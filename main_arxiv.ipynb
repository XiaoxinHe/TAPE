{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node idx</th>\n",
       "      <th>paper id</th>\n",
       "      <th>title</th>\n",
       "      <th>abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9657784</td>\n",
       "      <td>evasion attacks against machine learning at te...</td>\n",
       "      <td>In security-sensitive applications, the succes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>39886162</td>\n",
       "      <td>how hard is computing parity with noisy commun...</td>\n",
       "      <td>We show a tight lower bound of $\\Omega(N \\log\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>116214155</td>\n",
       "      <td>on the absence of the rip in real world applic...</td>\n",
       "      <td>The purpose of this paper is twofold. The firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>121432379</td>\n",
       "      <td>a promise theory perspective on data networks</td>\n",
       "      <td>Networking is undergoing a transformation thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>231147053</td>\n",
       "      <td>analysis of asymptotically optimal sampling ba...</td>\n",
       "      <td>Over the last 20 years significant effort has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169338</th>\n",
       "      <td>169338</td>\n",
       "      <td>3011696425</td>\n",
       "      <td>sentinet detecting localized universal attacks...</td>\n",
       "      <td>SentiNet is a novel detection framework for lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169339</th>\n",
       "      <td>169339</td>\n",
       "      <td>3011708313</td>\n",
       "      <td>interpretable mtl from heterogeneous domains u...</td>\n",
       "      <td>Multi-task learning (MTL) aims at improving th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169340</th>\n",
       "      <td>169340</td>\n",
       "      <td>3011798063</td>\n",
       "      <td>learning compositional rules via neural progra...</td>\n",
       "      <td>Many aspects of human reasoning, including lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169341</th>\n",
       "      <td>169341</td>\n",
       "      <td>3012226457</td>\n",
       "      <td>certified defenses for adversarial patches</td>\n",
       "      <td>Adversarial patch attacks are among one of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169342</th>\n",
       "      <td>169342</td>\n",
       "      <td>3012505757</td>\n",
       "      <td>fauras a proxy based framework for ensuring th...</td>\n",
       "      <td>HTTP/2 video streaming has caught a lot of att...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169343 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        node idx    paper id  \\\n",
       "0              0     9657784   \n",
       "1              1    39886162   \n",
       "2              2   116214155   \n",
       "3              3   121432379   \n",
       "4              4   231147053   \n",
       "...          ...         ...   \n",
       "169338    169338  3011696425   \n",
       "169339    169339  3011708313   \n",
       "169340    169340  3011798063   \n",
       "169341    169341  3012226457   \n",
       "169342    169342  3012505757   \n",
       "\n",
       "                                                    title  \\\n",
       "0       evasion attacks against machine learning at te...   \n",
       "1       how hard is computing parity with noisy commun...   \n",
       "2       on the absence of the rip in real world applic...   \n",
       "3           a promise theory perspective on data networks   \n",
       "4       analysis of asymptotically optimal sampling ba...   \n",
       "...                                                   ...   \n",
       "169338  sentinet detecting localized universal attacks...   \n",
       "169339  interpretable mtl from heterogeneous domains u...   \n",
       "169340  learning compositional rules via neural progra...   \n",
       "169341         certified defenses for adversarial patches   \n",
       "169342  fauras a proxy based framework for ensuring th...   \n",
       "\n",
       "                                                      abs  \n",
       "0       In security-sensitive applications, the succes...  \n",
       "1       We show a tight lower bound of $\\Omega(N \\log\\...  \n",
       "2       The purpose of this paper is twofold. The firs...  \n",
       "3       Networking is undergoing a transformation thro...  \n",
       "4       Over the last 20 years significant effort has ...  \n",
       "...                                                   ...  \n",
       "169338  SentiNet is a novel detection framework for lo...  \n",
       "169339  Multi-task learning (MTL) aims at improving th...  \n",
       "169340  Many aspects of human reasoning, including lan...  \n",
       "169341  Adversarial patch attacks are among one of the...  \n",
       "169342  HTTP/2 video streaming has caught a lot of att...  \n",
       "\n",
       "[169343 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bert import generate_node_embeddings, preprocessing\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import torch\n",
    "\n",
    "\n",
    "nodeidx2paperid = pd.read_csv(\n",
    "    'dataset/ogbn_arxiv/mapping/nodeidx2paperid.csv.gz', compression='gzip')\n",
    "\n",
    "raw_text = pd.read_csv('dataset/ogbn_arxiv/titleabs.tsv',\n",
    "                       sep='\\t', header=None, names=['paper id', 'title', 'abs'])\n",
    "X = pd.merge(nodeidx2paperid, raw_text, on='paper id')\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0\n",
    "device = f'cuda:{device}' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "dataset = PygNodePropPredDataset(\n",
    "    name='ogbn-arxiv', transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "data.adj_t = data.adj_t.to_symmetric()\n",
    "data = data.to(device)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx = split_idx['train'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/169343 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/xiaoxin/miniconda3/envs/gnn_ak/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 169343/169343 [09:05<00:00, 310.60it/s]\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "for ti, ab in zip(X['title'], X['abs']):\n",
    "    t = 'Title: ' + ti + '\\n'+'Abstract: ' + ab\n",
    "    # t = ti + ab\n",
    "    text.append(t)\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased', do_lower_case=True)\n",
    "for sample in tqdm(text):\n",
    "    encoding_dict = preprocessing(sample, tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids'])\n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "token_id = torch.cat(token_id, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Generating node embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10584/10584 [16:24<00:00, 10.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Prepare DataLoader\n",
    "batch_size = 16\n",
    "dataset = TensorDataset(token_id, attention_masks)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    sampler=RandomSampler(dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Load the BertForSequenceClassification model\n",
    "bert = BertModel.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Run on GPU\n",
    "print(\"[!] Generating node embeddings\")\n",
    "start = time.time()\n",
    "bert.cuda()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "features = generate_node_embeddings(bert, dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 4.0770, Train: 10.99%, Valid: 22.97% Test: 21.56%\n",
      "Epoch: 02, Loss: 4.4701, Train: 7.69%, Valid: 14.96% Test: 22.10%\n",
      "Epoch: 03, Loss: 4.0741, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
      "Epoch: 04, Loss: 3.9564, Train: 4.77%, Valid: 9.56% Test: 9.53%\n",
      "Epoch: 05, Loss: 3.4713, Train: 10.99%, Valid: 22.97% Test: 21.56%\n",
      "Epoch: 06, Loss: 3.4462, Train: 5.97%, Valid: 3.45% Test: 2.92%\n",
      "Epoch: 07, Loss: 3.4560, Train: 4.77%, Valid: 9.56% Test: 9.53%\n",
      "Epoch: 08, Loss: 3.3950, Train: 7.69%, Valid: 14.96% Test: 22.10%\n",
      "Epoch: 09, Loss: 3.3471, Train: 7.69%, Valid: 14.96% Test: 22.10%\n",
      "Epoch: 10, Loss: 3.2365, Train: 10.99%, Valid: 22.97% Test: 21.56%\n",
      "Epoch: 11, Loss: 3.1547, Train: 10.99%, Valid: 22.97% Test: 21.55%\n",
      "Epoch: 12, Loss: 3.1448, Train: 5.57%, Valid: 12.54% Test: 14.02%\n",
      "Epoch: 13, Loss: 3.1459, Train: 3.44%, Valid: 3.32% Test: 3.31%\n",
      "Epoch: 14, Loss: 3.1255, Train: 7.55%, Valid: 13.89% Test: 20.52%\n",
      "Epoch: 15, Loss: 3.0929, Train: 7.32%, Valid: 14.57% Test: 21.18%\n",
      "Epoch: 16, Loss: 3.0565, Train: 7.18%, Valid: 16.99% Test: 18.33%\n",
      "Epoch: 17, Loss: 3.0408, Train: 6.88%, Valid: 16.54% Test: 18.06%\n",
      "Epoch: 18, Loss: 3.0409, Train: 6.96%, Valid: 16.45% Test: 18.00%\n",
      "Epoch: 19, Loss: 3.0280, Train: 6.41%, Valid: 10.70% Test: 11.66%\n",
      "Epoch: 20, Loss: 3.0012, Train: 3.74%, Valid: 4.47% Test: 4.75%\n",
      "Epoch: 21, Loss: 2.9916, Train: 8.68%, Valid: 9.30% Test: 9.96%\n",
      "Epoch: 22, Loss: 2.9734, Train: 16.45%, Valid: 14.57% Test: 14.29%\n",
      "Epoch: 23, Loss: 2.9612, Train: 19.20%, Valid: 16.48% Test: 16.49%\n",
      "Epoch: 24, Loss: 2.9571, Train: 19.94%, Valid: 21.47% Test: 21.30%\n",
      "Epoch: 25, Loss: 2.9527, Train: 19.89%, Valid: 24.20% Test: 23.17%\n",
      "Epoch: 26, Loss: 2.9403, Train: 14.25%, Valid: 22.85% Test: 22.98%\n",
      "Epoch: 27, Loss: 2.9303, Train: 12.75%, Valid: 21.96% Test: 23.72%\n",
      "Epoch: 28, Loss: 2.9261, Train: 13.78%, Valid: 23.23% Test: 23.35%\n",
      "Epoch: 29, Loss: 2.9206, Train: 15.09%, Valid: 22.90% Test: 22.25%\n",
      "Epoch: 30, Loss: 2.9180, Train: 16.26%, Valid: 22.39% Test: 21.39%\n",
      "Epoch: 31, Loss: 2.9088, Train: 17.38%, Valid: 19.53% Test: 17.40%\n",
      "Epoch: 32, Loss: 2.8990, Train: 16.79%, Valid: 15.14% Test: 11.83%\n",
      "Epoch: 33, Loss: 2.8958, Train: 17.53%, Valid: 15.45% Test: 12.28%\n",
      "Epoch: 34, Loss: 2.8911, Train: 19.45%, Valid: 19.46% Test: 16.89%\n",
      "Epoch: 35, Loss: 2.8839, Train: 21.02%, Valid: 22.45% Test: 20.37%\n",
      "Epoch: 36, Loss: 2.8777, Train: 21.79%, Valid: 23.56% Test: 21.70%\n",
      "Epoch: 37, Loss: 2.8713, Train: 22.20%, Valid: 24.23% Test: 22.36%\n",
      "Epoch: 38, Loss: 2.8698, Train: 22.49%, Valid: 24.77% Test: 22.94%\n",
      "Epoch: 39, Loss: 2.8655, Train: 22.68%, Valid: 25.33% Test: 23.50%\n",
      "Epoch: 40, Loss: 2.8599, Train: 22.53%, Valid: 24.73% Test: 23.03%\n",
      "Epoch: 41, Loss: 2.8564, Train: 21.61%, Valid: 22.99% Test: 21.48%\n",
      "Epoch: 42, Loss: 2.8432, Train: 20.35%, Valid: 20.32% Test: 18.87%\n",
      "Epoch: 43, Loss: 2.8440, Train: 19.94%, Valid: 18.76% Test: 17.16%\n",
      "Epoch: 44, Loss: 2.8413, Train: 19.90%, Valid: 18.09% Test: 16.34%\n",
      "Epoch: 45, Loss: 2.8313, Train: 20.06%, Valid: 18.34% Test: 16.39%\n",
      "Epoch: 46, Loss: 2.8292, Train: 20.04%, Valid: 18.01% Test: 15.86%\n",
      "Epoch: 47, Loss: 2.8172, Train: 20.16%, Valid: 18.15% Test: 15.79%\n",
      "Epoch: 48, Loss: 2.8201, Train: 20.19%, Valid: 18.17% Test: 15.75%\n",
      "Epoch: 49, Loss: 2.8166, Train: 19.80%, Valid: 17.18% Test: 14.77%\n",
      "Epoch: 50, Loss: 2.8044, Train: 18.74%, Valid: 14.31% Test: 11.84%\n",
      "Epoch: 51, Loss: 2.8054, Train: 17.75%, Valid: 11.73% Test: 9.14%\n",
      "Epoch: 52, Loss: 2.7941, Train: 17.56%, Valid: 10.94% Test: 8.35%\n",
      "Epoch: 53, Loss: 2.7953, Train: 17.98%, Valid: 11.42% Test: 8.90%\n",
      "Epoch: 54, Loss: 2.7928, Train: 18.25%, Valid: 11.73% Test: 9.30%\n",
      "Epoch: 55, Loss: 2.7842, Train: 18.54%, Valid: 12.39% Test: 9.95%\n",
      "Epoch: 56, Loss: 2.7807, Train: 18.92%, Valid: 13.60% Test: 11.18%\n",
      "Epoch: 57, Loss: 2.7779, Train: 19.18%, Valid: 14.46% Test: 12.00%\n",
      "Epoch: 58, Loss: 2.7701, Train: 19.35%, Valid: 15.15% Test: 12.60%\n",
      "Epoch: 59, Loss: 2.7663, Train: 19.80%, Valid: 15.98% Test: 13.46%\n",
      "Epoch: 60, Loss: 2.7657, Train: 20.23%, Valid: 16.44% Test: 14.01%\n",
      "Epoch: 61, Loss: 2.7641, Train: 20.73%, Valid: 17.64% Test: 15.40%\n",
      "Epoch: 62, Loss: 2.7492, Train: 21.04%, Valid: 18.44% Test: 16.28%\n",
      "Epoch: 63, Loss: 2.7523, Train: 21.41%, Valid: 19.53% Test: 17.40%\n",
      "Epoch: 64, Loss: 2.7512, Train: 21.47%, Valid: 20.30% Test: 18.08%\n",
      "Epoch: 65, Loss: 2.7447, Train: 21.28%, Valid: 20.97% Test: 18.87%\n",
      "Epoch: 66, Loss: 2.7442, Train: 21.47%, Valid: 22.26% Test: 20.51%\n",
      "Epoch: 67, Loss: 2.7389, Train: 22.79%, Valid: 24.15% Test: 22.22%\n",
      "Epoch: 68, Loss: 2.7418, Train: 24.16%, Valid: 26.38% Test: 24.09%\n",
      "Epoch: 69, Loss: 2.7373, Train: 24.89%, Valid: 27.73% Test: 25.08%\n",
      "Epoch: 70, Loss: 2.7312, Train: 24.56%, Valid: 28.16% Test: 25.52%\n",
      "Epoch: 71, Loss: 2.7230, Train: 24.85%, Valid: 28.74% Test: 26.37%\n",
      "Epoch: 72, Loss: 2.7296, Train: 25.24%, Valid: 28.87% Test: 26.24%\n",
      "Epoch: 73, Loss: 2.7240, Train: 25.43%, Valid: 28.81% Test: 26.15%\n",
      "Epoch: 74, Loss: 2.7210, Train: 24.13%, Valid: 28.77% Test: 26.47%\n",
      "Epoch: 75, Loss: 2.7216, Train: 22.05%, Valid: 28.28% Test: 26.90%\n",
      "Epoch: 76, Loss: 2.7206, Train: 23.02%, Valid: 28.75% Test: 27.03%\n",
      "Epoch: 77, Loss: 2.7110, Train: 21.08%, Valid: 27.93% Test: 27.25%\n",
      "Epoch: 78, Loss: 2.7108, Train: 21.74%, Valid: 28.53% Test: 27.54%\n",
      "Epoch: 79, Loss: 2.7087, Train: 21.61%, Valid: 28.88% Test: 27.65%\n",
      "Epoch: 80, Loss: 2.7081, Train: 21.07%, Valid: 28.45% Test: 27.14%\n",
      "Epoch: 81, Loss: 2.7061, Train: 17.65%, Valid: 26.42% Test: 25.61%\n",
      "Epoch: 82, Loss: 2.7056, Train: 17.64%, Valid: 26.80% Test: 25.46%\n",
      "Epoch: 83, Loss: 2.6976, Train: 14.74%, Valid: 24.59% Test: 24.05%\n",
      "Epoch: 84, Loss: 2.6990, Train: 21.71%, Valid: 28.30% Test: 26.80%\n",
      "Epoch: 85, Loss: 2.7004, Train: 22.09%, Valid: 28.58% Test: 27.00%\n",
      "Epoch: 86, Loss: 2.6925, Train: 20.21%, Valid: 28.09% Test: 26.31%\n",
      "Epoch: 87, Loss: 2.6951, Train: 13.93%, Valid: 23.31% Test: 23.27%\n",
      "Epoch: 88, Loss: 2.6935, Train: 17.84%, Valid: 25.98% Test: 24.78%\n",
      "Epoch: 89, Loss: 2.6786, Train: 15.09%, Valid: 24.48% Test: 23.75%\n",
      "Epoch: 90, Loss: 2.6776, Train: 24.02%, Valid: 29.16% Test: 26.74%\n",
      "Epoch: 91, Loss: 2.6807, Train: 24.78%, Valid: 29.38% Test: 26.73%\n",
      "Epoch: 92, Loss: 2.6808, Train: 20.53%, Valid: 27.74% Test: 26.14%\n",
      "Epoch: 93, Loss: 2.6763, Train: 16.24%, Valid: 25.41% Test: 25.03%\n",
      "Epoch: 94, Loss: 2.6842, Train: 18.35%, Valid: 26.47% Test: 25.12%\n",
      "Epoch: 95, Loss: 2.6768, Train: 15.03%, Valid: 24.51% Test: 23.92%\n",
      "Epoch: 96, Loss: 2.6726, Train: 14.82%, Valid: 25.01% Test: 25.46%\n",
      "Epoch: 97, Loss: 2.6742, Train: 20.28%, Valid: 28.33% Test: 27.34%\n",
      "Epoch: 98, Loss: 2.6671, Train: 10.96%, Valid: 21.93% Test: 23.97%\n",
      "Epoch: 99, Loss: 2.6717, Train: 8.43%, Valid: 14.28% Test: 15.14%\n",
      "Epoch: 100, Loss: 2.6797, Train: 10.89%, Valid: 18.81% Test: 19.32%\n",
      "Epoch: 101, Loss: 2.6694, Train: 8.49%, Valid: 14.04% Test: 16.84%\n",
      "Epoch: 102, Loss: 2.6733, Train: 7.90%, Valid: 12.81% Test: 15.89%\n",
      "Epoch: 103, Loss: 2.6671, Train: 7.82%, Valid: 12.35% Test: 13.84%\n",
      "Epoch: 104, Loss: 2.6673, Train: 8.39%, Valid: 13.30% Test: 14.15%\n",
      "Epoch: 105, Loss: 2.6663, Train: 7.89%, Valid: 11.37% Test: 11.57%\n",
      "Epoch: 106, Loss: 2.6645, Train: 9.37%, Valid: 16.53% Test: 17.63%\n",
      "Epoch: 107, Loss: 2.6554, Train: 6.54%, Valid: 9.01% Test: 8.24%\n",
      "Epoch: 108, Loss: 2.6630, Train: 6.22%, Valid: 7.73% Test: 6.94%\n",
      "Epoch: 109, Loss: 2.6622, Train: 6.17%, Valid: 6.85% Test: 5.80%\n",
      "Epoch: 110, Loss: 2.6552, Train: 5.78%, Valid: 5.05% Test: 4.49%\n",
      "Epoch: 111, Loss: 2.6719, Train: 10.50%, Valid: 18.87% Test: 18.74%\n",
      "Epoch: 112, Loss: 2.6573, Train: 8.48%, Valid: 11.83% Test: 11.76%\n",
      "Epoch: 113, Loss: 2.6541, Train: 5.69%, Valid: 6.34% Test: 5.52%\n",
      "Epoch: 114, Loss: 2.6585, Train: 5.53%, Valid: 6.00% Test: 5.21%\n",
      "Epoch: 115, Loss: 2.6634, Train: 6.27%, Valid: 8.06% Test: 7.45%\n",
      "Epoch: 116, Loss: 2.6559, Train: 5.79%, Valid: 7.53% Test: 7.46%\n",
      "Epoch: 117, Loss: 2.6523, Train: 5.55%, Valid: 7.29% Test: 7.06%\n",
      "Epoch: 118, Loss: 2.6500, Train: 6.22%, Valid: 8.71% Test: 8.13%\n",
      "Epoch: 119, Loss: 2.6517, Train: 6.62%, Valid: 9.89% Test: 8.83%\n",
      "Epoch: 120, Loss: 2.6501, Train: 7.78%, Valid: 13.80% Test: 13.16%\n",
      "Epoch: 121, Loss: 2.6465, Train: 10.02%, Valid: 18.90% Test: 19.17%\n",
      "Epoch: 122, Loss: 2.6456, Train: 6.33%, Valid: 9.21% Test: 8.44%\n",
      "Epoch: 123, Loss: 2.6529, Train: 12.62%, Valid: 21.13% Test: 21.70%\n",
      "Epoch: 124, Loss: 2.6405, Train: 9.36%, Valid: 10.39% Test: 9.63%\n",
      "Epoch: 125, Loss: 2.6398, Train: 8.43%, Valid: 7.39% Test: 6.15%\n",
      "Epoch: 126, Loss: 2.6533, Train: 9.60%, Valid: 10.66% Test: 9.24%\n",
      "Epoch: 127, Loss: 2.6517, Train: 8.42%, Valid: 7.45% Test: 7.16%\n",
      "Epoch: 128, Loss: 2.6418, Train: 9.49%, Valid: 10.01% Test: 10.80%\n",
      "Epoch: 129, Loss: 2.6440, Train: 11.39%, Valid: 17.41% Test: 17.89%\n",
      "Epoch: 130, Loss: 2.6476, Train: 11.32%, Valid: 18.22% Test: 18.47%\n",
      "Epoch: 131, Loss: 2.6380, Train: 10.53%, Valid: 16.74% Test: 16.62%\n",
      "Epoch: 132, Loss: 2.6349, Train: 12.57%, Valid: 22.43% Test: 22.22%\n",
      "Epoch: 133, Loss: 2.6404, Train: 14.61%, Valid: 23.87% Test: 23.00%\n",
      "Epoch: 134, Loss: 2.6232, Train: 16.77%, Valid: 25.14% Test: 24.16%\n",
      "Epoch: 135, Loss: 2.6384, Train: 16.82%, Valid: 25.10% Test: 23.86%\n",
      "Epoch: 136, Loss: 2.6234, Train: 21.60%, Valid: 28.52% Test: 26.79%\n",
      "Epoch: 137, Loss: 2.6291, Train: 22.34%, Valid: 29.24% Test: 27.85%\n",
      "Epoch: 138, Loss: 2.6254, Train: 20.76%, Valid: 28.67% Test: 27.81%\n",
      "Epoch: 139, Loss: 2.6167, Train: 15.57%, Valid: 24.48% Test: 26.53%\n",
      "Epoch: 140, Loss: 2.6242, Train: 14.11%, Valid: 21.12% Test: 24.56%\n",
      "Epoch: 141, Loss: 2.6298, Train: 14.63%, Valid: 18.38% Test: 21.38%\n",
      "Epoch: 142, Loss: 2.6180, Train: 14.37%, Valid: 16.24% Test: 17.39%\n",
      "Epoch: 143, Loss: 2.6249, Train: 14.16%, Valid: 16.25% Test: 18.02%\n",
      "Epoch: 144, Loss: 2.6200, Train: 13.22%, Valid: 14.22% Test: 17.32%\n",
      "Epoch: 145, Loss: 2.6280, Train: 16.83%, Valid: 20.89% Test: 21.28%\n",
      "Epoch: 146, Loss: 2.6192, Train: 12.62%, Valid: 13.02% Test: 13.46%\n",
      "Epoch: 147, Loss: 2.6212, Train: 14.93%, Valid: 22.19% Test: 24.92%\n",
      "Epoch: 148, Loss: 2.6232, Train: 21.89%, Valid: 29.61% Test: 28.84%\n",
      "Epoch: 149, Loss: 2.6250, Train: 9.11%, Valid: 9.02% Test: 8.69%\n",
      "Epoch: 150, Loss: 2.6247, Train: 6.13%, Valid: 6.82% Test: 6.10%\n",
      "Epoch: 151, Loss: 2.6111, Train: 6.12%, Valid: 6.48% Test: 6.02%\n",
      "Epoch: 152, Loss: 2.6221, Train: 6.26%, Valid: 5.96% Test: 5.92%\n",
      "Epoch: 153, Loss: 2.6152, Train: 6.88%, Valid: 7.32% Test: 6.59%\n",
      "Epoch: 154, Loss: 2.6157, Train: 7.22%, Valid: 9.23% Test: 8.07%\n",
      "Epoch: 155, Loss: 2.6289, Train: 5.58%, Valid: 5.78% Test: 5.73%\n",
      "Epoch: 156, Loss: 2.6254, Train: 6.45%, Valid: 6.97% Test: 6.63%\n",
      "Epoch: 157, Loss: 2.6125, Train: 6.59%, Valid: 6.85% Test: 6.54%\n",
      "Epoch: 158, Loss: 2.6192, Train: 6.89%, Valid: 7.09% Test: 7.15%\n",
      "Epoch: 159, Loss: 2.6202, Train: 12.24%, Valid: 15.24% Test: 20.26%\n",
      "Epoch: 160, Loss: 2.6299, Train: 11.09%, Valid: 9.72% Test: 11.40%\n",
      "Epoch: 161, Loss: 2.6117, Train: 9.76%, Valid: 7.20% Test: 6.26%\n",
      "Epoch: 162, Loss: 2.6364, Train: 11.11%, Valid: 11.75% Test: 10.07%\n",
      "Epoch: 163, Loss: 2.6358, Train: 8.85%, Valid: 7.70% Test: 7.15%\n",
      "Epoch: 164, Loss: 2.6188, Train: 7.88%, Valid: 8.51% Test: 9.30%\n",
      "Epoch: 165, Loss: 2.6207, Train: 10.98%, Valid: 15.72% Test: 15.06%\n",
      "Epoch: 166, Loss: 2.6278, Train: 14.25%, Valid: 19.94% Test: 18.58%\n",
      "Epoch: 167, Loss: 2.6396, Train: 15.47%, Valid: 22.53% Test: 22.71%\n",
      "Epoch: 168, Loss: 2.6196, Train: 15.19%, Valid: 20.07% Test: 24.64%\n",
      "Epoch: 169, Loss: 2.6202, Train: 16.57%, Valid: 19.98% Test: 20.61%\n",
      "Epoch: 170, Loss: 2.6039, Train: 16.43%, Valid: 16.44% Test: 15.57%\n",
      "Epoch: 171, Loss: 2.6154, Train: 16.36%, Valid: 16.36% Test: 16.37%\n",
      "Epoch: 172, Loss: 2.6072, Train: 12.28%, Valid: 10.47% Test: 11.63%\n",
      "Epoch: 173, Loss: 2.6032, Train: 13.03%, Valid: 12.41% Test: 13.91%\n",
      "Epoch: 174, Loss: 2.5941, Train: 11.79%, Valid: 11.81% Test: 11.20%\n",
      "Epoch: 175, Loss: 2.5966, Train: 8.59%, Valid: 7.56% Test: 6.39%\n",
      "Epoch: 176, Loss: 2.6047, Train: 11.73%, Valid: 13.76% Test: 13.11%\n",
      "Epoch: 177, Loss: 2.5910, Train: 12.23%, Valid: 14.48% Test: 14.50%\n",
      "Epoch: 178, Loss: 2.5996, Train: 13.51%, Valid: 19.52% Test: 19.40%\n",
      "Epoch: 179, Loss: 2.5940, Train: 11.84%, Valid: 17.02% Test: 16.52%\n",
      "Epoch: 180, Loss: 2.5980, Train: 9.83%, Valid: 12.42% Test: 11.92%\n",
      "Epoch: 181, Loss: 2.5947, Train: 11.27%, Valid: 19.98% Test: 22.36%\n",
      "Epoch: 182, Loss: 2.5979, Train: 9.94%, Valid: 15.91% Test: 16.92%\n",
      "Epoch: 183, Loss: 2.5983, Train: 16.39%, Valid: 25.07% Test: 25.33%\n",
      "Epoch: 184, Loss: 2.5816, Train: 11.37%, Valid: 12.43% Test: 12.53%\n",
      "Epoch: 185, Loss: 2.6029, Train: 12.48%, Valid: 18.96% Test: 19.07%\n",
      "Epoch: 186, Loss: 2.5858, Train: 10.93%, Valid: 15.91% Test: 15.62%\n",
      "Epoch: 187, Loss: 2.5839, Train: 12.54%, Valid: 19.73% Test: 19.79%\n",
      "Epoch: 188, Loss: 2.5866, Train: 9.34%, Valid: 10.45% Test: 9.16%\n",
      "Epoch: 189, Loss: 2.5898, Train: 11.03%, Valid: 18.55% Test: 18.09%\n",
      "Epoch: 190, Loss: 2.5803, Train: 9.33%, Valid: 16.56% Test: 16.95%\n",
      "Epoch: 191, Loss: 2.5815, Train: 6.85%, Valid: 8.64% Test: 8.57%\n",
      "Epoch: 192, Loss: 2.5800, Train: 16.74%, Valid: 27.79% Test: 25.91%\n",
      "Epoch: 193, Loss: 2.5952, Train: 10.67%, Valid: 18.38% Test: 18.03%\n",
      "Epoch: 194, Loss: 2.5993, Train: 10.82%, Valid: 17.64% Test: 17.34%\n",
      "Epoch: 195, Loss: 2.5846, Train: 9.33%, Valid: 13.17% Test: 14.00%\n",
      "Epoch: 196, Loss: 2.5899, Train: 10.25%, Valid: 15.75% Test: 14.81%\n",
      "Epoch: 197, Loss: 2.5800, Train: 8.75%, Valid: 9.25% Test: 7.85%\n",
      "Epoch: 198, Loss: 2.5895, Train: 8.84%, Valid: 7.46% Test: 6.79%\n",
      "Epoch: 199, Loss: 2.5752, Train: 8.75%, Valid: 7.19% Test: 6.93%\n",
      "Epoch: 200, Loss: 2.5807, Train: 8.34%, Valid: 7.11% Test: 6.80%\n",
      "Epoch: 201, Loss: 2.5809, Train: 7.25%, Valid: 6.61% Test: 6.11%\n",
      "Epoch: 202, Loss: 2.5822, Train: 8.52%, Valid: 8.95% Test: 8.99%\n",
      "Epoch: 203, Loss: 2.5668, Train: 9.56%, Valid: 9.35% Test: 9.31%\n",
      "Epoch: 204, Loss: 2.5759, Train: 11.09%, Valid: 13.21% Test: 14.25%\n",
      "Epoch: 205, Loss: 2.5812, Train: 19.03%, Valid: 27.72% Test: 28.20%\n",
      "Epoch: 206, Loss: 2.5843, Train: 9.73%, Valid: 8.01% Test: 8.32%\n",
      "Epoch: 207, Loss: 2.5891, Train: 8.43%, Valid: 7.55% Test: 7.39%\n",
      "Epoch: 208, Loss: 2.5733, Train: 12.50%, Valid: 19.69% Test: 20.95%\n",
      "Epoch: 209, Loss: 2.5814, Train: 10.83%, Valid: 12.68% Test: 13.52%\n",
      "Epoch: 210, Loss: 2.5748, Train: 11.62%, Valid: 12.50% Test: 13.30%\n",
      "Epoch: 211, Loss: 2.5755, Train: 16.05%, Valid: 25.09% Test: 26.35%\n",
      "Epoch: 212, Loss: 2.5765, Train: 14.45%, Valid: 23.03% Test: 24.48%\n",
      "Epoch: 213, Loss: 2.5757, Train: 10.89%, Valid: 13.82% Test: 13.43%\n",
      "Epoch: 214, Loss: 2.5818, Train: 16.05%, Valid: 25.20% Test: 26.26%\n",
      "Epoch: 215, Loss: 2.5707, Train: 16.31%, Valid: 24.76% Test: 26.78%\n",
      "Epoch: 216, Loss: 2.5763, Train: 13.31%, Valid: 16.86% Test: 18.92%\n",
      "Epoch: 217, Loss: 2.5705, Train: 10.49%, Valid: 9.99% Test: 9.03%\n",
      "Epoch: 218, Loss: 2.5745, Train: 13.81%, Valid: 20.28% Test: 21.00%\n",
      "Epoch: 219, Loss: 2.5700, Train: 13.74%, Valid: 22.22% Test: 23.22%\n",
      "Epoch: 220, Loss: 2.5715, Train: 12.22%, Valid: 21.21% Test: 22.96%\n",
      "Epoch: 221, Loss: 2.5613, Train: 11.03%, Valid: 19.56% Test: 20.84%\n",
      "Epoch: 222, Loss: 2.5745, Train: 10.05%, Valid: 15.26% Test: 15.19%\n",
      "Epoch: 223, Loss: 2.5781, Train: 10.77%, Valid: 15.49% Test: 15.42%\n",
      "Epoch: 224, Loss: 2.5699, Train: 10.98%, Valid: 14.35% Test: 13.64%\n",
      "Epoch: 225, Loss: 2.5709, Train: 11.98%, Valid: 14.90% Test: 14.33%\n",
      "Epoch: 226, Loss: 2.5610, Train: 11.18%, Valid: 11.09% Test: 9.94%\n",
      "Epoch: 227, Loss: 2.5588, Train: 13.00%, Valid: 18.57% Test: 19.41%\n",
      "Epoch: 228, Loss: 2.5647, Train: 9.31%, Valid: 12.05% Test: 10.90%\n",
      "Epoch: 229, Loss: 2.5599, Train: 9.10%, Valid: 12.53% Test: 11.80%\n",
      "Epoch: 230, Loss: 2.5554, Train: 9.82%, Valid: 15.19% Test: 15.72%\n",
      "Epoch: 231, Loss: 2.5563, Train: 12.62%, Valid: 20.62% Test: 23.04%\n",
      "Epoch: 232, Loss: 2.5637, Train: 16.51%, Valid: 25.54% Test: 25.57%\n",
      "Epoch: 233, Loss: 2.5555, Train: 14.27%, Valid: 21.98% Test: 24.26%\n",
      "Epoch: 234, Loss: 2.5469, Train: 14.09%, Valid: 19.14% Test: 20.00%\n",
      "Epoch: 235, Loss: 2.5497, Train: 12.74%, Valid: 16.16% Test: 16.10%\n",
      "Epoch: 236, Loss: 2.5415, Train: 11.24%, Valid: 13.17% Test: 11.88%\n",
      "Epoch: 237, Loss: 2.5492, Train: 7.97%, Valid: 6.65% Test: 6.17%\n",
      "Epoch: 238, Loss: 2.5594, Train: 16.26%, Valid: 25.85% Test: 25.23%\n",
      "Epoch: 239, Loss: 2.5557, Train: 15.72%, Valid: 25.99% Test: 25.58%\n",
      "Epoch: 240, Loss: 2.5639, Train: 9.70%, Valid: 14.16% Test: 19.85%\n",
      "Epoch: 241, Loss: 2.5702, Train: 9.02%, Valid: 14.19% Test: 15.95%\n",
      "Epoch: 242, Loss: 2.5700, Train: 5.72%, Valid: 6.16% Test: 5.39%\n",
      "Epoch: 243, Loss: 2.5537, Train: 4.78%, Valid: 4.70% Test: 4.10%\n",
      "Epoch: 244, Loss: 2.5666, Train: 4.90%, Valid: 5.09% Test: 4.36%\n",
      "Epoch: 245, Loss: 2.5612, Train: 4.91%, Valid: 5.32% Test: 4.94%\n",
      "Epoch: 246, Loss: 2.5483, Train: 4.97%, Valid: 5.34% Test: 5.28%\n",
      "Epoch: 247, Loss: 2.5510, Train: 5.18%, Valid: 5.46% Test: 5.53%\n",
      "Epoch: 248, Loss: 2.5562, Train: 5.33%, Valid: 6.01% Test: 6.31%\n",
      "Epoch: 249, Loss: 2.5466, Train: 6.00%, Valid: 6.97% Test: 7.29%\n",
      "Epoch: 250, Loss: 2.5418, Train: 5.66%, Valid: 6.32% Test: 6.55%\n",
      "Epoch: 251, Loss: 2.5442, Train: 6.31%, Valid: 7.18% Test: 7.33%\n",
      "Epoch: 252, Loss: 2.5435, Train: 8.57%, Valid: 14.13% Test: 16.47%\n",
      "Epoch: 253, Loss: 2.5427, Train: 8.82%, Valid: 11.87% Test: 15.94%\n",
      "Epoch: 254, Loss: 2.5450, Train: 10.90%, Valid: 16.86% Test: 22.27%\n",
      "Epoch: 255, Loss: 2.5358, Train: 15.96%, Valid: 27.15% Test: 28.03%\n",
      "Epoch: 256, Loss: 2.5365, Train: 14.93%, Valid: 27.01% Test: 28.36%\n",
      "Epoch: 257, Loss: 2.5263, Train: 10.83%, Valid: 18.38% Test: 21.66%\n",
      "Epoch: 258, Loss: 2.5161, Train: 12.78%, Valid: 23.58% Test: 27.01%\n",
      "Epoch: 259, Loss: 2.5310, Train: 13.76%, Valid: 26.26% Test: 27.75%\n",
      "Epoch: 260, Loss: 2.5312, Train: 12.44%, Valid: 24.68% Test: 28.21%\n",
      "Epoch: 261, Loss: 2.5323, Train: 14.47%, Valid: 25.23% Test: 24.85%\n",
      "Epoch: 262, Loss: 2.5243, Train: 13.64%, Valid: 24.90% Test: 28.66%\n",
      "Epoch: 263, Loss: 2.5213, Train: 7.68%, Valid: 8.19% Test: 9.17%\n",
      "Epoch: 264, Loss: 2.5493, Train: 8.71%, Valid: 21.33% Test: 23.18%\n",
      "Epoch: 265, Loss: 2.5544, Train: 8.42%, Valid: 20.21% Test: 23.55%\n",
      "Epoch: 266, Loss: 2.5562, Train: 7.63%, Valid: 17.05% Test: 21.72%\n",
      "Epoch: 267, Loss: 2.5466, Train: 9.07%, Valid: 20.22% Test: 23.93%\n",
      "Epoch: 268, Loss: 2.5494, Train: 10.44%, Valid: 22.86% Test: 23.96%\n",
      "Epoch: 269, Loss: 2.5657, Train: 8.98%, Valid: 18.30% Test: 22.51%\n",
      "Epoch: 270, Loss: 2.5500, Train: 10.04%, Valid: 17.29% Test: 20.48%\n",
      "Epoch: 271, Loss: 2.5536, Train: 12.98%, Valid: 23.55% Test: 26.22%\n",
      "Epoch: 272, Loss: 2.5544, Train: 14.37%, Valid: 25.70% Test: 26.41%\n",
      "Epoch: 273, Loss: 2.5433, Train: 10.29%, Valid: 19.62% Test: 24.42%\n",
      "Epoch: 274, Loss: 2.5350, Train: 11.79%, Valid: 19.04% Test: 24.00%\n",
      "Epoch: 275, Loss: 2.5303, Train: 13.53%, Valid: 20.27% Test: 21.99%\n",
      "Epoch: 276, Loss: 2.5344, Train: 12.94%, Valid: 15.31% Test: 14.46%\n",
      "Epoch: 277, Loss: 2.5301, Train: 9.36%, Valid: 10.51% Test: 10.24%\n",
      "Epoch: 278, Loss: 2.5305, Train: 9.46%, Valid: 12.19% Test: 12.47%\n",
      "Epoch: 279, Loss: 2.5223, Train: 10.10%, Valid: 15.66% Test: 16.40%\n",
      "Epoch: 280, Loss: 2.5290, Train: 10.88%, Valid: 17.53% Test: 18.10%\n",
      "Epoch: 281, Loss: 2.5243, Train: 9.46%, Valid: 13.48% Test: 14.46%\n",
      "Epoch: 282, Loss: 2.5157, Train: 9.11%, Valid: 11.31% Test: 11.79%\n",
      "Epoch: 283, Loss: 2.5216, Train: 9.31%, Valid: 14.19% Test: 13.97%\n",
      "Epoch: 284, Loss: 2.5171, Train: 9.71%, Valid: 16.21% Test: 16.28%\n",
      "Epoch: 285, Loss: 2.5092, Train: 8.32%, Valid: 12.65% Test: 12.51%\n",
      "Epoch: 286, Loss: 2.5101, Train: 12.29%, Valid: 24.46% Test: 26.89%\n",
      "Epoch: 287, Loss: 2.5210, Train: 12.33%, Valid: 24.76% Test: 25.70%\n",
      "Epoch: 288, Loss: 2.5167, Train: 10.50%, Valid: 22.72% Test: 25.27%\n",
      "Epoch: 289, Loss: 2.5135, Train: 13.78%, Valid: 24.38% Test: 22.79%\n",
      "Epoch: 290, Loss: 2.5124, Train: 13.33%, Valid: 24.22% Test: 23.33%\n",
      "Epoch: 291, Loss: 2.5128, Train: 15.71%, Valid: 25.60% Test: 24.36%\n",
      "Epoch: 292, Loss: 2.5085, Train: 16.98%, Valid: 26.04% Test: 24.64%\n",
      "Epoch: 293, Loss: 2.5130, Train: 15.16%, Valid: 24.38% Test: 23.90%\n",
      "Epoch: 294, Loss: 2.5124, Train: 14.55%, Valid: 25.06% Test: 24.75%\n",
      "Epoch: 295, Loss: 2.4957, Train: 12.43%, Valid: 22.44% Test: 26.37%\n",
      "Epoch: 296, Loss: 2.5010, Train: 10.87%, Valid: 17.59% Test: 20.99%\n",
      "Epoch: 297, Loss: 2.4927, Train: 14.17%, Valid: 25.11% Test: 25.12%\n",
      "Epoch: 298, Loss: 2.4988, Train: 17.64%, Valid: 26.97% Test: 26.69%\n",
      "Epoch: 299, Loss: 2.5094, Train: 12.27%, Valid: 15.91% Test: 17.95%\n",
      "Epoch: 300, Loss: 2.5012, Train: 26.61%, Valid: 30.25% Test: 28.46%\n",
      "Epoch: 301, Loss: 2.5124, Train: 28.21%, Valid: 31.20% Test: 29.28%\n",
      "Epoch: 302, Loss: 2.5159, Train: 16.35%, Valid: 26.02% Test: 27.64%\n",
      "Epoch: 303, Loss: 2.5128, Train: 14.19%, Valid: 25.00% Test: 25.89%\n",
      "Epoch: 304, Loss: 2.5182, Train: 13.11%, Valid: 23.54% Test: 22.11%\n",
      "Epoch: 305, Loss: 2.5131, Train: 21.93%, Valid: 28.14% Test: 25.99%\n",
      "Epoch: 306, Loss: 2.5124, Train: 14.86%, Valid: 25.29% Test: 24.68%\n",
      "Epoch: 307, Loss: 2.5186, Train: 13.43%, Valid: 23.76% Test: 23.37%\n",
      "Epoch: 308, Loss: 2.4978, Train: 13.46%, Valid: 25.16% Test: 27.51%\n",
      "Epoch: 309, Loss: 2.5096, Train: 10.12%, Valid: 17.18% Test: 20.74%\n",
      "Epoch: 310, Loss: 2.5083, Train: 14.06%, Valid: 25.11% Test: 24.50%\n",
      "Epoch: 311, Loss: 2.5047, Train: 13.77%, Valid: 25.28% Test: 25.01%\n",
      "Epoch: 312, Loss: 2.5108, Train: 6.84%, Valid: 8.32% Test: 9.32%\n",
      "Epoch: 313, Loss: 2.4999, Train: 9.68%, Valid: 17.03% Test: 18.70%\n",
      "Epoch: 314, Loss: 2.5044, Train: 9.11%, Valid: 13.47% Test: 13.47%\n",
      "Epoch: 315, Loss: 2.5015, Train: 10.21%, Valid: 14.37% Test: 13.72%\n",
      "Epoch: 316, Loss: 2.4964, Train: 13.11%, Valid: 20.78% Test: 19.59%\n",
      "Epoch: 317, Loss: 2.5024, Train: 10.56%, Valid: 11.70% Test: 12.25%\n",
      "Epoch: 318, Loss: 2.5012, Train: 25.94%, Valid: 32.41% Test: 29.93%\n",
      "Epoch: 319, Loss: 2.5028, Train: 19.04%, Valid: 24.23% Test: 22.26%\n",
      "Epoch: 320, Loss: 2.5073, Train: 22.14%, Valid: 29.63% Test: 29.22%\n",
      "Epoch: 321, Loss: 2.4925, Train: 15.07%, Valid: 23.22% Test: 28.04%\n",
      "Epoch: 322, Loss: 2.5048, Train: 17.14%, Valid: 25.50% Test: 24.20%\n",
      "Epoch: 323, Loss: 2.5181, Train: 15.78%, Valid: 26.05% Test: 25.06%\n",
      "Epoch: 324, Loss: 2.5056, Train: 15.99%, Valid: 25.04% Test: 24.56%\n",
      "Epoch: 325, Loss: 2.4935, Train: 14.46%, Valid: 23.50% Test: 28.45%\n",
      "Epoch: 326, Loss: 2.4997, Train: 18.65%, Valid: 28.93% Test: 28.10%\n",
      "Epoch: 327, Loss: 2.5004, Train: 13.14%, Valid: 17.73% Test: 16.35%\n",
      "Epoch: 328, Loss: 2.4824, Train: 13.10%, Valid: 18.26% Test: 17.07%\n",
      "Epoch: 329, Loss: 2.4889, Train: 23.28%, Valid: 31.12% Test: 29.34%\n",
      "Epoch: 330, Loss: 2.4929, Train: 13.80%, Valid: 22.93% Test: 25.86%\n",
      "Epoch: 331, Loss: 2.4974, Train: 16.16%, Valid: 21.43% Test: 20.49%\n",
      "Epoch: 332, Loss: 2.4823, Train: 12.63%, Valid: 16.78% Test: 15.69%\n",
      "Epoch: 333, Loss: 2.4862, Train: 8.12%, Valid: 9.29% Test: 9.43%\n",
      "Epoch: 334, Loss: 2.4779, Train: 10.05%, Valid: 15.90% Test: 20.58%\n",
      "Epoch: 335, Loss: 2.4907, Train: 9.59%, Valid: 13.20% Test: 18.33%\n",
      "Epoch: 336, Loss: 2.4985, Train: 13.17%, Valid: 17.79% Test: 16.27%\n",
      "Epoch: 337, Loss: 2.4818, Train: 8.73%, Valid: 12.64% Test: 11.84%\n",
      "Epoch: 338, Loss: 2.4956, Train: 13.98%, Valid: 24.46% Test: 23.69%\n",
      "Epoch: 339, Loss: 2.4939, Train: 13.39%, Valid: 24.81% Test: 23.37%\n",
      "Epoch: 340, Loss: 2.5042, Train: 12.25%, Valid: 23.54% Test: 22.31%\n",
      "Epoch: 341, Loss: 2.4944, Train: 13.12%, Valid: 24.49% Test: 23.36%\n",
      "Epoch: 342, Loss: 2.4830, Train: 12.24%, Valid: 25.31% Test: 27.31%\n",
      "Epoch: 343, Loss: 2.4937, Train: 13.19%, Valid: 24.61% Test: 23.76%\n",
      "Epoch: 344, Loss: 2.4890, Train: 13.35%, Valid: 25.89% Test: 25.96%\n",
      "Epoch: 345, Loss: 2.5060, Train: 14.41%, Valid: 26.44% Test: 27.32%\n",
      "Epoch: 346, Loss: 2.4692, Train: 9.45%, Valid: 17.20% Test: 20.59%\n",
      "Epoch: 347, Loss: 2.4847, Train: 13.16%, Valid: 24.55% Test: 23.67%\n",
      "Epoch: 348, Loss: 2.4699, Train: 11.78%, Valid: 25.24% Test: 28.84%\n",
      "Epoch: 349, Loss: 2.4808, Train: 7.18%, Valid: 11.55% Test: 15.72%\n",
      "Epoch: 350, Loss: 2.4779, Train: 7.31%, Valid: 11.03% Test: 10.76%\n",
      "Epoch: 351, Loss: 2.4652, Train: 7.79%, Valid: 12.78% Test: 11.31%\n",
      "Epoch: 352, Loss: 2.4635, Train: 10.72%, Valid: 20.47% Test: 19.32%\n",
      "Epoch: 353, Loss: 2.4669, Train: 6.60%, Valid: 7.77% Test: 7.31%\n",
      "Epoch: 354, Loss: 2.4926, Train: 12.69%, Valid: 25.92% Test: 24.32%\n",
      "Epoch: 355, Loss: 2.4850, Train: 9.75%, Valid: 15.81% Test: 15.18%\n",
      "Epoch: 356, Loss: 2.4778, Train: 12.06%, Valid: 25.41% Test: 28.99%\n",
      "Epoch: 357, Loss: 2.4749, Train: 7.50%, Valid: 10.90% Test: 13.57%\n",
      "Epoch: 358, Loss: 2.4732, Train: 7.31%, Valid: 8.49% Test: 8.60%\n",
      "Epoch: 359, Loss: 2.4668, Train: 10.79%, Valid: 18.64% Test: 17.97%\n",
      "Epoch: 360, Loss: 2.4757, Train: 6.73%, Valid: 8.59% Test: 8.89%\n",
      "Epoch: 361, Loss: 2.4654, Train: 6.72%, Valid: 7.71% Test: 7.99%\n",
      "Epoch: 362, Loss: 2.4698, Train: 7.15%, Valid: 9.49% Test: 9.87%\n",
      "Epoch: 363, Loss: 2.4648, Train: 7.11%, Valid: 9.02% Test: 9.19%\n",
      "Epoch: 364, Loss: 2.4656, Train: 8.68%, Valid: 10.75% Test: 10.82%\n",
      "Epoch: 365, Loss: 2.4596, Train: 10.36%, Valid: 14.13% Test: 15.89%\n",
      "Epoch: 366, Loss: 2.4526, Train: 9.95%, Valid: 11.45% Test: 13.63%\n",
      "Epoch: 367, Loss: 2.4629, Train: 14.75%, Valid: 24.55% Test: 23.15%\n",
      "Epoch: 368, Loss: 2.4692, Train: 13.50%, Valid: 18.83% Test: 18.26%\n",
      "Epoch: 369, Loss: 2.4603, Train: 14.70%, Valid: 24.09% Test: 24.16%\n",
      "Epoch: 370, Loss: 2.4599, Train: 15.89%, Valid: 25.67% Test: 24.14%\n",
      "Epoch: 371, Loss: 2.4549, Train: 16.88%, Valid: 27.24% Test: 25.52%\n",
      "Epoch: 372, Loss: 2.4571, Train: 23.85%, Valid: 30.26% Test: 26.98%\n",
      "Epoch: 373, Loss: 2.4499, Train: 22.81%, Valid: 24.97% Test: 22.57%\n",
      "Epoch: 374, Loss: 2.4541, Train: 23.44%, Valid: 30.97% Test: 29.20%\n",
      "Epoch: 375, Loss: 2.4451, Train: 13.37%, Valid: 23.16% Test: 23.04%\n",
      "Epoch: 376, Loss: 2.4515, Train: 16.42%, Valid: 25.81% Test: 23.75%\n",
      "Epoch: 377, Loss: 2.4574, Train: 10.18%, Valid: 20.09% Test: 19.90%\n",
      "Epoch: 378, Loss: 2.4551, Train: 7.89%, Valid: 12.81% Test: 11.48%\n",
      "Epoch: 379, Loss: 2.4537, Train: 13.86%, Valid: 25.31% Test: 25.14%\n",
      "Epoch: 380, Loss: 2.4462, Train: 10.35%, Valid: 16.42% Test: 17.32%\n",
      "Epoch: 381, Loss: 2.4460, Train: 9.79%, Valid: 17.27% Test: 18.36%\n",
      "Epoch: 382, Loss: 2.4361, Train: 7.51%, Valid: 8.46% Test: 7.88%\n",
      "Epoch: 383, Loss: 2.4524, Train: 15.07%, Valid: 25.84% Test: 25.20%\n",
      "Epoch: 384, Loss: 2.4584, Train: 15.40%, Valid: 26.83% Test: 26.46%\n",
      "Epoch: 385, Loss: 2.4526, Train: 16.02%, Valid: 28.96% Test: 29.35%\n",
      "Epoch: 386, Loss: 2.4447, Train: 11.38%, Valid: 20.61% Test: 19.40%\n",
      "Epoch: 387, Loss: 2.4544, Train: 11.22%, Valid: 18.97% Test: 19.52%\n",
      "Epoch: 388, Loss: 2.4509, Train: 10.51%, Valid: 18.38% Test: 19.24%\n",
      "Epoch: 389, Loss: 2.4267, Train: 10.30%, Valid: 16.00% Test: 19.67%\n",
      "Epoch: 390, Loss: 2.4385, Train: 14.46%, Valid: 25.64% Test: 26.26%\n",
      "Epoch: 391, Loss: 2.4564, Train: 7.24%, Valid: 7.94% Test: 8.60%\n",
      "Epoch: 392, Loss: 2.4547, Train: 8.40%, Valid: 8.05% Test: 8.86%\n",
      "Epoch: 393, Loss: 2.4319, Train: 8.63%, Valid: 10.18% Test: 12.49%\n",
      "Epoch: 394, Loss: 2.4559, Train: 8.14%, Valid: 7.10% Test: 7.56%\n",
      "Epoch: 395, Loss: 2.4452, Train: 7.76%, Valid: 7.21% Test: 6.47%\n",
      "Epoch: 396, Loss: 2.4569, Train: 7.82%, Valid: 8.94% Test: 8.04%\n",
      "Epoch: 397, Loss: 2.4514, Train: 9.56%, Valid: 15.91% Test: 16.84%\n",
      "Epoch: 398, Loss: 2.4447, Train: 12.45%, Valid: 25.10% Test: 27.37%\n",
      "Epoch: 399, Loss: 2.4369, Train: 9.75%, Valid: 16.56% Test: 20.86%\n",
      "Epoch: 400, Loss: 2.4270, Train: 7.77%, Valid: 12.53% Test: 14.81%\n",
      "Epoch: 401, Loss: 2.4392, Train: 8.89%, Valid: 15.11% Test: 17.33%\n",
      "Epoch: 402, Loss: 2.4476, Train: 8.30%, Valid: 10.67% Test: 12.42%\n",
      "Epoch: 403, Loss: 2.4379, Train: 16.71%, Valid: 25.22% Test: 24.81%\n",
      "Epoch: 404, Loss: 2.4470, Train: 13.18%, Valid: 20.40% Test: 25.19%\n",
      "Epoch: 405, Loss: 2.4370, Train: 9.53%, Valid: 11.01% Test: 13.87%\n",
      "Epoch: 406, Loss: 2.4509, Train: 14.50%, Valid: 25.17% Test: 25.02%\n",
      "Epoch: 407, Loss: 2.4596, Train: 9.73%, Valid: 16.05% Test: 19.02%\n",
      "Epoch: 408, Loss: 2.4706, Train: 7.31%, Valid: 8.71% Test: 8.49%\n",
      "Epoch: 409, Loss: 2.4637, Train: 5.62%, Valid: 5.86% Test: 5.19%\n",
      "Epoch: 410, Loss: 2.4695, Train: 6.46%, Valid: 9.79% Test: 10.10%\n",
      "Epoch: 411, Loss: 2.4586, Train: 6.74%, Valid: 10.99% Test: 13.24%\n",
      "Epoch: 412, Loss: 2.4755, Train: 6.77%, Valid: 12.30% Test: 14.07%\n",
      "Epoch: 413, Loss: 2.4613, Train: 6.90%, Valid: 12.61% Test: 15.54%\n",
      "Epoch: 414, Loss: 2.4531, Train: 10.04%, Valid: 20.80% Test: 22.78%\n",
      "Epoch: 415, Loss: 2.4656, Train: 9.54%, Valid: 19.05% Test: 22.83%\n",
      "Epoch: 416, Loss: 2.4554, Train: 8.87%, Valid: 14.39% Test: 19.65%\n",
      "Epoch: 417, Loss: 2.4619, Train: 10.76%, Valid: 20.26% Test: 24.24%\n",
      "Epoch: 418, Loss: 2.4404, Train: 9.03%, Valid: 13.11% Test: 12.28%\n",
      "Epoch: 419, Loss: 2.4546, Train: 8.54%, Valid: 10.76% Test: 11.17%\n",
      "Epoch: 420, Loss: 2.4530, Train: 13.85%, Valid: 24.92% Test: 28.13%\n",
      "Epoch: 421, Loss: 2.4535, Train: 13.75%, Valid: 24.46% Test: 26.39%\n",
      "Epoch: 422, Loss: 2.4585, Train: 11.96%, Valid: 21.26% Test: 24.00%\n",
      "Epoch: 423, Loss: 2.4653, Train: 14.22%, Valid: 24.41% Test: 22.98%\n",
      "Epoch: 424, Loss: 2.4552, Train: 10.45%, Valid: 18.03% Test: 18.33%\n",
      "Epoch: 425, Loss: 2.4646, Train: 13.43%, Valid: 23.62% Test: 22.34%\n",
      "Epoch: 426, Loss: 2.4663, Train: 8.52%, Valid: 14.63% Test: 18.42%\n",
      "Epoch: 427, Loss: 2.4649, Train: 6.81%, Valid: 11.45% Test: 12.38%\n",
      "Epoch: 428, Loss: 2.4455, Train: 6.98%, Valid: 11.56% Test: 11.44%\n",
      "Epoch: 429, Loss: 2.4530, Train: 7.04%, Valid: 11.50% Test: 12.84%\n",
      "Epoch: 430, Loss: 2.4405, Train: 6.11%, Valid: 7.50% Test: 8.37%\n",
      "Epoch: 431, Loss: 2.4427, Train: 7.90%, Valid: 12.98% Test: 17.75%\n",
      "Epoch: 432, Loss: 2.4537, Train: 7.31%, Valid: 8.82% Test: 10.60%\n",
      "Epoch: 433, Loss: 2.4580, Train: 8.45%, Valid: 11.32% Test: 13.75%\n",
      "Epoch: 434, Loss: 2.4462, Train: 8.28%, Valid: 8.66% Test: 9.53%\n",
      "Epoch: 435, Loss: 2.4376, Train: 9.85%, Valid: 18.23% Test: 21.93%\n",
      "Epoch: 436, Loss: 2.4515, Train: 10.17%, Valid: 16.68% Test: 20.85%\n",
      "Epoch: 437, Loss: 2.4328, Train: 12.33%, Valid: 22.70% Test: 26.25%\n",
      "Epoch: 438, Loss: 2.4425, Train: 10.81%, Valid: 17.03% Test: 21.11%\n",
      "Epoch: 439, Loss: 2.4373, Train: 13.04%, Valid: 23.12% Test: 26.88%\n",
      "Epoch: 440, Loss: 2.4417, Train: 16.04%, Valid: 26.44% Test: 25.99%\n",
      "Epoch: 441, Loss: 2.4449, Train: 11.57%, Valid: 21.06% Test: 24.74%\n",
      "Epoch: 442, Loss: 2.4287, Train: 9.10%, Valid: 11.27% Test: 13.41%\n",
      "Epoch: 443, Loss: 2.4686, Train: 16.23%, Valid: 25.85% Test: 24.20%\n",
      "Epoch: 444, Loss: 2.4737, Train: 21.73%, Valid: 28.65% Test: 26.11%\n",
      "Epoch: 445, Loss: 2.4472, Train: 19.57%, Valid: 26.17% Test: 24.84%\n",
      "Epoch: 446, Loss: 2.4611, Train: 17.59%, Valid: 25.84% Test: 24.30%\n",
      "Epoch: 447, Loss: 2.4487, Train: 11.82%, Valid: 21.27% Test: 20.94%\n",
      "Epoch: 448, Loss: 2.4582, Train: 9.21%, Valid: 13.23% Test: 11.79%\n",
      "Epoch: 449, Loss: 2.4728, Train: 13.01%, Valid: 21.89% Test: 20.88%\n",
      "Epoch: 450, Loss: 2.4755, Train: 14.09%, Valid: 23.86% Test: 22.97%\n",
      "Epoch: 451, Loss: 2.4660, Train: 11.63%, Valid: 18.12% Test: 17.07%\n",
      "Epoch: 452, Loss: 2.4598, Train: 11.28%, Valid: 16.39% Test: 15.89%\n",
      "Epoch: 453, Loss: 2.4531, Train: 11.12%, Valid: 17.96% Test: 18.30%\n",
      "Epoch: 454, Loss: 2.4544, Train: 9.16%, Valid: 14.12% Test: 13.09%\n",
      "Epoch: 455, Loss: 2.4530, Train: 7.94%, Valid: 8.58% Test: 7.17%\n",
      "Epoch: 456, Loss: 2.4500, Train: 9.09%, Valid: 11.89% Test: 10.36%\n",
      "Epoch: 457, Loss: 2.4507, Train: 10.81%, Valid: 16.56% Test: 16.48%\n",
      "Epoch: 458, Loss: 2.4335, Train: 12.28%, Valid: 13.88% Test: 13.15%\n",
      "Epoch: 459, Loss: 2.4341, Train: 12.55%, Valid: 13.51% Test: 12.85%\n",
      "Epoch: 460, Loss: 2.4388, Train: 11.04%, Valid: 12.66% Test: 13.67%\n",
      "Epoch: 461, Loss: 2.4336, Train: 10.45%, Valid: 14.28% Test: 15.55%\n",
      "Epoch: 462, Loss: 2.4320, Train: 9.30%, Valid: 12.64% Test: 15.06%\n",
      "Epoch: 463, Loss: 2.4216, Train: 10.59%, Valid: 21.25% Test: 25.58%\n",
      "Epoch: 464, Loss: 2.4309, Train: 8.89%, Valid: 15.10% Test: 18.09%\n",
      "Epoch: 465, Loss: 2.4211, Train: 10.03%, Valid: 21.26% Test: 25.83%\n",
      "Epoch: 466, Loss: 2.4285, Train: 10.75%, Valid: 23.48% Test: 27.54%\n",
      "Epoch: 467, Loss: 2.4311, Train: 10.73%, Valid: 22.80% Test: 26.99%\n",
      "Epoch: 468, Loss: 2.4257, Train: 9.99%, Valid: 19.73% Test: 23.77%\n",
      "Epoch: 469, Loss: 2.4345, Train: 11.60%, Valid: 18.59% Test: 23.38%\n",
      "Epoch: 470, Loss: 2.4144, Train: 12.65%, Valid: 19.74% Test: 23.10%\n",
      "Epoch: 471, Loss: 2.4226, Train: 9.17%, Valid: 8.64% Test: 9.46%\n",
      "Epoch: 472, Loss: 2.4199, Train: 12.08%, Valid: 22.30% Test: 24.77%\n",
      "Epoch: 473, Loss: 2.4186, Train: 9.36%, Valid: 15.51% Test: 21.53%\n",
      "Epoch: 474, Loss: 2.4254, Train: 11.52%, Valid: 18.34% Test: 24.28%\n",
      "Epoch: 475, Loss: 2.4256, Train: 9.35%, Valid: 13.83% Test: 19.36%\n",
      "Epoch: 476, Loss: 2.4161, Train: 10.48%, Valid: 14.59% Test: 19.66%\n",
      "Epoch: 477, Loss: 2.4208, Train: 11.54%, Valid: 16.38% Test: 21.59%\n",
      "Epoch: 478, Loss: 2.4220, Train: 9.16%, Valid: 8.77% Test: 9.81%\n",
      "Epoch: 479, Loss: 2.4277, Train: 12.04%, Valid: 19.11% Test: 24.27%\n",
      "Epoch: 480, Loss: 2.4099, Train: 9.84%, Valid: 13.75% Test: 16.94%\n",
      "Epoch: 481, Loss: 2.4168, Train: 9.22%, Valid: 12.98% Test: 16.16%\n",
      "Epoch: 482, Loss: 2.4114, Train: 12.30%, Valid: 23.62% Test: 27.60%\n",
      "Epoch: 483, Loss: 2.4131, Train: 9.15%, Valid: 9.94% Test: 10.39%\n",
      "Epoch: 484, Loss: 2.4087, Train: 8.05%, Valid: 8.34% Test: 7.25%\n",
      "Epoch: 485, Loss: 2.4050, Train: 14.07%, Valid: 25.76% Test: 26.50%\n",
      "Epoch: 486, Loss: 2.4110, Train: 11.15%, Valid: 16.89% Test: 21.31%\n",
      "Epoch: 487, Loss: 2.4032, Train: 14.72%, Valid: 26.68% Test: 28.89%\n",
      "Epoch: 488, Loss: 2.4201, Train: 14.68%, Valid: 25.62% Test: 23.57%\n",
      "Epoch: 489, Loss: 2.4059, Train: 9.28%, Valid: 18.47% Test: 20.72%\n",
      "Epoch: 490, Loss: 2.4096, Train: 15.13%, Valid: 26.99% Test: 28.66%\n",
      "Epoch: 491, Loss: 2.4024, Train: 13.34%, Valid: 22.83% Test: 27.80%\n",
      "Epoch: 492, Loss: 2.4152, Train: 15.96%, Valid: 27.54% Test: 29.03%\n",
      "Epoch: 493, Loss: 2.3970, Train: 12.43%, Valid: 25.50% Test: 28.40%\n",
      "Epoch: 494, Loss: 2.3952, Train: 20.37%, Valid: 28.78% Test: 27.72%\n",
      "Epoch: 495, Loss: 2.3999, Train: 13.94%, Valid: 24.95% Test: 28.06%\n",
      "Epoch: 496, Loss: 2.4053, Train: 18.39%, Valid: 26.69% Test: 24.82%\n",
      "Epoch: 497, Loss: 2.4211, Train: 15.31%, Valid: 28.27% Test: 29.78%\n",
      "Epoch: 498, Loss: 2.4098, Train: 9.91%, Valid: 14.39% Test: 19.13%\n",
      "Epoch: 499, Loss: 2.4196, Train: 10.32%, Valid: 18.15% Test: 22.58%\n",
      "Epoch: 500, Loss: 2.3974, Train: 8.92%, Valid: 15.02% Test: 15.53%\n"
     ]
    }
   ],
   "source": [
    "from main_arxiv_gnn import GCN, train, test\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-arxiv',\n",
    "                                 transform=T.ToSparseTensor())\n",
    "\n",
    "data = dataset[0]\n",
    "data.adj_t = data.adj_t.to_symmetric()\n",
    "data.x = features\n",
    "data = data.cuda()\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx = split_idx['train'].to(device)\n",
    "\n",
    "gnn = GCN(data.num_features, hidden_channels=features.shape[1],\n",
    "          out_channels=dataset.num_classes, num_layers=3, dropout=0.5).cuda()\n",
    "\n",
    "evaluator = Evaluator(name='ogbn-arxiv')\n",
    "\n",
    "\n",
    "gnn.reset_parameters()\n",
    "EPOCHS = 500\n",
    "optimizer = torch.optim.Adam(gnn.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "for epoch in range(1, 1 + EPOCHS):\n",
    "    loss = train(gnn, data, train_idx, optimizer)\n",
    "    result = test(gnn, data, split_idx, evaluator)\n",
    "\n",
    "    train_acc, valid_acc, test_acc = result\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "          f'Loss: {loss:.4f}, '\n",
    "          f'Train: {100 * train_acc:.2f}%, '\n",
    "          f'Valid: {100 * valid_acc:.2f}% '\n",
    "          f'Test: {100 * test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2683, -0.3904, -0.9956,  0.3222,  0.8800, -0.0257, -0.6225,  0.1184,\n",
       "        -0.9612, -0.9998, -0.7657,  0.9723,  0.7014,  0.8677, -0.1716, -0.4991,\n",
       "         0.3457, -0.1709,  0.1376,  0.9450,  0.2497,  1.0000, -0.5088,  0.4366,\n",
       "         0.1875,  0.9898, -0.5866,  0.2618,  0.6200,  0.4965,  0.3178,  0.1491,\n",
       "        -0.9007, -0.0413, -0.9964, -0.9122,  0.4677,  0.0558,  0.1609,  0.0814,\n",
       "        -0.2902,  0.1601,  1.0000, -0.3534,  0.7455,  0.0467, -1.0000,  0.2013,\n",
       "        -0.0332,  0.9895,  0.9663,  0.9947,  0.1496,  0.2611,  0.4517, -0.7545,\n",
       "        -0.3114,  0.0227, -0.2535, -0.3701, -0.3778,  0.1775, -0.9339, -0.2378,\n",
       "         0.9820,  0.9800, -0.1038, -0.3181, -0.0381, -0.1220,  0.1029,  0.1095,\n",
       "        -0.7900, -0.4588,  0.9399,  0.1473, -0.7871,  1.0000, -0.1108, -0.7061,\n",
       "         0.9872,  0.9593,  0.6898, -0.8105,  0.9396, -1.0000,  0.6023,  0.0604,\n",
       "        -0.8297,  0.0989,  0.5366, -0.1621,  0.9774,  0.7457, -0.6837, -0.7392,\n",
       "        -0.1590, -0.9745, -0.3571, -0.6757,  0.3402, -0.1930, -0.4273, -0.3151,\n",
       "         0.4831, -0.4249,  0.6263,  0.8336,  0.2503,  0.4571,  0.6504, -0.4543,\n",
       "         0.2559, -0.4970,  0.3284, -0.3166, -0.9033, -0.7214, -0.8073,  0.1805,\n",
       "        -0.1733, -0.2142, -0.2118, -0.8655,  0.4139, -0.1926, -0.9916, -1.0000,\n",
       "        -0.7759, -0.8380, -0.4284, -0.3799, -0.6761, -0.8242,  0.4129,  0.7267,\n",
       "         0.2306,  0.9998, -0.3052,  0.4216, -0.4429, -0.8505,  0.8167, -0.1035,\n",
       "         0.9443, -0.6799,  0.4348,  0.0758, -0.7498,  0.5576, -0.8297, -0.1252,\n",
       "        -0.9722, -0.0673, -0.1555,  0.3791, -0.8255, -0.9896, -0.6263, -0.0584,\n",
       "        -0.0768,  0.2958,  0.9025,  0.1379, -0.4932,  0.3175,  0.4320,  0.4198,\n",
       "        -0.3074, -0.4538,  0.2736, -0.3957, -0.9926, -0.7966, -0.3052,  0.1864,\n",
       "         0.8105,  0.0358,  0.3383,  0.8543, -0.4925,  0.7108, -0.7344,  0.8100,\n",
       "         0.0826,  0.2700, -0.9369,  0.8581, -0.1656,  0.5457,  0.0525, -0.7896,\n",
       "        -0.3380, -0.1160, -0.5313, -0.1591, -0.9585,  0.1080, -0.0798, -0.1333,\n",
       "        -0.1244,  0.4306,  0.3125, -0.4182,  0.8747,  0.4589,  0.1534,  0.1107,\n",
       "        -0.0087,  0.0757,  0.0289,  0.7862, -0.9512,  0.1818, -0.2553, -0.7418,\n",
       "        -0.2826,  0.0786, -0.3190, -0.5540,  0.7025, -0.9221,  0.6627,  0.0503,\n",
       "         0.9186, -0.0936,  0.0565, -0.6183,  0.4401, -0.0858,  0.9805,  0.9959,\n",
       "        -0.4120, -0.9411,  0.9604, -0.9963, -0.5929, -0.6618, -0.2092,  0.3225,\n",
       "        -0.4728,  0.9085,  0.9830,  0.5485,  0.0945, -0.9654,  0.3806, -0.8151,\n",
       "        -0.0659,  0.6840,  0.9710,  0.7841,  0.4040, -0.3905, -0.2221, -0.7875,\n",
       "        -0.9781, -0.5991, -0.9826,  0.1706, -0.8814,  0.9475,  0.2816,  0.9410,\n",
       "        -0.4654, -0.2902, -0.4357, -0.3488, -0.0389,  0.0053, -0.6683,  0.1380,\n",
       "        -0.7028, -0.4553, -0.1173, -0.0303, -0.9128,  0.0463, -0.0703,  0.5236,\n",
       "         0.2532,  0.2956, -0.9866,  0.4150,  1.0000,  0.6288,  0.1732, -0.6728,\n",
       "        -1.0000, -0.9230,  0.9999, -0.9988, -1.0000, -0.1689, -0.4868,  0.0879,\n",
       "        -1.0000, -0.2341,  0.2367,  0.0807,  0.9049,  0.5079, -0.6429, -1.0000,\n",
       "         0.3167,  0.3887, -0.7686,  0.9593, -0.4990,  0.5821,  0.3109,  0.5891,\n",
       "         0.0665,  0.3278, -0.9904, -0.0772, -0.9312, -0.9402,  1.0000, -0.0139,\n",
       "        -0.7188,  0.4135,  0.8150,  0.1407, -0.0311, -0.6032, -0.3916,  0.6681,\n",
       "         0.7356,  0.2051,  0.3575,  0.2224,  0.3165,  0.9074, -0.5416,  0.6839,\n",
       "        -0.6231,  0.4623, -0.5183,  0.4776, -0.8591, -0.9224,  0.4264, -0.3512,\n",
       "         0.9783,  1.0000,  0.8577,  0.3056,  0.6666,  0.1320, -0.7976,  1.0000,\n",
       "         0.8934, -0.6968, -0.7663,  0.7782, -0.5217, -0.6645,  0.9936, -0.1675,\n",
       "        -0.9662, -0.8058,  0.8929, -0.7795,  0.9999, -0.0128, -0.6521,  0.4996,\n",
       "         0.4009, -0.5580, -0.6295, -0.2092, -0.9054,  0.3092,  0.4623,  0.4402,\n",
       "        -0.0361,  0.1160,  0.1405,  0.7626, -0.7917,  0.2430, -0.9031, -0.5724,\n",
       "         0.9961,  0.2400, -0.1890, -0.1238,  0.0415, -0.8632, -0.5975,  0.8858,\n",
       "         1.0000, -0.2690,  0.9717, -0.2336, -0.1285,  0.1711,  0.5899,  0.5497,\n",
       "        -0.0651, -0.4827,  0.9186,  0.4155, -0.8911, -0.5817,  0.1258, -0.0345,\n",
       "         0.9997,  0.5803,  0.2879,  0.6062,  0.9966,  0.0898, -0.5023,  0.9866,\n",
       "         0.8636, -0.1729,  0.8180, -0.4138, -0.9781, -0.0933, -0.4357,  0.0039,\n",
       "        -0.7727, -0.0952, -0.4683,  0.7213,  0.9937,  0.2900,  0.0810,  0.9606,\n",
       "         1.0000, -0.9966, -0.1720,  0.9851, -0.7931, -1.0000,  0.3204, -0.0654,\n",
       "        -0.1530, -0.9853, -0.3407,  0.1934, -0.5157,  0.9738,  0.8583,  0.4324,\n",
       "        -0.4552, -0.8564, -0.0309, -0.0863, -0.9990,  0.0474, -0.1922,  0.6622,\n",
       "        -0.3447,  0.1473, -0.7645, -0.3280,  0.4590, -0.1799,  0.7748,  0.9917,\n",
       "         0.7154, -0.9946, -0.6756, -0.1127, -0.1265,  0.1173, -0.4073, -0.9893,\n",
       "        -0.2330,  1.0000, -0.4030,  0.9919,  0.1171, -0.3483, -0.2534,  0.1845,\n",
       "         0.9950,  0.2355, -0.9184, -0.9837,  0.9899, -0.4347,  0.4218,  0.9518,\n",
       "         0.9707, -0.0571,  0.9602,  0.1062,  0.0641, -0.0342, -0.6205,  0.2030,\n",
       "        -0.0909, -0.1724, -0.0892, -0.2478,  0.9406,  1.0000,  0.1488,  0.8914,\n",
       "        -0.8004, -0.9861, -0.0132,  1.0000,  0.5882,  0.1984,  0.5793,  0.6683,\n",
       "        -0.0653, -0.4491, -0.1858, -0.1260,  0.0634, -0.0288,  0.2486, -0.3401,\n",
       "        -0.7798, -0.5884,  0.0971, -0.4945,  1.0000, -0.5462, -0.2166, -0.0126,\n",
       "        -0.6385, -0.9982, -0.0376, -0.5985, -0.1002,  0.1622,  0.3353,  0.2819,\n",
       "        -0.7164, -0.1238,  0.9864,  0.9086, -0.9860, -0.4702,  0.2976, -0.2543,\n",
       "         0.5543,  1.0000,  0.4861,  0.8873,  0.0828,  0.0965,  0.2030, -0.7220,\n",
       "         0.2082, -0.3115, -0.1545, -0.1773,  0.2021, -0.0085, -0.9694, -0.2636,\n",
       "         0.1759, -0.7430, -0.4751,  0.0623,  0.3541,  0.5277,  0.0133,  0.0117,\n",
       "         0.2192,  0.1065,  0.1653, -0.3610, -0.4806, -1.0000,  0.1516, -1.0000,\n",
       "         0.8912,  0.7799, -0.0700,  0.4123,  0.8449,  0.9285,  0.3742, -0.9860,\n",
       "        -0.0532,  0.1061, -0.2910, -0.6381,  0.3129,  0.4389,  0.1681, -0.1128,\n",
       "        -0.9046,  0.6434, -0.1830,  1.0000,  0.0658, -0.6853,  0.7362,  0.1496,\n",
       "        -0.1993,  1.0000,  0.5856, -0.6319,  0.2106, -0.8131, -0.1904,  0.4009,\n",
       "        -0.0035, -0.6767, -0.9925, -0.3810, -0.3018, -0.8009,  0.4774, -0.1128,\n",
       "        -0.2080,  0.1921,  0.9918,  0.7345,  0.5960,  0.0616, -0.8105, -0.5370,\n",
       "         0.6239,  0.3425, -0.8164, -0.0951,  1.0000,  0.3292, -0.6242,  0.0898,\n",
       "         0.0806, -0.1270, -0.3396,  0.1380,  0.3225,  0.5856, -0.1637,  0.1881,\n",
       "        -0.9862, -0.1000, -0.7625, -0.9458,  0.0896, -0.2713, -0.7327, -0.7101,\n",
       "         0.7434, -0.1356, -0.1364,  0.1206, -0.1093,  0.3155,  0.0514, -1.0000,\n",
       "         0.6247,  0.3960,  0.9820,  0.2335,  0.8354,  0.6845,  0.1411, -0.5536,\n",
       "         0.7993,  0.0160, -0.1933, -0.0640,  0.4100,  0.2210,  0.0751, -0.4020,\n",
       "        -0.6607, -0.9393, -0.9457, -0.8764,  0.3211, -0.8750,  0.7692,  0.7600,\n",
       "         0.2814,  0.1131, -0.6969, -0.9702, -0.8225,  0.3605, -0.4466, -0.2346,\n",
       "         0.3697, -0.2411,  0.0753,  0.6866, -0.9848, -0.0532, -0.9473,  0.4244,\n",
       "         0.9840, -0.8001,  0.1753,  0.7761, -0.1896,  0.2195, -0.2804,  0.7884,\n",
       "         0.9584, -0.2598,  0.3538, -0.4169, -0.0141, -0.2615, -0.2227, -0.3786,\n",
       "        -0.6751,  0.6506, -0.2273,  0.1128,  0.9521, -0.0367, -0.2820, -0.0169,\n",
       "        -0.9455, -0.3712, -0.5942,  0.1926, -0.7456,  0.9667, -0.1407,  0.9951,\n",
       "         0.6710, -0.2684, -0.0888, -0.2330,  0.0069, -0.8724, -0.6093, -0.3969,\n",
       "         0.2533,  0.2530,  1.0000, -0.9260, -0.9715, -0.7466, -0.2941,  0.5011,\n",
       "        -0.3911, -1.0000,  0.1704, -0.8673,  0.9503, -0.8060,  0.9672, -0.8114,\n",
       "        -0.0232, -0.3660,  0.9226,  0.9691, -0.2668, -0.5070,  0.6998, -0.8189,\n",
       "         0.9954, -0.1074, -0.8909, -0.7091,  0.7446, -0.9914, -0.2395, -0.1633])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygNodePropPredDataset(\n",
    "    name='ogbn-arxiv', transform=T.ToSparseTensor())\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([169343, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51be54f33a41cac59dddcbb17816709b9089a1f775c651da797be6dbfb614eca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gnn_ak')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
