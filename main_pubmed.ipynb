{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from bert import preprocessing, generate_node_embeddings\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from load_pubmed import get_pubmed_casestudy\n",
    "from main_pubmed_gnn import GCN\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "def train(lm, gnn, g, loader, optimizer_lm, optimizer_gnn,  device):\n",
    "    node_embs = []\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        output = lm(b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    output_hidden_states=True)\n",
    "        emb = output['hidden_states'][-1]  # outputs[0]=last hidden state\n",
    "        cls_token_emb = emb.permute(1, 0, 2)[0]\n",
    "        node_embs.append(cls_token_emb.detach().cpu())\n",
    "    node_embs = torch.cat(node_embs, dim=0)\n",
    "    torch.cuda.empty_cache() # PyTorch thing\n",
    "\n",
    "    # train gnn\n",
    "    \n",
    "    X = node_embs.to(device)\n",
    "    g = g.to(device)\n",
    "    X.requires_grad = True\n",
    "    X.retain_grad()\n",
    "    gnn.train()\n",
    "    optimizer_gnn.zero_grad()\n",
    "    out = gnn(X, g.edge_index)[g.train_mask]\n",
    "    loss = F.nll_loss(out, g.y[g.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer_gnn.step()\n",
    "    torch.cuda.empty_cache() # PyTorch thing\n",
    "\n",
    "    grad = X.grad\n",
    "    grad.requires_grad = True\n",
    "    print(grad.shape)\n",
    "    \n",
    "    lm.train()\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        optimizer_lm.zero_grad()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        # Forward pass\n",
    "        output = lm(b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    output_hidden_states=True)\n",
    "        emb = output['hidden_states'][-1]  # outputs[0]=last hidden state\n",
    "        cls_token_emb = emb.permute(1, 0, 2)[0]\n",
    "        loss = grad[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE].sum()\n",
    "        loss.backward()\n",
    "        optimizer_lm.step()\n",
    "        torch.cuda.empty_cache() # PyTorch thing\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=-1)\n",
    "    correct = pred.eq(data.y)\n",
    "\n",
    "    train_acc = correct[data.train_mask].sum().item() / \\\n",
    "        data.train_mask.sum().item()\n",
    "    val_acc = correct[data.val_mask].sum().item() / data.val_mask.sum().item()\n",
    "    test_acc = correct[data.test_mask].sum().item() / \\\n",
    "        data.test_mask.sum().item()\n",
    "\n",
    "    return train_acc, val_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "print(\"[!] Loading dataset\")\n",
    "f = open('pubmed.json')\n",
    "pubmed = json.load(f)\n",
    "df_pubmed = pd.DataFrame.from_dict(pubmed)\n",
    "\n",
    "# Preprocess\n",
    "print(\"[!] Preprocessing\")\n",
    "start = time.time()\n",
    "AB = df_pubmed['AB'].fillna(\"\")\n",
    "TI = df_pubmed['TI'].fillna(\"\")\n",
    "text = []\n",
    "for ti, ab in zip(TI, AB):\n",
    "    t = 'Title: ' + ti + '\\n'+'Abstract: ' + ab\n",
    "    # t = ti + ab\n",
    "    text.append(t)\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased', do_lower_case=True)\n",
    "for sample in tqdm(text):\n",
    "    encoding_dict = preprocessing(sample, tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids'])\n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "token_id = torch.cat(token_id, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "print(\"Time: \", time.time()-start)\n",
    "\n",
    "# Prepare DataLoader\n",
    "batch_size = 16\n",
    "dataset = TensorDataset(token_id, attention_masks)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False,\n",
    "    sampler=SequentialSampler(dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Load the BertForSequenceClassification model\n",
    "bert = BertModel.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run on GPU\n",
    "print(\"[!] Generating node embeddings\")\n",
    "start = time.time()\n",
    "bert.cuda()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "features = generate_node_embeddings(bert, dataloader, device)\n",
    "print(\"Time: \", time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_pubid = get_pubmed_casestudy()\n",
    "data.x = features\n",
    "gnn_model = GCN(\n",
    "    in_channels=data.x.shape[1], hidden_channels=128, out_channels=3, num_layers=4, dropout=0)\n",
    "gnn_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"[!] Start training\")\n",
    "start = time.time()\n",
    "data.cuda()\n",
    "optimizer_gnn = torch.optim.Adam(gnn_model.parameters(), lr=0.001)\n",
    "optimizer_lm = torch.optim.Adam(bert.parameters(), lr=0.001)\n",
    "for epoch in range(1, 1000):\n",
    "    loss = train(bert, gnn_model, data, dataloader,\n",
    "                 optimizer_lm, optimizer_gnn, device)\n",
    "    accs = test(gnn_model, data)\n",
    "    print(\n",
    "        f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train Acc: {accs[0]:.4f}, Val Acc: {accs[1]:.4f}, Test Acc: {accs[2]:.4f}')\n",
    "print(\"Time: \", time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# from bert import preprocessing, generate_node_embeddings\n",
    "\n",
    "# import json\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# import time\n",
    "\n",
    "# from load_pubmed import get_pubmed_casestudy\n",
    "# from main_pubmed_gnn import GCN\n",
    "\n",
    "# data, data_pubid = get_pubmed_casestudy()\n",
    "\n",
    "\n",
    "# a = torch.rand(data.x.shape[0], 768).detach()\n",
    "# x = a.cuda()\n",
    "# x.requires_grad = True\n",
    "# x.retain_grad()\n",
    "\n",
    "# data.cuda()\n",
    "# gnn_model = GCN(\n",
    "#     in_channels=x.shape[1], hidden_channels=128, out_channels=3, num_layers=4, dropout=0)\n",
    "# gnn_model.cuda()\n",
    "# optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.001)\n",
    "# out = gnn_model(x, data.edge_index)[data.train_mask]\n",
    "# loss = F.nll_loss(out, data.y[data.train_mask])\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "# print(x.grad.shape)\n",
    "\n",
    "# grad = x.grad.clone()\n",
    "# grad.requires_grad = True\n",
    "# grad.sum().backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_pubmed import get_pubmed_casestudy\n",
    "data, data_pubid = get_pubmed_casestudy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], train_id=[11830], val_id=[3943], test_id=[3944])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51be54f33a41cac59dddcbb17816709b9089a1f775c651da797be6dbfb614eca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gnn_ak')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
