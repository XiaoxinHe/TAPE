norman00argument
Argument in Multi-Agent Systems Multi-agent systems research is concerned both with the modelling of human and animal societies and with the development of principles for the design of practical distributed information management systems. This position paper will, rather than examine the various dierences in perspective within this area of research, discuss issues of communication and commitment that are of interest to multi-agent systems research in general. 1 Introduction A computational society is a collection of autonomous agents that are loosely dependent upon each other. The intentional stance [12] is often taken in describing the state of these agents. An agent may have beliefs, desires, intentions, and it may adopt a role or have relationships with others. Thus, multi-agent systems (MAS) as with most AI research is signi cantly inuenced, at least in its vocabulary, by philosophy and cognitive psychology. 1 So, what's the point? Computational societies are developed for two primary reasons:  Mode...
Agents
kusiak00decomposition
Decomposition in Data Mining: An Industrial Case Study Data mining offers tools for discovery of relationships, patterns, and knowledge in large databases. The knowledge extraction process is computationally complex and therefore a subset of all data is normally considered for mining. In this paper, numerous methods for decomposition of data sets are discussed. Decomposition enhances the quality of knowledge extracted from large databases by simplification of the data mining task. The ideas presented are illustrated with examples and an industrial case study. In the case study reported in this paper, a data mining approach is applied to extract knowledge from a data set. The extracted knowledge is used for the prediction and prevention of manufacturing faults in wafers.
ML
pant02exploration
Exploration versus Exploitation in Topic Driven Crawlers Topic driven crawlers are increasingly seen as a way to address the scalability limitations of universal search engines, by distributing the crawling process across users, queries, or even client computers. The context available to a topic driven crawler allows for informed decisions about how to prioritize the links to be explored, given time and bandwidth constraints. We have developed a framework and a number of methods to evaluate the performance of topic driven crawler algorithms in a fair way, under limited memory resources. Quality metrics are derived from lexical features, link analysis, and a hybrid combination of the two. In this paper we focus on the issue of how greedy a crawler should be. Given noisy quality estimates of links in a frontier, we investigate what is an appropriate balance between a crawler's need to exploit this information to focus on the most promising links, and the need to explore links that appear suboptimal but might lead to more relevant pages. We show that exploration is essential to locate the most relevant pages under a number of quality measures, in spite of a penalty in the early stage of the crawl.
IR
emmerich00software
Software Engineering and Middleware: A Roadmap The construction of a large class of distributed systems can be simplified by leveraging middleware, which is layered between network operating systems and application components. Middleware resolves heterogeneity, and facilitates communication and coordination of distributed components. State of-the-practice middleware products enable software engineers to build systems that are distributed across a localarea network. State-of-the-art middleware research aims to push this boundary towards Internet-scale distribution, adaptive systems, middleware for dependable and wireless systems. The challenge for software engineering research is to devise notations, techniques, methods and tools for distributed system construction that systematically build and exploit the capabilities that middleware products deliver, now and in the future.
DB
386938
Dynamic-Agents for Dynamic Service Provisioning We claim that a dynamic-agent infrastructure can provide a shift from static distributed computing to dynamic distributed computing, and we have developed such an infrastructure to realize such a shift. We shall show its impact on software engineering through a comparison with other distributed object-oriented systems such as CORBA and DCOM, and demonstrate its value in highly dynamic system integration and service provisioning.  The infrastructure is Java-based, light-weight, and extensible. It differs from other agent platforms and client/server infrastructures in its support of dynamic behavior modification of agents. A dynamic-agent is not designed to have a fixed set of predefined functions but instead, to carry application-specific actions, which can be loaded and modified on the fly. This allows a dynamic-agent to adjust its capability for accommodating environment and requirement changes, and play different roles across multiple applications.  The above features are supported b...
Agents
532373
A Hybrid Mobile Robot Architecture with Integrated Planning and Control Research in the planning and control of mobile robots has received much attention in the past two decades. Two basic approaches have emerged from these research efforts: deliberative vs. reactive. These two approaches can be distinguished by their different usage of sensed data and global knowledge, speed of response, reasoning capability, and complexity of computation. Their strengths are complementary and their weaknesses can be mitigated by combining the two approaches in a hybrid architecture. This paper describes a method for goal-directed, collision-free navigation in unpredictable environments that employs a behavior-based hybrid architecture with asynchronously operating behavioral modules. It differs from existing hybrid architectures in two important ways: (1) the planning module produces a sequence of checkpoints instead of a conventional complete path, and (2) in addition to obstacle avoidance, the reactive module also performs target reaching under the control of a self-organizing neural network. The neural network is trained to perform fine, smooth motor control that moves the robot through the checkpoints. These two aspects facilitate a tight integration between high-level planning and low-level control, which permits real-time performance and easy path modification even when the robot is en route to the goal position.
Agents
arge02optimal
Optimal External Memory Interval Management  In this paper we present the external interval tree, an optimal external memory data structure for answering stabbing queries on a set of dynamically maintained intervals. The external interval tree can be used in an optimal solution to the dynamic interval management problem, which is a central problem for object-oriented and temporal databases and for constraint logic programming. Part of the structure uses a weight-balancing technique for efficient worst-case manipulation of balanced trees, which is of independent interest. The external interval tree, as well as our new balancing technique, have recently been used to develop several efficient external data structures.
DB
pandurangan02using
Using PageRank to Characterize Web Structure Recent work on modeling the Web graph has dwelt on capturing  the degree distributions observed on the Web. Pointing out that  this represents a heavy reliance on "local" properties of the Web graph,  we study the distribution of PageRank values (used in the Google search  engine) on the Web. This distribution is of independent interest in optimizing  search indices and storage. We show that PageRank values on  the Web follow a power law. We then develop detailed models for the  Web graph that explain this observation, and moreover remain faithful  to previously studied degree distributions. We analyze these models,  and compare the analyses to both snapshots from the Web and to graphs  generated by simulations on the new models. To our knowledge this represents  the first modeling of the Web that goes beyond fitting degree  distributions on the Web.
IR
18832
XM2VTSDB: The Extended M2VTS Database In this paper we describe the acquisition and content of a large multi-modal database intended for training and testing of multi-modal verification systems. The XM2VTSDB database offers synchronised video and speech data as well as image sequences allowing multiple views of the face. It consists of digital video recordings taken of 295 hundred subjects at one month intervals taken over a period of five months. We also describe a protocol for evaluating verification algorithms on the database. The database has been made available to anyone on request to the University of Surrey through http://www.ee.surrey.ac.uk/Research/VSSP/xm2vtsdb.
DB
binder01design
Design And Implementation Of The J-Seal2 Mobile Agent Kernel J-SEAL2 is a secure, portable, and efficient execution environment for mobile agents. The core of the system is a micro-kernel fulfilling the same functions as a traditional operating system kernel: protection, communication, domain termination, and resource control. This paper describes the key concepts of the J-SEAL2 micro-kernel and how they are implemented in pure Java.  
Agents
slivinskas00foundation
A Foundation for Conventional and Temporal Query Optimization Addressing Duplicates and Ordering AbstractÐMost real-world databases contain substantial amounts of time-referenced, or temporal, data. Recent advances in temporal query languages show that such database applications may benefit substantially from built-in temporal support in the DBMS. To achieve this, temporal query representation, optimization, and processing mechanisms must be provided. This paper presents a foundation for query optimization that integrates conventional and temporal query optimization and is suitable for both conventional DBMS architectures and ones where the temporal support is obtained via a layer on top of a conventional DBMS. This foundation captures duplicates and ordering for all queries, as well as coalescing for temporal queries, thus generalizing all existing approaches known to the authors. It includes a temporally extended relational algebra to which SQL and temporal SQL queries may be mapped, six types of algebraic equivalences, concrete query transformation rules that obey different equivalences, a procedure for determining which types of transformation rules are applicable for optimizing a query, and a query plan enumeration algorithm. The presented approach partitions the work required by the database implementor to develop a provably correct query optimizer into four stages: The database implementor has to 1) specify operations formally, 2) design and prove correct appropriate transformation rules that satisfy any of the six equivalence types, 3) augment the mechanism that determines when the different types of rules are applicable to ensure that the enumeration algorithm applies the rules correctly, and 4) ensure that the mapping generates a correct initial query plan. Index TermsÐTemporal databases, query optimization, transformation rules, temporal algebra, duplicate elimination, coalescing. 1
DB
496719
Use of Satellite Image Referencing Algorithms to Characterize Asphaltic Concrete Mixtures A natural way to test the structural integrity of a pavement is to send signals with different frequencies through the pavement and compare the results with the signals passing through an ideal pavement. For this comparison, we must determine how, for the corresponding mixture, the elasticity E depends on the frequency f in the range from 0.1 to 10  5  Hz. It is very expensive to perform measurements in high frequency area (above 20 Hz). To avoid these measurements, we can use the fact that for most of these mixtures, when we change a temperature, the new dependence changes simply by scaling. Thus, instead of performing expensive measurements for different frequencies, we can measure the dependence of E on moderate frequencies f for different temperatures, and then combine the resulting curves into a single "master" curve. In this paper, we show how fuzzy techniques can help to automate this "combination".
ML
kitano99robocup
RoboCup Rescue: Search and Rescue in Large-Scale Disasters as a Domain for Autonomous Agents Research Disaster rescue is one of the most serious social issue which involves very large numbers of hetergenious agents in the hostile environment. RoboCup-Rescue intends to promote research and development in this socially significant domain by creating a standard simulator and forum for researchers and practitioners. While the rescue domain intuitively appealing as large scale multi-agent domains, it has not yet given through analysis on its domain characteristics. In this paper, we present detailed analysis on the task domain and elucidate characteristics necessary for multi-agent systems for this domain.  1 Introduction  In this paper, we propose RoboCup-Rescue, as a secondary domain for RoboCup activities [ Kitano, et al., 1997]. The aim of RoboCup-Rescue are (1) to ensure smooth transfer of technologies invented through RoboCup activity to a socially significant real world domain, (2) to establish a domain which complements features that are missing in soccer, and (3) to examine funderm...
HCI
leonhardi99virtual
Virtual Information Towers - A Metaphor for Intuitive, Location-Aware Information Access in a Mobile Environment This paper introduces Virtual Information Towers (VITs) as a concept for presenting and accessing location-aware information with mobile clients. A VIT is a means of structuring location-aware information, which is assigned to a certain geographical position while having a certain area of visibility. A user equipped with a mobile, wearable computer has access to the VITs which are "visible" from his/her current location. The architecture and protocols of a system are described, which allows its users to create VITs and to access the information on them using Internet mechanisms. We have implemented a prototype of this system and a VIT client for a wearable computer and will present some aspects of this implementation. 1 Introduction  Mobile information access is an important field of application for wearable computers. In a mobile environment much of the accessed information is location-dependent, i.e. the content of the information or the user's interest in the information depends on ...
HCI
prasad96offline
Off-line Learning of Coordination in Functionally Structured Agents for Distributed Data Processing When we design multi-agent systems for realistic, worth-oriented environments, coordination problems they present involve intricate and sophisticated interplay between the domain and the various system components. Achieving effective coordination in such systems is a difficult problem for a number of reasons like local views of problem-solving task and uncertainty about the outcomes of interacting non-local tasks. In this paper, we present a learning algorithm that endows agents with the capability to choose an appropriate coordination algorithm based on the present problem solving situation in the domain of distributed data processing. 1 Introduction Achieving effective coordination in a multi-agent system is a difficult problem for a number of reasons. The first is that an agent's control decisions, based only on its local view of problem-solving task structures, may lead to inappropriate decisions about which activity it should do next, what results it should transmit to other agen...
ML
datta98case
A Case for Parallelism in Data Warehousing and OLAP In recent years the database community has experienced a tremendous increase in the availability of new technologies to support efficient storage and retrieval of large volumes of data, namely data warehousing and  On-Line Analytical Processing (OLAP) products. Efficient query processing is critical in such an environment, yet achieving quick response times with OLAP queries is still largely an open issue. In this paper we propose a solution approach to this problem by applying  parallel processing techniques to a warehouse environment. We suggest an efficient partitioning strategy based on the relational representation of a data warehouse (i.e., star schema). Furthermore, we incorporate a particular indexing strategy, DataIndexes, to further improve query processing times and parallel resource utilization, and propose a preliminary parallel star-join strategy. 1 Introduction  In recent years, there has been an explosive growth in the use of databases for decision support. This phenome...
DB
100598
Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-ofspeech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a "treebank" corpus; then the grammar is improved by selecting rules with high "benefit" scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal.  1 Introduction  Finding base noun phrases is a sensible first step for many natural language processing (NLP) tasks: Accurate identification of base noun phrases is arguably the most critical comp...
IR
snoek02stateart
A State-of-the-art Review on Multimodal Video Indexing Efficient and effective handling of video documents depends on the availability of indexes. Manual indexing is unfeasible for large video collections. Effective indexing requires a multimodal approach in which either the most appropriate modality is selected or the different modalities are used in collaborative fashion. In this paper we focus on the similarities and differences between the modalities, and survey several methods aiming at automating the time and resource consuming process of video indexing. Furthermore, we put forward a unifying and multimodal framework, which views a video document from the perspective of its author. This framework forms the guiding principle for identifying index types, for which automatic methods are found in literature. It furthermore forms the basis for categorizing these different methods.
IR
zaiane98discovering
Discovering Web Access Patterns and Trends by Applying OLAP and Data Mining Technology on Web Logs As a confluence of data mining and WWW technologies, it is now possible to perform data mining on web log records collected from the Internet web page access history. The behaviour of the web page readers is imprinted in the web server log files. Analyzing and exploring regularities in this behaviour can improve system performance, enhance the quality and delivery of Internet information services to the end user, and identify population of potential customers for electronic commerce. Thus, by observing people using collections of data, data mining can bring considerable contribution to digital library designers. In a joint effort between the TeleLearning-NCE project on Virtual University and NCE-IRIS project on data mining, we have been developing the knowledge discovery tool, WebLogMiner, for mining web server log files. This paper presents the design of the WebLogMiner, reports the current progress, and outlines the future work in this direction.
DB
kosch99managing
Managing the Operator Ordering Problem in Parallel Databases This paper focuses on parallel query optimization. We consider the operator problem and introduce a new class of execution strategies called Linear-Oriented Bushy Trees (LBT). Compared to the related approach of the General Bushy Trees (GBT) a significant complexity reduction of the operator ordering problem can be derived theoretically and demonstrated experimentally (e.g. compared with GBTs, LBTs authorize optimization time improvement that can reach up-to 49%) without loosing quality. Finally we demonstrate that existing commercial parallel query optimizers need little extension modifications in order to handle LBTs.  Key words: Parallel databases, parallel query optimization, linear-oriented bushy  trees, extending existing optimizers.  1 Introduction  Modern database applications, such as data mining and decision support pose several new challenges to query optimization and processing [1]. One of the main issues concerns the processing of complex queries (e.g. recent Teradata rela...
DB
vanthong00speechbot
SpeechBot: a Speech Recognition based Audio Indexing System for the Web We have developed an audio search engine incorporating speech recognition technology. This allows indexing of spoken documents from the World Wide Web when no transcription is available. This site indexes several talk and news radio shows covering a wide range of topics and speaking styles from a selection of public Web sites with multimedia archives. Our Web site is similar in spirit to normal Web search sites; it contains an index, not the actual multimedia content. The audio from these shows suffers in acoustic quality due to bandwidth limitations, coding, compression, and poor acoustic conditions. The shows are typically sampled at 8 kHz and transmitted, RealAudio compressed, at 6.5 kbps. Our word-error rate results using appropriately trained acoustic models show remarkable resilience to the high compression, though many factors combine to increase the average word-error rates over standard broadcast news benchmarks. We show that, even if the transcription is inaccurate, we can st...
IR
362156
Context Awareness by Analysing Accelerometer Data In this paper we describe continuing work being carried out as part of the Bristol Wearable Computing Initiative. We are researching processing techniques for data from accelerometers which enable the wearable computer to determine the user's activity.  We have experimented with, and review, techniques already employed by others; and then propose new methods for analysing the data delivered by these devices. We try to minimise the number of devices needed, and use a single X-Y accelerometer device.  Using our techniques we have adapted our GPS based Tourist Guide wearable Computer application to include a multimedia presentation which gives the user information using different media depending on the user's activity as well as location.  1 Introduction and Background  This is a condensed version of a technical report. [1]  Our interests in wearable computing are centred around determining the context of the user and developing applications which make use of this information. We are expl...
HCI
konda01actorcritic
Actor-Critic Algorithms We propose and analyze a class of actor-critic algorithms for  simulation-based optimization of a Markov decision process over  a parameterized family of randomized stationary policies. These  are two-time-scale algorithms in which the critic uses TD learning  with a linear approximation architecture and the actor is updated  in an approximate gradient direction based on information provided  by the critic. We show that the features for the critic should  span a subspace prescribed by the choice of parameterization of the  actor. We conclude by discussing convergence properties and some  open problems.  1 Introduction  The vast majority of Reinforcement Learning (RL) [9] and Neuro-Dynamic Programming (NDP) [1] methods fall into one of the following two categories:  (a) Actor-only methods work with a parameterized family of policies. The gradient of the performance, with respect to the actor parameters, is directly estimated by simulation, and the parameters are updated in a direction o...
ML
marchionini98interfaces
Interfaces and Tools for the Library of Congress National Digital Library Program This paper describes a collaborative effort to explore user needs in a digital library, develop interface prototypes for a digital library, and suggest and prototype tools for digital librarians and users at the Library of Congress (LC). Interfaces were guided by an assessment of user needs and aimed to maximize interaction with primary resources and support both browsing and analytical search strategies. Tools to aid users and librarians in overviewing collections, previewing objects, and gathering results were created and serve as the beginnings of a digital librarian toolkit. The design process and results are described and suggestions for future work are offered. Digital Libraries (DL) offer new challenges to an emerging breed of digital librarians who must combine the principles and practices of information management with rapidly evolving technological developments to create new information products and services. This paper describes a collaborative effort to explore user needs ...
HCI
loyer99computing
Computing and Comparing Semantics of Programs in Four-valued Logics The different semantics that can be assigned to a logic program correspond to different assumptions made concerning the atoms whose logical values cannot be inferred from the rules. Thus, the well founded semantics corresponds to the assumption that every such atom is false, while the Kripke-Kleene semantics corresponds to the assumption that every such atom is unknown. In this paper, we propose to unify and extend this assumption-based approach by introducing parameterized semantics for logic programs. The parameter holds the value that one assumes for all atoms whose logical values cannot be inferred from the rules. We work within Belnap's four-valued logic, and we consider the class of logic programs defined by Fitting. Following Fitting's approach, we define a simple operator that allows us to compute the parameterized semantics, and to compare and combine semantics obtained for different values of the parameter. The semantics proposed by Fitting corresponds to the value false. We also show that our approach captures and extends the usual semantics of conventional logic programs thereby unifying their computation.
DB
petrak95objectoriented
An Object-Oriented Case-Based Learning System This thesis first gives an overview of the subfield of classification in the area of machine learning. The numerous variants of Case-Based Learning algorithms are compared according to what kind of data is processed, how knowledge and hypotheses are represented, and what kind of reasoning or learning is performed. The strengths and weaknesses of these learning methods are compared to each other and to other groups of learning methods. A modular object-oriented LISP environment, VIE-CBR2 is introduced, that implements a number of algorithms for Case-Based Learning. This system allows to easily combine preprogrammed learning algorithms. and provides a framework for simple integration of new learning algorithms and other components that make use of the basic system.  
ML
borchers00pattern
A Pattern Approach to Interaction Design To create successful interactive systems, user interface designers need to cooperate with developers and application domain experts in an interdisciplinary team. These groups, however, usually miss a common terminology to exchange ideas, opinions, and values. This paper presents an approach that uses pattern languages to capture this knowledge in software development, HCI, and the application domain. A formal, domain-independent definition of design patterns allows for computer support without sacrificing readability, and pattern use is integrated into the usability engineering lifecycle. As an example, experience from building an award-winning interactive music exhibit was turned into a pattern language, which was then used to inform follow-up projects and support HCI education.
HCI
saif01communication
Communication Primitives for Ubiquitous Systems or RPC Considered Harmful RPC is widely used to access and modify remote state. Its procedural call semantics are argued as an efficient unifying paradigm for both local and remote access. Our experience with ubiquitous device control systems has shown otherwise. RPC semantics of a synchronous, blocking invocation on a statically typed interface are overly restrictive, inflexible, and fail to provide an efficient unifying abstraction for accessing and modifying state in ubiquitous systems. This position paper considers other alternatives and proposes the use of comvets (conditional, mobility aware events) as the unifying generic communication paradigm for such systems.  Keywords: RPC, RMI, Events, Comvets, CORBA, Jini  1 Introduction  Ubiquitous environments or active spaces are the next generation of device control networks. A user interacts with an active space by using novel interfaces like speech and gesture input [1] to control her environment, and the system interacts with the user using audio/video outpu...
Agents
478335
Multi-User and Security Support for Multi-Agent Systems This paper discusses the requirements an agent system needs to be secure. In particular, the paper introduces a classification of modern distributed systems, and examines the delegation concept from a security point of view. After discussing the peculiar security and delegation issues present in distributed object systems, mobile agent systems and in multi agent systems, a case study is presented, describing the multi-user and security support that is being built into the JADE platform.
Agents
nambiar01benchmarking
Benchmarking XML Management Systems: The XOO7 Way The effectiveness of existing XML query languages has been studied by many who focused  on the comparison of linguistic features, implicitly reflecting the fact that most XML tools exist  only on paper. In this paper, with a focus on efficiency and concreteness, we propose a pragmatic  first step toward the systematic benchmarking of XML query processing platforms. We begin by  identifying the necessary functionalities an XML data management system should support. We  review existing approaches for managing XML data and the query processing capabilities of these  approaches. We then compare three XML query benchmarks XMach-1, XMark and XOO7 and  discuss the applicability, strengths and limitations of these benchmarks. We highlight the bias of  these benchmarks towards the data centric view of XML and motivate our selection of XOO7 to  extend with document centric queries. We complete XOO7 to capture the information retrieval  capabilities of XML management systems. Finally we summarize our contributions and discuss  future directions.
DB
315017
dQUOB: Managing Large Data Flows Using Dynamic Embedded Queries The dQUOB system satisfies client need for specific information from high-volume data streams. The data streams we speak of are the flow of data existing during large-scale visualizations, video streaming to large numbers of distributed users, and high volume business transactions. We introduces the notion of conceptualizing a data stream as a set of relational database tables so that a scientist can request information with an SQL-like query. Transformation or computation that often needs to be performed on the data en-route can be conceptualized ascomputation performed onconsecutive views of the data, with computation associated with each view. The dQUOB system moves the query code into the data stream as a quoblet; as compiled code. The relational database data model has the significant advantage of presenting opportunities for efficient reoptimizations of queries and sets of queries. Using examples from global atmospheric modeling, we illustrate the usefulness of the dQUOB system. We carry the examples through the experiments to establish the viability of the approach for high performance computing with a baseline benchmark. We define a cost-metric of end-to-end latency that can be used to determine realistic cases where optimization should be applied. Finally, we show that end-to-end latency can be controlled through a probability assigned to a query that a query will evaluate to true.  
DB
301891
A Multi-version Approach to Conflict Resolution in Distributed Groupware Systems Groupware systems are a special class of distributed computing systems which support human-computer-human interaction. Real-time collaborative graphics editors allow a group of users to view and edit the same graphics document at the same time from geographically dispersed sites connected by communication networks. Resolving conflict access to shared objects is one of the core issues in the design of this type of systems. This paper proposes a novel distributed multi-version approach to conflict resolution. This approach aims to preserve the work concurrently produced by multiple users in the face of conflicts, and to minimize the number of object versions for accommodating combined effects of conflicting and compatible operations. Major technical contributions of this work include a formal specification of a unique combined effect for any group of conflicting and compatible operations, a distributed algorithm for incremental creation of multiple object versions, and a consistent objec...
HCI
dunlop00development
Development and Evaluation of Clustering Techniques for Finding People Typically in a large organisation much   expertise and  knowledge  is held informally   within employees' own memories. When  employees leave an organisation many  documented links that go through that person  are broken and no mechanism is usually  available to overcome these broken links. This  matchmaking problem is related to the  problem of finding potential work partners in a  large and distributed organisation. This paper  reports a comparative investigation into using  standard information retrieval techniques to  group employees together based on their web  pages. This information can, hopefully, be  subsequently used to redirect broken links to  people who worked closely with a departed  employee or used to highlight people, say in  different departments, who work on similar  topics. The paper reports the design and  positive results of an experiment conducted at  Ris National Laboratory comparing four  different IR searching and clustering  approaches using real users' we...
IR
292710
Use Case Maps as a Feature Description Notation . We propose Use Case Maps (UCMs) as a notation for describing features. UCMs capture functional requirements in terms of causal scenarios bound to underlying abstract components. This particular view proved very useful in the description of a wide range of reactive and telecommunications systems. This paper presents some of the most interesting constructs and benefits of the notation in relation to a question on a User Requirements Notation recently approved by ITU-T Study Group 10, which will lead to a new Recommendation by 2003. Tool support, current research on UCMs, and related notations are also discussed.  1 Introduction  The modeling of reactive systems requires an early emphasis on behavioral aspects such as interactions between the system and the external world (including the users), on the cause-to-e#ect relationships among these interactions, and on intermediate activities performed by the system. Scenarios are particularly good at representing such aspects so that various ...
DB
klein99computer
Computer Response to User Frustration Use of computer technology often has unpleasant side effects, some of which are strong, negative emotional states that arise in humans during interaction with computers. Frustration, confusion, anger, anxiety and similar emotional states can affect not only the interaction itself, but also productivity, learning, social relationships, and overall well-being. This thesis presents the idea of designing human-computer interaction systems to actively support human users in their ability to regulate, manage, and recover from their own negative emotional states, particularly frustration. This document describes traditional theoretical strategies for emotion regulation, the design of a human-computer interaction agent built by the author to actively help relieve frustration, and an evaluation that shows the effectiveness of the agent. A study designed to test this agent was conducted: A system was built that elicits frustration in human subjects. The interaction agent then initiated several social, emotional-content feedback strategies with some of the subjects, in an effort to help relieve their emotional state. These strategies were designed to provide many of the same cues that skilled, human listeners employ when helping relieve strong, negative emotions in others. Two control groups were exposed to the same frustrating stimuli, one of which was given no emotional support at all; the other enabled subjects to report problems and "vent" at the computer. Subsequent behavior was then observed, and self-report data was collected. Behavioral results showed the agent was significantly more effective than the two controls in helping relieve frustration levels in subjects. These results demonstrate that strategic, social, emotional-content interaction with a computer by users who ...
HCI
faensen01hermes
Hermes - A Notification Service for Digital Libraries The high publication rate of scholarly material makes searching and browsing an inconvenient way to keep oneself up-todate. Instead of being the active part in information access, researchers want to be notified whenever a new paper in one's research area is published.
IR
slaughter00open
Open video: A framework for a test collection This paper provides a framework for such a test collection and describes the Open  Video Project that has begun to develop a test collection based on this framework. The  proposed test collection is meant to be used to study a wide range of problems, such  as tests of algorithms for creating surrogates for video content or interfaces that display  result sets from queries. An important challenge in developing such a collection is storing  and distributing video objects. This paper is meant to layout video management issues  that may influence distributed storage solutions. More specifically, this paper describes  the first phase for creating the test collection, sets guidelines for building the collection,  and serves as a basis for discussion to inform subsequent phases and invite research  community involvement.  2000 Academic Press  1. Introduction  It is inevitable that the technical limitations that impede widespread usage of video libraries will dimi
IR
malec00implementing
Implementing Teams of Agents Playing Simulated Robot Soccer . This article is intended to present an overview of the issues  related to implementing teams of cooperating agents playing simulated  robot soccer. First we introduce the concept of robot soccer and the  simulated environment. Then we discuss why the (simulated) robot soccer  is an interesting application area from the point of view of robotics  and articial intelligence. The main part of the paper contains a discussion  of agent architectures, both from theoretical and practical point  of view. Later we discuss how to combine individual agents into teams  having common strategies and goals. We also discuss learning, both on  individual and team levels.  1 Introduction  Robot soccer is a growing area of interest for the robotics and articial intelligence communities. There are many reasons for that. The main one is the complexity of the domain together with a set of well-dened rules governing the behaviour of the agents in this domain. The domain is suited for experiments both in ...
Agents
384644
Formal Concepts of Learning Systems Validation in Use In the problem area of evaluating complex software  systems, there are two distinguished areas of research,  development, and application identified by the two  buzzwords validation and verification, respectively.  From the perspective adopted by the authors  (cf. (O'Keefe & O'Leary 1993), e.g.), verification is  usually more formally based and, thus, can be supported  by formal reasoning tools like theorem provers,  for instance.  The scope of verification approaches is limited by the  difficulty of finding a sufficiently complete formalization  to built upon. In paramount realistic problem  domains, validation seems to be more appropriate, although  it is less stringent in character and, therefore,  validation results are often less definite.  The aim of this paper is to exemplify a validation approach  based on a clear and thoroughly formal theory.  In this way, validation and verification should be  brought closer to each other, for the benefit of a concerted  action towards depend...
ML
punin01web
Web Usage Mining - Languages and Algorithms We propose two new XML applications, XGMML and LOGML. XGMML is a graph description language and LOGML is a web-log report description language. We generate a web graph in XGMML format for a web site using the web robot of the WWWPal system (developed for web visualization and organization). We generate web-log reports in LOGML format for a web site from web log files and the web graph. In this paper, we further illustrate the usefulness of these two XML applications with a web data mining example. Moreover, we show the simplicity with which this mining algorithm can be specified and implemented efficiently using our two XML applications. We provide sample results, namely frequent patterns of users in a web site, with our web data mining algorithm.
IR
li01minimizing
Minimizing View Sets without Losing Query-Answering Power The problem of answering queries using views has been studied extensively, due to its relevance in a wide variety of data-management applications. In these applications, we often need to select a subset of views to maintain, due to limited resources. In this paper, we show that traditional query containment is not a good basis for deciding whether or not a view should be selected. Instead, we should minimize the view set without losing query-answering power. To formalize this notion, we rst introduce the concept of "p-containment." That is, a view set V is p-contained in another view set W, if W can answer all the queries that can be answered by V. We show that p-containment and the traditional query containment are not related; i.e., one does not imply the other. We then discuss how to minimize a view set while retaining its query-answering power. We develop the idea further by considering p-containment of two view sets with respect to a given set of queries, and consider their relationship in terms of maximally-contained rewritings of queries using the views.  
DB
killijian01towards
Towards Group Communication for Mobile Participants (Extended Abstract) Group communication will undoubtedly be a useful paradigm for many applications of wireless networking in which reliability and timeliness are requirements. Moreover, location awareness is clearly central to mobile applications such as traffic management and smart spaces. In this paper, we introduce our definition of proximity groups in which group membership depends on location and then discuss some requirements for a group membership management service suitable for proximity groups. We describe a novel approach to efficient coverage estimation, giving applications feedback on the proportion of the area of interest covered by a proximity group, and also discuss our approach to partition anticipation.
HCI
mitzenmacher01estimating
Estimating Resemblance of MIDI Documents Abstract. Search engines often employ techniques for determining syntactic similarity of Web pages. Such a tool allows them to avoid returning multiple copies of essentially the same page when a user makes a query. Here we describe our experience extending these techniques to MIDI music files. The music domain requires modification to cope with problems introduced in the musical setting, such as polyphony. Our experience suggests that when used properly these techniques prove useful for determining duplicates and clustering databases in the musical setting as well. 1
IR
piater00distinctive
Distinctive Features Should Be Learned Most existing machine vision systems perform recognition based on a xed set  of hand-crafted features, geometric models, or eigen-subspace decomposition.  Drawing from psychology, neuroscience and intuition, we show that certain  aspects of human performance in visual discrimination cannot be explained by  any of these techniques. We argue that many practical recognition tasks for  articial vision systems operating under uncontrolled conditions critically depend  on incremental learning. Loosely motivated by visuocortical processing,  we present feature representations and learning methods that perform biologically  plausible functions. The paper concludes with experimental results  generated by our method.  1 Introduction  How exible are the representations for visual recognition, encoded by the neurons of the human visual cortex? Are they predetermined by a xed developmental schedule, or does their development depend on their stimulation? Does their development cease at some poin...
ML
232670
Interaction between Path and Type Constraints XML [7], which is emerging as an important standard for data exchange on the World-Wide Web, highlights the importance of semistructured data. Although the XML standard itself does not require any schema or type system, a number of proposals [6, 17, 19] have been developed that roughly correspond to data definition languages. These allow one to constrain the structure of XML data by imposing a schema on it. These and other proposals also advocate the need for integrity constraints, another form of constraints that should, for example, be capable of expressing inclusion constraints and inverse relationships. The latter have recently been studied as path constraints in the context of semistructured data [4, 9]. It is likely that future XML proposals will involve both forms of constraints, and it is therefore appropriate to understand the interaction between them. This paper investigates that interaction. In particular it studies constraint implication problems, which are important both i...
DB
jennings01aspects
Aspects of Network Edge Intelligence Is it the case that the migration of intelligence from the core of networks to the  periphery is simply a function of the IP protocols? Or are there more fundamental  forces at work? This report addresses this issue from the perspective of core network  protocols, mobile networking and the emerging embedded Internet. It identifies the  forces at work, and concludes that the long term trends are driven by more than simply  the end-to-end argument. If the logical progression of this trend is that intelligence  will migrate to the embedded Internet, then a new type of peripheral intelligence may  form the basis for further progress. The report concludes by identifying the challenges  for the embedded Internet in constructed environments.  Keywords: IP networking, intelligence, architecture, smart spaces  Aspects of Network Edge Intelligence  I. 
HCI
92649
Robustness of Case-Initialized Genetic Algorithms We investigate the robustness of Case Initialized Genetic AlgoRithm (CIGAR) systems with respect to problem indexing. When confronted with a series of similar problems CIGAR stores potential solutions in a case-base or an associative memory and retrieves and uses these solutions to help improve a genetic algorithm 's performance over time. Defining similarity among the problems, or indexing, is key to performance improvement. We study four indexing schemes on a class of simple problems and provide empirical evidence of CIGAR's robustness to imperfect indexing.
ML
ciravegna00learning
Learning to Tag for Information Extraction from Text . LearningPINOCCHIO is an algorithm for adaptive information extraction. It learns template filling rules that insert SGML tags into texts. LearningPINOCCHIO is based on a covering algorithm that learns rules by bottom-up generalization of instances in a tagged corpus. It has been tested on three scenarios in informal domains in two languages (Italian and English). Experiments report excellent results with respect to the current state of the art. 1 Introduction By general agreement the main barriers to wide use and commercialization of IE are the difficulties in adapting systems to new applications and domains. In the last years there has been increasing interest in applying machine learning to Information Extraction from text [13, 3, 9, 1, 11]. In particular there is an increasing interest in the application of adaptive IE to Web pages [12, 10] and to informal domains (rental ads, email messages, etc.) [15, 8, 2] for building fully automated systems. This is due from one side to the...
ML
457459
A Natural Interface to a Virtual Environment through Computer Vision-estimated Pointing Gestures . This paper describes the development of a natural interface to a virtual  environment. The interface is through a natural pointing gesture and replaces  pointing devices which are normally used to interact with virtual environments.  The pointing gesture is estimated in 3D using kinematic knowledge of the arm  during pointing and monocular computer vision. The latter is used to extract the  2D position of the user's hand and map it into 3D. Off-line tests of the system  show promising results with an average errors of 76mm when pointing at a screen  2m away. The implementation of a real time system is currently in progress and  is expected to run with 25Hz.  1 
HCI
hannebauer99rapid
Rapid Concurrent Software Engineering in Competitive Situations This article is an experience report on the evolutionary development process of AT Humboldt, a multi agent system which has become World Champion 1997 and Vice World Champion 1998 of RoboCup simulator league. It details why the artifical soccer initiative RoboCup is a tempting domain for rapid concurrent software engineering. Both the development processes in 1997 and 1998 are described, compared and evaluated. Lessons learned for development projects in distributed control conclude this report. 1 Introduction  In this article the project managers describe the evolutionary development process of the software project AT (AgentTeam) Humboldt, which has become World Champion 1997 and Vice World Champion 1998 in the simulator league of the artifical soccer contest RoboCup ([10]). The RoboCup initiative recently gets more and more popular among scientists in robotics, distributed systems and distributed artificial intelligence because of its strong competitive character and tight resource b...
Agents
bredin99economic
Economic Markets as a Means of Open Mobile-Agent Systems Mobile-agent systems have gained popularity in use because they ease the application design process by giving software engineers greater flexibility. Although the value of any network is dependent on both the number of users and the number of sites participating in the network, there is little motivation for systems to donate resources to arbitrary agents. We propose to remedy the problem by imposing an economic market on mobile-agent systems where agents purchase resources from host sites and sell services to users and other agents. Host sites accumulate revenues, which are distributed to users to be used to launch more agents. We argue for the use of markets to regulate mobile-agent systems and discuss open issues in implementing market-based mobile-agent systems. 1 Introduction  One of the more recent items in a network programmer's tool box is code mobility. The technique is becoming more common in applications programming, network management [BPW98], video conferencing [BPR98], so...
Agents
oliver98graphical
Graphical Models for Recognizing Human Interactions We describe a real-time computer vision and machine learning system for modeling and recognizing human actions and interactions. Two different domains are explored: recognition of two-handed motions in the martial art 'Tai Chi', and multiple-person interactions in a visual surveillance task. Our system combines top-down with bottom-up information using a feedback loop, and is formulated with a Bayesian framework. Two different graphical models (HMMs and Coupled HMMs) are used for modeling both individual actions and multiple-agent interactions, and CHMMs are shown to work more efficiently and accurately for a given amount of training. Finally, to overcome the limited amounts of training data, we demonstrate that `synthetic agents' (Alife-style agents) can be used to develop flexible prior models of the person-to-person interactions.   1 INTRODUCTION  We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in two different scenari...
Agents
509627
Adaptive Load Sharing for Network Processors A novel scheme for processing packets in a router is presented, which provides for load sharing among multiple network processors distributed within the router. It is complemented by a feedback control mechanism designed to prevent processor overload. Incoming traffic is scheduled to multiple processors based on a deterministic mapping. The mapping formula is derived from the robust hash routing (also known as the highest random weight - HRW) scheme, introduced in K.W. Ross, IEEE Network, 11(6), 1997, and D.G. Thaler et al., IEEE Trans. Networking, 6(1), 1998. No state information on individual flow mapping needs to be stored, but for each packet, a mapping function is computed over an identifier vector, a predefined set of fields in the packet. An adaptive extension to the HRW scheme is provided in order to cope with biased traffic patterns. We prove that our adaptation possesses the minimal disruption property with respect to the mapping and exploit that property in order to minimize the probability of flow reordering. Simulation results indicate that the scheme achieves significant improvements in processor utilization. A higher number of router interfaces can thus be supported with the same amount of processing power. I.
IR
chan98approximate
Approximate Nearest Neighbor Queries Revisited This paper proposes new methods to answer approximate nearest neighbor queries on a set of n points in d-dimensional Euclidean space. For any fixed constant d, a data structure with  O("  (1\Gammad)=2  n log n) preprocessing time and O("  (1\Gammad)=2  log n) query time achieves approximation factor 1 + " for any given 0 ! " ! 1; a variant reduces the "-dependence by a factor of "  \Gamma1=2  . For any arbitrary d, a data structure with O(d  2  n log n) preprocessing time and O(d  2  log n) query time achieves approximation factor O(d  3=2  ). Applications to various proximity problems are discussed. 1 Introduction  Let P be a set of n point sites in d-dimensional space IR  d  . In the well-known post office problem, we want to preprocess P into a data structure so that a site closest to a given query point q (called the  nearest neighbor of q) can be found efficiently. Distances are measured under the Euclidean metric. The post office problem has many applications within computational...
ML
138451
A Geometric Framework for Specifying Spatiotemporal Objects We present a framework for specifying spatiotemporal objects using spatial and temporal objects, and a geometric transformation. We define a number of classes of spatiotemporal objects and study their closure properties.  1 Introduction  Many natural or man-made phenomena have both a spatial and a temporal extent. Consider for example, a forest fire or property histories in a city. To store information about such phenomena in a database one needs appropriate data modeling constructs. We claim that a new concept, spatiotemporal object, is necessary. In this paper, we introduce a very general framework for specifying spatiotemporal objects. To define a spatiotemporal object we need a spatial object, a temporal object, and a continuous geometric transformation (specified using a parametric representation) that determines the image of the spatial object at different time instants belonging to the temporal object. In this framework, a number of classes of spatiotemporal objects arise quite ...
DB
benetti01sidesigner
SI-Designer: a tool for intelligent integration of information SI-Designer (Source Integrator Designer) is a designer support tool for semi � automatic integration of heterogeneous sources schemata (relational, object and semi � structured sources); it has been implemented within the MOMIS project and it carries out integration following a semantic approach which uses intelligent Description Logics-based techniques, clustering techniques and an extended ODMG-ODL language, �������� � , to represent schemata, extracted, integrated information. Starting from the sources ’ �������� � descriptions (local schemata) SI-Designer supports the designer in the creation of an integrated view of all the sources (global schema) which is expressed in the same �������� � language. We propose SI-Designer as a tool to build virtual catalogs in the E-Commerce environment. 1.
DB
324100
Information-Theoretic Learning This chapter seeks to extend the ubiquitous mean-square error criterion (MSE) to cost functions that include more information about the training data. Since the learning process ultimately should transfer the information carried in the data samples onto the system's parameters, a natural goal is to find cost functions that directly manipulate information. Hence the name informationtheoretic learning (ITL). In order to be useful, ITL should be independent of the learning machine architecture, and require solely the availability of the data, i.e. it should not require a priori assumptions about the data distributions. The chapter presents our current efforts to develop ITL criteria based on the integration of nonparametric density estimators with Renyi's quadratic entropy definition. As a motivation we start with an application of the MSE to manipulate information using the nonlinear characteristics of the learning machine. This section illustrates the issues faced when we attempt to use...
IR
wu99firstorder
First-Order Polynomial Based Theorem Proving Introduction  The Boolean ring or first-order polynomial based theorem proving began with the work of Hsiang (1982, 1985). Hsiang extended the idea of using Boolean polynomials to represent propositional formulae to the case of first-order predicate calculus. Based on the completion procedure of Knuth and Bendix (1970), the N-strategy was proposed. Later on, by imitating the framework of Buchberger 's algorithm to compute the Grobner bases of polynomial ideals (Buchberger 1985), Kapur and Narendran (1985) developed another approach which is also referred to as the Grobner basis method.  One obvious advantage of using Boolean polynomials is that every propositional formula has a unique representation, and sometimes it is easy to be generalized to some non-classical logic systems (Chazarain et al. 1991; Wu and Tan 1994). Stimulated by them, some approaches and results have been reported (Bachmair and Dershowitz 1987; Dietrich 1986; Kapur and Zhang 1989; Wu and Liu 1998; Zhang 198
DB
turney00learning
Learning Algorithms for Keyphrase Extraction Many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article. Since these key words are often phrases of two or more words, we prefer to call them keyphrases. There is a wide variety of tasks for which keyphrases are useful, as we discuss in this paper. We approach the problem of automatically extracting keyphrases from text as a supervised learning task. We treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Our first set of experiments applies the C4.5 decision tree induction algorithm to this learning task. We evaluate the performance of nine different configurations of C4.5. The second set of experiments applies the GenEx algorithm to the task. We developed the GenEx algorithm specifically for automatically extracting keyphrases from text. The experimental results support the claim that a custom-designed algorithm (GenEx)...
IR
sawhney99nomadic
Nomadic Radio: Scaleable and Contextual Notification for Wearable Audio Messaging Mobile workers need seamless access to communication and information services on portable devices. However current solutions overwhelm users with intrusive and ambiguous notifications. In this paper, we describe scaleable auditory techniques and a contextual notification model for providing timely information, while minimizing  interruptions. User's actions influence local adaptation in the model. These techniques are demonstrated in Nomadic Radio, an audio-only wearable computing platform. 
HCI
369356
Object-Based Multimedia Content Description Schemes and Applications for MPEG-7 In this paper, we describe description schemes (DSs) for image, video, multimedia, home media, and archive content proposed to the MPEG-7 standard. MPEG-7 aims to create a multimedia content description standard in order to facilitate various multimedia searching and filtering applications. During the design process, special care was taken to provide simple but powerful structures that represent generic multimedia data. We use the eXtensible Markup Language (XML) to illustrate and exemplify the proposed DSs because of its interoperability and flexibility advantages. The main components of the image, video, and multimedia description schemes are object, feature classification, object hierarchy, entity-relation graph, code downloading, multi-abstraction levels, and modality transcoding. The home media description instantiates the former DSs proposing the 6-W semantic features for objects, and 1-P physical and 6-W semantic object hierarchies. The archive description scheme aims to describ...
IR
443403
Error-Tolerant Agents . The use of agents in today's Internet world is expanding rapidly. Yet, agent developers
Agents
286829
Approximate Query Translation across Heterogeneous Information Sources (Extended Version) In this paper we present a mechanism for  approximately translating Boolean query constraints  across heterogeneous information sources.  Achieving the best translation is challenging because  sources support different constraints for formulating  queries, and often these constraints cannot  be precisely translated. For instance, a query  [score ? 8] might be "perfectly" translated as  [rating ? 0.8] at some site, but can only be approximated  as [grade = A] at another. Unlike  other work, our general framework adopts a customizable  "closeness" metric for the translation  that combines both precision and recall. Our results  show that for query translation we need to  handle interdependencies among both query conjuncts  as well as disjuncts. As the basis, we identify  the essential requirements of a rule system for  users to encode the mappings for atomic semantic  units. Our algorithm then translates complex  queries by rewriting them in terms of the semantic  units. We show that, un...
DB
roobaert00directsvm
DirectSVM: A Fast And Simple Support Vector Machine Perceptron .  We propose a simple implementation of the Support Vector Machine (SVM) for pattern recognition, that is not based on solving a complex quadratic optimization problem. Instead we propose a simple, iterative algorithm that is based on a few simple heuristics. The proposed algorithm nds high-quality solutions in a fast and intuitively-simple way. In experiments on the COIL database, on the extended COIL database and on the Sonar database of the UCI Irvine repository, DirectSVM is able to nd solutions that are similar to these found by the original SVM. However DirectSVM is able to nd these solutions substantially faster, while requiring less computational resources than the original SVM.  INTRODUCTION  Support Vector Machines (SVMs) belong to the best-performing learning algorithms available. They have produced remarkable performance in a number of dicult learning tasks without requiring prior knowledge. We mention amongst others the following examples in pattern recognition: handwr...
ML
119642
Bias, Variance, and Error Correcting Output Codes for Local Learners : This paper focuses on a bias variance decomposition analysis of a local learning algorithm, the nearest neighbor classifier, that has been extended with error correcting output codes. This extended algorithm often considerably reduces the 0-1 (i.e., classification) error in comparison with nearest neighbor (Ricci & Aha, 1997). The analysis presented here reveals that this performance improvement is obtained by drastically reducing bias at the cost of increasing variance. We also show that, even in classification problems with few classes (m5), extending the codeword length beyond the limit that assures column separation yields an error reduction. This error reduction is not only in the variance, which is due to the voting mechanism used for error-correcting output codes, but also in the bias. Keywords: Case-based learning, classification, error-correcting output codes, bias and variance Email: ricci@irst.itc.it, aha@aic.nrl.navy.mil Phone: ++39 461 314334 FAX: ++39 461 302040 Bi...
ML
457569
Schema Evolution in Heterogeneous Database Architectures, A Schema Transformation Approach In previous work we have a developed general framework to support schema transformation and integration in heterogeneous database architectures. The framework consists of a hypergraph-based common data model and a set of primitive schema transformations defined for this model. Higher-level common data models and primitive schema transformations for them can be defined in terms of this lower-level model. A key feature of the framework is that both primitive and composite schema transformations are automatically reversible. We have shown in earlier work how this allows automatic query translation from a global schema to a set of source schemas. In this paper we show how our framework also readily supports  evolution of source schemas, allowing the global schema and the query translation pathways to be easily repaired, as opposed to having to be regenerated, after changes to source schemas. 1 
DB
neaves98dynamic
Dynamic Connection of Wearable Computers to Companion Devices Using Near-Field Radio Hewlett-Packard Laboratories, Bristol and the University of Bristol Department of Computer Science are engaged in an initiative to explore the design, technology and use of wearable computers.  We describe a way of connecting a wearable computer to companion devices such as displays or cameras using near-field radio technology. The shortrange nature of near-field radio allows relatively high data rates (300 kbps -- 1Mbit), low power consumption and the interpretation of gestures as configuration requests.  Keywords: Near-field radio, dynamic connectivity.  INTRODUCTION  We are particularly interested in communication technologies that exhibit low-power, short range (up to 1 foot) and modest data rates (300 kbps -- 1 Mbs). The action of picking up a companion device (such as a display) establishes the communication link due to the very short range. An important aspect of a suitable communication technology is that the user is not required to touch an electrode and therefore handling of ...
HCI
147739
SIC: Satisfiability Checking for Integrity Constraints SIC is an interactive prototype to assist in the design of finitely satisfiable integrity constraints. Thus SIC addresses the constraint satisfiability problem during the schema design phase of a database. SIC combines two systems, a reasoning component and an interactive visual interface. This paper outlines the functionality of both components and the theoretical background and implementation aspects of the reasoning component.  
DB
stocky02conveying
Conveying Routes: Multimodal Generation and Spatial Intelligence In Embodied Conversational Agents In creating an embodied conversational agent (ECA) capable of conveying routes, it is necessary to understand how to present spatial information in an effective and natural manner. When conveying routes to someone, a person uses multiple modalities  e.g., speech, gestures, and reference to a map  to present information, and it is important to know precisely how these modalities are coordinated. With an understanding of how humans present spatial intelligence to give directions, it is then possible to create an ECA with similar capabilities. Two empirical studies were carried out to observe natural human-to-human direction-giving interactions. From the results, a direction-giving model was created, and then implemented in the MACK (Media Lab Autonomous Conversational Kiosk) system.
HCI
paradiso99cybershoe
The CyberShoe: A Wireless Multisensor Interface for a Dancer's Feet : As a bridge between our interest in Wearable Computer systems and new performance interfaces for digital music, we have built a highly instrumented pair of sneakers for interactive dance. These shoes each measure 16 different, continuous parameters expressed by each foot and are able to transmit them wirelessly to a base station placed well over 30 meters away, updating all values up to 60 times per second. This paper describes our system, illustrates its performance, and outlines a few musical mappings that we have created for demonstrations in computer-augmented dance.  ____________________________________  Electronic sensors have been incorporated into footwear for several different applications over the last several years. Employing force-sensing resistor arrays or pixelated capacitive sensing, insoles with very dense pressure sampling have been developed for research at the laboratories of footwear manufacturers and pediatric treatment facilities (Cavanaugh, et. al., 1992). Alth...
HCI
nodine98overview
An Overview of Active Information Gathering in InfoSleuth InfoSleuth is a system of collaborating software agents that can be configured to perform many different information management activities in a distributed environment. InfoSleuth agents advertise semantic constraints about themselves to InfoSleuth brokers using a global domain ontology. When queried, a broker reasons over these constraints to determine the minimal set of agents that can provide a solution to the query. InfoSleuth's architecture is based on a generic agent shell that provides basic agent communication behaviors over a subset of Knowledge Query Manipulation Language. Individual agents are subclasses of this generic shell that provide specific kinds of functionality. InfoSleuth agents perform a number of complex query activities that require resolving ontology-based queries over dynamically changing, distributed, heterogeneous resources, including distributed query, location-independent single-resource updates, event monitoring by means of subscription/notification servi...
DB
110962
Logical Transactions and Serializability The concept of logic databases can serve as a clear and expressive foundation of various information systems. However, the classical logic language only refers to a single database state, although in modern information systems it is necessary to take the dynamics into account. For this purpose, several update languages were proposed, among them [1, 3, 4, 8, 9, 11], which allow to specify complex transitions from one database state to another. From the evaluation point of view, a complex state transition can and should be considered as a transaction. Up to now, the isolation property of transactions has been poorly addressed in the context of logic update languages, although it is an important problem even for classical, sequential transactions (see [2] for instance). In this paper we investigate how the serializability of logical transactions can be supported and what this means for the implementation of a transaction manager. 1 Introduction and Motivation  In [11, 12] we propose an up...
DB
agarwal01time
Time Responsive Indexing Schemes for Moving Points We develop new indexing schemes for storing a set of points in one or two dimensions, each  moving along a linear trajectory, so that a range query at a given future time t q can be answered  efficiently. The novel feature of our indexing schemes is that the number of I/Os required to  answer a query depends not only on the size of the data set and on the number of points in the  answer but also on the difference between t q and the current time; queries close to the current  time are answered fast, while queries that are far away in the future or in the past may take  more time.  Center for Geometric Computing, Department of Computer Science, Duke University, Durham, NC 27708, USA. Supported in part by Army Research Office MURI grant DAAH04-96-1-0013, by a Sloan fellowship, by NSF grants EIA--9870724, EIA--997287, and CCR--9732787, and by grant from the U.S.-Israeli Binational Science Foundation. Email: pankaj@cs.duke.edu.  y  Center for Geometric Computing, Department of Computer Sci...
DB
31326
Inference and Learning in Hybrid Bayesian Networks We survey the literature on methods for inference and learning in Bayesian Networks composed of discrete and continuous nodes, in which the continuous nodes have a multivariate Gaussian distribution, whose mean and variance depends on the values of the discrete nodes. We also briefly consider hybrid Dynamic Bayesian Networks, an extension of switching Kalman filters. This report is meant to summarize what is known at a sufficient level of detail to enable someone to implement the algorithms, but without dwelling on formalities.  1 1 Introduction  We discuss Bayesian networks (BNs [Jen96]) in which each node is either discrete or continuous, scalar or vector-valued, and in which the joint distribution over all the nodes is Conditional Gaussian (CG) [LW89, Lau92] i.e., for each instantiation i of the discrete nodes Y, the distribution over the continuous nodes X has the form f(xjY = i) = N (x; ~¯(i); \Sigma(i)), where N () represents a multivariate Gaussian (MVG) or Normal density. (Note...
ML
539227
EQUIP: a Software Platform for Distributed Interactive Systems EQUIP is a new software platform designed and engineered to support the development and deployment of distributed interactive systems, such as mixed reality user interfaces that combine distributed input and output devices to create a coordinated experience. EQUIP emphasises: cross-language development (currently C++ and Java), modularisation, extensibility, interactive performance, and heterogeneity of devices (from handheld devices to large servers and visualisation machines) and networks (including both wired and wireless technologies). A key element of EQUIP is its shared data service, which combines ideas from tuplespaces, general event systems and collaborative virtual environments. This data service provides a uniquely balanced treatment of state and event-based communication. It also supports distributed computation -- through remote class loading -- as well as passive data distribution. EQUIP has already been used in several projects within the EQUATOR Interdisciplinary Research Collaboration (IRC) in the UK, and is freely available in source form (currently known to work on Windows, IRIX and MacOS-X platforms).
HCI
29319
Optimising Propositional Modal Satisfiability for Description Logic Subsumption . Effective optimisation techniques can make a dramatic difference in the performance of knowledge representation systems based on expressive description logics. Because of the correspondence between description logics and propositional modal logic many of these techniques carry over into propositional modal logic satisfiability checking. Currently-implemented representation systems that employ these techniques, such as FaCT and DLP, make effective satisfiable checkers for various propositional modal logics. 1 Introduction  Description logics are a logical formalism for the representation of knowledge about individuals and descriptions of individuals. Description logics represent and reason with descriptions similar to "all people whose friends are both doctors and lawyers" or "all people whose children are doctors or lawyers or who have a child who has a spouse". The computations performed by systems that implement description logics are based around determining whether one descriptio...
AI
triesch98gesture
A Gesture Interface for Human-Robot-Interaction We present a person-independent gesture interface implemented on a real robot which allows the user to give simple commands ,e.g., how to grasp an object and where to put it. The gesture analysis relies on realtime tracking of the user's hand and a re\Thetaned analysis of the hand's shape in the presence of varying complex backgrounds. 1. Introduction  Robots of the future will interact with humans in a natural way. They will understand spoken and gestural commands and will articulate themselves by speech and gesture. We are especially interested in gestural interfaces for robots operating in uncontrolled real world environments. This imposes several constraints on human-robot-interaction as a special case of human-computer-interaction:  1. The robot visual system must cope with variable and possibly complex backgrounds. A system requiring uniform background is not exible enough for real world applications. 2. The system must be person independent. Many users should be able to operate ...
AI
galhardas01improving
Improving Data Cleaning Quality using a Data Lineage Facility The problem of data cleaning, which consists of  removing inconsistencies and errors from original  data sets, is well known in the area of decision  support systems and data warehouses. However,  for some applications, existing ETL (Extraction  Transformation Loading) and data cleaning  tools for writing data cleaning programs are insufficient.  One important challenge with them is the  design of a data flow graph that effectively generates  clean data. A generalized difficulty is the lack  of explanation of cleaning results and user interaction  facilities to tune a data cleaning program.  This paper presents a solution to handle this problem  by enabling users to express user interactions  declaratively and tune data cleaning programs.  1 
DB
330661
Maude: Specification and Programming in Rewriting Logic Maude is a high-level language and a high-performance system supporting executable specification and declarative programming in rewriting logic. Since rewriting logic contains equational logic, Maude also supports equational specification and programming in its sublanguage of functional modules and theories. The underlying equational logic chosen for Maude is membership equational logic, that has sorts, subsorts, operator overloading, and partiality definable by membership and equality conditions. Rewriting logic is reflective, in the sense of being able to express its own metalevel at the object level. Reflection is systematically exploited in Maude endowing the language with powerful metaprogramming capabilities, including both user-definable module operations and declarative strategies to guide the deduction process. This paper explains and illustrates with examples the main concepts of Maude's language design, including its underlying logic, functional, system and object-oriented modules, as well as parameterized modules, theories, and views. We also explain how Maude supports reflection, metaprogramming and internal strategies. The paper outlines the principles underlying the Maude system implementation, including its semicompilation techniques. We conclude with some remarks about applications, work on a formal environment for Maude, and a mobile language extension of Maude.
DB
poupyrev98virtual
Virtual Notepad: Handwriting in Immersive VR We present Virtual Notepad, a collection of interface tools that allows the user to take notes, annotate documents and input text using a pen, while still immersed in virtual environments (VEs). Using a spatially-tracked, pressure-sensitive graphics tablet, pen and handwriting recognition software, Virtual Notepad explores handwriting as a new modality for interaction in immersive VEs. This paper reports details of the Virtual Notepad interface and interaction techniques, discusses implementation and design issues, reports the results of initial evaluation and overviews possible applications of virtual handwriting.  1. Introduction  Writing is a ubiquitous everyday activity. We jot down ideas and memos, scribble comments in the margins of a book or an article, annotate blueprints and design plans. Using computers, we type documents, complete forms and enter database queries. However, writing, taking notes or entering text in immersive VEs is almost impossible. Cut off from conventional...
DB
dzeroski98detecting
Detecting traffic problems with ILP Expert systems for decision support have recently been successfully introduced in road transport management. These systems include knowledge on traffic problem detection and alleviation. The paper describes experiments in automated acquisition of knowledge on traffic problem detection. The task is to detect road sections where a problem has occured (critical sections) from sensor data. It is necessary to use inductive logic programming (ILP) for this purpose as relational background knowledge on the road network is essential. In this paper, we apply three state-of-the art ILP systems to learn how to detect traffic problems and compare their performance to the performance of a propositional learning system on the same problem. 1 Introduction Expert systems for decision support have recently been successfully introduced in road transport management. Some of the proposals in this direction are TRYS [5], KITS [4] and ARTIST [9]. From a general perspective, the goal of a real time traffi...
DB
uhrmacher00modeling
Modeling And Simulation Of Mobile Agents Agent-oriented software implies the realization of software components, which are mobile, autonomous, and solve problems by creating new software components during run-time, moving between locations, initiating or joining groups of other software components. Modeling and simulating those multiagent systems requires specific mechanisms for variable structure modeling.  JAMES, a Java-Based Agent Modeling Environment for Simulation, realizes variable structure models including mobility from the perspective of single autonomous agents. JAMES itself is based on parallel DEVS and adopts its abstract simulator model. Simulation takes place as a sending of messages between concurrently active and locally distributed entities which reflect the model's current structure. Thus, modeling and simulation are coined equally by an agent-based perspective.  1 Introduction  The definition of agents subsumes a multitude of different facets [30]. Agents are reactive, deliberative or combine reactive with ...
Agents
513722
Parameterized Logic Programs where Computing Meets Learning Abstract. In this paper, we describe recent attempts to incorporate learning into logic programs as a step toward adaptive software that can learn from an environment. Although there are a variety of types of learning, we focus on parameter learning of logic programs, one for statistical learning by the EM algorithm and the other for reinforcement learning by learning automatons. Both attempts are not full- edged yet, but in the former case, thanks to the general framework and an e cient EM learning algorithm combined with a tabulated search, we have obtained very promising results that open up the prospect of modeling complex symbolic-statistical phenomena. 1
ML
25809
Object-relational Queries into Multidimensional Databases with the Active Data Repository As computational power and storage capacity increase, processing and analyzing large volumes  of multi-dimensional datasets play an increasingly important role in many domains of scientific research. Scientific applications that make use of very large scientific datasets have several  important characteristics: datasets consist of complex data and are usually multi-dimensional;  applications usually retrieve a subset of all the data available in the dataset; various application-specific operations are performed on the data items retrieved. Such applications can be supported  by object-relational database management systems (OR-DBMSs). In addition to providing  functionality to define new complex datatypes and user-defined functions, an OR-DBMS  for scientific datasets should contain runtime support that will provide optimized storage for  very large datasets and an execution environment for user-defined functions involving expensive  operations. In this paper we describe an infrastructure, t...
DB
hustadt00verification
Verification within the KARO Agent Theory Abstract. This paper discusses automated reasoning in the KARO framework. The KARO framework accommodates a range of expressive modal logics for describing the behaviour of intelligent agents. We concentrate on a core logic within this framework, in particular, we describe two new methods for providing proof methods for this core logic, discuss some of the problems we have encountered in their design, and present an extended example of the use of the KARO framework and the two proof methods. 1
Agents
537667
Least Squares Conformal Maps for Automatic Texture Atlas Generation A Texture Atlas is an efficient color representation for 3D Paint Systems. The model to be textured is decomposed into charts homeomorphic to discs, each chart is parameterized, and the unfolded charts are packed in texture space. Existing texture atlas methods for triangulated surfaces suffer from several limitations, requiring them to generate a large number of small charts with simple borders. The discontinuities between the charts cause artifacts, and make it difficult to paint large areas with regular patterns.
AI
dicaro98antnet
AntNet: Distributed Stigmergetic Control for Communications Networks This paper introduces AntNet, a novel approach to the adaptive learning of routing tables in communications networks. AntNet is a distributed, mobile agents based Monte Carlo system that was inspired by recent work on the ant colony metaphor for solving optimization problems. AntNet's agents concurrently explore the network and exchange collected information. The communication among the agents is indirect and asynchronous, mediated by the network itself. This form of communication is typical of social insects and is called stigmergy. We compare our algorithm with six state-of-the-art routing algorithms coming from the telecommunications and machine learning elds. The algorithms' performance is evaluated over a set of realistic testbeds. We run many experiments over real and arti cial IP datagram networks with increasing number of nodes and under several paradigmatic spatial and temporal tra c distributions. Results are very encouraging. AntNet showed superior performance under all the experimental conditions with respect to its competitors. We analyze the main characteristics of the algorithm and try to explain the reasons for its superiority. 1.
ML
rauber01automatically
Automatically Analyzing and Organizing Music Archives . We are experiencing a tremendous increase in the amount of  music being made available in digital form. With the creation of large multimedia  collections, however, we need to devise ways to make those collections  accessible to the users. While music repositories exist today, they mostly  limit access to their content to query-based retrieval of their items based on  textual meta-information, with some advanced systems supporting acoustic  queries. What we would like to have additionally, is a way to facilitate exploration  of musical libraries. We thus need to automatically organize music  according to its sound characteristics in such a way that we nd similar  pieces of music grouped together, allowing us to nd a classical section, or  a hard-rock section etc. in a music repository.  In this paper we present an approach to obtain such an organization of  music data based on an extension to our SOMLib digital library system  for text documents. Particularly, we employ the Self-Organizing Map to  create a map of a musical archive, where pieces of music with similar sound  characteristics are organized next to each other on the two-dimensional map  display. Locating a piece of music on the map then leaves you with related  music next to it, allowing intuitive exploration of a music archive.  Keywords: Multimedia, Music Library, Self-Organizing Map (SOM), Exploration  of Information Spaces, User Interface, MP3  1 
IR
yang00improving
Improving Text Categorization Methods for Event Tracking Automated tracking of events from chronologically ordered document streams is a new challenge for statistical text classification. Existing learning techniques must be adapted or improved in order to effectively handle difficult situations where the number of positive training instances per event is extremely small, the majority of training documents are unlabelled, and most of the events have a short duration in time. We adapted several supervised text categorization methods, specifically several new variants of the k-Nearest Neighbor (kNN) algorithm and a Rocchio approach, to track events. All of these methods showed significant improvement (up to 71% reduction in weighted error rates) over the performance of the original kNN algorithm on TDT benchmark collections, making kNN among the top-performing systems in the recent TDT3 official evaluation. Furthermore, by combining these methods, we significantly reduced the variance in performance of our event tracking system over different ...
IR
32409
The Well-founded Semantics Is the Principle of Inductive Definition . Existing formalisations of (transfinite) inductive definitions in constructive mathematics are reviewed and strong correspondences with LP under least model and perfect model semantics become apparent. I point to fundamental restrictions of these existing formalisations and argue that the well-founded semantics (wfs) overcomes these problems and hence, provides a superior formalisation of the principle of inductive definition. The contribution of this study for LP is that it (re- )introduces the knowledge theoretic interpretation of LP as a logic for representing definitional knowledge. I point to fundamental differences between this knowledge theoretic interpretation of LP and the more commonly known interpretations of LP as default theories or auto-epistemic theories. The relevance is that differences in knowledge theoretic interpretation have strong impact on knowledge representation methodology and on extensions of the LP formalism, for example for representing uncertainty. Keywo...
AI
171304
An Empirical Comparison of Decision Trees and Other Classification Methods Twenty two decision tree, nine statistical, and two neural network classifiers are compared on thirtytwo datasets in terms of classification error rate, computational time, and (in the case of trees) number of terminal nodes. It is found that the average error rates for a majority of the classifiers are not statistically significant but the computational times of the classifiers differ over a wide range. The statistical POLYCLASS classifier based on a logistic regression spline algorithm has the lowest average error rate. However, it is also one of the most computationally intensive. The classifier based on standard polytomous logistic regression and a decision tree classifier using the QUEST algorithm with linear splits have the second lowest average error rates and are about 50 times faster than POLYCLASS. Among decision tree classifiers with univariate splits, the classifiers based on the C4.5, IND-CART, and QUEST algorithms have the best combination of error rate and speed, althoug...
ML
530008
A Framework for Knowledge Management on the Semantic Web The Semantic Web can be a very promising platform for developing knowledge management systems. However, the problem is how to represent knowledge in the machine-understandable form, so that relevant knowledge can be found by machine agents. In this paper we present a knowledge management approach based on RDF-compatible format for representing rules and on a novel technique for the annotation of knowledge sources by using conditional statements. The approach is based on our existing Semantic Web tools. The main benefit is high improvement in the precision by searching for knowledge, as well as the possibility to retrieve a composition of knowledge sources which are relevant for the problem solving.
IR
schiele99attentional
Attentional Objects for Visual Context Understanding This paper exploits wearable computers' unique opportunity to record and index the visual environment of the user from the "first-person" perspective. We propose to use a hat-mounted wearable camera to record what the user sees during the day with a wearable computer. This camera can be used to make the computer more contextually aware of the user and their actions. Furthermore, the camera can be used to record, analyze and index the visual environment of the user. By keeping track of the actions of the user upon and within the environment the system can be more aware of the interactions of the user within the environment. An important aspect of the system is to automatically extract objects of user interest, and their motion within the environment and relative to the user.  1 Introduction  Wearable computers have the potential to "see" as the user sees, "hear" as the user hears, and experience the life and the environment of the user in a "first-person" sense. As has been pointed out ...
HCI
445758
Extending a Multi-Agent System for Genomic Annotation . The explosive growth in genomic (and soon, expression and proteomic)  data, exemplified by the Human Genome Project, is a fertile domain for the application  of multi-agent information gathering technologies. Furthermore, hundreds  of smaller-profile, yet still economically important organisms are being studied  that require the efficient and inexpensive automated analysis tools that multiagent  approaches can provide. In this paper we give a progress report on the use  of the DECAF multi-agent toolkit to build reusable information gathering systems  for bioinformatics. We will briefly summarize why bioinformatics is a classic  application for information gathering, how DECAF supports it, and recent  extensions underway to support new analysis paths for genomic information.  1 
Agents
wolski98fuzzy
Fuzzy Triggers: Incorporating Imprecise Reasoning into Active Databases Traditional Event-Condition-Action triggers (active database rules) include a Boolean predicate as a trigger condition. We propose fuzzy triggers whereby fuzzy inference is utilized in the condition evaluation. This way, approximate reasoning may be integrated with a traditional crisp database. The new approach paves the way for intuitive expression of application semantics of imprecise nature, in database-bound applications. Two fuzzy triggers models are proposed. Firstly, a set of fuzzy rules is encapsulated into a Boolean-valued function called a rule set function, leading to the C-fuzzy trigger model. Subsequently, actions are expressed also in fuzzy terms, and the corresponding CA-fuzzy trigger model is proposed. Examples are provided to illustrate how fuzzy triggers can be applied to a real-life drive control system in an industrial installation.  1 . Introduction  There has been considerable interest in active database rules (called triggers in commercial applications, and in th...
ML
chen00websail
WebSail: From On-line Learning to Web Search In this paper we investigate the applicability of on-line learning algorithms to the real-world problem of web search. Consider that web documents are indexed using n Boolean features. We first present a practically efficient on-line learning algorithm TW2 to search for web documents represented by a disjunction of at most k relevant features. We then design and implement WebSail, a real-time adaptive web search learner, with TW2 as its learning component. WebSail learns from the user's relevance feedback in real-time and helps the user to search for the desired web documents. The architecture and performance of WebSail are also discussed.
IR
pinheirodasilva00user
User Interface Modelling with UML The Unified Modeling Language (UML) is a natural candidate  for user interface (UI) modelling since it is the standard notation  for object oriented modelling of applications. However, it is by no means  clear how to model UIs using UML. This paper presents a user interface  modelling case study using UML. This case study identifies some  aspects of UIs that cannot be modelled using UML notation, and a set  of UML constructors that may be used to model UIs. The modelling  problems indicate some weaknesses of UML for modelling UIs, while  the constructors exploited indicate some strengths. The identification  of such strengths and weaknesses can be used in the formulation of a  strategy for extending UML to provide greater support for user interface  design.
HCI
klesen00exploiting
Exploiting Models of Personality and Emotions to Control the Behavior of Animated Interactive Agents The German Research Centre for Artificial Intelligence (DFKI) recently started three new projects1 to advance our understanding of the fundamental technology required to drive the social behaviour of interactive animated agents. This initiative has been timed to catch the current wave of research and commercial interest in the field of lifelike characters [1] and
HCI
289242
Offering a Precision-Performance Tradeoff for Aggregation Queries over Replicated Data Strict consistency of replicated data is infeasible or  not required by many distributed applications, so current  systems often permit stale replication,inwhich  cached copies of data values are allowed to become  out of date. Queries over cached data return an answer  quickly, but the stale answer may be unboundedly  imprecise. Alternatively, queries over remote  master data return a precise answer, but with potentially  poor performance. To bridge the gap between  these two extremes, we propose a new class of replication  systems called TRAPP (Tradeoff in Replication  Precision and Performance). TRAPP systems  give each user fine-grained control over the tradeoff  between precision and performance: Caches store  ranges that are guaranteed to bound the current data  values, instead of storing stale exact values. Users  supply a quantitative precision constraint along with  each query. To answer a query, TRAPP systems automatically  select a combination of locally cached  bounds and exact master data stored remotely to deliver  a bounded answer consisting of a range that is  no wider than the specified precision constraint, that  is guaranteed to contain the precise answer, and that  is computed as quickly as possible. This paper defines  the architecture of TRAPP replication systems  and covers some mechanics of caching data ranges. It  then focuses on queries with aggregation, presenting  optimization algorithms for answering queries with  precision constraints, and reporting on performance  experiments that demonstrate the fine-grained control  of the precision-performance tradeoff offered by  TRAPP systems.
DB
greenberg98using
Using a Room Metaphor to Ease Transitions in Groupware Many groupware systems contain gaps that hinder or block natural social interaction or that do not let people easily move between different styles of work. We believe that the adoption of a room metaphor can ease people's transitions across these gaps, allowing them to work together more naturally. Using the TeamWave Workplace system as an example, we show how particular gaps are removed. First, we ease a person's transition between single user and groupware applications by making rooms suitable for both individual and group activity. Second, people can move fluidly between asynchronous and synchronous work because room artifacts persist. People can leave messages, documents and annotations for others, or work on them together when occupying the room at the same time. Third, we ease the difficulty of initiating real time work by providing people with awareness of others who may be available for real-time interactions, and by automatically establishing connections as users enter a commo...
HCI
dotsch00tic
TIC - A Toolkit for Validation in Formal Language Learning Quite often, heuristics and common sense suggest  directions for improving well--known learning algorithms.  However it seems not an easy task to verify  that the modifications are indeed helpful.  This is made more complicated through various additional  influences inherent in different application domains.  In order to obtain a faithful impression of phenomena  that are intrinsic to the algorithms, the role  of specific domains should be minimized.  Our validation toolkit TIC allows to explore the behaviour  of various algorithms for learning formal languages.  This is a well-examined and standardized application  domain.  TIC is operated by interactive as well as automatic  control.  Motivation and Introduction  Today, a lot of different learning approaches and algorithms do exist. There are "classical" as well as "brand new" approaches, and all of them come in many versions and refinements. On the one hand this indicates a desirable improvement of methods, but on the other hand it ...
ML
goller99connectionist
A Connectionist Approach for Learning Search-Control Heuristics for Automated Deduction Systems The central problem in automated deduction is the explosive growth of search spaces when proof length increases. In this paper, a connectionist approach for learning search-control heuristics for automated deduction systems is presented. In particular, we show how folding architecture networks, a new type of neural networks capable of solving supervised learning tasks on structured data, can be used for learning heuristics evaluation functions for algebraic (logical) expressions and how these evaluation functions can then be used to control the search process for new proof problems. Experimental results with the automated deduction system  Setheo in an algebraic domain show a considerable performance improvement. Controlled by heuristics which had been learned from simple problems in this domain the system is able to solve several problems from the same domain which had been out of reach for the original system.   1 Introduction  The goal in automated deduction (AD) is to automatically...
ML
cremonini00ruling
Ruling Agent Motion in Structured Environments . The design and development of cooperative Internet applications  based on mobile agents require appropriate modelling of both the physical  space where agents roam and the conceptual space of mobile agent interaction.  The paper discusses how an open, Internet-based, organisation network can be  modelled as a hierarchical collection of locality domains, where agents can  dynamically acquire information about resource location and availability  according to their permissions. It also analyses the issue of how agent motion  can be ruled and constrained within a structured environment by means of an  appropriate coordination infrastructure.  1 Introduction  Mobile agents are a promising technology for the design and development of cooperative applications on the Internet [3, 5, 12, 13]. Due to their capability of autonomously roaming the Internet, mobile agents can move locally to the resources they need -- let them be users, data, or services -- and there interact with them. This can p...
Agents
jain99statistical
Statistical Pattern Recognition: A Review AbstractÐThe primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.
ML
amiri00dynamic
Dynamic Function Placement for Data-intensive Cluster Computing Optimally partitioning application andfilesystem functionality within a cluster of clients and servers is a difficult problem dueto dynamic variations in application behavior, resource availability, and workload mixes. Thispaper presents A BACUS, a run-time systemthat monitors and dynamically changes function placement for applications that manipulate largedata sets. Several examples of data-intensive workloads are used to show the importance ofproper function placement and its dependence on dynamic run-time characteristics, withperformance differences frequently reaching 2-10X. We evaluate how well the ABACUSprototype adapts to run-time system behavior, including both long-term variation (e.g., filterselectivity) and short-term variation (e.g., multi-phase applications and inter-applicationresource contention). Our experiments with ABACUS indicate that it is possible to adapt inall of these situations and that the adaptation converges most quickly in those cases where theperformance impact is most significant. 1
DB
297471
Subsumption for XML Types . XML data is often used (validated, stored, queried, etc) with  respect to different types. Understanding the relationship between these  types can provide important information for manipulating this data. We  propose a notion of subsumption for XML to capture such relationships.  Subsumption relies on a syntactic mapping between types, and  can be used for facilitating validation and query processing. We study  the properties of subsumption, in particular the notion of the greatest  lower bound of two schemas, and show how this can be used as a guide  for selecting a storage structure. While less powerful than inclusion, subsumption  generalizes several other mechanisms for reusing types, notably  extension and refinement from XML Schema, and subtyping.  1 Introduction  XML [5] is a data format for Web applications. As opposed to e.g., relational databases, XML documents do not have to be created and used with respect to a fixed, existing schema. This is particularly useful in Web ap...
DB
lerman00coalition
Coalition Formation for Large-Scale Electronic Markets Coalition formation is a desirable behavior in a multiagent system, when a group of agents can perform a task more efficiently than any single agent can. Computational and communications complexity of traditional approaches to coalition formation, e.g., through negotiation, make them impractical for large systems. We propose an alternative, physics-motivated mechanism for coalition formation that treats agents as randomly moving, locally interacting entities. A new coalition may form when two agents encounter one another, and it may grow when a single agent encounters it. Such agent-level behavior leads to a macroscopic model that describes how the number and distribution of coalitions change with time. We increase the generality and complexity of the model by letting the agents leave coalitions with some probability. The model is expressed mathematically as a series of differential equations. These equations have steady state solutions that describe the equilibrium distribution of coa...
Agents
dolan00benchmarking
Benchmarking Optimization Software with COPS 1 Introduction 1 Testing Methods 2 1 Largest Small Polygon 3 2 Distribution of Electrons on a Sphere 5 3 Hanging Chain 7 4 Shape Optimization of a Cam 9 5 Isometrization of ff-pinene 11 6 Marine Population Dynamics 13 7 Flow in a Channel 16 8 Robot Arm 18 9 Particle Steering 21 10 Goddard Rocket 23 11 Hang Glider 26 12 Catalytic Cracking of Gas Oil 29 13 Methanol to Hydrocarbons 31 14 Catalyst Mixing 33 15 Elastic-Plastic Torsion 35 16 Journal Bearing 37 17 Minimal Surface with Obstacle 39 Acknowledgments 41 References 41 ii  Benchmarking Optimization Software with COPS  by  Elizabeth D. Dolan and Jorge J. Mor'e Abstract  We describe version 2.0 of the COPS set of nonlinearly constrained optimization problems. We have added new problems, as well as streamlined and improved most of the problems. We also provide a comparison of the LANCELOT, LOQO, MINOS, and SNOPT solvers on these problems. Introduction  The COPS [5] test set provides a modest selection of difficult nonlinearly constrai...
AI
sturm00tableau
A Tableau Calculus for Temporal Description Logic: The Expanding Domain Case . In this paper we present a tableau calculus for a temporal extension of the  description logic ALC, called T LALC . This logic is based on the temporal language with  `Until' interpreted over the natural numbers with expanding ALC-domains. The tableau  calculus forms an elaborated combination of Wolper's tableau calculus for propositional  linear temporal logic, the standard tableau-algorithm for ALC, and the method of quasimodels  as it has been introduced by Wolter and Zakharyaschev. Based on those three  ingredients the paper provides a new method of how tableau-based decision procedures  can be constructed for many-dimensional logics which lack the finite model property. The  method can be applied to deal with other temporalized formalisms as well.  1 Introduction  In many application domains of logic in Computer Science and Artificial Intelligence it is no longer enough to describe the static aspect of the world. In particular, there is a need to formalize its temporal evolution...
DB
lesh99using
Using Plan Recognition in Human-Computer Collaboration . Human-computer collaboration provides a practical and useful application for  plan recognition techniques. We describe a plan recognition algorithm which is tractable by  virtue of exploiting properties of the collaborative setting, namely: the focus of attention, the  use of partially elaborated hierarchical plans, and the possibility of asking for clarification.  We demonstrate how the addition of our plan recognition algorithm to an implemented  collaborative system reduces the amount of communication required from the user.  1 Introduction  An important trend in recent work on human-computer interaction and user modeling has been to view human-computer interaction as a kind of collaboration (e.g, Ferguson and Allen, 1998, Guinn, 1996, Rich and Sidner, 1998, Rickel and Johnson, 1998). In this approach, the human user and the computer (often personified as an "agent") coordinate their actions toward achieving shared goals. A common setting for collaboration, illustrated in Figure 1...
HCI
5188
Empirical Risk Approximation: An Induction Principle for Unsupervised Learning Unsupervised learning algorithms are designed to extract structure from data without reference to explicit teacher information. The quality of the learned structure is determined by a cost function which guides the learning process. This paper proposes Empirical Risk Approximation as a new induction principle for unsupervised learning. The complexity of the unsupervised learning models are automatically controlled by the two conditions for learning: (i) the empirical risk of learning should uniformly converge towards the expected risk; (ii) the hypothesis class should retain a minimal variety for consistent inference. The maximal entropy principle with deterministic annealing as an efficient search strategy arises from the  Empirical Risk Approximation principle as the optimal inference strategy for large learning problems. Parameter selection of learnable data structures is demonstrated for the case of  k-means clustering. 1 What is unsupervised learning?  Learning algorithms are desi...
ML
simmons00first
First Results in the Coordination of Heterogeneous Robots for Large-Scale Assembly : While many multi-robot systems rely on fortuitous cooperation  between agents, some tasks, such as the assembly of large structures, require  tighter coordination. We present a general software architecture for coordinating  heterogeneous robots that allows for both autonomy of the individual agents as  well as explicit coordination. This paper presents recent results with three robots  with very different configurations. Working as a team, these robots are able to perform  a high-precision docking task that none could achieve individually.  1. Introduction  As robots become more autonomous and sophisticated, they are increasingly being used for more complex and demanding tasks. Often, single robots are insufficient to perform the tasks. For some types of tasks, such as exploration or demining, multiple robots can be used to increase efficiency and reliability. For many other tasks, however, not only are multiple robots necessary, but explicit coordination amongst the robots is imper...
Agents
9683
A Control Architecture for Flexible Internet Auction Servers The flexibility to support both high activity and low activity auctions is required by any system that allows bidding by both humans and software agents. We present the control architecture of the Michigan Internet AuctionBot, and discuss some of the system engineering issues that arose in its design. 1 Introduction  The Michigan Internet AuctionBot is a highly configurable auction server built to support research on electronic commerce and multiagent negotiation [3]. The first generation architecture was simple and robust, and allowed us to concentrate on other aspects of the system. However, several inefficiencies made it problematic to run auctions with very fast interactions. We have redesigned the core AuctionBot architecture in order to improve overall performance, while still meeting the original goal: a system that is configurable, maintainable, and capable of conducting a large number of simultaneous auctions. In AuctionBot architecture nomenclature, we say an auction is open ...
DB
534081
Active Proxy-G: Optimizing the Query Execution Process in the Grid The Grid environment facilitates collaborative work and allows many users to query and process data over geographically dispersed data repositories. Over the past several years, there has been a growing interest in developing applications that interactively analyze datasets, potentially in a collaborative setting. We describe an Active Proxy-G service that is able to cache query results, use those results for answering new incoming queries, generate subqueries for the parts of a query that cannot be produced from the cache, and submit the subqueries for final processing at application servers that store the raw datasets. We present an experimental evaluation to illustrate the effects of various design tradeoJj5 . We also show the benefits that two real applications gain from using the middleware.
IR
510049
A Semiotic Communication Model for Interface Design This research wants to contribute to the creation of a semiotic framework for interface design. Using the Jakobson's communication model to analyse the HCI approach to interface development, we explain how some central factors of communication are not enough considered by designers.
HCI
103027
Modeling Emotions and Other Motivations in Synthetic Agents We present Cathexis, a distributed, computational model which offers an alternative approach to model the dynamic nature of different affective phenomena, such as emotions, moods and temperaments, and provides a flexible way of modeling their influence on the behavior of synthetic autonomous agents. The model has been implemented as part of an extensible, object-oriented framework which provides enough functionality for agent developers to design emotional agents that can be used in a variety of applications including entertainment (e.g. synthetic agents for interactive drama, video games, etc.), education (e.g. Intelligent Tutoring Systems), and human-computer interfaces.  Introduction  Emotions are an essential part of our lives, they influence how we think and behave, and how we communicate with others. Several researchers have acknowledged their importance in human thinking [Minsky 1986; Toda 1993], and recent neurological evidence seems to support these ideas [LeDoux 1996; Damasio...
Agents
srinivasan02web
Web Crawling Agents for Retrieving Biomedical Information Autonomous agents for topic driven retrieval of information from the Web are currently a very active area of research. The ability to conduct real time searches for information is important for many users including biomedical scientists, health care professionals and the general public. We present preliminary research on different retrieval agents tested on their ability to retrieve biomedical information, whose relevance is assessed using both genetic and ontological expertise. In particular, the agents are judged on their performance in fetching information about diseases when given information about genes. We discuss several key insights into the particular challenges of agent based retrieval learned from our initial experience in the biomedical domain.
IR
fan01splitting
On splitting and Cloning Agents Embedded with cloning mechanisms, an agent can balance its own loads by discharging computing tasks to its clones when it is over-loaded. In addition, it's more reasonable to transfer the smarter, smaller clones of an agent rather than the bulky agent itself in mobile computing. In this paper, a simple BDI agent model is formally established. Using this model, the semantics of constructing new agents by inheritance and self-identifying behavior of existing agents are precisely de  ned. Four kinds of cloning mechanisms are identi  ed, the properties of each cloning mechanism and the relationships in between are studied, and some implementation issues are also discussed.
Agents
baumgartner01supervised
Supervised Wrapper Generation with Lixto We illustrate basic features of the Lixto wrapper  generator such as the user and system interaction,  the capacious visual interface, the marking and  selecting procedures, and the extraction tasks by  describing the construction of a simple example  program in the current Lixto prototype.  1. 
IR
carbonell98report
Report on the CONALD Workshop on Learning from Text and the Web Moo], organization and presentation of documents in information retrieval systems [GS, Hof], collaborative filtering [dVN], lexicon learning [GBGH], query reformulation [KK], text generation [Rad] and analysis of the statistical properties of text [MA]. In short, the state of the art in learning from text and the web is that a broad range of methods are currently being applied to many important and interesting tasks. There remain numerous open research questions, however. Broadly, the goals of the work presented at the workshop fall into two overlapping categories: (i) making textual information available in a structured format so that it can be used for complex queries and problem solving, and (ii) assisting users in finding, organizing and managing information represented in text sources. As an example of research aimed at the former goal, Muslea, Minton and Knoblock [MMK] have developed an approach to learning wrappers for semi-structured Web sources, such as restau
IR
515869
A Two-stage Scheme for Dynamic Hand Gesture Recognition In this paper a scheme is presented for recognizing hand gestures using the output of a hand tracker which tracks a rectangular window bounding the hand region. A hierarchical scheme for dynamic hand gesture recognition is proposed based on state representation of the dominant feature trajectories using an a priori knowledge of the way in which each gesture is performed.
HCI
coppin00eventscope
EventScope: Amplifying Human Knowledge and Experience via Intelligent Robotic Systems and Information Interaction The EventScope program develops publicly accessible "reality browsers" that display both archived and updating representations of remote environments derived from on-site robotic sensors. The interface encourages collaborative work within a community of users. Public exploration of real remote sites presents a variety of interface issues addressed by EventScope, including time delay, public exploration via a single robot and communication between geographically separate users from diverse backgrounds. Merging public interface with educational and contextual information extends the notion of "interface" to "remote reality library." EventScope is a NASA and private foundationfunded project based at Carnegie Mellon University. 1. Introduction  Publicly funded Earth and planetary exploration is conducted to increase knowledge of our universe. The public traditionally accesses this knowledge passively, through the media. However, the development of the Web and of robotic remote-sensing tech...
HCI
bertelsen99dynamics
Dynamics in Wastewater Treatment: A Framework for Understanding Formal Constructs in Complex Technical Settings . Based on the study of unskilled work in a Danish wastewater treatment plant, the problem of formalisation of work is discussed and extended to technical processes. Five symmetrical levels of dynamics in complex technical work arrangements are proposed as a tool for understanding the limits of formalisation and for designing formal constructs in such settings. The analysis is based on concepts of heterogeneity, granularity of goals and motives, and process and structure.  Introduction  An inevitable problem in the design of CSCW systems is that work is not standing still. Rather, work settings are dynamic: routines evolve over time and unusual situations force deviations from the routine. For this reason, purely formal constructs and descriptions of work have proven inadequate when designing effective, real world CSCW systems.  Suchman and Wynn (1984) set off the debate about the role of formalism in CSCW, and their empirical studies clearly illustrated that there is more to office wo...
HCI
warshaw98monitoring
Monitoring Network Logs for Anomalous Activity We report on the progress of the VenusDB active-database system as driven by WatchDog, an application in network intrusion detection. The application is typical of a class of problems we coin monotonic log monitoring systems. These are systems where real-time data sources are logged to a database for transactional assurances and the database further provides services for decision support. Milestones comprise the successful layering of a Venus language executable with Oracle through the use of the Venus Abstract Machine Interface (AMI, a data abstraction interface) and Oracle's native trigger mechanism. The identification of monotonic-logging systems as an interesting application class enables us to limit coupling modes and to identify an effective layered architecture.
DB
davison00topical
Topical Locality in the Web: Experiments and Observations Most web pages are linked to others with related content. This idea, combined with another that says that text in, and possibly around, HTML anchors describe the pages to which they point, is the foundation for a usable World-Wide Web. In this paper, we examine to what extent these ideas hold by empirically testing whether topical locality mirrors spatial locality of pages on the Web. In particular, we find that the likelihood of linked pages having similar textual content to be high; the similarity of sibling pages increases when the links from the parent are close together; titles, descriptions, and anchor text represent at least part of the target page; and that anchor text may be a useful discriminator among unseen child pages. These results present the foundations necessary for the success of many web systems, including search engines, focused crawlers, linkage analyzers, and intelligent web agents. 1 Introduction Most web pages are linked to others with related content...
IR
kollios99nearest
Nearest Neighbor Queries in a Mobile Environment Nearest neighbor queries have received much interest in recent  years due to their increased importance in advanced database applications. However, past work
DB
bassiliades00edevice
E-DEVICE: An Extensible Active Knowledge Base System with Multiple Rule Type Support This paper describes E-DEVICE, an extensible active knowledge base system (KBS) that supports the processing of event-driven, production, and deductive rules into the same active OODB system. E-DEVICE provides the infrastructure for the smooth integration of various declarative rule types, such as production and deductive rules, into an active OODB system that supports low-level event-driven rules only by a) mapping each declarative rule into one event-driven rule, offering centralized rule selection control for correct run-time behavior and conflict resolution, and b) using complex events to map the conditions of declarative rules and monitor the database to incrementally match those conditions. E-DEVICE provides the infrastructure for easily extending the system by adding a) new rule types as subtypes of existing ones and b) transparent optimizations to the rule matching network. The resulting system is a flexible, yet efficient, KBS that gives the user the ability to express knowledge in a variety of high-level forms for advanced problem solving in data intensive applications.
DB
camerer00ewa
EWA Learning in Bilateral Call Markets This chapter extends the EWA learning model to bilateral call market games (also known as the "sealed-bid mechanism" in two-person bargaining). In these games, a buyer and seller independently draw private values from commonly-known distributions and submit bids. If the buyer's bid is above the seller's, they trade at the midpoint of the two bids; otherwise they don't trade. We apply EWA by assuming that players have value-dependent bidding strategies, and they partially generalize experience from one value/cost condition to another in response to the incentives from nonlinear optimal bid functions. The same learning model can be applied to other market institutions where subjects economize on learning by taking into consideration similarity between past experience and a new environment while still recognizing the difference in market incentives between them. The chapter also presents a new application of EWA to a "continental divide" coordination game, and reviews 32 earlier studies comparing EWA, reinforcement, and belief learning. The application shows the advantages of a generalized adaptive model of behavior that includes elements of reinforcement, belief-based and direction learning as special cases at some cost of complexity for the benefit of generality and psychological appeal. It is a good foundation to build upon to extend our understanding of adaptive behavior in more general games and market institutions. In future work, we should investigate the similarity parameters, y and w, to better characterize their magnitude and significance in different market institutions. Keywords: Experimental economics, call markets, sealed-bid mechanism, learning JEL Classification: D44, D83, C92 August 2, 2000. Thanks to Terry Daniel for supplying data. This research has been...
ML
koch99simple
A Simple Query Facility for the Objectivity/DB Persistent Object Manager This document discusses the reasons that lead to the development of a query faciliy within the CRISTAL project, its design criteria, syntax, semantics, use, and restrictions. It is furthermore intended to serve as a preliminary manual. The query facility is discussed in its immediately next development stage which should be finished within the next few weeks
DB
crabbe01multiple
Multiple Goal Q-Learning: Issues and Functions This paper addresses the concerns of agents using reinforcement learning to learn to achieve multiple simultaneous goals. It proves that an algorithm based on acting upon the maximal goal at any one time will, in many cases, not not produce the Maximal Expected Utility for the agent. The paper then examines the type of function approximator necessary for the agent's reinforcement learning system, and concludes that a bi-linear function is the best compromise between expressive power and speed of learning.
ML
flake02extracting
Extracting Query Modifications from Nonlinear SVMs When searching the WWW, users often desire results restricted to a particular document category. Ideally, a user would be able to filter results with a text classifier to minimize false positive results; however, current search engines allow only simple query modifications. To automate the process of generating effective query modifications, we introduce a sensitivity analysis-based method for extracting rules from nonlinear support vector machines. The proposed method allows the user to specify a desired precision while attempting to maximize the recall. Our method performs several levels of dimensionality reduction and is vastly faster than searching the combination feature space; moreover, it is very effective on real-world data.
IR
142848
Extending the ODMG Object Model with Time Although many temporal extensions of the relational data model have been proposed, there is no comparable amount of work in the context of object-oriented data models. Moreover, extensions to the relational model have been proposed in the framework of SQL standards, whereas no attempts have been made to extend the standard for object-oriented databases, defined by ODMG. This paper presents T ODMG, a temporal extension of the ODMG-93 standard data model. The main contributions of this work are, thus, the formalization of the ODMG standard data model and its extension with time. Another contribution of this work is the investigation, on a formal basis, of the main issues arising from the introduction of time in an object-oriented model.
DB
aiello01ontological
Ontological Overhearing The collaboration between two intelligent agents can be greatly enhanced  if a third agent, who has some understanding of the communication between  the first two, intervenes giving appropriate information or acting helpfully  without having been explicitly involved. The behavior of this third agent, quite  common in human interaction, is called overhearing. We present an agent architecture  modeling this behavior. In particular, we focus on overhearing based on  ontological reasoning; that is, the overhearer semantically selects pieces of communication  according to his own knowledge (ontologically organized) and goals.
Agents
schweighofer01improving
Improving the Quality of Labels for Self-Organising Maps Using Fine-Tuning Vector representation of legal documents is still the best way for computing classification clusters and labelling of its contents. A very special problem occurs with self organising maps: strong clusters tend to dominate neighbouring smaller clusters in terms of their weight vector structure, which influences the labels extracted from these. This unwelcome side-effect can be overcome efficiently with a dedicated fine-tuning phase at the end of the training process, in which the neighbourhood radius of the training function is set to zero. Experiments with our text collection have shown the high improvement of the quality of labelling.  
IR
gaskett00reinforcement
Reinforcement Learning for Visual Servoing of a Mobile Robot A novel reinforcement learning algorithm is applied to a visual servoing task on a real mobile robot. There is no requirement for camera calibration, an actuator model or a knowledgeable teacher. The controller learns from a critic which gives a scalar reward. The learning algorithm handles continuously valued states and actions and can learn from good and bad experiences including data gathered while performing unrelated behaviours and from historical data. Experimental results are presented. 1 Introduction Visual servoing consists of moving some part of a robot to a desired position using visual feedback [ Hutchinson et al., 1996 ] . It is a basic building block for purposeful robot behaviours such as foraging, target pursuit and landmark based navigation. Some degree of calibration is generally required to achieve visual servoing. This calibration can be a time consuming and error prone process. In this work we show that reinforcement based learning can eliminate the ca...
ML
duch00optimization
Optimization and Interpretation of Rule-Based Classifiers Abstract. Machine learning methods are frequently used to create rule-based classifiers. For continuous features linguistic variables used in conditions of the rules are defined by membership functions. These linguistic variables should be optimized at the level of single rules or sets of rules. Assuming the Gaussian uncertainty of input values allows to increase the accuracy of predictions and to estimate probabilities of different classes. Detailed interpretation of relevant rules is possible using (probabilistic) confidence intervals. A real life example of such interpretation is given for personality disorders. The approach to optimization and interpretation described here is applicable to any rule-based system. 1 Introduction. In many applications rule-based classifiers are created starting from machine learning, fuzzy logic or neural network methods [1]–[3]. If the number of rules is relatively small and accuracy is sufficiently high such classifiers are an optimal choice, because the reasons for their decisions are easily verified. Crisp logical rules are desirable
ML
ester00dctree
The DC-tree: A Fully Dynamic Index Structure for Data Warehouses :  Many companies have recognized the strategic importance of the knowledge hidden in their large databases and have built data warehouses. Typically, updates are collected and applied to the data warehouse periodically in a batch mode, e.g., over night. Then, all derived information such as index structures has to be updated as well. The standard approach of bulk incremental updates to data warehouses has some drawbacks.First, the average runtime for a single update is small but the total runtime for the whole batch of updates may become rather large. Second, the contents of the data warehouse is not always up to date. In this paper, we introduced the DC-tree, a fully dynamic index structure for data warehouses modeled as a data cube. This new index structure is designed for applications where the above drawbacks of the bulk update approach are critical. The DC-tree is a hierarchical index structure - similar to the X-tree - exploiting the concept hierarchies typically defined for the...
DB
chen99introducing
Introducing a New Advantage of Crossover: Commonality-Based Selection The Commonality-Based Crossover Framework defines crossover as a two-step process: 1) preserve the maximal common schema of two parents, and 2) complete the solution with a construction heuristic. In these “heuristic ” operators, the first step is a form of selection. This commonality-based form of selection has been isolated in GENIE. Using random parent selection and a non-elitist generational replacement scheme, GENIE does not include fitness-based selection. However, a theoretical analysis shows that “ideal ” construction heuristics in GENIE can potentially converge to optimal solutions. Experimentally, results show that the effectiveness of practical construction heuristics can be amplified by commonalitybased restarts. Overall, it is shown that the commonality hypothesis is valid--schemata common to above-average solutions are indeed above average. Since common schemata can only be identified by multi-parent operators, commonality-based selection is a unique advantage that crossover can enjoy over mutation. 1
ML
kon00flexible
A Flexible, Interoperable Framework for Active Spaces this paper we describe the requirements faced by such a system and propose an integrated architecture meeting these requirements. The paper focuses on a representation of Active Spaces using standard Naming and Trading mechanisms and on an object-oriented framework for managing heterogeneous devices.
HCI
mcdonald00heterogeneous
Heterogeneous Database Integration Using Agent-Oriented Information Systems : The Department of Defense (DOD) has an extensive family of models used to simulate the mission level interaction of weapon systems. Interoperability and reuse of the underlying data files used to create simulation scenarios pose great challenges in this regard. Unlike traditional data integration methods common to federated database research, the emerging field of agent-oriented information systems (AOIS) views data as the central focus of an application while also providing an overall architectural framework for application development. We develop an AOIS solution relevant to this problem domain by combining object-oriented data modeling (OMT), a persistent programming language using a commercial objectoriented database (ObjectStore#), and an agentoriented analysis and design methodology (MaSE). Requirements from a contractor-led effort at the Air Force Research Laboratory (AFRL) known as CERTCORT are the basis for analysis and design of our system. We implement prototypical information-layer applications to conceptually demonstrate the reusability and integration of scenarios across simulation models. Keywords: AOIS, Agents, Modeling and Simulations, Heterogeneous Database Integration 1.
DB
vanschooten01structuring
Structuring Distributed Virtual Environments Using a Relational Database Model This paper discusses a specification technique that is based on a traditional (entity-relationship) database model to model the architecture of complex interactive systems, in particular multimodal and multi-user user interfaces. User interface components and other software components ...
HCI
455752
Chart of Darkness: Mapping a Large Intranet We introduce and de ne the concept of dark matter on the Web. Dark matter for a person or Web crawler consists of pages that they cannot reach and view, but which another observer can. Dark matter is important to our understanding of the Web in that the portion of the Web any of us can see depends on our viewpoint. Diffrent observers see different overlapping sections of the Web. However, no one can see all of the Web, even if they want to. We categorise the various types of dark matter that exist and how they may be discovered. Formal definitions of what constitutes lightness and darkness on the Web are formulated in terms of reachability. Our case study of dark matter within the Australian National University's intranet is reported. We estimate that 87% of the ANU intranet's information is dark to our local search service, and 37% is potentially loadable Web data unreachable to almost every Web user. Finally, we discuss some of the implications of dark matter for estimating the size of the Web and for general Web searching.
IR
huffman94instructable
Instructable Autonomous Agents INSTRUCTABLE AUTONOMOUS AGENTS by Scott Bradley Huffman Chair: John E. Laird In contrast to current intelligent systems, which must be laboriously programmed for each task they are meant to perform, instructable agents can be taught new tasks and associated knowledge. This thesis presents a general theory of learning from tutorial instruction and its use to produce an instructable agent. Tutorial instruction is a particularly powerful form of instruction, because it allows the instructor to communicate whatever kind of knowledge a student needs at whatever point it is needed. To exploit this broad flexibility, however, a tutorable agent must support a full range of interaction with its instructor to learn a full range of knowledge. Thus, unlike most machine learning tasks, which target deep learning of a single kind of knowledge from a single kind of input, tutorability requires a breadth of learning from a broad range of instructional interactions. The theory of learning from tutorial...
ML
cohn01qualitative
Qualitative Spatial Representation and Reasoning: An Overview . The paper is a overview of the major qualitative spatial representation and reasoning techniques. We survey the main aspects of the representation of qualitative knowledge including ontological aspects, topology, distance, orientation and shape. We also consider qualitative spatial reasoning including reasoning about spatial change. Finally there is a discussion of theoretical results and a glimpse of future work. The paper is a revised and condensed version of [33, 34]. Keywords: Qualitative Spatial Reasoning, Ontology. The text is in a slightly di erent format from the FI format. Cohn
HCI
131492
Intelligent Gradient-Based Search of Incompletely Defined Design Spaces Gradient-based numerical optimization of complex engineering designs offers the promise of rapidly producing better designs. However, such methods generally assume that the objective function and constraint functions are continuous, smooth, and defined everywhere. Unfortunately, realistic simulators tend to violate these assumptions. We present a rule-based technique for intelligently computing gradients in the presence of such pathologies in the simulators, and show how this gradient computation method can be used as part of a gradient-based numerical optimization system. We tested the resulting system in the domain of conceptual design of supersonic transport aircraft, and found that using rule-based gradients can decrease the cost of design space search by one or more orders of magnitude.  Keywords: Optimization, gradients, sequential quadratic programming, rule-based systems.  1 Introduction  Automated search of a space of candidate designs seems an attractive way to improve the tr...
ML
249151
CAT: the Copying Approach to Tabling The SLG-WAM is an abstract machine that can be characterized as a sharing approach to implementing tabling: The execution environments of suspended computations are interspersed in the WAM stacks. Stacks are frozen using a set of freeze registers, and the WAM trail mechanism is extended so that the suspended computations can be resumed. This technique has a reasonably small execution overhead, but it is not easy to implement on top of an existing Prolog system. It is also quite difficult to understand. We propose a new technique for the implementation of tabling: the copying approach to tabling. CAT does not impose any overhead to the execution of Prolog code and can be introduced into an existing Prolog system orthogonally. Also, CAT is easier to understand. We have implemented CAT in the XSB system by taking out SLG-WAM and adding CAT. We describe the additions needed for adopting CAT in a WAM implementation. We show a case in which CAT performs arbitrarily worse than SLG-WAM, but on the other hand we present empirical evidence that CAT is competitive and often faster than the SLG-WAM. We also briefly discuss issues related to memory management and to the scheduling.
DB
yang01pms
PMS: a PVC Management System for ATM Networks . Reported in this paper is the developed PMS, a PVC management  system for ATM networks. PMS provides a scalable, end-to-end path  management solution required for managing todays complex ATM networks.  It aims to assist the network operators to perform PVC operations with  simplified procedures and automatic optimum route selection. It also aims to  provide effective decision-making support for PVC fault identification and  prevention to the network operators.  1 Introduction  ATM communication network is playing more and more important role in todays  telecommunication networks. It has been widely used in backbone networks, transmission networks, access networks, and even enterprise networks. Such emerging large heterogeneous ATM networks have raised many new challenges for researchers and developers in the area of network management. In the management of ATM communication networks that have increased dramatically in size and complexity, the PVC (Permanent Virtual Circuit) managemen...
Agents
66208
An Efficient Boosting Algorithm for Combining Preferences We study the problem of learning to accurately rank a set of objects by combining a given collection  of ranking or preference functions. This problem of combining preferences arises in several  applications, such as that of combining the results of different search engines, or the "collaborativefiltering  " problem of ranking movies for a user based on the movie rankings provided by other  users. In this work, we begin by presenting a formal framework for this general problem. We then  describe and analyze an efficient algorithm called RankBoost for combining preferences based on  the boosting approach to machine learning. We give theoretical results describing the algorithm's  behavior both on the training data, and on new test data not seen during training. We also describe  an efficient implementation of the algorithm for a particular restricted but common case. We next  discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment,  we used the algorithm to combine different web search strategies, each of which is a query  expansion for a given domain. The second experiment is a collaborative-filtering task for making  movie recommendations.
ML
cui01lineage
Lineage Tracing for General Data Warehouse Transformations Data warehousing systems integrate information from operational data sources into a central repository to enable analysis and mining of the integrated information. During the integration process, source data typically undergoes a series of transformations, which may vary from simple algebraic operations or aggregations to complex "data cleansing" procedures. In a warehousing environment, the data lineage problem is that of tracing warehouse data items back to the original source items from which they were derived. We formally define the lineage tracing problem in the presence of general data warehouse transformations, and we present algorithms for lineage tracing in this environment. Our tracing procedures take advantage of known structure or properties of transformations when present, but also work in the absence of such information. Our results can be used as the basis for a lineage tracing tool in a general warehousing setting, and also can guide the design of data warehouses that enable efficient lineage tracing.  1 
DB
64654
Estimating Dependency Structure as a Hidden Variable This paper introduces a probability model, the mixture of trees that can account for sparse, dynamically changing dependence relationships. We present a family of efficient algorithms that use EM and the Minimum Spanning Tree algorithm to find the ML and MAP mixture of trees for a variety of priors, including the Dirichlet and the MDL priors.  1 INTRODUCTION  A fundamental feature of a good model is the ability to uncover and exploit independencies in the data it is presented with. For many commonly used models, such as neural nets and belief networks, the dependency structure encoded in the model is fixed, in the sense that it is not allowed to vary depending on actual values of the variables or with the current case. However, dependency structures that are conditional on values of variables abound in the world around us. Consider for example bitmaps of handwritten digits. They obviously contain many dependencies between pixels; however, the pattern of these dependencies will vary acr...
ML
eriksson98to
To Each and Everyone an Agent: Augmenting Web-Based Commerce with Agents Internet has evolved from an information space to a market space with thousands, potentially millions, of electronic storefronts, auctions and other commercial services. This creates great opportunities, but is not without problems. One major problem is the difficulty of finding relevant offers. Another problem is coping with the multitude of different styles of web-based user interfaces to different marketplaces. Yet another problem is how to automate routine tasks in such an environment. We present one possible solution to these problems. An agent-based market infrastructure, in which agents support all users and services, helps customers and commercial sites find matching interests, and, if desired, negotiate and close deals. The infrastructure is entirely open and decentralized. Each participant has an agent that acts in the interest of its owner. Interaction is entirely symmetric. Any participant can play any role on a market. In this paper we present an integration of such an infrastructure, SICS MarketSpace, with the web. Personal assistant agents help users in their interaction with services and are able to handle routine tasks off-line. Agent-enabled services are able to adapt to the interests of their users, even on their first visit, and are provided with a mechanism to take the first initiative (push) in a highly focused manner. Keywords: agent-based markets, software agents, worldwide web, electronic commerce, personal assistants 1.
Agents
455229
Group Task Analysis for Groupware Usability Evaluations Techniques for inspecting the usability of groupware applications have recently been proposed. These techniques focus on the mechanics of collaboration rather than the work context in which a system is used, and offer time and cost savings by not requiring actual users or fully-functional prototypes. Although these techniques are valuable, adding information about task and work context could improve the quality of inspection results. We introduce a method for analysing group tasks that can be used to add context to discount groupware evaluation techniques. Our method allows for the specification of collaborative scenarios and tasks by considering the mechanics of collaboration, levels of coupling during task performance, and variability in task execution. We describe how this type of task analysis could be used in a new inspection technique based on cognitive walkthrough. 
HCI
291240
An Open Framework for Distributed Multimedia Retrieval This article describes a framework for distributed multimedia retrieval which permits the  connection of compliant user interfaces with a variety of multimedia retrieval engines via an  open communication protocol, MRML (Multi Media Retrieval Markup Language). It allows  the choice of image collection, feature set and query algorithm during run{time, permitting  multiple users to query a system adapted to their needs, using the query paradigm adapted  to their problem such as query by example (QBE), browsing queries, or query by annotation.  User interaction is implemented over several levels and in diverse ways. Relevance feedback  is implemented using positive and negative example images that can be used for a  best{match QBE query. In contrast, browsing methods try to approach the searched image  by giving overviews of the entire collection and by successive renement. In addition to these  query methods, Long term o line learning is implemented. It allows feature preferences per  ...
HCI
acharya99selectivity
Selectivity Estimation in Spatial Databases Selectivity estimation of queries is an important and wellstudied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries...
DB
114091
A Multimodal Approach To Term Extraction Using A Rhetorical Structure Theory Tagger And Formal Concept Analysis This paper reports on knowledge extraction using Rhetorical Structure Theory (RST) and Formal Concept Analysis (FCA). The research is multimodal in two ways: (i) it uses a text tagger to identify key terms in free text, these terms are then used as indexation filters over the free text; (ii) it aims to normalise the contents of multiple text sources into a single knowledge base. The aim is semi-automated extraction of semantic content in texts derived from different sources and merging them into a single coherent knowledge base. We use RST ([7]) to automate the identification of discourse markers in multiple texts dealing with a single subject matter. Marcu ([8, 10]) has shown that RST can be used for the semiautomated mark up of natural language texts. Marcu uses discourse trees, useful to store information about the rhetorical structure, and has shown that the identification of discourse markers from prototypical texts can be automated with 88% precision ([9]). We have adapted Marcu's algorithm in our approach. Although our work draws on recent results from natural language processing, progress in that field is not the objective. The research is motivated by the analysis of texts generated by different sources, their translation to a formal knowledge representation followed by a consolidation into a single knowledge corpus. Our interest is in the analysis of this corpus to determine the reliability of information obtained from multiple agencies ([11]) and then to visually navigate this knowledge. This involves FCA ([14, 15, 17, 18, 6]) for browsing and retrieving text documents ([2, 3, 4, 1]). FCA is typically a propositional knowledge representation technique, i.e., it can only express monadic relations. Recently, Wille ([16]) has shown that FCA can be used to repres...
AI
palen99social
Social, Individual & Technological Issues for Groupware Calendar Systems Designing and deploying groupware is difficult. Groupware evaluation and design are often approached from a single perspective, with a technologically-, individually-, or socially-centered focus. A study of Groupware Calendar Systems (GCSs) highlights the need for a synthesis of these multiple perspectives to fully understand the adoption challenges these systems face. First, GCSs often replace existing calendar artifacts, which can impact users' calendaring habits and in turn influence technology adoption decisions. Second, electronic calendars have the potential to easily share contextualized information publicly over the computer network, creating opportunities for peer judgment about time allocation and raising concerns about privacy regulation. However, this situation may also support coordination by allowing others to make useful inferences about one's schedule. Third, the technology and the social environment are in a reciprocal, co-evolutionary relationship: the use context is affected by the constraints andaffordances of the technology, and the technology also co-adapts to the environment in important ways. Finally,  GCSs, despite being below the horizon of everyday notice, can affect the nature of temporal coordination beyond the expected meeting scheduling practice.
HCI
roth01relational
Relational Learning via Propositional Algorithms: An Information Extraction Case Study This paper develops a new paradigm for relational  learning which allows for the representation and  learning of relational information using propositional  means. This paradigm suggests different  tradeoffs than those in the traditional approach to  this problem -- the ILP approach -- and as a result  it enjoys several significant advantages over it. In  particular, the new paradigm is more flexible and  allows the use of any propositional algorithm, including  probabilistic algorithms, within it.  We evaluate the new approach on an important  and relation-intensive task - Information Extraction  - and show that it outperforms existing methods  while being orders of magnitude more efficient.  1 
IR
441053
Direct value-approximation for factored MDPs We present a simple approach for computing reasonable policies  for factored Markov decision processes (MDPs), when the optimal  value function can be approximated by a compact linear form.
ML
prentzas01webbased
A Web-Based ITS Controlled by an Expert System Intelligent Tutoring System (ITS) for teaching high school teachers how to use new technologies. It offers course units covering the needs of users with different knowledge levels and characteristics. It tailors the presentation of the educational material to the users' diverse needs by using AI techniques to specify each user's model as well as to make pedagogical decisions. This is achieved via an expert system that uses a hybrid knowledge representation formalism integrating symbolic rules with neurocomputing.
AI
coetzee00feature
Feature Selection in Web Applications Using ROC Inflections and Power Set Pruning A basic problem of information processing is selecting enough features to ensure that events are accurately  represented for classification problems, while simultaneously minimizing storage and processing  of irrelevant or marginally important features. To address this problem, feature selection procedures  perform a search through the feature power set to find the smallest subset meeting performance requirements.  Major restrictions of existing procedures are that they typically explicitly or implicitly assume a  fixed operating point, and make limited use of the statistical structure of the feature power set. We present  a method that combines the Neyman-Pearson design procedure on finite data, with the directed set structure  of the Receiver Operating Curves on the feature subsets, to determine the maximal size of the feature  subsets that can be ranked in a given problem. The search can then be restricted to the smaller subsets, resulting  in significant reductions in computational...
IR
shehory99emergent
Emergent Cooperative Goal-Satisfaction in Large Scale Automated-Agent Systems Cooperation among autonomous agents has been discussed in the DAI community for several years. Papers about cooperation [6, 45], negotiation [33], distributed planning [5], and coalition formation [28, 48], have provided a variety of approaches and several algorithms and solutions to situations wherein cooperation is possible. However, the case of cooperation in large-scale multi-agent systems (MAS) has not been thoroughly examined. Therefore, in this paper we present a framework for cooperative goal-satisfaction in large-scale environments focusing on a low complexity physics-oriented approach. The multi-agent systems with which we deal are modeled by a physics-oriented model. According to the model, MAS inherit physical properties, and therefore the evolution of the computational systems is similar to the evolution of physical systems. To enable implementation of the model, we provide a detailed algorithm to be used by a single agent within the system. The model and the algorithm are a...
Agents
5541
Exploiting Schema Knowledge for the Integration of Heterogeneous Sources . Information sharing from multiple heterogeneous sources is a challenging issue which ranges from database to ontology areas. In this paper, we propose an intelligent approach to information integration which takes into account semantic conflicts and contradictions, caused by the lack of a common shared ontology. Our goal is to provide an  integrated access to information sources, allowing a user to pose a single query and to receive a single unified answer. We propose a "semantic" approach to integration where the conceptual schema of each source is provided, adopting a common standard data model and language, and Description Logics plus clustering techniques  are exploited.  Description Logics is used to obtain a semi-automatic generation of a Common Thesaurus (to solve semantic heterogeneities and to derive a common ontology). Clustering techniques are used to build the global schema, i.e. the unified view of the data to be used for query processing.  keywords: Intelligent Informat...
DB
340027
Improving the Performance of High-Energy Physics Analysis through Bitmap Indices Bitmap indices are popular multi-dimensional structures for accessing read-mostly data such as data warehouse (DW) applications, decision support systems (DSS) and on-line analytical processing (OLAP). One of their main strengths is that they provide good performance characteristics for complex adhoc and an efficient combination of multiple index in one query. Considerable research work has been done in the area of finite (and low) attribute cardinalities. However, additional complexity is imposed on the design of bitmap indices for high cardinality or even non-discrete attributes, where different optimisation techniques than the ones proposed so far have to be applied. In this paper we discuss the design and implementation of bitmap indices for High-Energy Physics (HEP) analysis, where the potential search space consists of hundreds of independent dimensions. A single HEP query typically covers 10 to 100 dimensions out of the whole searchs space. In this context we evaluated two different bitmap encoding techniques, namely equality encoding and range encoding. For both methods the number of bit slices (or bitmap vectors) per attribute is a a central optimisation parameter. The paper presents some (first) results for choosing the optimal number of bit slices for multi-dimensional indices with attributes of different value distribution and query selectivity. We believe taht this discussion is not only applicable to HEP but also to DW, DSS and OLAP type problems in general.
DB
lisetti00automatic
Automatic Facial Expression Interpretation: Where Human-Computer Interaction, Artificial Intelligence and Cognitive Science Intersect this paper is to attempt to bring together people, results and questions from these three different disciplines -- HCI, AI, and Cognitive Science -- to explore the potential of building computer interfaces which understand and respond to the richness of the information conveyed in the human face. Until recently, information has been conveyed from the computer to the user mainly via the visual channel, whereas inputs from the user to the computer have been made from the keyboard and pointing devices via the user's motor channel. The recent emergence of multimodal interfaces as our everyday tools might restore a better balance between our physiology and sensory/motor skills, and impact (for the better we hope), the richness of activities we will find ourselves involved in. Given recent progress in user-interface primitives composed of gesture, speech, context and affect, it seems feasible to design environments which do not impose themselves as computer environments, but have a much more natural feeling associated with them.
ML
383150
Logical Structure Detection for Heterogeneous Document Classes We present a fully implemented system based on generic document knowledge for detecting the logical structure of documents for which only general layout information is assumed. In particular, we focus on detecting the reading order. Our system integrates components based on computer vision, articial intelligence, and natural language processing techniques. The prominent feature of our framework is its ability to handle documents from heterogeneous collections. The system has been evaluated on a standard collection of documents to measure the quality of the reading order detection. Experimental results for each component and the system as a whole are presented and discussed in detail. The performance of the system is promising, especially when considering the diversity of the document collection.  Keywords: Document Analysis, Logical Structure Detection, Reading Order Detection, Natural Language Processing, Spatial Reasoning.  1. INTRODUCTION  The goal of document analysis is to automa...
IR
dreilinger96information
An Information Gathering Agent for Querying Web Search Engines Information gathering agents have attracted much attention of late. As a new application, they are attractive because the need for intelligent assistance in navigating the World Wide Web and large databases is acute. Information agents provide an open-ended and complex, yet easily accessible environment in which ideas from many areas can be integrated. We have developed an information gathering agent called SavvySearch for intelligently searching multiple search engines on the Web. SavvySearch tracks responses from existing search engines to manage resource usage and submit queries only to the most appropriate search engines. To implement SavvySearch, we adapted simple ideas from machine learning, information retrieval and planning and tested two issues in the designs: Can search engine selection knowledge be acquired to improve performance? Do users find that high quality results are being returned early within the limited parallelism provided by SavvySearch? Current results indicate ...
IR
346889
Yarrow: A Real-Time Client Side Meta-Search Learner In this paper we report our research on building Yarrow - an intelligent web meta-search engine. The predominant feature of Yarrow is that in contrast to the lack of adaptive learning features in existing metasearch engines, Yarrow is equipped with a practically efficient on-line learning algorithm so that it is capable of helping the user to search for the desired documents with as little feedback as possible. Currently, Yarrow can query eight of the most popular search engines and is able to perform document parsing and indexing, and learning in real-time on client side. Its architecture and performance are also discussed. 1. Introduction As the world wide web evolves and grows so rapidly, web search, an interface between the human users and the vast information gold mine of the web, is becoming a necessary part of people's daily life. Designing and implementing practically effective web search tools is a challenging task. It calls for innovative methods and strategies f...
IR
baker00hallucinating
Hallucinating Faces In most surveillance scenarios there is a large distance between the camera and the objects of interest in the scene. Surveillance cameras are also usually set up with wide fields of view in order to image as much of the scene as possible. The end result is that the objects in the scene normally appear very small in surveillance imagery. It is generally possible to detect and track the objects in the scene, however, for tasks such as automatic face recognition and license plate reading, resolution enhancement techniques are often needed. Although numerous resolution enhancement algorithms have been proposed in the literature, most of them are limited by the fact that they make weak, if any, assumptions about the scene. We propose an algorithm that can be used to learn a prior on the spatial distribution of the image gradient for frontal images of faces. We proceed to show how such a prior can be incorporated into a super-resolution algorithm to yield 4-8 fold improvements in resolution #...
ML
538003
Two Views of Classifier Systems This work suggests two ways of looking at Michigan classifier systems; as Genetic Algorithm-based systems, and as Reinforcement Learning-based systems, and argues that the former is more suitable for traditional strength-based systems while the latter is more suitable for accuracy-based XCS. The dissociation of the Genetic Algorithm from policy determination in XCS is noted, and the two types of Michigan classifier system are contrasted with Pittsburgh systems.
ML
tan01trust
Trust Relationships in a Mobile Agent System . The notion of trust is presented as an important component  in a security infrastructure for mobile agents. A trust model that can  be used in tackling the aspect of protecting mobile agents from hostile  platforms is proposed. We dene several trust relationships in our model,  and present a trust derivation algorithm that can be used to infer new  relationships from existing ones. An example of how such a model can  be utilized in a practical system is provided.  1 
Agents
turner00improving
Improving the Scalability of Multi-agent Systems . There is an increasing demand for designers and developers  to construct ever larger multi-agent systems. Such systems will be composed  of hundreds or even thousands of autonomous agents. Moreover,  in open and dynamic environments, the number of agents in the system  at any one time will uctuate signicantly. To cope with these twin  issues of scalability and variable numbers, we hypothesize that multiagent  systems need to be both self-building (able to determine the most  appropriate organizational structure for the system by themselves at runtime)  and adaptive (able to change this structure as their environment  changes). To evaluate this hypothesis we have implemented such a multiagent  system and have applied it to the domain of automated trading.  Preliminary results supporting the rst part of this hypothesis are presented:  adaption and self-organization do indeed make the system better  able to cope with large numbers of agents.  1 Introduction  When designing or buildin...
Agents
460643
Consensus-Based Methods Applied to the Intelligent User Interface Development In today's world properly designed user interfaces are becoming crucial for every information systems. Population of users is very differentiated, so it is almost impossible to design for every information system a single, equally appropriate, user interface for each user. Instead, we postulate to construct adoptive interfaces that take into account experiences of all the population to build users profile by means of consensus methods. Keywords: intelligent user interface, user profile, consensus method 1
HCI
billinghurst98wearable
A Wearable Spatial Conferencing Space Wearable computers provide constant access to computing and communications resources. In this paper we describe how the computing power of wearables can be used to provide spatialized 3D graphics and audio cues to aid communication. The result is a wearable augmented reality communication space with audio enabled avatars of the remote collaborators surrounding the user. The user can use natural head motions to attend to the remote collaborators, can communicate freely while being aware of other side conversations and can move through the communication space. In this way the conferencing space can support dozens of simultaneous users. Informal user studies suggest that wearable communication spaces may offer several advantages, both through the increase in the amount of information it is possible to access and the naturalness of the interface.  1: Introduction  One of the broad trends emerging in human-computer interaction is the increasing portability of computing and communication fac...
HCI
499374
A Software Fault Tree Approach to Requirements Analysis of an Intrusion Detection System Requirements analysis for an Intrusion Detection System (IDS) involves deriving requirements for the IDS from analysis of the intrusion domain. When the IDS is, as here, a collection of mobile agents that detect, classify, and correlate system and network activities, the derived requirements include what activities the agent software should monitor, what intrusion characteristics the agents should correlate, where the IDS agents should be placed to feasibly detect the intrusions, and what countermeasures the software should initiate. This paper describes the use of software fault trees for requirements identification and analysis in an IDS. Intrusions are divided into seven stages (following Ruiu), and a fault subtree is developed to model each of the seven stages (reconnaissance, penetration, etc.). Two examples are provided. This approach was found to support requirements evolution (as new intrusions were identified), incremental development of the IDS, and prioritization of countermeasures.
Agents
biskup00constraints
Constraints in Object-Oriented Databases Normal forms in relational database theory, like 3NF or BCNF, are dened by  means of semantic contraints. Since for these constraints sound and complete axiomatisations  exist and, additionally, for some of these constraints the implication  problem is decidable, computer aided database design is possible for relational  data models. Object-oriented database theory lacks such normal forms, partly  because neither a classication of semantic constraints nor sound and complete  axiomatisations exist.  In this work we present three classes of semantic constraints for object-oriented  data models and show that these constraints have a sound and complete axiomatisation.  Thus we prepare the grounds for normal forms in object-oriented data  models and subsequently for computer aided object-oriented database design.  1 Introduction  The theory of database design for relational data models identies a number of properties to characterise good database schemas. These properties lead then to no...
DB
105684
Bargaining with Deadlines This paper analyzes automated distributive negotiation where agents have firm deadlines that are private information. The agents are allowed to make and accept offers in any order in continuous time. We show that the only sequential equilibrium outcome is one where the agents wait until the first deadline, at which point that agent concedes everything to the other. This holds for pure and mixed strategies. So,    This material is based upon work supported by the National Science Foundation under CAREER Award IRI-9703122, Grant IRI-9610122, and Grant IIS-9800994.  y  This is based upon work supported by the EPSRC Award GR M07052.  interestingly, rational agents can never agree to a nontrivial split because offers signal enough weakness of bargaining power (early deadline) so that the recipient should never accept. Similarly, the offerer knows that it offered too much if the offer gets accepted: the offerer could have done better by out-waiting the opponent. In most cases, the deadline ...
Agents
boncz98flattening
Flattening an Object Algebra to Provide Performance Algebraic transformation and optimization techniques have been the method of choice in relational query execution, but applying them in OODBMS has been difficult due to the complexity of object-oriented query languages. This paper demonstrates that the problem can be simplified by mapping a complex storage model to the flat binary model implemented by Monet, a state-of-theart database kernel. We present a generic mapping scheme to flatten data models and study the case of a straightforward object-oriented model. We show how flattening enabled us to implement a full-fledged query algebra on it, using only a very limited set of simple operations. The required primitives and query execution strategies are discussed, and their performance is evaluated on the 1GB TPC-D benchmark, showing that our divide-and-conquer approach yields excellent results. 1 Introduction  During the last decade, relational database technology has grown towards industrial maturity, and the attention of the research...
DB
ali98implementing
Implementing Schema-theoretic Models of Animal Behavior in Robotic Systems Formal models of animal sensorimotor behavior can provide effective methods for generating robotic intelligence. In this paper we describe how schema-theoretic models of the praying mantis are implemented on a hexapod robot equipped with a real-time color vision system. The model upon which the implementation is based was developed by ethologists studying mantids. This implementation incorporates a wide range of behaviors, including obstacle avoidance, prey acquisition, predator avoidance, mating, and chantlitaxia behaviors. 1 Introduction  Ecological robotics refers to incorporating aspects of the relationship a robot maintains with its environment into its control system (i.e., its ecology) [4]. One means for developing such a control system is by exploiting models of behavior developed by ethologists or neuroscientists. Although considerable research has been conducted in the modeling of neural controllers based on animal models (e.g., [3, 5, 14]), incorporation of environmental int...
AI
263737
ROL2: A Real Deductive Object-Oriented Database Language . ROL is a strongly typed deductive object-oriented database  language. It integrates many important features of deductive databases  and object-oriented databases. However, it is only a structurally objectoriented  language. In this paper, we describe our extension of ROL called  ROL2. ROL2 keeps all the important features of ROL. In addition, it incorporates  important behaviorally object-oriented features such as rulebased  methods and encapsulation so that it is a real deductive objectoriented  database language. It supports object identity, complex objects,  class hierarchy, methods, non-monotonic multiple structural and behavioral  inheritance with overriding and blocking.  1 Introduction  In the past decade, a number of deductive object-oriented database languages have been proposed, such as O-logic [21], revised O-logic [15], IQL [1], LOGRES [7], Datalog  meth  [2], CORAL++[26], Gulog [10], Rock & Roll [3] Flogic [14], and ROL [18, 19]. However, most of them are only structural...
DB
voncollani99neurofuzzy
A Neuro-Fuzzy Solution for Integrated Visual and Force Control In this paper the use of a B-spline neuro-fuzzy model for different tasks such as vision-based fine-positioning using uncalibrated cameras and force control is presented. It is shown that neuro-fuzzy controllers can be used not only for low-dimensional problems like force control but also for high-dimensional problems like vision-based sensorimotor control and for fusing input from different sensors. Controllers of this type can be modularly combined to solve a given assembly problem.  1 Introduction  It is well-known that general fuzzy rule descriptions of systems with a large number of input variables suffer from the problem of the "curse of dimensionality." In many realworld applications it is difficult to identify the decisive input parameters and thus to reduce the number of input variables to the minimum. A general solution to building fuzzy models is not only interesting from a theoretical point, it may also extend the range of applications of fuzzy control to more complex intel...
ML
liechti99nonobtrusive
A Non-obtrusive User Interface for Increasing Social Awareness on the World Wide Web Arguing for the need of increasing social awareness on the World Wide Web, we describe a user interface based on the metaphor of windows bridging electronic and physical spaces. We present a system that, with the aim of making on-line activity perceptible in the physical world, makes it possible to hear people visiting one's Web site. The system takes advantage of the seamless and continuous network connection offered by handheld Web-appliances such as PDA's.
IR
261862
The Morph Node We discuss potential and limitations of a Morph Node, inspired by the corresponding construct in Java3D. A Morph Node in Java3D interpolates vertex attributes among several homeomorphic geometries. This node is a promising candidate for the delivery of 3D animation in a very compact form. We review the state-of-the-art in Web 3D techniques, allowing for the possibility of interpolating among several geometries. This review leads to a simple extension for VRML-97 as well as a recommendation for necessary changes in Java3D. Furthermore, we discuss various optimization issues for Morph Nodes.  CR Categories and Subject Descriptors. I.3.6 [Computer Graphics ] Methodology and Techniques: Standards - VRML; I.3.7 [Computer Graphics] Three Dimensional Graphics and Realism: Animation; I.3.8 [Computer Graphics] Applications.  Additional Keywords. Animation, Avatars, Morphing, Virtual Humans, VRML.  INTRODUCTION  Animation of three-dimensional shapes involves the change of vertex attributes over ...
HCI
518526
HYSSOP: Natural Language Generation Meets Knowledge Discovery in Databases. In this paper, we present HYSSOP, a system that generates natural language hypertext summaries of insights resulting from a knowledge discovery process. We discuss the synergy between the two technologies underlying HYSSOP: Natural Language Generation (NLG) and Knowledge Discovery in Databases (KDD). We first highlight the advantages of natural language hypertext as a summarization medium for KDD results, showing the gains that it provides over charts and tables in terms of conciseness, expressive versatility and ease of interpretation for decision makers. Second, we highlight how KDD technologies, and in particular OLAP and data mining, can implement key tasks of automated natural language data summary generation, in a more domain-independent and scalable way than the human written heuristic rule approach of previous systems.
DB
bennett00combinations
Combinations of Modal Logics  Combining logics for modelling purposes has become a rapidly expanding enterprise that is inspired mainly by concerns about modularity and the wish to join together different kinds of information. As any interesting real world system is a complex, composite entity, decomposing its descriptive requirements (for design, verification, or maintenance purposes) into simpler, more restricted, reasoning tasks is not only appealing but is often the only plausible way forward. It would be an exaggeration to claim that we currently have a thorough understanding of `combined methods.' However, a core body of notions, questions and results has emerged for an important class of combined logics, and we are beginning to understand how this core theory behaves when it is applied outside this particular class.  In this paper we will consider the combination of modal (including temporal) logics, identifying leading edge research that we, and others, have carried out. Such combined sys
Agents
gullickson98using
Using Experience to Guide Web Server Selection We examine the use of the anycasting communication paradigm to improve client performance when accessing replicated multimedia objects. Anycasting supports dynamic selection of a server amongst a group of servers that provide equivalent content. If the selection is done well, the client will experience improved performance. A key issue in anycasting is the method used to maintain performance information used in server selection. We explore using past performance or experience to predict future performance. We conduct our work in the context of a customized web prefetching application called WebSnatcher. We examine a variety of algorithms for selecting a server using past performance and find that the overall average and weighted average algorithms are closest to optimal performance. In addition to the WebSnatcher application, this work has implications for responsible network behavior by other applications that generate network traffic automatically. By using the techniques we present ...
IR
474167
A multi-agent system for advising and monitoring students navigating instructional Web sites A growing community of teachers, at all levels of the educational system, provides course material in the form of hypertext/multimedia documents. In most cases this is done by creating a course Web site. This paper explores the issues related to the design of software  systems that aid teachers in monitoring how students use their sites and proactively advise students navigating the sites. In connection to these functions two important topics in current applications of technology to education are discussed. Firstly the definition of a set of criteria allowing the evaluation of the appropriateness of multi-media and hypertext technologies vis vis to classic course support  material and in particular textbooks. Secondly the issue of the utility and acceptability of proactive user interfaces such as interface agents or personal assistant agents. A multi agent system capable of advising and monitoring students navigating instructional Web sites is introduced and it is used as a basis for discussion of the above two topics. The system generates and uses a set of indicators evaluating how much use is made of hypertext and multimedia tools as well as indicators of usefulness and cognitive support of the proactive user interface.  Keywords: Tutoring systems, Multi agent systems, World Wide Web, Autonomous Interface Agents, Digital Course Material, XML.  1. 
Agents
greiff98theory
A Theory of Term Weighting Based on Exploratory Data Analysis Techniques of exploratory data analysis are used to study the weight of evidence that the occurrence of a query term provides in support of the hypothesis that a document is relevant to an information need. In particular, the relationship between the document frequency and the weight of evidence is investigated. A correlation between document frequency normalized by collection size and the mutual information between relevance and term occurrence is uncovered. This correlation is found to be robust across a variety of query sets and document collections. Based on this relationship, a theoretical explanation of the efficacy of inverse document frequency for term weighting is developed which differs in both style and content from theories previously put forth. The theory predicts that a "flattening" of idf at both low and high frequency should result in improved retrieval performance. This altered idf formulation is tested on all TREC query sets. Retrieval results corroborate the predicti...
IR
lyons00guided
Guided by Voices: An Audio Augmented Reality System This paper presents an application of a low cost, lightweight audio-only augmented reality infrastructure. The system uses a simple wearable computer and a RF based location system to play digital sounds corresponding to the user's location and current state. Using this infrastructure we implemented a game in the fantasy genre where players move around in the real world and trigger actions in the virtual game world. We present some of the issues involved in creating audio-only augmented reality games and show how our location infrastructure is generalizable to other audio augmented realities.  Keywords  Audio, augmented reality, wearable computing, context-awareness  INTRODUCTION  This paper presents a lightweight and inexpensive infrastructure for augmented realities that uses a simple wearable computer. Whereas most traditional augmented reality systems overlay graphics onto the user's environment, this system employs only audio. Furthermore, we have created a positioning infrastruct...
HCI
hoste00rule
A Rule Induction Approach to Modeling Regional Pronunciation Variation. This 1)~q)er descril)es the use of rule indue-tion techniques fi)r the mli;omatic exl;ra(:l;ion of l)honemic knowledge mM rules fl'om pairs of l:,romm(:intion lexi(:a. This (:xtra(:ted knowl-edge allows the ndat)tntion of sl)ee(:h pro(:ess-ing systelns tO regional vm'iants of a language. As a case sl;u(ty, we apply the approach to Northern Dutch and Flemish (the wtriant of Dutch spoken in Flan(lers, a t)art; of Bel-gium), based Oll C(?lex and l'bnilex, promm-clarion lexi(:a tbr Norttmrn l)utch mM Fhm,-ish, r(~sl)e(:tively. In our study, we (:omt)ar(~ l;wo rule ilMu(:tion techniques, franslbrmation-B;tsed Error-l)riven Learning ('I'I/E])I,) (Brill, 1995) mM C5.0 (Quinl~m, 1993), and (,valu-ate the extr~tct(xl knowh;dge quanl:it~l;ively (a(:-(:ura.cy) mM qualitatively (linguistic r(;levanc:e of the rules). We (:onchMe that. whereas classificntion-1)ased rule. induct;ion with C5.0 is 11101.'0 a(;(:(lr&l;e ~ th(? |;rallSt~)rnl;~l;ion l&quot;ules le;~rne(t with TBE1)I, can 1)e more easily ini;ert)reted. 1.
ML
dautenhahn99bringing
Bringing up Robots or - The Psychology of Socially Intelligent Robots: From Theory to Implementation We discuss robotic experiments in a framework based on theories in developmental psychology. 1 Introduction  Piaget's theory of cognitive development has strongly influenced many approaches in artificial intelligence and agent research. His theory has been challenged from various directions, and recent experiments have confirmed Vygotsky's belief in the essential role of social interaction and teaching as a scaffolding mechanism which is important for the child in order to reach higher levels of competence and control based on current skills. Hereby concepts are not taught directly but through social interaction, the child's experiences are re-arranged, a shared understanding develops between the child and its interaction partner. A Piagetean viewpoint sees language as a product of the cognitive development of mental representations, while Vygotsky believes that the sole primary function of language is communication with peers and adults, and that language develops exactly in this cont...
AI
argamon98routing
Routing Documents According to Style Most research on automated text categorization has focused on determining the topic of a given text. While topic is generally the main characteristic of an information need, there are other characteristics that are useful for information retrieval. In this paper we consider the problem of text categorization according to style. For example, in searching the web, we may wish to automatically determine if a given page is promotional or informative, was written by a native English speaker or not, and so on. Learning to determine the style of a document is a dual to that of determining its topic, in that those document features which capture the style of a document are precisely those which are independent of its topic. We here define the features of a document to be the frequencies of each of a set of function words and parts-of-speech triples. We then use machine learning techniques to classify documents. We test our methods on four collections of downloaded newspaper and magazine articl...
IR
35592
Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods Abstract. One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated hypothesis usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik’s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition. 1
ML
ordonez01mining
Mining Constrained Association Rules to Predict Heart Disease This work describes our experiences on discovering association rules in medical data to predict heart disease. We focus on two aspects in this work: mapping medical data to a transaction format suitable for mining association rules and identifying useful constraints. Based on these aspects we introduce an improved algorithm to discover constrained association rules. We present an experimental section explaining several interesting discovered rules.  1. 
AI
svidzinska01world
A World Wide Web Meta Search Engine Using an Automatic Query Routing Algorithm CONTENTS 1. INTRODUCTION......................................................... ........................................ 6 2. LITERATURE REVIEW............................................................... ........................ 9 2.1 Overview of conventional search techniques............................................. 9 2.2 Conventional query routing systems......................................................... 11 2.2.1 Manual query routing services......................................................... 11 2.2.2 Automated query routing systems based on centroids..................... 12 2.2.3 Automated query routing systems without centroids....................... 12 3. SYSTEM STRUCTURE............................................................ .......................... 14 3.1 System overview............................................................. .......................... 14 3.2 Off-line operations.......................................................
IR
duch98optimization
Optimization and Global Minimization Methods Suitable for Neural Networks Neural networks are usually trained using local, gradient-based procedures. Such methods frequently find suboptimal solutions being trapped in local minima. Optimization of neural structures and global minimization methods applied to network cost functions have strong influence on all aspects of network performance. Recently genetic algorithms are frequently combined with neural methods to select best architectures and avoid drawbacks of local minimization methods. Many other global minimization methods are suitable for that purpose, although they are used rather rarely in this context. This paper provides a survey of such global methods, including some aspects of genetic algorithms. CONTENTS 1 Introduction 2 2 Monte Carlo and its improvements 4 3 Simulated annealing and its variants 6 3.1 Adaptive Simulated Annealing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.2 Alopex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....
ML
tourapis01advanced
Advanced DeInterlacing techniques with the use of Zonal Based Algorithms This paper describes a new highly efficient aleinterlacing approach based on motion estimation and compensation techniques. The proposed technique mainly benefits from the motion vector properties of zonal based algorithms, such as the Advanced Predictive Diamond Zonal Search (APDZS) and the Predictive Motion Vector Field Adaptive Search Technique (PMVFAST), multihypothesis motion compensation, but also an additional motion classification phase where, depending on the motion of a pixel, additional spatial and temporal information is also considered to further improve performance. Extensive simulations demonstrate the efficacy of these algorithms, especially when compared to standard deinterlacing techniques such as the line doubling and line averaging algorithms.
ML
488749
LivingLab: A white paper The LivingLab is a planned research infrastructure that is pivotal for user-system interaction research in the next decade. This article presents the concept and outlines a research programme that will be served by this facility. These future plans are motivated by a vision of future developments concerning interaction with intelligent environments.
HCI
tenhagen01continuous
Continuous State Space Q-Learning for Control of Nonlinear Systems Contents 1 Introduction 1 1.1 Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1.1 Designing the state feedback controller . . . . . . . . . . . . . . . . 3 1.1.2 Unknown systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.3 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.4 Overview of this Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2 Reinforcement Learning 11 2.1 A Discrete Deterministic Optimal Control Task . . . . . . . . . . . . . . . 11 2.1.1 The problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.1.2 The solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.2 The Stochastic Optimization Tasks . . . . . . . . . . . . . . . . . . . . . . 13 2.2.1 The Markov Decision Process . . . . . . . . . . . . . . . . . . .
ML
deloach02analysis
Analysis and Design of Multiagent Systems Using Hybrid Coordination Media Over the last few years, two advances in agent-oriented  software engineering have had a significant impact. The first is  the identification of interaction and coordination as the central  focus of multiagent systems design and the second is the   realization that the multiagent organization is distinct from the  individual agents that populate the system. Also, the evolution  of new, more powerful hybrid coordination models, which  combine data-centered and control-centered coordination  approaches, have given us the capability to model and  implement the rules that govern organizations independently   from the individual agents in the system. This paper  investigates how to combine the power of these hybrid  coordination capabilities with the concept of organizational   rules using traditional conversation-based approaches to  designing multiagent systems.   1. 
Agents
hespanha99multipleagent
Multiple-Agent Probabilistic Pursuit-Evasion Games In this paper we develop a probabilistic framework for pursuit-evasion games. We propose a "greedy" policy to control a swarm of autonomous agents in the pursuit of one or several evaders. At each instant of time this policy directs the pursuers to the locations that maximize the probability of finding an evader at that particular time instant. It is shown that, under mild assumptions, this policy guarantees that an evader is found in finite time and that the expected time needed to find the evader is also finite. Simulations are included to illustrate the results. 1 Introduction  This paper addresses the problem of controlling a swarm of autonomous agents in the pursuit of one or several evaders. To this effect we develop a probabilistic framework for pursuit-evasion games involving multiple agents. The problem is nondeterministic because the motions of the pursuers/evaders and the devices they use to sense their surroundings require probabilistic models. It is also assumed that when ...
Agents
367649
Let's Browse: A Collaborative Browsing Agent Web browsing, like most of today's desktop applications, is usually a solitary activity. Other forms of media, such as watching television, are often done by groups of people, such as families or friends. What would it be like to do collaborative Web browsing? Could the computer provide assistance to group browsing by trying to help find mutual interests among the participants? Let's Browse is an experiment in building an agent to assist a group of people in browsing, by suggesting new material likely to be of common interest. It is built as an extension to the single user Web browsing agent Letizia. Let's Browse features automatic detection of the presence of users, automated "channel surfing" browsing, and dynamic display of the user profiles and explanation of recommendations. # 1999 Elsevier Science B.V. All rights reserved.  Keywords: Browsing; Collaboration; Agents; User profiles  1. Collaborative browsing  Increasingly, Web browsing will be performed in collaborative  settings, ...
HCI
milo00typechecking
Typechecking for XML Transformers We study the typechecking problem for XML transformers: given an XML transformation program and a DTD for the input XML documents, check whether every result of the program conforms to a specified output DTD. We model XML transformers using a novel device called a k- pebble transducer, that can express most queries without data-value joins in XML-QL, XSLT, and other XML query languages. Types are modeled by regular tree languages, a robust extension of DTDs. The main result of the paper is that typechecking for k-pebble transducers is decidable. Consequently, typechecking can be performed for a broad range of XML transformation languages, including XMLQL and a fragment of XSLT.  1. INTRODUCTION  Traditionally, database query languages have focused on data retrieval, with complex data transformations left to applications. The new XML data exchange standard for the Web, and emerging applications requiring data wrapping and integration, have shifted the focus towards data transformations....
DB
huntsberger98bismarc
BISMARC: A Biologically Inspired System for Map-based Autonomous Rover Control As the complexity of the missions to planetary surfaces increases, so too does the need for autonomous rover systems. This need is complicated by the power, mass and computer storage restrictions on such systems (Miller, 1992). To address these problems, we have recently developed a system called BISMARC (Biologically Inspired System for Map-based Autonomous Rover Control) for planetary missions involving multiple small, lightweight surface rovers (Huntsberger, 1997). BISMARC is capable of cooperative planetary surface retrieval operations such as a multiple cache recovery mission to Mars. The system employs autonomous navigation techniques, behavior-based control for surface retrieval operations, and an action selection mechanism based on a modified form of free flow hierarchy (Rosenblatt and Payton, 1989). This paper primarily describes the navigation and map-mapping subsystems of BISMARC. They are inspired by some recent studies of London taxi drivers indicating that the right hippo...
AI
lenzmann97contractnetbased
Contract-Net-Based Learning in a User-Adaptive Interface Agency . This paper describes a multi-agent learning approach to adaptation to users' preferences realized by an interface agency. Using a contract-net-based negotiation technique, agents as contractors as well as managers negotiate with each other to pursue the overall goal of dynamic user adaptation. By learning from indirect user feedback, the adjustment of internal credit vectors and the assignment of contractors that gained maximal credit with respect to the user's current preferences, the preceding session, and current situational circumstances can be realized. In this way, user adaptation is achieved without accumulating explicit user models but by the use of implicit, distributed user models. 1 Introduction Interface agents are computer programs that enhance the human-computer interaction by mediating a relationship between technical systems and users [Lau90]. On the one hand, they provide assistance to users by acting on his/her behalf and automating his/her actions [Nor94...
HCI
leuschel98creating
Creating Specialised Integrity Checks Through Partial Evaluation Of Meta-Interpreters Interpretation". Danny De Schreye is senior research associate of the Belgian National Fund for Scientific Research.  We would like to thank Bern Martens for proof-reading several versions of this paper and for his helpful insights and comments on the topic of this paper. We would also like to thank him for his huge pile of references on integrity checking and for introducing the first author to the subject. We thank Bart Demoen for sharing his expertise on writing efficient Prolog programs. Our thanks also go to John Gallagher for pointing out several errors in an earlier version of the paper and for the fruitful discussions on partial evaluation and integrity checking. We are also grateful for discussions and extremely helpful comments by Hendrik Decker. Other interesting discussions about the topic of this paper were held with Stefan Decker, the members of the Compulog II project as well as with the participants of the 1996 Dagstuhl seminar on "Logic and the Meaning of Change". Fina...
DB
131669
On the Correspondence between Neural Folding Architectures and Tree Automata The folding architecture together with adequate supervised training algorithms is a special recurrent neural network model designed to solve inductive inference tasks on structured domains. Recently, the generic architecture has been proven as a universal approximator of mappings from rooted labeled ordered trees to real vector spaces. In this article we explore formal correspondences to the automata (language) theory in order to characterize the computational power (representational capabilities) of different instances of the generic folding architecture. As the main result we prove that simple instances of the folding architecture have the computational power of at least the class of deterministic bottom-up tree automata. It is shown how architectural constraints like the number of layers, the type of the activation functions (first-order vs. higher-order) and the transfer functions (threshold vs. sigmoid) influence the representational capabilities. All proofs are carried out in a c...
ML
matzinger98computational
On Computational Representations of Herbrand Models . Finding computationally valuable representations of models of predicate logic formulas is an important issue in the field of automated theorem proving, e.g. for automated model building or semantic resolution. In this article we treat the problem of representing single models independently of building them and discuss the power of different mechanisms for this purpose. We start with investigating context-free languages for representing single Herbrand models. We show their computational feasibility and prove their expressive power to be exactly the finite models. We show an equivalence with "ground atoms and ground equations" concluding equal expressive power. Finally we indicate how various other well known techniques could be used for representing essentially infinite models (i.e. models of not finitely controllable formulas), thus motivating our interest in relating model properties with syntactical properties of corresponding Herbrand models and in investigating connections betwe...
DB
baltsavias00integrating
Integrating Spatial Information And Image Analysis - One Plus One Makes Ten Photogrammetry and remote sensing have proven their efficiency for spatial data collection in many ways. Interactive mapping at digital workstations is performed by skilled operators, which guarantees excellent quality in particular of the geometric data. In this way, worldwide acquisition of a large number of national GIS databases has been supported and still a lot of production effort is devoted to this task. In the field of image analysis, it has become evident that algorithms for scene interpretation and 3D reconstruction of topographic objects, which rely on a single data source, cannot function efficiently. Research in two directions promises to be more successful. Multiple, largely complementary, sensor data like range data from laser scanners, SAR and panchromatic or multi-/hyper-spectral aerial images have been used to achieve robustness and better performance in image analysis. On the other hand, given GIS databases, e.g. layers from topographic maps, can be considered as vi...
DB
483730
QUEST - Querying specialized collections on the Web Ensuring access to specialized web-collections in a fast evolving web  environment requires flexible techniques for orientation and querying. The adoption  of meta search techniques for web-collections is hindered by the enormous  heterogeneity of the resources. In this paper we introduce QUEST --- a system  for querying specialized collections on the web.
IR
frank99naive
Naive Bayes for Regression Abstract. Despite its simplicity, the naive Bayes learning scheme performs well on most classification tasks, and is often significantly more accurate than more sophisticated methods. Although the probability estimates that it produces can be inaccurate, it often assigns maximum probability to the correct class. This suggests that its good performance might be restricted to situations where the output is categorical. It is therefore interesting to see how it performs in domains where the predicted value is numeric, because in this case, predictions are more sensitive to inaccurate probability estimates. This paper shows how to apply the naive Bayes methodology to numeric prediction (i.e., regression) tasks by modeling the probability distribution of the target value with kernel density estimators, and compares it to linear regression, locally weighted linear regression, and a method that produces “model trees”—decision trees with linear regression functions at the leaves. Although we exhibit an artificial dataset for which naive Bayes is the method of choice, on real-world datasets it is almost uniformly worse than locally weighted linear regression and model trees. The comparison with linear regression depends on the error measure: for one measure naive Bayes performs similarly, while for another it is worse. We also show that standard naive Bayes applied to regression problems by discretizing the target value performs similarly badly. We then present empirical evidence that isolates naive Bayes ’ independence assumption as the culprit for its poor performance in the regression setting. These results indicate that the simplistic statistical assumption that naive Bayes makes is indeed more restrictive for regression than for classification.
ML
ashri00paradigma
Paradigma: Agent Implementation through Jini One of the key problems of recent years has been the divide between theoretical work in agent-based systems and its practical complement which have, to a large extent, developed along different paths. The Paradigma implementation framework has been designed with the aim of narrowing this gap. It relies on an extensive formal agent framework implemented using recent advances in Java technology. Specifically, Paradigma uses Jini connectivity technology to enable the creation of on-line communities in support of the development of agent-based systems.  1 Introduction  In a networked environment that is highly interconnected, interdependent and heterogeneous, we are faced with an explosion of information and available services that are increasingly hard to manage. Agent-based systems can provide solutions to these problems as a consequence of their dynamics of social interaction; communication and cooperation can be used to effectively model problem domains through the interaction of agent...
AI
goasdoue00rewriting
Rewriting Conjunctive Queries Using Views in Description Logics with Existential Restrictions this paper, extending the work of [8], we study the problem of rewriting conjunctive queries over DL expressions into conjunctive queries using a set of views that are a set of distinguished DL expressions, for three DLs allowing existential restrictions: FLE , ALE and ALNE . Thus, our rewriting problem is: given a conjunctive query over expressions from a DL L 2 fFLE;ALE ; ALNEg and a set of views V over expressions from L, we want to compute a representative set of all the rewritings of the query that are conjunctive queries over V. By representative set we mean that this set contains at least the rewritings that are maximally contained in the query.
DB
davison98applying
Applying Parallelism to Improve Genetic Algorithm-based Design Optimization Introduction  The abundance of powerful workstations makes course-grained parallelization an obvious enhancement to many optimization techniques, including genetic algorithms [Gol89, DM97]. While initial modifications have been made to GADO (Genetic Algorithm for Design Optimization [Ras98, RHG97]), such changes have not been carefully analyzed for potential impacts on quality. More generally, parallelization has the potential to improve GA performance through the use of alternative models of computation. Parallelism can certainly reduce the total elapsed clock-time for a solution, but as a change in model of computation (either real or simulated) , it can change the number of simulator calls and even make new solutions achievable. The effects of parallelization on GADO were investigated during my summer internship at the Center for Computational Design. 2 Objectives  Since a straightforward parallelized implementation already existed, my first tasks were to ana
ML
79013
Using the CONDENSATION Algorithm for Robust, Vision-based Mobile Robot Localization To navigate reliably in indoor environments, a mobile robot must know where it is. This includes both the ability of globally localizing the robot from scratch, as well as tracking the robot's position once its location is known. Vision has long been advertised as providing a solution to these problems, but we still lack efficient solutions in unmodified environments. Many existing approaches require modification of the environment to function properly, and those that work within unmodified environments seldomly address the problem of global localization.  In this paper we present a novel, vision-based localization method based on the CONDENSATION algorithm [17, 18], a Bayesian filtering method that uses a samplingbased density representation. We show how the CONDEN- SATION algorithm can be used in a novel way to track the position of the camera platform rather than tracking an object in the scene. In addition, it can also be used to globally localize the camera platform, given a visua...
ML
panayiotopoulos98intelligent
An Intelligent Agent Framework In VRML Worlds actions, e.g. move to next room, are received by it and consequently send to the EAC. Finally, the abstract action arrives at the Virtual Reality Management Unit that specifies in detail the received actions. It provides specific values concerning the orientation and position of the avatar, e.g. it specifies the coordinates, orientation and path so that it can successfully move to the next room, and sends them as commands to the Virtual Reality World Browser. The browser executes the command by altering the virtual environment appropriately. When changes have been performed the AEC unit notifies the logical core that the action has been successfully executed and the logical core goes on by updating its internal and external state. Consequently, the agent looks around into the virtual space, gathers any additional information and decides the next step it should take to satisfy its goals.
Agents
467872
Model-Free Least-Squares Policy Iteration We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efficient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly influenced by the visitation distribution over states. Our new algorithm, Least-Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We test LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efficiently by merely observing a relatively small number of completely random trials.
ML
gurret99basis
The BASIS System: a Benchmarking Approach for Spatial Index Structures This paper describes the design of the BASIS prototype system, which is currently under implementation. BASIS stands for Benchmarking Approach for Spatial Index Structures. It is a prototype system aiming at performance evaluation of spatial access methods and query processing strategies, under different data sets, various query types, and different workloads. BASIS is based on a modular architecture, composed of a simple storage manager, a query processor, and a set of algorithmic techniques to facilitate benchmarking. The main objective of BASIS is twofold: (i) to provide a benchmarking environment for spatial access methods and related query evaluation techniques, and (ii) to allow comparative studies of spatial access methods in different cases but under a common framework. We currently extend it to support the fundamental features of spatiotemporal data management and access methods.
DB
antifakos01exploration
Exploration of Perceptual Computing for Smart-Its The future success of ubiquitous computing depends to a big part on how well applications can adapt to their environment and act accordingly. This thesis has set itself the goal of exploring perceptual computing for Smart-Its, which is one such ubiquitous computing vision.
HCI
barbosa01tox
ToX - The Toronto XML Engine Abstract. We present ToX – the Toronto XML Engine – a repository for XML data and metadata, which supports real and virtual XML documents. Real documents are stored as files or mapped into relational or object databases, depending on their structuredness; indices are defined according to the storage method used. Virtual documents can be remote documents, defined as arbitrary WebOQL queries, or views, defined as queries over documents registered in the system. The system catalog contains metadata for the documents, especially their schemata, used for query processing and optimization. Queries can range over both the catalog and the documents, and multiple query languages are supported. In this paper we describe the architecture and main of ToX; we present our indexing and storage strategies, including two novel techniques; and we discuss our query processing strategy. The project started recently and is under active development. 1
DB
68623
Logic-Based Subsumption Architecture In this paper we describe a logic-based AI architecture based on Brooks'  Subsumption Architecture. We axiomatize each of the layers of control in  his system separately and use independent theorem provers to derive each  layer's output actions given its inputs. We implement the subsumption  of lower layers by higher layers using circumscription. We give formal  semantics to our approach.  1 Introduction  In [?], Brooks proposed a reactive architecture embodying an approach to robot control different on various counts from traditional approaches. He decomposed the problem into layers corresponding to levels of behavior, rather than according to a sequential, functional form. Within this setting he introduced the idea of subsumption, that is, that more complex layers could not only depend on lower, more reactive layers, but could also influence their behavior. The resulting architecture was one that could service simultaneously multiple, potentially conflicting goals in a reactive fashi...
AI
531929
REFEREE: An open framework for practical testing of recommender systems using ResearchIndex Automated recommendation (e.g., personalized product recommendation on an ecommerce web site) is an increasingly valuable service associated with many databases--typically online retail catalogs and web logs. Currently, a major obstacle for evaluating recommendation algorithms is the lack of any standard, public, real-world testbed appropriate for the task. In an attempt to fill this gap, we have created REFEREE, a framework for building recommender systems using ResearchIndex--a huge online digital library of computer science research papers--so that anyone in the research community can develop, deploy, and evaluate recommender systems relatively easily and quickly. Research Index is in many ways ideal for evaluating recommender systems, especially so-called hybrid recommenders that combine information filtering and collaborative filtering techniques. The documents in the database are associated with a wealth of content information (author, title, abstract, full text) and collaborative information (user behaviors), as well as linkage information via the citation structure. Our framework supports more realistic evaluation metrics that assess user buy-in directly, rather than resorting to offline metrics like prediction accuracy that may have little to do with end user utility. The sheer scale of ResearchIndex (over 500,000 documents with thousands of user accesses per hour) will force algorithm designers to make real-world trade-offs that consider performance, not just accuracy. We present our own tradeoff decisions in building an example hybrid recommender called PD-Live. The algorithm uses content-based similarity information to select a set of documents from which to recommend, and collaborative information to rank the documents. PD-Live performs reasonably well compared to other recommenders in ResearchIndex.
IR
386535
Performance Analysis of Mobile Agents for Filtering Data Streams on Wireless Networks Wireless networks are an ideal environment for mobile agents, since their mobility allows them to move across an unreliable link to reside on a wired host, next to or closer to the resources that they need to use. Furthermore, clientspecific data transformations can be moved across the wireless link and run on a wired gateway server, reducing bandwidth demands. In this paper we examine the tradeoffs faced when deciding whether to use mobile agents in a datafiltering application where numerous wireless clients filter information from a large data stream arriving across the wired network. We develop an analytical model and use parameters from filtering experiments conducted during a U.S. Navy Fleet Battle Experiment (FBE) to explore the model's implications.  1. Introduction  Mobile agents are programs that can migrate from host to host in a network of computers, at times and to places of their own choosing. Unlike applets, both the code and the execution state (heap and stack) move with...
Agents
157253
Latent Semantic Indexing: A Probabilistic Analysis Latent semantic indexing (LSI) is an information retrieval technique based on the spectral analysis of the term-document matrix, whose empirical success had heretofore been without rigorous prediction and explanation. We prove that, under certain conditions, LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance. We also propose the technique of random projection as a way of speeding up LSI. We complement our theorems with encouraging experimental results. We also argue that our results may be viewed in a more general framework, as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative filtering.   Computer Science Division, U. C. Berkeley, Berkeley, CA 94720. Email: christos@cs.berkeley.edu.  y  IBM Almaden Research Center, 650 Harry Road, San Jose, CA 95120. Email: pragh@almaden.ibm.com.  z  Computer Science Department, Meiji University, Tokyo, Japan. Email: htamaki@cs.meiji....
DB
dahl99eel
The Eel Programming Language and Internal Concurrency in Logic Agents This paper describes work done on creating the logic programming language  Eel. The language is designed for implementing agents with a behaviour based,  concurrent internal architecture. The paper also suggests a new such architecture  which improves on the ones currently available. It gives examples of how  parts of that architecture are implemented in Eel and comments that the Agent  Oriented Programming paradigm currently contains two different metaphors for  concurrency.  Eel's event based approach to process communication and process initiation  introduces an explicit representation of state to a logic program. A new declarative  approach to object states is demonstrated as a part of the object oriented implementation  of the suggested agent architecture.  As well as being a programming language, Eel is a formalism which is well  suited for logic based machine learning of behaviour and interaction. This paper  briefly outlines the scope for using such learning to improve on exist...
Agents
rui99image
Image Retrieval: Current Techniques, Promising Directions And Open Issues This paper provides a comprehensive survey of the technical achievements in the research area of image retrieval, especially content-based image retrieval, an area that has been so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified and future promising research directions are suggested. C ○ 1999 Academic Press 1.
IR
157013
Multi-Layer Incremental Induction . This paper describes a multi-layer incremental induction algorithm, MLII, which is linked to an existing nonincremental induction algorithm to learn incrementally from noisy data. MLII makes use of three operations: data partitioning, generalization and reduction. Generalization can either learn a set of rules from a (sub)set of examples, or refine a previous set of rules. The latter is achieved through a redescription operation called reduction: from a set of examples and a set of rules, we derive a new set of examples describing the behaviour of the rule set. New rules are extracted from these behavioral examples, and these rules can be seen as meta-rules, as they control previous rules in order to improve their predictive accuracy. Experimental results show that MLII achieves significant improvement on the existing nonincremental algorithm HCV used for experiments in this paper, in terms of rule accuracy. 1 Introduction  Existing machine learning algorithms can be generally distin...
ML
50068
Super Logic Programs Recently, considerable interest and research e#ort has been given to the problem of finding a suitable extension of the logic programming paradigm beyond the class of normal logic programs. In order to demonstrate that a class of programs can be justifiably called an extension of logic programs one should be able to argue that:  .  the proposed syntax of such programs resembles the syntax of logic programs but it applies to a significantly broader class of programs;  .  the proposed semantics of such programs constitutes an intuitively natural extension of the semantics of normal logic programs;  .  there exists a reasonably simple procedural mechanism allowing, at least in principle, to compute the semantics;  .  the proposed class of programs and their semantics is a special case of a more general non-monotonic formalism which clearly links it to other well-established non-monotonic formalisms. In this paper we propose a specific class of extended logic programs which will be (modestly) called super logic programs or just super-programs. We will argue that the class of super-programs satisfies all of the above conditions, and, in addition, is su#ciently flexible to allow various application-dependent extensions and modifications. We also provide a brief description of a Prolog implementation of a query-answering interpreter for the class of super-programs which is available via FTP and WWW.  Keywords: Non-Monotonic Reasoning, Logics of Knowledge and Beliefs, Semantics of Logic Programs and Deductive Databases.  # An extended abstract of this paper appeared in the Proceedings of the Fifth International Conference on Principles of Knowledge Representation and Reasoning (KR'96), Boston, Massachusetts, 1996, pp. 529--541.  + Partially supported by the National Science Fou...
AI
73962
Supporting Flexibility. A Case-Based Reasoning Approach This paper presents a case-based reasoning system  TA3. We address the flexibility of the case-based reasoning process, namely flexible retrieval of relevant experiences, by using a novel similarity assessment theory. To exemplify the advantages of such an approach, we have experimentally evaluated the system and compared its performance to the performance of non-flexible version of TA3 and to other machine learning algorithms on several domains. Introduction  Flexible Computation  There are many situation when resources are scarce and when the system has to make decision as to how to proceed with further computation in order to make the right tradeoff between quality of the answer and resources needed. In several domains, such as medicine, robotics, financing, etc., if the answer to a query is not produced within a certain time limit it may become useless. Such family of problems has been a motivation for designing anytime algorithms (Dean & Boddy 1988; Frisch & Haddawy 1994), algorit...
ML
254693
Indexing Moving Points We propose three indexing schemes for storing a set S of N points in the plane, each moving  along a linear trajectory, so that a query of the following form can be answered quickly: Given  a rectangle R and a real value t q , report all K points of S that lie inside R at time t q . We  first present an indexing structure that, for any given constant " ? 0, uses O(N=B) disk blocks,  where B is the block size, and answers a query in O((N=B)  1=2+"  + K=B) I/Os. It can also  report all the points of S that lie inside R during a given time interval. A point can be inserted  or deleted, or the trajectory of a point can be changed, in O(log  2  B N) I/Os. Next, we present a  general approach that improves the query time if the queries arrive in chronological order, by  allowing the index to evolve over time. We obtain a tradeoff between the query time and the  number of times the index needs to be updated as the points move. We also describe an indexing  scheme in which the number of I/Os required to answer a query depends monotonically on the  difference between t q and the current time. Finally, we develop an efficient indexing scheme to  answer approximate nearest-neighbor queries among moving points.  An extended abstract of this paper appeared in the Proceedings of the 19th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems.  y  Center for Geometric Computing, Department of Computer Science, Duke University, Box 90129, Durham, NC 27708--0129; pankaj@cs.duke.edu; http://www.cs.duke.edu/  pankaj. Supported in part by National Science Foundation grants EIA--9870734, EIA--9972879, and CCR--9732787, by Army Research Of fice MURI grant DAAH04-- 96--1--0013, by a Sloan fellowship, and by a grant from the U.S.-Israeli Binational Science Foundation.  z  Center ...
DB
dejong01multilayer
Multi-Layer Methods and the Optimal Optimizer Multi-Layer Methods are methods that act on several layers simultaneously. Examples of multi-layer methods are found in multi-agent systems (global and per-agent behavior), in learning (e.g. boosting, bias tuning), in self-adaptive methods (such as evolution strategies), in hybrid approaches, and in optimization (e.g. multiple runs, result pooling). We give a formal definition of what a multi-layer method is. We discuss the relationship with the no free lunch theorem, to show that such a thing as the optimal optimizer exists, and how multi-layer methods can be used to approximate it.
Agents
hanson99scalable
Scalable Trigger Processing +  Current database trigger systems have extremely limited scalability. This paper proposes a way to develop a truly scalable trigger system. Scalability to large numbers of triggers is achieved with a trigger cache to use main memory effectively, and a memory-conserving selection predicate index based on the use of unique expression formats called expression signatures. A key observation is that if a very large number of triggers are created, many will have the same structure, except for the appearance of different constant values. When a trigger is created, tuples are added to special relations created for expression signatures to hold the trigger's constants. These tables can be augmented with a database index or main-memory index structure to serve as a predicate index. The design presented also uses a number of types of concurrency to achieve scalability, including token (tuple)-level, condition-level, rule action-level, and datalevel concurrency.  1. Introduction  Trigger feature...
DB
332050
Agent-Based Modeling for Holonic Manufacturing Systems with Fuzzy Control Agent-based systems technologies are of emerging interest in the specification and implementation of complex systems. This article introduces the CASA agent development system which seamlessly combines the BDI (Belief Desire Intention) approach with the FIPA agent communication language standard and an integrated specification of fuzzy controllers. The behavior of agents is defined by strategies which basically correspond to extended guarded horn clauses with priorities. The presented concepts are introduced by an example from Computer Integrated Manufacturing (CIM). The example gives the specification of a fuzzy controller for a manufacturing station in the context of a holonic manufacturing system (HMS).  1 Introduction  Agent-based systems technologies in the sense of distributed computing is an area of emerging interest in the domain of complex systems design. The agent-based paradigm can be seen as a real enhancement of the objectoriented paradigm where objects become autonomous, ...
Agents
hero98robust
Robust Entropy Estimation Strategies Based on Edge Weighted Random Graphs (with corrections) In this paper we treat the problem of robust entropy estimation given a multidimensional random sample from an unknown distribution. In particular, we consider estimation of the Renyi entropy of fractional order which is insensitive to outliers, e.g. high variance contaminating distributions, using the k-point minimal spanning tree (kMST) . A greedy algorithm for approximating the NP-hard problem of computing the k-minimal spanning tree is given which is a generalization of the potential function partitioning method of Ravi etal.  1  The basis for our approach is an asymptotic theorem establishing that the log of the overall length or weight of the greedy approximation is a strongly consistent estimator of the Renyi entropy. Quantitative robustness of the estimator to outliers is established using Hampel's method of inuence functions.  2  The structure of the inuence function indicates that the k-MST is a natural extension of the one dimensional -trimmed mean for multi-dimensional...
ML
bouguettaya99using
Using Java and CORBA for Implementing Internet Databases We describe an architecture called WebFINDIT that allows dynamic couplings of Web accessible databases based on their content and interest. We propose an implementation using WWW, Java, JDBC, and CORBA's ORBs that communicate via the CORBA's IIOP protocol. The combination of these technologies offers a compelling middleware infrastructure to implement wide-area enterprise applications. In addition to a discussion of WebFINDIT's core concepts and implementation architecture, we also discuss an experience of using WebFINDIT in a healthcare application.  1 Introduction  The growth of the Internet and the Web increased dramatically the need for data sharing. The Web has brought a wave of new users and service providers to the Internet. It contains a huge quantity of heterogeneous information and services (e.g., home pages, online digital libraries, product catalogs, and so on) (Bouguettaya et al. 1998). The result is that the Web is now accepted as the de facto support in all domains of li...
DB
487862
Cross Entropy Guided Ant-like Agents Finding Dependable Primary/Backup Path Patterns in Networks Telecommunication network owners and operators have for half a century been well aware of the potential loss of revenue if a major trunk is damaged, thus dependability at high cost has been implemented. A simple, effective and common dependability scheme is 1:1 protection with 100% capacity redundancy in the network. A growing number of applications in need of dependable connections with specific requirements to bandwidth and delay have started using the internet (which only provides best effort transport) as their base communication service. In this paper we adopt the 1:1 protection scheme and incorporate it as part of a routing system applicable for internet infrastructures. 100% capacity redundancy is no longer required. A distributed stochastic path finding (routing) algorithm based on Rubinstein's Cross Entropy method for combinatorial optimisation is presented. Early results from Monte Carlo simulations indeed indicate that the algorithm is capable of finding pairs of independent primary and backup paths satisfying specific bandwidth a constraints.
Agents
corradi01policy
Policy Controlled Mobility The mobility of software components seems an interesting solution for the deployment of web services and applications in the Internet global infrastructure and also in mobile ad-hoc networks. The network infrastructure already supports several forms of code mobility to make possible to dynamically reconfigure bindings between code fragments and locations where they are to be executed. However, more work is still to be done to facilitate the specification and the enforcement of the mobility behaviour of software components. The traditional approach to embed the migration strategy into the component at design time can not suit the dynamicity of the new network scenarios. Mobile components should instead be enhanced with the possibility to adapt their mobility behaviour to evolving application and environment conditions and to react to unforeseen events. To reach this goal, the paper proposes the adoption of policy based systems to abstract away the specification of migration strategies from the component code. This approach permits to change the mobility behaviour of components without intervention on the component code. We have experienced the dynamicity and flexibility of the proposed approach in the framework obtained by integrating a policy-based management system in a mobile agent environment.  Keywords: Mobility, Migration Policies, Adaptation  Mobile Agents, Reconfiguration.  1 
Agents
539969
User Behavior Analysis of Location Aware Search Engine Rapid growth of internet access from mobile users puts much importance on location specific information on the web. An unique web service called Mobile Info Search (MIS) from NTT Laboratories gathers the information and provide location aware search facilities. We performed association rule mining and sequence pattern mining against the access log which was accumulated at the MIS site in order to get some insight into the behavior of mobile users regarding the spatial information on the web. Detail web log mining process and the rules we derived are reported in this paper.
IR
vasconcelos00bayesian
Bayesian Representations and Learning Mechanisms for Content-Based Image Retrieval We have previously introduced a Bayesian framework for content-based image retrieval (CBIR) that relies on a generative model for feature representation based on embedded mixtures. This is a truly generic image representation that can jointly model color and texture and has been shown to perform well across a broad spectrum of image databases. In this paper, we expand the Bayesian framework along two directions.  First, we show that the formulation of CBIR as a problem of Bayesian inference leads to a natural criteria for evaluating local image similarity without requiring any image segmentation. This allows the practical implementation of retrieval systems where users can provide image regions, or objects, as queries. Region-based queries are significantly less ambiguous than queries based on entire images leading to significant improvements in retrieval precision.  Second, we present a Bayesian learning algorithm that relies on belief propagation to integrate feedback provided by the...
ML
claypool99ounce
An Ounce of Prevention is Worth A Pound of Cure: Formal Verification for Consistent Database Evolution Consistency of a database is as an important property that must be preserved at all times. In most OODB systems today, application code can directly access and alter both the data as well as the structure of the database. As a consequence application code can potentially violate the integrity of the database, in terms of the invariants of the data model, the user-specified application constraints, and even the referential integrity of the objects themselves. A common form of consistency management in most databases today is to encode constraints at the system level (e.g., foreign keys), or at the trigger based level (e.g., user constraints) and to perform transaction rollback on discovery of any violation of these constraints. However, for programs that alter the structure as well as the objects in a database, such as an extensible schema evolution program, roll-backs are expensive and add to the already astronomical cost of doing schema evolution. In this paper, pre-execution ...
DB
marini00specification
Specification of Heterogeneous Agent Architectures . Agent-based software applications need to incorporate agents having  heterogeneous architectures in order for each agent to optimally perform its  task. HEMASL is a simple meta-language used to specify intelligent agents and  multi-agent systems when different and heterogeneous agent architectures must  be used. HEMASL specifications are based on an agent model that abstracts several  existing agent architectures. The paper describes some of the features of the  language, presents examples of its use and outlines its operational semantics. We  argue that adding HEMASL to CaseLP, a specification and prototyping environment  for MAS, can enhance its flexibility and usability.  1 Introduction  Intelligent agents and multi-agent systems (MAS) are increasingly being acknowledged as the "new" modelling techniques to be used to engineer complex and distributed software applications [17, 9]. Agent-based software development is concerned with the realization of software applications modelled ...
Agents
bowling02multiagent
Multiagent Learning Using a Variable Learning Rate Learning to act in a multiagent environment is a difficult problem since the normal definition of an optimal policy no longer applies. The optimal policy at any moment depends on the policies of the other agents. This creates a situation of learning a moving target. Previous learning algorithms have one of two shortcomings depending on their approach. They either converge to a policy that may not be optimal against the specific opponents ’ policies, or they may not converge at all. In this article we examine this learning problem in the framework of stochastic games. We look at a number of previous learning algorithms showing how they fail at one of the above criteria. We then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings. Specifically, we introduce the WoLF principle, “Win or Learn Fast, ” for varying the learning rate. We examine this technique theoretically, proving convergence in self-play on a restricted class of iterated matrix games. We also present empirical results on a variety of more general stochastic games, in situations of self-play and otherwise, demonstrating the wide applicability of this method. Key words: Multiagent learning, reinforcement learning, game theory 1
Agents
504171
Smart Playing Cards A Ubiquitous Computing Game Abstract. Recent technological advances allow for turning parts of our everyday environment into so–called smart environments. In this paper we present the “Smart Playing Cards ” application, a ubiquitous computing game that augments a classical card game with information–technological functionality, in contrast to developing new games around the abilities of available technology. Furthermore, we present the requirements such an application makes on a supporting software infrastructure for ubiquitous computing.
HCI
232896
Query Optimization for Semistructured Data using Path Constraints in a Deterministic Data Model . Path constraints have been studied for semistructured data  modeled as a rooted edge-labeled directed graph [4, 11--13]. In this model,  the implication problems associated with many natural path constraints  are undecidable [11, 13]. A variant of the graph model, called the deterministic  data model , was recently proposed in [10]. In this model, data is  represented as a graph with deterministic edge relations, i.e., the edges  emanating from any node in the graph have distinct labels. This model is  more appropriate for representing, e.g., ACeDB [27] databases and Web  sites. This paper investigates path constraints for the deterministic data  model. It demonstrates the application of path constraints to, among  others, query optimization. Three classes of path constraints are considered:  the language Pc introduced in [11], an extension of Pc , denoted by  P  w  c , by including wildcards in path expressions, and a generalization of  P  w  c , denoted by P  c , by representing pa...
DB
hart99comparing
Comparing Evolutionary Programs and Evolutionary Pattern Search Algorithms: A Drug Docking Application Evolutionary programs (EPs) and evolutionary pattern search algorithms (EPSAs) are two general classes of evolutionary methods for optimizing on continuous domains. The relative performance of these methods has been evaluated on standard global optimization test functions, and these results suggest that EPSAs more robustly converge to nearoptimal solutions than EPs. In this paper we evaluate the relative performance of EPSAs and EPs on a real-world application: flexible ligand binding in the Autodock docking software. We compare the performance of these methods on a suite of docking test problems. Our results confirm that EPSAs and EPs have comparable performance, and they suggest that EPSAs may be more robust on larger, more complex problems. 1 Introduction  Evolutionary programs (EPs) and evolutionary pattern search algorithms (EPSAs) are two classes of evolutionary algorithms (EAs) that have been specifically developed for solving problems of the form min  x2R  n  f(x): In particula...
ML
yao98making
Making Use of Population Information in Evolutionary Artificial Neural Networks This paper is concerned with the simultaneous evolution of artificial neural network (ANN) architectures and weights. The current practice in evolving ANN's is to choose the best ANN in the last generation as the final result. This paper proposes a different approach to form the final result by combining all the individuals in the last generation in order to make best use of all the information contained in the whole population. This approach regards a population of ANN's as an ensemble and uses a combination method to integrate them. Although there has been some work on integrating ANN modules [2], [3], little has been done in evolutionary learning to make best use of its population information. Four linear combination methods have been investigated in this paper to illustrate our ideas. Three real-world data sets have been used in our experimental studies, which show that the recursive least-square (RLS) algorithm always produces an integrated system that outperforms the best individual. The results confirm that a population contains more information than a single individual. Evolutionary learning should exploit such information to improve generalization of learned systems.
ML
zambonelli01organisational
Organisational Rules as an Abstraction for the Analysis and Design of Multi-Agent Systems Multi-agent systems... In this paper we introduce three additional organisational concepts - organisational rules, organisational structures, and organisational patterns - and discuss why we believe they are necessary for the complete specification of computational organisations. In particular, we focus on the concept of organisational rules and introduce a formalism, based on temporal logic, to specify them. This formalism is then used to drive the definition of the organisational structure and the identification of the organisational patterns. Finally, the paper sketches some guidelines for a methodology for agent-oriented systems based on our expanded set of organisational abstractions.
Agents
119409
An Agent-Based Approach to the Construction of Floristic Digital Libraries This paper describes an agent-assisted approach to the construction of floristic digital libraries, which consist of very large botanical data repositories and related services. We propose an environment, termed Chrysalis, in which authors of plant morphologic descriptions can enter data into a digital library via a web-based editor. An agent that runs concurrently with the editor suggests potentially useful morphologic descriptions based on similar documents existing in the library. Benefits derived from the introduction of Chrysalis include reduced potential for errors and data inconsistencies, increased parallelism among descriptions, and considerable savings in the time regularly spent in visually checking for parallelism and manually editing data.  KEYWORDS: agents, agent-based interfaces, floristic digital libraries, FNA, Chrysalis.  INTRODUCTION  Constructing the vast data repositories that will support knowledge-intensive activities in digital libraries poses problems of enormo...
HCI
kim99effective
Effective Temporal Aggregation using Point-based Trees . Temporal databases introduce the concept of time into underlying  data, and provide built-in facilities that allow users to store and  retrieve time-varying data. The aggregation in temporal databases, that  is, temporal aggregation is an extension of conventional aggregation on  the domain and range of aggregates to include time concept. Temporal  aggregation is important for various applications, but is very expensive.  In this paper, we propose a new tree structure for temporal aggregation,  called PA-tree, and aggregate processing method based on the PA-tree.  We show that the time complexity of the proposed method is better  than those of the existing methods. The time complexity of the proposed  method is shown to be indeed the lower bound of the problem. We  perform comparative experiments and show the performance advantage  of our proposed method in practice.  1 Introduction  While conventional database systems store the most recent snapshots of the real world, temporal datab...
DB
barros96business
Business Suitability Principles for Workflow Modelling By incorporating aspects of coordination and collaboration, workflow implementations of information systems require a sound conceptualisation of business processing semantics. Traditionally, the success of conceptual modelling techniques has depended largely on the adequacy of conceptualisation, expressive power, comprehensibility and formal foundation. An equally important requirement, particularly with the increased conceptualisation of business aspects, is business suitability.  In this paper, the focus is on the business suitability of workflow modelling for a commonly encountered class of (operational) business processing, e.g. those of insurance claims, bank loans and land conveyancing. A general assessment is first conducted on some integrated  techniques characterising well-known paradigms - structured process modelling, object-oriented modelling, behavioural process modelling and business-oriented modelling. Through this, an insight into business suitability within the broader...
HCI
539185
A Scalable and Ontology-Based P2P Infrastructure for Semantic Web Services Semantic Web Services are a promising combination of Semantic Web and Web service technology, aiming at providing means of automatically executing, discovering and composing semantically marked-up Web services. We envision peer-to-peer networks which allow for carrying out searches in real-time on permanently reconfiguring networks to be an ideal injastructure for deploying a network of Semantic Web Service providers. However, P2P networks evolving in an unorganized manner suffer jom serious scalability problems, limiting the number of nodes in the network, creating network overload and pushing search times to unacceptable limits. We address these problems by imposing a deterministic shape on P2P networks: We propose a graph topology which allows for very efficient broadcast and search, and we provide an efficient topology construction and maintenance algorithm which, crucial to symmetric peer-to-peer networks, does neither require a central server nor super nodes in the network. We show how our scheme can be made even more efficient by using a globally known ontology to determine the organization of peers in the graph topology, allowing for efficient concept-based search.
HCI
armstrong99boticelli
Boticelli: A Single-Camera Mobile Robot Using New Approaches to Range Data Fusion, World Modeling, and Navigation Planning ID: A083  Abstract  Boticelli is a mobile robot, designed and built for testing new approaches in stereo vision, world modeling, data fusion, map extraction, reinforcement learning and navigation planning. A single camera is used to capture depth information by taking advantage of camera movements. The main thrust of the new approaches is to replace well-known techniques, that depend upon grids of points in space with techniques that use continuous, piecewise linear functions. These techniques scale well to large, complex environments.  Keywords: mobile agents, mapping and exploration, reinforcement learning  Acknowledgements  This research was supported mainly by the Defence Research Establishment Suffield (contract W7702-6R594 /001 to Dendronic Decisions Limited). We are very grateful to the Scientific Authority Dr. Simon Barton for his guidance. Travel was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC). Help of Kyle Palmer in designing the multi...
AI
114630
Response Generation in Collaborative Negotiation In collaborative planning activities, since the agents are autonomous and heterogeneous, it is inevitable that conflicts arise in their beliefs during the planning process. In cases where such conflicts are relevant to the task at hand, the agents should engage in collaborative negotiation  as an attempt to square away the discrepancies in their beliefs. This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution. Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise, and of selecting appropriate evidence to justify the need for such modification. Furthermore, by capturing the negotiation process in a recursive  Propose-Evaluate-Modify cycle of actions, our model can successfully handle embedded negotiation subdialogues.  1 Introduction  In collaborative consultat...
Agents
rosenthal00view
View Security as the Basis for Data Warehouse Security Access  .  permissions in a data warehouse are  currently managed in a separate world from  the sources' policies. The consequences are  inconsistencies, slow response to change, and  wasted administrative work. We present a  different approach, which treats the sources'  exported tables and the warehouse as part of  the same distributed database. Our main result  is a way to control derived products by  extending SQL grants rather than creating  entirely new mechanisms. We provide a  powerful, sound inference theory that derives  permissions on warehouse tables (both  materialized and virtual), making the system  easier to administer and its applications more  robust. We also propose a new permission  construct suitable for views that filter data  from mutually-suspicious parties.  1 Introduction  A key challenge for data warehouse security is how to manage the entire system coherently -- from sources and their export tables, to warehouse stored tables  (conventional and cubes) and vi...
DB
mazumdar99achieving
Achieving Consistency in Mobile Databases through Localization in PRO-MOTION There is great need and potential for traditional transaction support in a mobile computing environment. However, owing to the inherent limitations of mobile computing, we need to augment the well-developed techniques of Database Management Systems with new approaches. In this paper, we focus on the challenge of assuring data consistency. Our approach of localization is to reformulate global constraints so as to enhance the autonomy of the mobile hosts. We show how this approach unifies techniques of maintaining replicated data with methods of enforcing polynomial inequalities. We also discuss how localization can be implemented in PRO-MOTION, a flexible infrastructure for transaction processing in a mobile environment.  1. Introduction  Thanks to the relentless advances in semiconductors, the number of users with mobile computers (we will refer to these machines as mobile hosts or MHs) continues to increase. These users have discovered that exciting developments in wireless technology...
DB
claypool99optimizatizing
Optimizatizing the Performance of Schema Evolution Sequences More than ever before schema transformation is a prevalent problem that needs to be addressed to accomplish for example the migration of legacy systems to the newer OODB systems, the generation of structured web pages from data in database systems, or the integration of systems with different native data models. Such schema transformations are typically composed of a sequence of schema evolution operations. The execution of such sequences can be very timeintensive, possibly requiring many hours or even days and thus effectively making the database unavailable for unacceptable time spans. While researchers have looked at the deferred execution approach for schema evolution in an effort to improve availability of the system, to the best of our knowledge ours is the first effort to provide a direct optimization strategy for a sequence of changes. In this paper, we propose heuristics for the iterative elimination and cancellation of schema evolution primitives as well as for the merging of...
DB
kosala00web
Web Mining Research: A Survey With the huge amount of information available online, the World Wide Web is a fertile area for data mining research. The Web mining research is at the cross road of research from several research communities, such as database, information retrieval, and within AI, especially the sub-areas of machine learning and natural language processing. However, there is a lot of confusions when comparing research efforts from different point of views. In this paper, we survey the research in the area of Web mining, point out some confusions regarded the usage of the term Web mining and suggest three Web mining categories. Then we situate some of the research with respect to these three categories. We also explore the connection between the Web mining categories and the related agent paradigm. For the survey, we focus on representation issues, on the process, on the learning algorithm, and on the application of the recent works as the criteria. We conclude the paper with some research issues.
IR
wilensky00digital
Digital Library Resources as a Basis for Collaborative Work The creation of large, networked, digital document resources has greatly facilitated information access and dissemination. We suggest that such resources can further enhance how we work with information, namely, that they can provide a substrate that supports collaborative work. We focus on one form of collaboration, annotation, by which we mean any of an open-ended number of creative document manipulations which are useful to record and to share with others.  Widespread digital document dissemination required technological enablers, such as web clients and servers. The resulting infrastructure is one in which information may be widely shared by individuals across administrative boundaries. To achieve the same ubiquitous availability for annotation requires providing support for spontaneous collaboration, that is, for collaboration across administrative boundaries without significant prior agreements. Annotation is not more commonplace, we suggest, because the technological needs of sp...
IR
laviola99msvt
MSVT: A Virtual Reality-Based Multimodal Scientific Visualization Tool Recent approaches to providing users with more natural methods of interacting with virtual environment applications have shown that more than one mode of input can be both beneficial and intuitive as a communication medium between humans and computer applications. Although there are many different modes that could be used in these applications, hand gestures and speech appear to be two of the most logical since users will typically be in environments that will have them immersed in a virtual world with limited access to traditional input devices such as the keyboard or mouse. In this paper, we describe a prototype application, MSVT (Multimodal Scientific Visualization Tool), for visualizing fluid flow around a dataset. MSVT uses a multimodal interface which combines whole-hand and voice input to allow users to visualize and interact with the dataset in a natural manner. A discussion of the various interaction techniques, and the results of an informal user evaluation are presented.  KE...
HCI
vardy99wristcam
The WristCam as Input Device We show how images of a user's hand from a video camera attached to the underside of the wrist can be processed to yield finger movement information. Discrete (and discreet) movements of the fingers away from a rest position are translated into a small set of base symbols. These are interpreted as input to a wearable computer, providing unobtrusive control.
HCI
bradford98pruning
Pruning Decision Trees with Misclassification Costs . We describe an experimental study of pruning methods for decision tree classifiers when the goal is minimizing loss rather than error. In addition to two common methods for error minimization, CART's cost-complexity pruning and C4.5's error-based pruning, we study the extension of cost-complexity pruning to loss and one pruning variant based on the Laplace correction. We perform an empirical comparison of these methods and evaluate them with respect to loss. We found that applying the Laplace correction to estimate the probability distributions at the leaves was beneficial to all pruning methods. Unlike in error minimization, and somewhat surprisingly, performing no pruning led to results that were on par with other methods in terms of the evaluation criteria. The main advantage of pruning was in the reduction of the decision tree size, sometimes by a factor of ten. While no method dominated others on all datasets, even for the same domain different pruning mechanisms are better for ...
ML
wicke99compiling
Compiling for Fast State Capture of Mobile Agents Saving, transporting, and restoring the state of a mobile agent is one of the main problems in implementing a mobile agents system. We present an approach, implemented as part of our Messengers system, that represents a trade-off between the unrestricted use of pointers and the ability to perform fully transparent state capture. When writing the code for an agent, the programmer has a choice between two types of functions. C functions are fully general and may use unrestricted pointers, but they are not allowed to invoke any migration commands. Messengers functions may cause migration but their use of pointers is restricted to only a special type of a dynamic array structure. Under these restrictions, the local variables, the program counter, and the calling stack of an agent can all be made machine-independent and can be captured/restored transparently during migration. 1 Introduction  Saving, transporting, and restoring the state of a mobile agent is one of the main problem in implem...
Agents
krum02speech
Speech and Gesture Multimodal Control of a Whole Earth 3D Visualization Environment A growing body of research shows several advantages to multimodal interfaces including increased expressiveness, flexibility, and user freedom. This paper investigates the design of such an interface that integrates speech and hand gestures. The interface has the additional property of operating relative to the user and can be used while the user is in motion or stands at a distance from the computer display. The paper then describes an implementation of the multimodal interface for a whole earth 3D visualization environment which presents navigation interface challenges due to the large magnitude of scale and extended spaces that is available. The characteristics of the multimodal interface are examined, such as speed, recognizability of gestures, ease and accuracy of use, and learnability under likely conditions of use. This implementation shows that such a multimodal interface can be e#ective in a real environment and sets some parameters for the design and use of such interfaces.
HCI
hodkinson99decidable
Decidable Fragments of First-Order Temporal Logics In this paper, we introduce a new fragment of the first-order temporal language, called the monodic fragment, in which all formulas beginning with a temporal operator (Since or Until) have at most one free variable. We show that the satisfiability problem for monodic formulas in various linear time structures can be reduced to the satisfiability problem for a certain fragment of classical first-order logic. This reduction is then used to single out a number of decidable fragments of first-order temporal logics and of two-sorted first-order logics in which one sort is intended for temporal reasoning. Besides standard first-order time structures, we consider also those that have only finite first-order domains, and extend the results mentioned above to temporal logics of finite domains. We prove decidability in three different ways: using decidability of monadic second-order logic over the intended flows of time, by an explicit analysis of structures with natural numbers time, and by a composition method that builds a model from pieces in finitely many steps. 1
DB
455651
Text Database Selection for Longer Queries A metasearch engine is a system that supports unified access to multiple local search engines. One of the main challenges in building a large-scale metasearch engine is to solve the database (search engine) selection problem, which is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query. For the database of each search engine, a representative which indicates approximately the contents of the database is created in advance to enable database selection. The representatives of all databases can be integrated into a single representative to make the selection process more scalable. While an integrated representative with high scalability has just been proposed and has been found to be effective for short queries, its effectiveness for longer queries is significantly lower. In the Internet environment, most queries initially submitted by users are short queries. However, it has been found that better search effectiveness can often be achieved when additional terms are added to the initial queries through query expansion or relevance feedback. The resulting queries are usually longer than the initial queries. In this paper, we propose a new method to construct database representatives and to decide which databases to select for longer queries. Experimental results are given to compare the performance of the new method with that of a previous method.
IR
ram90incremental
Incremental Learning of Explanation Patterns and their Indices This paper describes how a reasoner can improve its understanding of an incompletely understood domain through the application of what it already knows to novel problems in that domain. Recent work in AI has dealt with the issue of using past explanations stored in the reasoner's memory to understand novel situations. However, this process assumes that past explanations are well understood and provide good "lessons" to be used for future situations. This assumption is usually false when one is learning about a novel domain, since situations encountered previously in this domain might not have been understood completely. Instead, it is reasonable to assume that the reasoner would have gaps in its knowledge base. By reasoning about a new situation, the reasoner should be able to fill in these gaps as new information came in, reorganize its explanations in memory, and gradually evolve a better understanding of its domain. We present a story understanding program that retrieves past explan...
ML
omlin98equivalence
Equivalence in Knowledge Representation: Automata, Recurrent Neural Networks, and Dynamical Fuzzy Systems Neurofuzzy systems-the combination of artificial neural networks with fuzzy logic-have become useful in many application domains. However, conventional neurofuzzy models usually need enhanced representation power for applications that require context and state (e.g., speech, time series prediction, control). Some of these applications can be readily modeled as finite state automata. Previously, it was proved that deterministic finite state automata (DFA) can be synthesized by or mapped into recurrent neural networks by directly programming the DFA structure into the weights of the neural network. Based on those results, a synthesis method is proposed for mapping fuzzy finite state automata (FFA) into recurrent neural networks. Furthermore, this mapping is suitable for direct implementation in very large scale integration (VLSI), i.e., the encoding of FFA as a generalization of the encoding of DFA in VLSI systems. The synthesis method requires FFA to undergo a transformation prior to being mapped into recurrent networks. The neurons are provided with an enriched functionality in order to accommodate a fuzzy representation of FFA states. This enriched neuron functionality also permits fuzzy parameters of FFA to be directly represented as parameters of the neural network. We also prove the stability of fuzzy finite state dynamics of the constructed neural networks for finite values of network weight and, through simulations, give empirical validation of the proofs. Hence, we prove various knowledge equivalence representations between neural and fuzzy systems and models of automata.
ML
32215
Using an Explicit Teamwork Model and Learning in RoboCup: An Extended Abstract Stacy Marsella, Jafar Adibi, Yaser Al-Onaizan, Ali Erdem, Randall Hill Gal A. Kaminka, Zhun Qiu, Milind Tambe  Information Sciences Institute and Computer Science Department University of Southern California 4676 Admiralty Way, Marina del Rey, CA 90292, USA robocup-sim@isi.edu 1 Introduction  The RoboCup research initiative has established synthetic and robotic soccer as testbeds for pursuing research challenges in Artificial Intelligence and robotics. This extended abstract focuses on teamwork and learning, two of the multiagent research challenges highlighted in RoboCup. To address the challenge of teamwork, we discuss the use of a domain-independent explicit model of teamwork, and an explicit representation of team plans and goals. We also discuss the application of agent learning in RoboCup. The vehicle for our research investigations in RoboCup is ISIS (ISI Synthetic),  a team of synthetic soccer-players that successfully participated in the simulation league of RoboCup'97, by win...
ML
460667
Hardware/Software Implementation for Localization and Classification Systems Pattern localization and classification are CPU time intensive being normally implemented in software, however with lower performance than custom implementations. Custom implementation in hardware (ASIC) allows real-time processing, having higher cost and time-to-market than software implementation. We present an alternative that represents a good trade-off between performance and cost. This paper presents initially some systems dealing with object localization and classification, analyzing the performance and implementation of each work. After we propose a system for localization and classification of shapes using reconfigurable devices (FPGA) and a signal processor (DSP) available in a flexible codesign platform. The system will be described using C and VHDL languages, for the software and hardware parts respectively, and has been implemented in an APTIX prototyping platform.  1. 
ML
chiu99meta
A Meta Modeling Approach to Workflow Management Systems Supporting Exception Handling Workflow Management Systems (WFMSs) facilitate the definition of structure and decomposition of business processes and assists in management of coordinating, scheduling, executing and monitoring of such activities. Most of the current WFMSs are built on traditional relational database systems and/or using an objectoriented database system for storing the definition and run time data about the workflows. However, a WFMS requires advanced modeling functionalities to support adaptive features, such as on-line exception handling. This article describes our advanced meta-modeling approach using various enabling technologies (such as object orientation, roles, rules, active capabilities) supported by an integrated environment, the ADOME, as a solid basis for a flexible WFMS involving dynamic match making, migrating workflows and exception handling. Copyright 1999 Elsevier Science Ltd  Key words: Meta-modeling, Object-Orientation, Workflow Management, Match-Making, Exception Handling, Workflo...
DB
touzet01side
SIDE Surfer: a Spontaneous Information Discovery and Exchange System Development of wireless communications enables the rise of networking applications in embedded systems. Web interactions, which are the most spread, are nowadays available on wireless PDAs. Moreover, we can observe a development of ubiquitous computing. Based on this concept, many works aim to consider user's context as part of the parameters of the applications. The context notion can include the user's location, his social activity . . . Taking part from emerging technologies enabling short range and direct wireless communications (which allow to define a proximity context), the aim of our study is to design a new kind of application, extending the Web paradigm: spontaneous and proximate Web interactions.
HCI
beigi97metaseek
MetaSEEk: A Content-Based Meta-Search Engine for Images Search engines are the most powerful resources for finding information on the rapidly expanding World Wide Web (WWW). Finding the desired search engines and learning how to use them, however, can be very time consuming. The integration of such search tools enables the users to access information across the world in a transparent and efficient manner. These systems are called meta-search engines. The recent emergence of visual information retrieval (VIR) search engines on the web is leading to the same efficiency problem. This paper describes and evaluates MetaSEEk, a content-based meta-search engine used for finding images on the Web based on their visual information. MetaSEEk is designed to intelligently select and interface with multiple on-line image search engines by ranking their performance for different classes of user queries. User feedback is also integrated in the ranking refinement. We compare MetaSEEk with a base line version of meta-search engine, which does not use the past performance of the different search engines in recommending target search engines for future queries.
IR
langley95acquisition
Acquisition of Place Knowledge Through Case-Based Learning In this paper we define the task of place learning and describe one approach to this problem. The framework represents distinct places as evidence grids, a probabilistic description of occupancy. Place recognition relies on case-based classification, augmented by a registration process to correct for translations. The learning mechanism is also similar to that in case-based systems, involving the simple storage of inferred evidence grids. Experimental studies with physical and simulated robots suggest that this approach improves place recognition with experience, that it can handle significant sensor noise, that it benefits from improved quality in stored cases, and that it scales well to environments with many distinct places. Previous researchers have studied evidence grids and place learning, but they have not combined these two powerful concepts, nor have they used the experimental methods of machine learning to evaluate their methods' abilities. Keywords: place acquisition, case-b...
ML
joho02hierarchical
Hierarchical Presentation of Expansion Terms Different presentations of candidate expansion terms have not been fully explored in interactive query expansion (IQE). Most existing systems that offer an IQE facility use a list form of presentation. This paper examines an hierarchical presentation of the expansion terms which are automatically generated from a set of retrieved documents, organised in a general to specific manner, and visualised by cascade menus. To evaluate the effectiveness of the presentation, a user test was carried out to compare the hierarchical form with the conventional list form. This shows that users of the hierarchy can complete the expansion task in less time and with fewer terms over those using the lists. Relations between initial query terms and selected expansion terms were also investigated.  Keywords  Information retrieval, interactive query expansion, concept hierarchies 1. 
HCI
kadobayashi98seamless
Seamless Guidance by Personal Agent in Virtual Space Based on User Interaction in Real World This paper describes a personal agent that guides a visitor through a virtual space by using context, i.e., visitor interaction, in the real world. #Context-aware# guidance in the real world has become an increasingly important research demand since the advent of small and powerful palmtop computers, or PDAs (Personal Digital Assistants), that can obtain useful information about objects in the real world based on context (e.g., current location and visit history) in the real world. We have tried to extend this context-aware guidance into a virtual environment by using context not from the virtual environment but from the real world; such seamless guidance allows visitors to concentrate their attention on the understanding of actual elements. This paper reports our experiment on seamless guidance by a personal agent in a virtual ancient village using real-world context. This was presented at an open house exhibition in our research laboratories. 1 Introduction  We have proposed a notion...
AI
hunter00merging
Merging Potentially Inconsistent Items of Structured Text Structured text is a general concept that is implicit in a variety of approaches to handling  information. Syntactically, an item of structured text is a number of grammatically simple  phrases together with a semantic label for each phrase. Items of structured text may be nested  within larger items of structured text. The semantic labels in a structured text are meant to  parameterize a stereotypical situation, and so a particular item of structured text is an instance  of that stereotypical situation. Much information is potentially available as structured text  including tagged text in XML, text in relational and object-oriented databases, and the output  from information extraction systems in the form of instantiated templates. In this paper, we  formalize the concept of structured text, and then focus on how we can identify inconsistency  in the logical representation of items of structured text. We then present a new framework  for merging logical theories that can be employed t...
DB
cabri00mars
MARS: a Programmable Coordination Architecture for Mobile Agents Mobile agents represent a promising technology for the development of Internet  applications. However, mobile computational entities introduce peculiar problems  w.r.t. the coordination of the application components. The paper outlines the  advantages of Linda-like coordination models, and shows how a programmable  coordination model based on reactive tuple spaces can provide further desirable  features for Internet applications based on mobile agents. Accordingly, the paper  presents the design and the implementation of the MARS coordination architecture for  Java-based mobile agents. MARS defines Linda-like tuple spaces, which can be  programmed to react with specific actions to the accesses made by mobile agents.
Agents
534078
Text-Based Content Search and Retrieval in ad hoc P2P Communities We consider the problem of content search and retrieval in peer-to-peer (P2P) communities. P2P computing is a potentially powerful model for information sharing between ad hoc groups' of users because of its' low cost of entry and natural model for resource scaling with community size. As P2P communities grow in size, however, locating information distributed across the large number of peers becomes problematic. We present a distributed text-based content search and retrieval algorithm to address this' problem. Our algorithm is' based on a state-of-the-art text-based document ranking algorithm: the vector-space model instantiated with the TFxlDF ranking rule. A naive application of TFxlDF wouM require each peer in a community to collect an inverted index of the entire community. This' is' costly both in terms of bandwidth and storage. Instea & we show how TFxlDF can be approximated given compact summaries of peers' local inverted indexes. We make three contributions: (a) we show how the TFxlDF rule can be adapted to use the index summaries, (b) we provide a heuristic for adaptively determining the set of peers that shouM be contacted for a query, and (c) we show that our algorithm tracks' TFxlDF's performance very closely, regardless of how documents' are distributed throughout the community. Furthermore, our algorithm preserves the main flavor of TFxlDF by retrieving close to the same set of documents for any given query.
IR
84828
Projective Rotations applied to a Pan-Tilt Stereo Head A non-metric pan-tilt stereo-head consists of a weakly calibrated stereo rig mounted on a pan-tilt mechanism. It is called non-metric since neither the kinematics of the mechanism, nor camera calibration are required. The Lie group of "projective rotations"- homographies of projective space corresponding to pure rotations -- is an original formalism to model the geometry of such a pan-tilt system. A Rodrigues alike formula as well as a minimal parameterization of projective rotations are introduced. Based on this, the practical part devises a numerical optimization technique for accurately estimating projective rotations from point correspondences, only. This procedure recovers sufficient geometry to operate the system. The experiments validate and evaluate the proposed approach on real image data. They show the weak calibration, image prediction, and homing of a non-metric pan-tilt head.  1 Introduction  One of the most useful sensors in computer vision is a pan and tilt stereo head. ...
ML
minar99cooperating
Cooperating Mobile Agents for Dynamic Network Routing this paper we present a contrasting model, a dynamic, wireless, peer to peer network with routing tasks performed in a decentralized and distributed fashion by mobile software agents that cooperate to accumulate and distribute connectivity information. Our agents determine system topology by exploring the network, then store this information in the nodes on the network. Other agents use this stored information to derive multi-hop routes across the network. We study these algorithms in simulation as an example of using populations of mobile agents to manage networks
Agents
hellerstein98optimization
Optimization Techniques For Queries with Expensive Methods Object-Relational database management systems allow knowledgeable users to de ne new data types, as well as new methods (operators) for the types. This exibility produces an attendant complexity, which must be handled in new ways for an Object-Relational database management system to be e cient. In this paper we study techniques for optimizing queries that contain time-consuming methods. The focus of traditional query optimizers has been on the choice of join methods and orders; selections have been handled by \pushdown " rules. These rules apply selections in an arbitrary order before as many joins as possible, using the assumption that selection takes no time. However, users of Object-Relational systems can embed complex methods in selections. Thus selections may take signi cant amounts of time, and the query optimization model must be enhanced. In this paper, we carefully de ne a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial Object-Relational database management system Illustra, and discuss practical issues that a ect our earlier assumptions. We compare Predicate Migration to a variety of simpler optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we presentmaybe useful for constrained workloads.
DB
braumandl00functional
Functional Join Processing . Inter-object references are one of the key concepts of object-relational and object-oriented database systems. In this work, we investigate alternative techniques to implement inter-object references and make the best use of them in query processing, i.e., in evaluating functional joins. We will give a comprehensive overview and performance evaluation of all known techniques for simple (singlevalued) as well as multi-valued functional joins. Furthermore, we will describe special order-preserving functionaljoin techniques that are particularly attractive for decision support queries that require ordered results. While most of the presentation of this paper is focused on object-relational and object-oriented database systems, some of the results can also be applied to plain relational databases because index nested-loop joins along key/foreign-key relationships, as they are frequently found in relational databases, are just one particular way to execute a functional join.  Key words: O...
DB
michaud99managing
Managing Robot Autonomy and Interactivity Using Motives and Visual Communication An autonomous mobile robot operating in everyday life conditions will have to face a huge variety of situations and to interact with other agents (living or artificial). Such a robot needs flexible and robust methods for managing its goals and for adapting its control mechanisms to face the contingencies of the world. It also needs to communicate with others in order to get useful information about the world. This paper describes an approach based on a general architecture and on internal variables called `motives' to manage the goals of an autonomous robot. These variables are also used as a basis for communication using a visual communication system. Experiments using a vision- and sonar-based Pioneer I robot, equipped with a visual signaling device, are presented.  1 Introduction  Designing an autonomous mobile robot to operate in unmodified environments, i.e., environments that have not been specifically engineered for the robot, is a very challenging problems. Dynamic and unpredic...
AI
chang99approximate
Approximate Query Translation across Heterogeneous Information Sources In this paper we present a mechanism for  approximately translating Boolean query constraints  across heterogeneous information sources.  Achieving the best translation is challenging because  sources support different constraints for formulating  queries, and often these constraints cannot  be precisely translated. For instance, a query  [score ? 8] might be "perfectly" translated as  [rating ? 0.8] at some site, but can only be approximated  as [grade = A] at another. Unlike  other work, our general framework adopts a customizable  "closeness" metric for the translation  that combines both precision and recall. Our results  show that for query translation we need to  handle interdependencies among both query conjuncts  as well as disjuncts. As the basis, we identify  the essential requirements of a rule system for  users to encode the mappings for atomic semantic  units. Our algorithm then translates complex  queries by rewriting them in terms of the semantic  units. We show that, un...
DB
muller01cube
The Cub-e, a Novel Virtual 3D Display Device We have designed, and are in the process of building, a visualisation device,  the Cub-e. The Cub-e consists of six TFT screens, arranged in a perspex cube,  with a StrongARM processor and batteries inside. It is a multipurpose device  with applications including teleconferencing, interaction with virtual worlds, and  games.  1 
HCI
schweighofer01automatic
Automatic Text Representation, Classification and Labeling in European Law The huge text archives and retrieval systems of legal information have not achieved yet the representation in the wellknown subject-oriented structure of legal commentaries. Content-based classification and text analysis remains a high priority research topic. In the joint KONTERM, SOM and LabelSOM projects, learning techniques of neural networks are used to achieve similar high compression rates of classification and analysis like in manual legal indexing. The produced maps of legal text corpora cluster related documents in units that are described with automatically selected descriptors. Extensive tests with text corpora in European case law have shown the feasibility of this approach. Classification and labeling proved very helpful for legal research. The Growing Hierarchical Self-Organizing Map represents very interesting generalities and specialties of legal text corpora. The segmentation into document parts improved very much the quality of labeling. The next challenge would be a change from tfxidf vector representation to a modified vector representation taking into account thesauri or ontologies considering learned properties of legal text corpora.
IR
schuldt99concurrency
Concurrency Control and Recovery in Transactional Process Management The classical theory of transaction management contains two different aspects, namely concurrency control and recovery, which ensure serializability and atomicity of transaction executions, respectively. Although concurrency control and recovery are not independent of each other, the criteria for these two aspects were developed orthogonally and as a result, in most cases these criteria are incompatible with each other. Recently a unified theory of concurrency control and recovery for databases with read and write operations has been introduced in [SWY93, AVA  +  94] that allows reasoning about serializability and atomicity within the same framework. In [SWY93, AVA  +  94] a class of schedules was introduced (called prefix reducible),  which guarantees both serializability and atomicity in a failure prone environment with read/write operations. Several protocols were developed to generate such schedules by a database concurrency control mechanism. We present here a unified transaction ...
DB
klein91supporting
Supporting Conflict Resolution in Cooperative Design Systems Complex modern-day artifacts are designed cooperatively by groups of experts, each with their own areas of expertise. The interaction of such experts inevitably involves conflict. This paper presents an implemented computational model, based on studies of human cooperative design, for supporting the resolution of such conflicts. This model is based centrally on the insights that general conflict resolution expertise exists separately from domain-level design expertise, and that this expertise can be instantiated in the context of particular conflicts into specific advice for resolving those conflicts. Conflict resolution expertise consists of a taxonomy of design conflict classes in addition to associated general advice suitable for resolving conflicts in these classes. The abstract nature of conflict resolution expertise makes it applicable to a wide variety of design domains. This paper describes this conflict resolution model and provides examples of its operation from an implemente...
Agents
29551
CMUnited-97: RoboCup-97 Small-Robot World Champion Team Robotic soccer is a challenging research domain which involves multiple agents that need to collaborate in an  adversarial environment to achieve specificobjectives. In this paper, we describe CMUnited, the team of small robotic  agents that we developed to enter the RoboCup-97 competition. We designed and built the robotic agents, devised  the appropriate vision algorithm, and developed and implemented algorithms for strategic collaboration between the  robots in an uncertain and dynamic environment. The robots can organize themselves in formations, hold specific  roles, and pursue their goals. In game situations, they have demonstrated their collaborative behaviors on multiple  occasions. We present an overview of the vision processing algorithm which successfully tracks multiple moving  objects and predicts trajectories. The paper then focusses on the agent behaviors ranging from low-level individual  behaviors to coordinated, strategic team behaviors. CMUnited won the RoboCup-97 small-robot competition at  IJCAI-97 in Nagoya, Japan.
AI
fang98computing
Computing Iceberg Queries Efficiently Many applications compute aggregate functions (such as COUNT, SUM) over an attribute (or set of attributes) to find aggregate values above some specified threshold. We call such queries iceberg queries because the number of above-threshold results is often very small (the tip of an iceberg), relative to the large amount of input data (the iceberg). Such iceberg queries are common in many applications, including data warehousing, information-retrieval, market basket analysis in data mining, clustering and copy detection. We propose efficient algorithms to evaluate iceberg queries using very little memory and significantly fewer passes over data, as compared to current techniques that use sorting or hashing. We present an experimental case study using over three gigabytes of Web data to illustrate the savings obtained by our algorithms.  1 Introduction  In this paper we develop efficient execution strategies for an important class of queries that we call iceberg queries. An iceberg query...
IR
michail99assessing
Assessing Software Libraries by Browsing Similar Classes, Functions, and Relationships Comparing and contrasting a set of software libraries is useful for reuse related activities such as selecting a library from among several candidates or porting an application from one library to another. The current state of the art in assessing libraries relies on qualitative methods. To reduce costs and/or assess a large collection of libraries, automation is necessary. Although there are tools that help a developer examine an individual library in terms of architecture, style, etc., we know of no tools that help the developer directly compare several libraries. With existing tools, the user must manually integrate the knowledge learned about each library. Automation to help developers directly compare and contrast libraries requires matching of similar components (such as classes and functions) across libraries. This is different than the traditional component retrieval problem in which components are returned that best match a user's query. Rather, we need to find those component...
IR
538013
Self-Organization in Ad Hoc Sensor Networks: An Empirical Study Research in classifying and recognizing complex concepts  has been directing its focus increasingly on distributed  sensing using a large amount of sensors. The colossal  amount of sensor data often obstructs traditional algorithms  in centralized approaches, where all sensor data is directed  to one central location to be processed. Spreading the  processing of sensor data over the network seems to be a  promising option, but distributed algorithms are harder to  inspect and evaluate. Using self-sufficient sensor boards  with short-range wireless communication capabilities, we  are exploring approaches to achieve an emerging  distributed perception of the sensed environment in realtime  through clustering. Experiments in both simulation  and real-world platforms indicate that this is a valid  methodology, being especially promising for computation  on many units with limited resources.
HCI
ganti98clustering
Clustering Large Datasets in Arbitrary Metric Spaces Clustering partitions a collection of objects into groups called clusters, such that similar objects fall into the same group. Similarity between objects is defined by a distance function satisfying the triangle inequality; this distance function along with the collection of objects describes a distance space. In a distance space, the only operation possible on data objects is the computation of distance between them. All scalable algorithms in the literature assume a special type of distance space, namely a k-dimensional vector space, which allows vector operations on objects. We present two scalable algorithms designed for clustering very large datasets in distance spaces. Our first algorithm BUBBLE is, to our knowledge, the first scalable clustering algorithm for data in a distance space. Our second algorithm BUBBLE-FM improves upon BUBBLE by reducing the number of calls to the distance function, which may be computationally very expensive. Both algorithms make only a single scan ov...
ML
grumbach00manipulating
Manipulating Interpolated Data is Easier than You Thought Data defined by interpolation is frequently  found in new applications involving geographical  entities, moving objects, or spatiotemporal  data. These data lead to potentially  infinite collections of items, (e.g., the elevation  of any point in a map), whose definitions  are based on the association of a collection of  samples with an interpolation function. The  naive manipulation of the data through direct  access to both the samples and the interpolation  functions leads to cumbersome or inaccurate  queries. It is desirable to hide the  samples and the interpolation functions from  the logical level, while their manipulation is  performed automatically.  We propose to model such data using infinite  relations (e.g., the map with elevation yields  an infinite ternary relation) which can be manipulated  through standard relational query  languages (e.g., SQL), with no mention of the  interpolated definition. The clear separation  between logical and physical levels ensures the  accu...
DB
cheverst00developing
Developing a Context-aware Electronic Tourist Guide: Some Issues and Experiences In this paper, we describe our experiences of developing and evaluating GUIDE, an intelligent electronic tourist guide. The GUIDE system has been built to overcome many of the limitations of the traditional information and navigation tools available to city visitors. For example, group-based tours are inherently inflexible with fixed starting times and fixed durations and (like most guidebooks) are constrained by the need to satisfy the interests of the majority rather than the specific interests of individuals. Following a period of requirements capture, involving experts in the field of tourism, we developed and installed a system for use by visitors to Lancaster. The system combines mobile computing technologies with a wireless infrastructure to present city visitors with information tailored to both their personal and environmental contexts. In this paper we present an evaluation of GUIDE, focusing on the quality of the visitors experience when using the system.  Keywords  Mobile c...
HCI
51542
Reasoning with Concrete Domains Description logics are knowledge representation and reasoning formalisms  which represent conceptual knowledge on an abstract logical  level. Concrete domains are a theoretically well-founded approach to  the integration of description logic reasoning with reasoning about  concrete objects such as numbers, time intervals or spatial regions. In  this paper, the complexity of combined reasoning with description logics  and concrete domains is investigated. We extend ALC(D), which  is the basic description logic for reasoning with concrete domains, by  the operators "feature agreement" and "feature disagreement". For  the extended logic, called ALCF(D), an algorithm for deciding the  ABox consistency problem is devised. The strategy employed by this  algorithm is vital for the efficient implementation of reasoners for description  logics incorporating concrete domains. Based on the algorithm,  it is proved that the standard reasoning problems for both  logics ALC(D) and ALCF(D) are PSpace-co...
AI
amiri00highly
Highly Concurrent Shared Storage . Shared storage arrays enable thousands of storage devices to be shared and directly  accessed by end hosts over switched system-area networks, promising databases and filesystems highly  scalable, reliable storage. In such systems, hosts perform access tasks (read and write) and management  tasks (migration and reconstruction of data on failed devices.) Each task translates into multiple  phases of low-level device I/Os, so that concurrent host tasks can span multiple shared devices and  access overlapping ranges potentially leading to inconsistencies for redundancy codes and for data  read by end hosts. Highly scalable concurrency control and recovery protocols are required to coordinate  on-line storage management and access tasks. While expressing storage-level tasks as ACID transactions  ensures proper concurrency control and recovery, such an approach imposes high performance  overhead, results in replication of work and does not exploit the available knowledge about storage  le...
DB
lee01collaborative
Collaborative Learning for Recommender Systems Recommender systems use ratings from users on items such as movies and music for the purpose of predicting the user preferences on items that have not been rated. Predictions are normally done by using the ratings of other users of the system, by learning the user preference as a function of the features of the items or by a combination of both these methods. In this paper, we pose the problem as one of collaboratively learning of preference functions by multiple users of the recommender system. We study several mixture models for this task. We show, via theoretical analyses and experiments on a movie rating database, how the models can be designed to overcome common problems in recommender systems including the new user problem, the recurring startup problem, the sparse rating problem and the scaling problem. 1.
IR
87014
Dynamic Service Matchmaking Among Agents in Open Information Environments Introduction  The amount of services and deployed software agents in the most famous offspring of the Internet, the World Wide Web, is exponentially increasing. In addition, the Internet is an open environment, where information sources, communication links and agents themselves may appear and disappear unpredictably. Thus, an effective, automated search and selection of relevant services or agents is essential for human users and agents as well. We distinguish three general agent categories in the Cyberspace, service providers, service requester,  and middle agents. Service providers provide some type of service, such as finding information, or performing some particular domain specific problem solving. Requester agents need provider agents to perform some service for them. Agents that help locate others are called middle agents[2]. Matchmaking  is the process of finding an appropriate provider for a requester thr
AI
jeh02scaling
Scaling Personalized Web Search Recent web search techniques augment traditional text matching with a global notion of “importance ” based on the linkage structure of the web, such as in Google’s PageRank algorithm. For more refined searches, this global notion of importance can be specialized to create personalized views of importance—for example, importance scores can be biased according to a user-specified set of initially-interesting pages. Computing and storing all possible personalized views in advance is impractical, as is computing personalized views at query time, since the computation of each view requires an iterative computation over the web graph. We present new graph-theoretical results, and a new technique based on these results, that encode personalized views as partial vectors. Partial vectors are shared across multiple personalized views, and their computation and storage costs scale well with the number of views. Our approach enables incremental computation, so that the construction of personalized views from partial vectors is practical at query time. We present efficient dynamic programming algorithms for computing partial vectors, an algorithm for constructing personalized views from partial vectors, and experimental results demonstrating the effectiveness and scalability of our techniques. 1
IR
smola01regularized
Regularized Principal Manifolds . Many settings of unsupervised learning can be viewed as quantization problems --- the minimization of the expected quantization error subject to some restrictions. This allows the use of tools such as regularization from the theory of (supervised) risk minimization for unsupervised settings. This setting turns out to be closely related to principal curves, the generative topographic map, and robust coding. We explore this connection in two ways: 1) we propose an algorithm for finding principal manifolds that can be regularized in a variety of ways. 2) We derive uniform convergence bounds and hence bounds on the learning rates of the algorithm. In particular, we give bounds on the covering numbers which allows us to obtain nearly optimal learning rates for certain types of regularization operators. Experimental results demonstrate the feasibility of the approach. Keywords: Regularization, Uniform Convergence, Kernels, Entropy Numbers, Principal Curves, Clustering, Generative Topograph...
IR
cardie99integrating
Integrating Case-Based Learning and Cognitive Biases for Machine Learning of Natural Language This paper shows that psychological constraints on human information processing can be used effectively  to guide feature set selection for case-based learning of linguistic knowledge. Given as input a  baseline case representation for a natural language learning task, our algorithm selects the relevant cognitive  biases for the task and then automatically modifies the representation in response to those biases  by changing, deleting, and weighting features appropriately. We apply the cognitive bias approach to  feature set selection to four natural language learning problems and show that performance of the casebased  learning algorithm improves significantly when relevant cognitive biases are incorporated into the  baseline instance representation. We argue that the cognitive bias approach offers new possibilities for  case-based learning of natural language: it simplifies the process of instance representation design and, in  theory, obviates the need for separate instance represent...
ML
454876
A Study of Approaches to Hypertext Categorization Hypertext poses new research challenges for text classification. Hyperlinks, HTML  tags, category labels distributed over linked documents, and meta data extracted from related Web sites all provide rich information for classifying hypertext documents. How to appropriately represent that information and automatically learn statistical patterns for solving hypertext classification problems is an open question. This paper seeks a principled approach to providing the answers. Specifically, we define five hypertext regularities which may (or may not) hold in a particular application domain, and whose presence (or absence) may significantly influence the optimal design of a classifier. Using three hypertext datasets and three well-known learning algorithms (Naive Bayes, Nearest Neighbor, and First Order Inductive Learner), we examine these regularities in different domains, and compare alternative ways to exploit them. Our results show that the identification of hypertext regularities in the data and the selection of appropriate representations for hypertext in particular domains are crucial, but seldom obvious, in real-world problems. We find that adding the words in the linked neighborhood to the page having those links (both inlinks and outlinks) were helpful for all our classifiers on one data set, but more harmful than helpful for two out of the three classifiers on the remaining datasets. We also observed that extracting meta data from related Web sites was extremely useful for improving classification accuracy in some of those domains. Finally, the relative performance of the classifiers being tested provided insights into their strengths and limitations for solving classification problems involving diverse and often noisy Web pages.
IR
270469
Patterns of Search: Analyzing and Modeling Web Query Refinement . We discuss the construction of probabilistic models centering on temporal patterns of query refinement. Our analyses are derived from a large corpus of Web search queries extracted from server logs recorded by a popular Internet search service. We frame the modeling task in terms of pursuing an understanding of probabilistic relationships among temporal patterns of activity, informational goals, and classes of query refinement. We construct Bayesian networks that predict search behavior, with a focus on the progression of queries over time. We review a methodology for abstracting and tagging user queries. After presenting key statistics on query length, query frequency, and informational goals, we describe user models that capture the dynamics of query refinement. 1 Introduction The evolution of the World Wide Web has provided rich opportunities for gathering and analyzing anonymous log data generated by user interactions with network-based services. Web-based search engine...
IR
335912
OBDD-based Universal Planning for Synchronized Agents in Non-Deterministic Domains Recently model checking representation and search techniques were shown to be efficiently  applicable to planning, in particular to non-deterministic planning. Such planning  approaches use Ordered Binary Decision Diagrams (obdds) to encode a planning  domain as a non-deterministic finite automaton and then apply fast algorithms from model  checking to search for a solution. obdds can effectively scale and can provide universal  plans for complex planning domains. We are particularly interested in addressing the  complexities arising in non-deterministic, multi-agent domains. In this article, we present  umop, a new universal obdd-based planning framework for non-deterministic, multi-agent  domains. We introduce a new planning domain description language, NADL, to specify  non-deterministic, multi-agent domains. The language contributes the explicit definition  of controllable agents and uncontrollable environment agents. We describe the syntax and  semantics of NADL and show how to bu...
ML
moreno95alisis
An'alisis Din'amico De Las Creencias El modelo de los mundos posibles y su sem'antica de Kripke asociada dan una sem'antica intuitiva para las l'ogicas dox'asticas, pero parecen llevar inevitablemente a modelizar agentes l'ogicamente omniscientes y razonadores perfectos. Estos problemas son evitables, si los mundos posibles dejan de considerarse como descripciones completas y consistentes del mundo real. Adoptando una definici'on sint'actica de mundos posibles, en este art'iculo se sugiere c'omo se pueden analizar las creencias de una forma puramente l'ogica, usando el m'etodo de los tableros anal'iticos (con ciertas modificaciones). Este an'alisis constituye un primer paso hacia la modelizaci'on de la investigaci'on racional. Palabras clave: l'ogicas de creencias y de conocimiento, omnisciencia l'ogica, mundos posibles, relaci'on de accesibilidad, tableros anal'iticos Temas: representaci'on del conocimiento, razonamiento 1 Introducci'on Las l'ogicas de creencias y de conocimiento ([HALP92], [FAGI95]) son herramienta...
Agents
roobaert00comparison
Comparison of Learning Approaches to Appearance-based 3D Object Recognition with and without cluttered background We re-evaluate the application of Support Vector Machines (SVM) to appearance-based 3D object recognition, by comparing it to two other learning approaches: the system developed at Columbia University ("Columbia") and a simple image matching system using a nearest neighbor classifier ("NNC").  In a first set of experiments, we compare correct recognition rates of the segmented 3D object images of the COIL database. We show that the performance of the simple "NNC" system compares to the more elaborated "Columbia" and "SVM" systems. Only when the experimental setting is more demanding, i.e. when we reduce the number of views during the training phase, some difference in performance can be observed. In a second set of experiments, we consider the more realistic task of 3D object recognition with cluttered background. Also in this case, we obtain that the performance of the three systems are comparable. Only with the recently proposed black/white background training scheme ("BW") applied t...
ML
prasad96distributed
Distributed Case-Based Learning Multi-agent systems exploiting case based reasoning techniques have to deal with the problem of retrieving episodes that are themselves distributed across a set of agents. From a Gestalt perspective, a good overall case may not be the one derived from the summation of best subcases. In this paper we deal with issues involved in learning and exploiting the learned knowledge in multiagent case-based systems. Introduction Case Based Reasoning (CBR) has been attracting much attention recently as a paradigm with a wide variety of applications [Kolodner 93]. In this paper, we discuss issues pertaining to cooperative retrieval and composition of a case in which subcases are distributed across different agents in a multi-agent system. A multi-agent system comprises a group of intelligent agents working towards a set of common global goals or separate individual goals that may interact. In such a system, each of the agents may not be individually capable of achieving the global goal and/or ...
ML
sun99optional
Optional Locking Integrated with Operational Transformation in Distributed Real-Time Group Editors Locking is a standard technique in traditional distributed computing and database systems to ensure data integrity by prohibiting concurrent conflicting updates on shared data objects. Operational transformation is an innovative technique invented by groupware research for consistency maintenance in real-time group editors. In this paper, we will examine and explore the complementary roles of locking and operational transformation in consistency maintenance. A novel optional locking scheme is proposed and integrated with operation transformation to maintain both generic and context-specific consistency in a distributed, interactive, and collaborative environment. The integrated optional locking and operational transformation technique is fully distributed, highly responsive, non-blocking, and capable of avoiding locking overhead in the most common case of collaborative editing.  Keywords: Locking, operational transformation, consistency maintenance, group editors, groupware, distribute...
DB
pfister98room
Rooms, Protocols, and Nets: Metaphors for Computer Supported Cooperative Learning of Distributed Groups : We discuss an integrative design for computer supported cooperative learning (CSCL) environments. Three common problems of CSCL are addressed: How to achieve social orientation and group awareness, how to coordinate goal-directed interaction, and how to construct a shared knowledge base. With respect to each problem, we propose a guiding metaphor which links theoretical, technical, and usability requirements. If appropriately implemented, each metaphor resolves one problem: Virtual rooms support social orientation, learning protocols guide interactions aimed at knowledge acquisition, and learning nets represent socially shared knowledge. Theoretically, the metaphor of virtual rooms originates in work on virtual spaces in human computer interaction, learning protocols are related to speech act theory, and learning nets are based on models of knowledge representation. A prototype system implementing the virtual room metaphor is presented. We argue that by further integrating these thre...
HCI
chen00algebraic
An Algebraic Compression Framework for Query Results Decision-support applications in emerging environments require that SQL query results or intermediate results be shipped to clients for further analysis and presentation. These clients may use low bandwidth connections or have severe storage restrictions. Consequently, there is a need to compress the results of a query for efficient transfer and client-side access. This paper explores a variety of techniques that address this issue. Instead of using a fixed method, we choose a combination of compression methods that use statistical and semantic information of the query results to enhance the effect of compression. To represent such a combination, we present a framework of "compression plans" formed by composing primitive compression operators. We also present optimization algorithms that enumerate valid compression plans and choose an optimal plan. Our experiments show that our techniques achieve significant performance improvement over standard compression tools like WinZip.  1. Intro...
DB
ormoneit01learning
Learning and Tracking Cyclic Human Motion We present methods for learning and tracking human motion in  video. We estimate a statistical model of typical activities from a  large set of 3D periodic human motion data by segmenting these  data automatically into "cycles". Then the mean and the principal  components of the cycles are computed using a new algorithm  that accounts for missing information and enforces smooth transitions  between cycles. The learned temporal model provides a  prior probability distribution over human motions that can be used  in a Bayesian framework for tracking human subjects in complex  monocular video sequences and recovering their 3D motion.
HCI
combs99does
Does Zooming Improve Image Browsing? We describe an image retrieval system we built based on a Zoomable User Interface (ZUI). We also discuss the design, results and analysis of a controlled experiment we performed on the browsing aspects of the system. The experiment resulted in a statistically significant difference in the interaction between number of images (25, 75, 225) and style of browser (2D, ZUI, 3D). The 2D and ZUI browser systems performed equally, and both performed better than the 3D systems. The image browsers tested during the experiment include Cerious Software's Thumbs Plus, TriVista Technology's Simple LandScape and Photo GoRound, and our Zoomable Image Browser based on Pad++. Keywords Evaluation, controlled experiment, image browsers, retrieval systems, real-time computer graphics, Zoomable User Interfaces (ZUIs), multiscale interfaces, Pad++. INTRODUCTION In the past two decades, with the emergence of faster computers, the declining cost of memory, the popularity of digital cameras, online archives...
HCI
veloso94learning
Learning Strategy Knowledge Incrementally Modern industrial processes require advanced computer tools that should adapt to the user requirements and to the tasks being solved. Strategy learning consists of automating the acquisition of patterns of actions used while solving particular tasks. Current intelligent strategy learning systems acquire operational knowledge to improve the efficiency of a particular problem solver. However, these strategy learning tools should also provide a way of achieving low-cost solutions according to user-specific criteria. In this paper, we present a learning system, hamlet, which is integrated in a planning architecture, prodigy, and acquires control knowledge to guide prodigy to efficiently produce cost-effective plans. hamlet learns from planning episodes, by explaining why the correct decisions were made, and later refines the learned strategy knowledge to make it incrementally correct with experience.
ML
panzarasa99modeling
Modeling Sociality In The BDI Framework . We present a conceptual model for how the social nature of agents impacts upon their individual mental states. Roles and social relationships provide an abstraction upon which we develop the notion of social mental shaping . 1 Introduction Belief-Desire-Intention (BDI) architectures for deliberative agents are based on the physical symbol system assumption that agents maintain and reason about internal representations of their world [2]. However, while such architectures conceptualise individual intentionality and behaviour, they say nothing about the social aspects of agents being situated in a multi-agent system. The main reason for this limitation is that mental attitudes are taken to be internal to a particular agent (or team) and are modeled as a relation between the agent (or a team) and a proposition. The purpose of this paper is, therefore, to extend BDI models in order to investigate the problem of how the social nature of agents can impact upon their individual mental ...
Agents
shaw98clause
Clause Aggregation Using Linguistic Knowledge By combining multiple clauses into one single sentence, a text generation system can express the same amount of information in fewer words and at the same time, produce a great variety of complex constructions. In this paper, we describe hypotactic and paratactic operators for generating complex sentences from clause-sized semantic representations. These two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar. 1 Introduction  An expression is more concise than another expression if it conveys the same amount of information in fewer words. Complex sentences generated by combining clauses are more concise than corresponding simple sentences because multiple references to the recurring entities are removed. For example, clauses like "Jones is a patient" and "Jones has hypertension" can be combined into a more concise sentence "Jones is a hypertensive patient." To illustrate the common occurrence of such repeated enti...
DB
abraham00hybrid
Hybrid Heuristics for Optimal Design of Artificial Neural Networks Designing the architecture and correct parameters for the learning algorithm is a tedious task for modeling an optimal Artificial Neural Network (ANN), which is smaller, faster and with a better generalization performance. In this paper we explain how a hybrid algorithm integrating Genetic algorithm (GA), Simulated Annealing (SA) and other heuristic procedures can be applied for the optimal design of an ANN. This paper is more concerned with the understanding of current theoretical developments of Evolutionary Artificial Neural Networks (EANNs) using GAs and how the proposed hybrid heuristic procedures can be combined to produce an optimal ANN. The proposed meta-heuristic can be regarded as a general framework for adaptive systems, that is, systems that can change their connection weights, architectures and learning rules according to different environments without human intervention.
AI
cummins99automatic
Automatic Discrimination Among Languages Based on Prosody Alone The development of methods for the automatic identification of languages is motivated both by speech-based applications intended for use in a multi-lingual environment, and by theoretical questions of cross-linguistic variation and similarity. We evaluate the potential utility of two prosodic variables, F 0 and amplitude envelope modulation, in a pairwise language discrimination task. Discrimination is done using a novel neural network which can successfully attend to temporal information at a range of timescales. Both variables are found to be useful in discriminating among languages, and confusion patterns, in general, reflect traditional intonational and rhythmic language classes. The methods employed allow empirical determination of prosodic similarity across languages. Die Entwicklung von Methoden zur automatischen Sprachidentifikation wird motiviert sowohl durch sprach-basierte Anwendungen, die zum Einsatz in einer mehrsprachigen Umgebung bestimmt sind, als auch durch theoretisch...
ML
57340
Class Representation and Image Retrieval with Non-Metric Distances One of the key problems in appearance-based vision is understanding how to use a set of labeled images to classify new images. Classification systems that can model human performance, or that use robust image matching methods, often make use of similarity judgments that are non-metric; but when the triangle inequality is not obeyed, most existing pattern recognition techniques are not applicable. We note that exemplar-based (or nearest-neighbor) methods can be applied naturally when using a wide class of non-metric similarity functions. The key issue, however, is to find methods for choosing good representatives of a class that accurately characterize it. We show that existing condensing techniques for finding class representatives are ill-suited to deal with non-metric dataspaces. We then focus on developing techniques for solving this problem, emphasizing two points: First, we show that the distance between two images is not a good measure of how well one image can represent another in non-metric spaces. Instead, we use the vector correlation between the distances from each image to other previously seen images. Second, we show that in non-metric spaces, boundary points are less significant for capturing the structure of a class
ML
451065
Ontology-Related Services in Agent-Based Distributed Information Infrastructures Ontologies are an emerging paradigm to support declarativity, interoperability, and intelligent services in many areas, such as Agent--based Computation, Distributed Information Systems, and Expert Systems. In the context of designing a scalable, agent-based middleware for the realization of distributed Organizational Memories (OM), we examine the question what ontology--related services must be provided as middleware components. To this end, we discuss three basic dimensions of information that have fundamental impact on the usefulness of ontologies for OMs, namely formality, stability, and sharing scope of information. A short discussion of techniques which are suited to find a balance in each of these dimensions leads to a characterization of roles of ontology--related actors in the OM scenario. We describe the several roles with respect to their goals, knowledge, competencies, rights, and obligations. These actor classes and the related competencies are candidates to define agent types, speech acts, and standard services in the envisioned OM middleware. 1.
AI
emmerich99incremental
Incremental Code Mobility with XML We demonstrate how XML and related technologies can be used for code mobility at any granularity, thus overcoming the restrictions of existing approaches. By not fixing a particular granularity for mobile code, we enable complete programs as well as individual lines of code to be sent across the network. We define the concept of incremental code mobility as the ability to migrate and add, remove, or replace code fragments (i.e., increments) in a remote program. The combination of fine-grained and incremental mobility achieves a previously unavailable degree of flexibility. We examine the application of incremental and fine-grained code mobility to a variety of domains, including user interface management, application management on mobile thin clients, for example PDAs, and management of distributed documents.  Keywords  Incremental Code Mobility, XML Technologies  1 INTRODUCTION  The increasing popularity of Java and the spread of Webbased technologies are contributing to a growing int...
HCI
301461
Organisational Abstractions for the Analysis and Design of Multi-Agent Systems Abstract. The architecture of a multi-agent system can naturally be viewed as a computational organisation. For this reason, we believe organisational abstractions should play a central role in the analysis and design of such systems. To this end, the concepts of agent roles and role models are increasingly being used to specify and design multi-agent systems. However, this is not the full picture. In this paper we introduce three additional organisational concepts — organisational rules, organisational structures, and organisational patterns — that we believe are necessary for the complete specification of computational organisations. We view the introduction of these concepts as a step towards a comprehensive methodology for agent-oriented systems. 1
Agents
hannebauer99dynamic
Dynamic Reconfiguration in Collaborative Problem Solving In this article we will describe our research efforts in coping with a trade-off that can be often found in the control and optimization of todays business processes. Though centralized control may achieve nearto -optimum results in optimizing the system behavior, there are usually social, technical and security restrictions on applying centralized control. Distributed control on the other hand may cope with these restrictions but also entails sub-optimality and communicational overhead. Our concept of composable agents tries to allow a dynamic and fluent transition between globalization and localization in business process control by adapting to the current real-world system structure. We are currently evaluating this concept in the framework of patient flow control at Charit'e Berlin.  Introduction  Research in Distributed Artificial Intelligence (DAI, (Bond & Gasser 1988)) has been traditionally divided into Distributed Problem Solving (DPS) and Multi Agent Systems (MAS). However, r...
Agents
296568
Combining Multiple Learning Strategies for Effective Cross Validation Parameter tuning through cross-validation  becomes very difficult when the validation  set contains no or only a few examples of  the classes in the evaluation set. We address  this open challenge by using a combination  of classifiers with different performance  characteristics to effectively reduce the performance  variance on average of the overall  system across all classes, including those  not seen before. This approach allows us to  tune the combination system on available but  less-representative validation data and obtain  smaller performance degradation of this  system on the evaluation data than using a  single-method classifier alone. We tested this  approach by applying k-Nearest Neighbor,  Rocchio and Language Modeling classifiers  and their combination to the event tracking  problem in the Topic Detection and Tracking  (TDT) domain, where new classes (events)  are created constantly over time, and representative  validation sets for new classes are  often difficult to ob...
IR
alvarez00web
Web Metasearch as Belief Aggregation Web metasearch requires a mechanism for combining rank-ordered lists of ratings returned by multiple search engines in response to a given user query. We view this as being analogous to the need for combining degrees of belief in probabilistic and uncertain reasoning in artificial intelligence. This paper describes a practical method for performing web metasearch based on a novel transformationbased theory of belief aggregation. The consensus ratings produced by this method take into account the item ratings/rankings output by individual search engines as well as the user's preferences. Copyright c fl 2000, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. Introduction Web search engines (WSE) use tools ranging from simple text-based search to more sophisticated methods that attempt to understand the intended meanings of both queries and data items. There has been much work in this area in recent years. The link structure of the web has...
IR
vandendoel01foleyautomatic
FOLEYAUTOMATIC: Physically-based Sound Effects for Interactive Simulation and Animation Animations for which sound effects were automatically added by our system, demonstrated in the accompanying video. (a) A real wok in which a pebble is thrown; the pebble rattles around the wok and comes to rest after wobbling. (b) A simulation of a pebble thrown in wok, with all sound effects automatically generated. (c) A ball rolling back and forth on a ribbed surface. (d) Interaction with a sonified object. We describe algorithms for real-time synthesis of realistic sound effects for interactive simulations (e.g., games) and animation. These sound effects are produced automatically, from 3D models using dynamic simulation and user interaction. We develop algorithms that are efficient, physicallybased, and can be controlled by users in natural ways. We develop effective techniques for producing high quality continuous contact sounds from dynamic simulations running at video rates which are slow relative to audio synthesis. We accomplish this using modal models driven by contact forces modeled at audio rates, which are much higher than the graphics frame rate. The contact forces can be computed from simulations or can be custom designed. We demonstrate the effectiveness with complex realistic simulations.
HCI
maneewongvatana99analysis
Analysis of Approximate Nearest Neighbor Searching with Clustered Point Sets this paper we study the performance of two other splitting methods, and compare them against the kd-tree splitting method. The first, called slidingmidpoint, is a splitting method that was introduced by Mount and Arya in the ANN library for approximate nearest neighbor searching [30]. This method was introduced into the library in order to better handle highly clustered data sets. We know of no analysis (empirical or theoretical) of this method. This method was designed as a simple technique for addressing one of the most serious flaws in the standard kd-tree splitting method. The flaw is that when the data points are highly clustered in low dimensional subspaces, then the standard kd-tree splitting method may produce highly elongated cells, and these can lead to slow query times. This splitting method starts with a simple midpoint split of the longest side of the cell, but if this split results in either subcell containing no data points, it translates (or "slides") the splitting plane in the direction of the  points until hitting the first data point. In Section 3.1 we describe this splitting method and analyze some of its properties. The second splitting method, called minimum-ambiguity, is a query-based technique. The tree is given not only the data points, but also a collection of sample query points, called the training points. The algorithm applies a greedy heuristic to build the tree in an attempt to minimize the expected query time on the training points. We model query processing as the problem of eliminating data points from consideration as the possible candidates for the nearest neighbor. Given a collection of query points, we can model any stage of the nearest neighbor algorithm as a bipartite graph, called the candidate graph, whose vertices correspond t...
ML
gelati01agents
Agents Supporting Information Integration: The Miks Framework During past years we have developed the MOMIS (Mediator envirOnment for Multiple Information Sources) system for the integration of data from structured and semi-structured data sources.  In this paper we propose a new system, MIKS (Mediator agent for Integration of Knowledge Sources), which enriches the MOMIS architecture exploiting the intelligent and mobile agent features.  1. Motivation  The web explosion, both at Internet and intranet level, has transformed the electronic information system from single isolated node to an entry point into a worldwide network of information exchange and business transactions. One of the main challenges for the designers of the e-commerce infrastructures is the information sharing, retrieving data located in different sources thus obtaining an integrated view to overcome any contradiction or redundancy.  During past years we have developed the MOMIS (Mediator envirOnment for Multiple Information Sources) system for the integration of data from struc...
Agents
28981
Relational Learning Techniques for Natural Language Information Extraction The recent growth of online information available in the form of natural language documents creates a greater need for computing systems with the ability to process those documents to simplify access to the information. One type of processing appropriate for many tasks is information extraction, a type of text skimming that retrieves specific types of information from text. Although information extraction systems have existed for two decades, these systems have generally been built by hand and contain domain specific information, making them difficult to port to other domains. A few researchers have begun to apply machine learning to information extraction tasks, but most of this work has involved applying learning to pieces of a much larger system. This paper presents a novel rule representation specific to natural language and a learning system, Rapier, which learns information extraction rules. Rapier takes pairs of documents and filled templates indicating the information to be ext...
DB
brabrand99ltbigwiggt
&lt;bigwig&gt; -- A language for developing interactive Web services &lt;bigwig&gt; is a high-level programming language and a compiler for developing interactive Web services. The overall goal of the language design is to remove many of the obstacles that face current developers of Web services in order to lower cost while increasing functionality and reliability. The compiler translates programs into a conglomerate of lower-level standard technologies such as CGI-scripts, HTML, JavaScript, and HTTP Authentication. This paper describes the major facets of the language design and the techniques used in their implementation, and compares the design with alternative Web service technologies.  
DB
zhou00implementation
Implementation of a Linear Tabling Mechanism Delaying-based tabling mechanisms, such as the one adopted in XSB, are nonlinear  in the sense that the computation state of delayed calls has to be preserved.
DB
tice00key
Key Instructions: Solving the Code Location Problem for Optimized Code There are many difficulties to be overcome in the process of designing and implementing a debugger for optimized code. One of the first problems facing the designer of such a debugger is determining how to accurately map between locations in the source program and locations in the corresponding optimized binary. The solution to this problem is critical for many aspects of debugger design, from setting breakpoints, to implementing single-stepping, to reporting error locations. Previous approaches to debugging optimized code have presented many different techniques for solving this location mapping problem (commonly known as the code location problem). These techniques are often very complex and sometimes incomplete.  Identifying key instructions allows for a simple yet formal way of mapping between locations in the source program and the optimized target program. In this paper we present the concept of key instructions. We give a formal definition of key instructions and present algorit...
HCI
forlizzi99data
A Data Model and Data Structures for Moving Objects Databases We consider spatio-temporal databases supporting spatial objects with continuously  changing position and extent, termed moving objects databases. We formally  define a data model for such databases that includes complex evolving spatial  structures such as line networks or multi-component regions with holes. The data  model is given as a collection of data types and operations which can be plugged  as attribute types into any DBMS data model (e.g. relational, or object-oriented)  to obtain a complete model and query language. A particular novel concept is the  sliced representation which represents a temporal development as a set of units,  where unit types for spatial and other data types represent certain "simple" functions  of time. We also show how the model can be mapped into concrete physical  data structures in a DBMS environment.  1 Introduction  A wide and increasing range of database applications has to deal with spatial objects whose position and/or extent changes over time...
DB
himberg01time
Time Series Segmentation for Context Recognition in Mobile Devices Recognizing the context of use is important in making mobile devices as simple to use as possible. Finding out what the user's situation is can help the device and underlying service in providing an adaptive and personalized user interface. The device can infer parts of the context of the user from sensor data: the mobile device can include sensors for acceleration, noise level, luminosity, humidity, etc. In this paper we consider context recognition by unsupervised segmentation of time series produced by sensors.  Dynamic programming can be used to find segments that minimize the intra-segment variances. While this method produces optimal solutions, it is too slow for long sequences of data. We present and analyze randomized variations of the algorithm. One of them, Global Iterative Replacement or GIR, gives approximately optimal results in a fraction of the time required by dynamic programming. We demonstrate the use of time series segmentation in context recognition for mobile phone applications.  1 
HCI
ng01stable
Stable Algorithms for Link Analysis The Kleinberg HITS and the Google PageRank algorithms are eigenvector methods for identifying "authoritative" or "influential" articles, given hyperlink or citation information. That such algorithms should give reliable or consistent answers is surely a desideratum, and in [10], we analyzed when they can be expected to give stable rankings under small perturbations to the linkage patterns. In this paper, we extend the analysis and show how it gives insight into ways of designing stable link analysis methods. This in turn motivates two new algorithms, whose performance we study empirically using citation data and web hyperlink data. 1.
IR
yeates01memory
Memory Hierarchies as a Metaphor for Academic Library Collections Research libraries and their collections are a cornerstone of the academic tradition, representing 2000 years of development of the Western Civilization; they make written history widely accessible at low cost. Computer memories are a range of physical devices used for storing digital information that have undergone much formal study over 40 years and are well understood. This paper draws parallels between the organisation of research collections and computer memories, in particular examining their hierarchical structure, and examines the implication for digital libraries.
IR
231484
Autonomous Evolution of Gaits with the Sony Quadruped Robot A trend in robotics is towards legged robots. One of the issues with legged robots is the development of gaits. Typically gaits are developed manually. In this paper we report our results of autonomous evolution of dynamic gaits for the Sony Quadruped Robot. Fitness is determined using the robot's digital camera and infrared sensors. Using this system we evolve faster dynamic gaits than previously manually developed. 1 INTRODUCTION  In this paper we present an implementation of an autonomous evolutionary algorithm (EA) for developing locomotion gaits. All processing is handled by the robot's onboard computer and individuals are evaluated using the robot's sensors. Our implementation successfully evolves trot and pace gaits for our robot for which the pace gait significantly outperforms previous hand-developed gaits. In addition to achieving our desired goal of automatically developing gaits these results show that EAs can be used on real robots to evolve non-trivial behaviors. A method...
ML
462179
Infinite-Horizon Policy-Gradient Estimation Gradient-based approaches to direct policy search in reinforcement learning have received  much recent attention as a means to solve problems of partial observability and to avoid some of  the problems associated with policy degradation in value-function methods. In this paper we introduce   GPOMDP, a simulation-based algorithm for generating a biased estimate of the gradient of  the average reward in Partially Observable Markov Decision Processes (POMDPs) controlled by  parameterized stochastic policies. A similar algorithm was proposed by Kimura, Yamamura, and  Kobayashi (1995). The algorithm's chief advantages are that it requires storage of only twice the  number of policy parameters, uses one free parameter   2 [0; 1) (which has a natural interpretation  in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove  convergence of GPOMDP, and show how the correct choice of the parameter   is related to the  mixing time of the controlled POMDP. We briefly describe extensions of GPOMDP to controlled  Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order  derivatives, and a version for training stochastic policies with internal states. In a companion paper  (Baxter, Bartlett, & Weaver, 2001) we show how the gradient estimates generated by GPOMDP  can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure  to find local optima of the average reward.
ML
jr99identifying
Identifying and Handling Structural Incompleteness for Validation of Probabilistic Knowledge-Bases The PESKI (Probabilities, Expert Systems, Knowledge,  and Inference) system attempts to address some  of the problems in expert system design through the  use of the Bayesian Knowledge Base (BKB) representation.
AI
broekstra02enabling
Enabling knowledge representation on the Web by extending RDF Schema Recently, there has been a wide interest in using ontologies on the Web. As a basis for this, RDF Schema (RDFS) provides means to define vocabulary, structure and constraints for expressing metadata about Web resources. However, formal semantics are not provided, and the expressivity of it is not enough for full-fledged ontological modeling and reasoning. In this paper, we will show how RDFS can be extended in such a way that a full knowledge representation (KR) language can be expressed in it, thus enriching it with the required additional expressivity and the semantics of this language. We do this by describing the ontology language OIL as an extension of RDFS. An important advantage of our approach is a maximal backward compatability with RDFS: any meta-data in OIL format can still be partially interpreted by any RDFS-only-processor. The OIL extension of RDFS has been carefully engineered so that such a partial interpretation of OIL meta-data is still correct under the intended semantics of RDFS: simply ignoring the OIL specific portions of an OIL document yields a correct RDF(S) document whose intended RDFS semantics is precisely a subset of the semantics of the full OIL statements. In this way, our approach ensures maximal sharing of meta-data on the Web: even partial interpretation of meta-data by less semantically aware processors will yield a correct partial interpretation of the meta-data. We conclude that our method of extending is equally applicable to other KR formalisms. 1 1
IR
itti99comparison
A Comparison of Feature Combination Strategies for Saliency-Based Visual Attention Systems Bottom-up or saliency-based visual attention allows primates to detect non-specific conspicuous targets in cluttered scenes. A classical metaphor, derived from electrophysiological and psychophysical studies, describes attention as a rapidly shiftable "spotlight". The model described here reproduces the attentional scanpaths of this spotlight: Simple multi-scale "feature maps" detect local spatial discontinuities in intensity, color, orientation or optical flow, and are combined into a unique "master" or "saliency" map. The saliency map is sequentially scanned, in order of decreasing saliency, by the focus of attention. We study the problem of combining feature maps, from different visual modalities and with unrelated dynamic ranges (such as color and motion), into a unique saliency map. Four combination strategies are compared using three databases of natural color images: (1) Simple normalized summation, (2) linear combination with learned weights, (3) global non-linear normalization...
ML
lind01issues
Issues in Agent-Oriented Software Engineering   In this paper, I will discuss the conceptual foundation of agent-oriented software development by relating the fundamental elements of the agent-oriented view to those of other, well established programming paradigms, especially the object-oriented approach. Furthermore, I will motivate the concept of autonomy as the basic property of the agent-oriented school and discuss the development history of programming paradigms that lead to this perspective on software systems. The paper will be concluded by an outlook on how the new paradigm can change the way we think about software systems. 
DB
233508
Developments in Spatio-Temporal Query Languages In contrast to the field view of spatial data that basically views spatial data as a mapping from points into some features, the object view clusters points by features and their values into spatial objects of type point, line, or region. When embedding these objects into a data model, such as the relational model, an additional clustering according to conceptually identified objects takes place. For example, we could define a relation City(name: string,center: point,area: region) that combines different features for cities in one relation. An important aspect of this kind of modeling is that clustering happens on two different levels: (i) points are grouped into spatial objects like regions and (ii) different attributes/features are grouped into a perceived object. When talking about data modeling there is no reason why this grouping should be limited to two levels. For example, we can consider storing regions of different population densities for each city in an attribute density: num → region. Although then the relation is not in first normal form anymore, we can “recover” the first normal form by encapsulating the function num → region in an abstract data type. The important aspect is that all the required operations on such a type as well as on regions and other complex types can be defined to a large degree independently from the data model. 1 The most important point about the preceding discussion is the way in which complex types can be easily
DB
casillas00methodology
A Methodology to Improve Ad Hoc Data-Driven Linguistic Rule Learning Methods by Inducing Cooperation Among Rules Within the Linguistic Modeling eld |one of the most important applications of Fuzzy  Rule-Based Systems|, a family of ecient and simple methods guided by covering criteria  of the data in the example set, called \ad hoc data-driven methods", has been proposed in the  literature in the last few years. Their high performance, in addition to their quickness and  easy understanding, have make them very suitable for learning tasks. In this paper we are  going to perform a double task analyzing these kinds of learning methods and introducing  a new methodology to signicantly improve their accuracy keeping their descriptive power  unalterable.  On the one hand, a taxonomy of ad hoc data-driven learning methods based on the way  in which the available data is used to guide the learning will be made. In this sense, we  will distinguish between two approaches: the example-based and the fuzzy-grid-based one.  Whilst in the former each rule is obtained from a specic example, in the latter the e...
ML
ulrich00appearancebased
Appearance-Based Obstacle Detection with Monocular Color Vision This paper presents a new vision-based obstacle detection  method for mobile robots. Each individual image pixel is  classified as belonging either to an obstacle or the ground  based on its color appearance. The method uses a single  passive color camera, performs in real-time, and provides a  binary obstacle image at high resolution. The system is  easily trained by simply driving the robot through its  environment. In the adaptive mode, the system keeps  learning the appearance of the ground during operation. The  system has been tested successfully in a variety of  environments, indoors as well as outdoors.  1. Introduction  Obstacle detection is an important task for many mobile robot applications. Most mobile robots rely on range data for obstacle detection. Popular sensors for range-based obstacle detection systems include ultrasonic sensors, laser rangefinders, radar, stereo vision, optical flow, and depth from focus. Because these sensors measure the distances from obstacles t...
ML
steiner98omsjava
OMS/Java: Model Extensibility of OODBMS for Advanced Application Domains . We showhow model extensibility of object-oriented data  management systems can be achieved through the combination of a highlevel  core object data model and an architecture designed with model  extensibility in mind. The resulting system, OMS#Java, is both a general  data management system and a framework for the developmentof  advanced database application systems. All aspects of the core model #  constructs, query language and constraints # can easily be generalised to  support, for example, the management of temporal, spatial and versioned  data. Speci#cally,we showhow the framework was used to extend the  core system to a temporal object-oriented database management system.  1 Introduction  Extensibility has often been considered a purely architectural issue in database management systems #DBMS#. In the 1980s, there was an increase in the various forms of DBMS that appeared --- many of whichwere tailored to speci#c application domains such as Geographical Information Systems or ...
DB
kueng01holistic
A Holistic Process Performance Analysis through a Performance Data Warehouse This paper describes how a performance data warehouse can be used to facilitate business process improvement that is based on holistic performance measurement. The feasibility study shows how management and analysis of performance data can be facilitated by a data warehouse approach. It is argued that many of the shortcomings of traditional measurement systems can be overcome with this performance data warehouse approach.
DB
stone98layered
A Layered Approach to Learning Client Behaviors in the RoboCup Soccer Server In the past few years, Multiagent Systems (MAS) has emerged as an active subfield of Artificial Intelligence (AI). Because of the inherent complexity of MAS, there is much interest in using Machine Learning (ML) techniques to help build multiagent systems. Robotic soccer is a particularly good domain for studying MAS and Multiagent Learning. Our approach to using ML as a tool for building Soccer Server clients involves layering increasingly complex learned behaviors. In this article, we describe two levels of learned behaviors. First, the clients learn a low-level individual skill that allows them to control the ball effectively. Then, using this learned skill, they learn a higher-level skill that involves multiple players. For both skills, we describe the learning method in detail and report on our extensive empirical testing. We also verify empirically that the learned skills are applicable to game situations. 1 Introduction  In the past few years, Multiagent Systems (MAS) has emerge...
AI
472783
A Meta-search Method Reinforced by Cluster Descriptors A meta-search engine acts as an agent for the participant search engines. It receives queries from users and redirects them to one or more of the participant search engines for processing. A meta-search engine incorporating many participant search engines is better than a single global search engine in terms of the number of pages indexed and the freshness of the indexes. The meta-search engine stores descriptive data (i.e., descriptors) about the index maintained by each participant search engine so that it can estimate the relevance of each search engine when a query is received. The ability for the meta-search engine to select the most relevant search engines determines the quality of the final result. To facilitate the selection process, the document space covered by each search engine must be described not only concisely but also precisely. Existing methods tend to focus on the conciseness of the descriptors by keeping a descriptor for a search engine 's entire index. This paper proposes to cluster a search engine's document space into clusters and keep a descriptor for each cluster. We show that cluster descriptors can provide a finer and more accurate representation of the document space, and hence enable the meta-search engine to improve the selection of relevant search engines. Two cluster-based search engine selection scenarios (i.e., independent and high-correlation) are discussed in this paper. Experiments verify that the cluster-based search engine selection can effectively identify the most relevant search engines and improve the quality of the search results consequently.  1 
IR
skow02security
A Security Architecture for Application Session Handoff Ubiquitous computing across a variety of wired and wireless connections still lacks an effective security architecture. In our research work, we address the specific issue of designing and building a security architecture for Application Session Handoff, a functionality which we envision will be a key component enabling ubiquitous computing. Our architecture incorporates a number of proven approaches into the new context of ubiquitous computing. We employ the Bell-LaPadula and capability models to realise access control and adopt Public Key Infrastructure (PKI)-based approaches to provide efficient and authenticated end-to-end security. To demonstrate the effectiveness of our design, we implemented an application enabled with this security architecture and showed that it incurred low latency.
HCI
karlgren98iterative
Iterative Information Retrieval Using Fast Clustering and Usage-Specific Genres This paper describes how collection specific empirically defined stylistics based genre prediction can be brought together together with rapid topical clustering to build an interactive information retrieval interface with multi-dimensional presentation of search results. The prototype presented addresses two specific problems of information retrieval: how to enrich the information seeking dialog by encouraging and supporting iterative refinement of queries, and how to enrich the document representation past the shallow semantics allowed by term frequencies.  Searching For More Than Words  Today's tools for searching information in a document database are based on term occurrence in texts. The searcher enters a number of terms and a number of documents where those terms or closely related terms appear comparatively frequently are retrieved and presented by the system in list form. This method works well up to a point. It is intuitively understandable, and for competent users and well e...
IR
488116
Agent-mediated Electronic Commerce: Scientific and Technological roadmap. this report is that a big part of Internet users have already sampled buying over the web (f.i. 40% in the UK), and a significat part qualify themselves as regular shoppers (f.i. 10% in the UK). Again, important differences between countries may be detected with respect to the expenses produced. For instance, Finland spent 20 times more that Spain on a per capita basis. The forecasts for European buying goods and services for the year 2002 suggest that the current 5.2 million shoppers will increase until 28.8 millions, and the European revenues from the current EUR3,032 million to EUR57,210 million. Finally, a significant increase in the number of European executives that believe in the future of electronic commerce has been observed (33% in 1999 up from 23% in 1998)
Agents
abonyi01modified
Modified Gath-Geva Fuzzy Clustering for Identification of Takagi-Sugeno Fuzzy Models The construction of interpretable Takagi--Sugeno (TS) fuzzy models by means of clustering is addressed. First, it is shown how the antecedent fuzzy sets and the corresponding consequent parameters of the TS model can be derived from clusters obtained by the Gath--Geva algorithm. To preserve the partitioning of the antecedent space, linearly transformed input variables can be used in the model. This may, however, complicate the interpretation of the rules. To form an easily interpretable model that does not use the transformed input variables, a new clustering algorithm is proposed, based on the Expectation Maximization (EM) identification of Gaussian mixture models. This new technique is applied to two well-known benchmark problems: the MPG (miles per gallon) prediction and a simulated second-order nonlinear process. The obtained results are compared with results from the literature.
ML
blockeel00executing
Executing Query Packs in ILP Inductive logic programming systems usually send large numbers  of queries to a database. The lattice structure from which these queries  are typically selected causes many of these queries to be highly similar. As a consequence, independent execution of all queries may involve a lot of redundant computation. We propose a mechanism for executing a hierarchically  structured set of queries (a "query pack") through which a lot of redundancy in the computation is removed. We have incorporated our  query pack execution mechanism in the ILP systems Tilde and Warmr by implementing a new Prolog engine ilProlog which provides support  for pack execution at a lower level. Experimental results demonstrate  significant efficiency gains. Our query pack execution mechanism is very  general in nature and could be incorporated in most other ILP systems,  with similar efficiency improvements to be expected.  
DB
dixon00resolutionbased
Resolution-Based Proof for Multi-Modal Temporal Logics of Knowledge Temporal logics of knowledge are useful in order to specify complex systems in which agents are both dynamic and have information about their surroundings. We present a resolution method for propositional temporal logic combined with multi-modal S5 and illustrate its use on examples. This paper corrects a previous proposal for resolution in multi-modal temporal logics of knowledge.  Keywords: temporal and modal logics, non-classical  resolution, theorem-proving  1 Introduction  Combinations of logics have been useful for specifying and reasoning about complex situations, for example multi-agent systems [21, 24], accident analysis [15], and security protocols [18]. For example, logics to formalise multi-agent systems often incorporate a dynamic component representing change of over time; an informational component to capture the agent's knowledge or beliefs; and a motivational component for notions such as goals, wishes, desires or intentions. Often temporal or dynamic logic is used for...
Agents
castillo99gpropii
G-Prop-II: Global Optimization of Multilayer Perceptrons using GAs A general problem in model selection is to obtain the right parameters that make a model fit observed data. For a Multilayer Perceptron (MLP) trained with Backpropagation (BP), this means finding appropiate layer size and initial weights. This paper proposes a method (G-Prop, genetic backpropagation) that attempts to solve that problem by combining a genetic algorithm (GA) and BP to train MLPs with a single hidden layer. The GA selects the initial weights and changes the number of neurons in the hidden layer through the application of specific genetic operators. G-Prop combines the advantages of the global search performed by the GA over the MLP parameter space and the local search of the BP algorithm. The application of the G-Prop algorithm to several real-world and benchmark problems shows that MLPs evolved using G-Prop are smaller and achieve a higher level of generalization than other perceptron training algorithms, such as QuickPropagation or RPROP, and other evolutive algorithms, s...
ML
federici95advances
Advances in Analogy-Based Learning: False Friends and Exceptional Items in Pronunciation By Paradigm-Driven Analogy When looked at from a multilingual perspective, grapheme-to-phoneme conversion is a challenging task, fraught with most of the classical NLP "vexed questions": bottle-neck problem of data acquisition, pervasiveness of exceptions, difficulty to state range and order of rule application, proper treatment of context-sensitive phenomena and long-distance dependencies, and so on. The hand-crafting of transcription rules by a human expert is onerous and time-consuming, and yet, for some European languages, still stops short of a level of correctness and accuracy acceptable for practical applications. We illustrate here a self-learning multilingual system for analogy-based pronunciation which was tested on Italian, English and French, and whose performances are assessed against the output of both statistically and rule-based transcribers. The general point is made that analogy-based self-learning techniques are no longer just psycholinguistically-plausible models, but competitive tools, combining the advantages of using language-independent, self-learning, tractable algorithms, with the welcome bonus of being more reliable for applications than traditional text-to-speech systems.
ML
192612
Ontobroker: The Very High Idea The World Wide Web (WWW) is currently one of the most important electronic information sources. However, its query interfaces and the provided reasoning services are rather limited. Ontobroker consists of a number of languages and tools that enhance query access and inference service of the WWW. The technique is based on the use of ontologies.  Ontologies are applied to annotate web documents and to provide query access and inference service that deal with the semantics of the presented information. In consequence, intelligent brokering services for web documents can be achieved without requiring to change the semiformal nature of web documents. Introduction  The World Wide Web (WWW) contains huge amounts of knowledge about almost all subjects you can think of. HTML documents enriched by multi-media applications provide knowledge in different representations (i.e., text, graphics, animated pictures, video, sound, virtual reality, etc.). Hypertext links between web documents represent r...
IR
445880
Market Protocols for Decentralized Supply Chain Formation   In order to effectively respond to changing market conditions, business partners must be able to rapidly form supply chains. This thesis approaches the problem of automating supply chain formation---the process of determining the participants in a supply chain, who will exchange what with whom, and the terms of the exchanges---within an economic framework.  In this thesis, supply chain formation is formalized as task dependency networks. This model captures subtask decomposition in the presence of resource contention---two important and challenging aspects of supply chain formation.  In order to form supply chains in a decentralized fashion, price systems provide an economic framework for guiding the decisions of self-interested agents. In competitive price equilibrium, agents choose optimal allocations with respect to prices, and outcomes are optimal overall. Approximate competitive equilibria yield approximately optimal allocations. Different market protocols are proposed for agents to negotiate the allocation of resources to form supply chains. In the presence of resource contention, these protocols produce better solutions than the greedy protocols common in the artificial intelligence and multiagent systems literature. The first protocol proposed is based on distributed, progressive, price-based auctions, and is analyzed with non-strategic, agent bidding policies. The protocol often converges to high-value supply chains, and when competitive equilibria exist, typically to approximate competitive equilibria. However, complemen- tarities in agent production technologies can cause the protocol to wastefully allocate inputs to agents that do not produce their out...
Agents
spelt99theorem
A Theorem Prover-Based Analysis Tool for Object-Oriented Databases We present a theorem-prover based analysis tool for object-oriented database systems with integrity  constraints. Object-oriented database specifications are mapped to higher-order logic (HOL). This allows  us to reason about the semantics of database operations using a mechanical theorem prover such  as Isabelle or PVS. The tool can be used to verify various semantics requirements of the schema (such as  transaction safety, compensation, and commutativity) to support the advanced transaction models used in  workflow and cooperative work. We give an example of method safety analysis for the generic structure  editing operations of a cooperative authoring system.  1 Introduction  Object-oriented specification methodologies and object-oriented programming have become increasingly important in the past ten years. Not surprisingly, this has recently led to an interest in object-oriented program verification in the theorem prover community, mainly using higher-order logic (HOL). Several dif...
DB
koskela00picsom
The PicSOM Retrieval System: Description and Evaluations We have developed an experimental system called PicSOM for retrieving images similar to a given set of reference  images in large unannotated image databases. The technique is based on a hierarchical variant of the Self-Organizing  Map (SOM) called the Tree Structured Self-Organizing Map (TS-SOM). Given a set of reference images, PicSOM  is able to retrieve another set of images which are most similar to the given ones. Each TS-SOM is formed using a  different image feature representation like color, texture, or shape. A new technique introduced in PicSOM facilitates  automatic combination of the responses from multiple TS-SOMs and their hierarchical levels. This mechanism adapts  to the user's preferences in selecting which images resemble each other. In this paper, a brief description of the system  and a set of methods applicable to evaluating retrieval performance of image retrieval applications are presented.  1 Introduction  Content-based image retrieval (CBIR) has been a subject...
HCI
37804
Embodied Evolution: Embodying an Evolutionary Algorithm in a Population of Robots We introduce Embodied Evolution (EE) as a methodology for the automatic design of robotic controllers. EE is an evolutionary robotics (ER) technique that avoids the pitfalls of the simulate-and-transfer method, allows the speed-up of evaluation time by utilizing parallelism, and is particularly suited to future work on multi-agent behaviors. In EE, an evolutionary algorithm is distributed amongst and embodied within a population of physical robots that reproduce with one another while situated in the task environment. We have built a population of eight robots and successfully implemented our first experiments. The controllers evolved by EE compare favorably to hand-designed solutions for a simple task. We detail our methodology, report our initial results, and discuss the application of EE to more advanced and distributed robotics tasks. 1. Introduction  Our work is inspired by the following vision. A large number of robots freely interact with each other in a shared environment, atte...
AI
chen99dynamic
Dynamic Agents this paper, we shall explain how dynamic behaviors are obtained and utilized through automatic action installation, and inter-agent communication. We also describe intra-agent communication between the carrier-part and the action part of a dynamic agent, and between di#erent actions carried by the same dynamic agent. We shall also discuss three triggering mechanisms for dynamic behavior modi#cation: planning-based, request-driven, and event-based.
Agents
devanbu99chime
CHIME: Customizable Hyperlink Insertion and Maintenance Engine for Software Engineering Environments Source code browsing is an important part of program comprehension. Browsers expose semantic and syntactic relationships (such as between object references and definitions) in GUI-accessible forms. These relationships are derived using tools which perform static analysis on the original software documents. Implementing such browsers is tricky. Program comprehension strategies vary, and it is necessary to provide the right browsing support. Analysis tools to derive the relevant crossreference relationships are often difficult to build. Tools to browse distributed documents require extensive coding for the GUI, as well as for data communications. Therefore, there are powerful motivations for using existing static analysis tools in conjunction with WWW technology to implement browsers for distributed software projects. The chime framework provides a flexible, customizable platform for inserting HTML links into software documents using information generated by existing software analysis tools. Using the chime specification language, and a simple, retargetable database interface, it is possible to quickly incorporate a range of different link insertion tools for software documents, into an existing, legacy software development environment. This enables tool builders to offer customized browsing support with a well-known GUI. This paper describes the chime architecture, and describes our experience with several re-targeting efforts of this system. 1
DB
mihalcea98word
Word Sense Disambiguation based on Semantic Density This paper presents a Word Sense Disambiguation method based on the idea of semantic density between words. The disambiguation is done in the context of WordNet. The Internet is used as a raw corpora to provide statistical information for word associations. A metric is introduced and used to measure the semantic density and to rank all possible combinations of the senses of two words. This method provides a precision of 58% in indicating the correct sense for both words at the same time. The precision increases as we consider more choices: 70% for top two ranked and 73% for top three ranked.  1 Introduction  Word Sense Disambiguation (WSD) is an open problem in Natural Language Processing. Its solution impacts other tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et al.1992), (Miller et al.1994), (Agirre and Rig...
AI
lagoze01open
The Open Archives Initiative: Building a low-barrier interoperability framework The Open Archives Initiative (OAI) develops and promotes interoperability solutions that aim to facilitate the efficient dissemination of content. The roots of the OAI lie in the E-Print community. Over the last year its focus has been extended to include all content providers. This paper describes the recent history of the OAI – its origins in promoting E-Prints, the broadening of its focus, the details of its technical standard for metadata harvesting, the applications of this standard, and future plans. Categories and Subject Descriptors
IR
neti00perceptual
Perceptual Interfaces For Information Interaction: Joint Processing Of Audio And Visual Information For Human-Computer Interaction We are exploiting the human perceptual principle of sensory integration (the joint use of audio and visual information) to improve the recognition of human activity (speech recognition, speech event detection and speaker change), intent (intent to speak) and human identity (speaker recognition), particularly in the presence of acoustic degradation due to noise and channel. In this paper, we present experimental results in a variety of contexts that demonstrate the benefit of joint audio-visual processing.
HCI
pouwelse99feasible
A Feasible Low-Power Augmented-Reality Terminal This paper studies the requirements for a truly wearable augmented-reality (AR) terminal. The requirements translate into a generic hardware architecture consisting of programmable modules communicating through a central interconnect. Careful selection of low-power components shows that it is feasible to construct an AR terminal that weighs about 2 kg and roughly dissipates 26 W. With stateof -the-art batteries and a 50% average resource utilization, the terminal can operate for about 10 hours. 1. Introduction  The goal of ubiquitous computing is to have computers act as "human assistants" that support us instantly. Computers should move out of our awareness instead of being at the center of our attention [14]. For ubiquitous computing to become reality we need two important technologies to mature: wireless communication (wearability) and augmented reality (user-interface). Wireless communication is obviously required to obtain services provided by an arbitrary computer regardless the ...
HCI
chen98learningbased
Learning-Based Vision and Its Application to Autonomous Indoor Navigation Learning-Based Vision and Its Application to Autonomous Indoor Navigation  By  Shaoyun Chen Adaptation is critical to autonomous navigation of mobile robots. Many adaptive mechanisms have been implemented, ranging from simple color thresholding to complicated learning with artificial neural networks (ANN). The major focus of this thesis lies in machine learning for vision-based navigation. Two well known vision-based navigation systems are ALVINN and ROBIN developed by Carnegie-Mellon University and University of Maryland, respectively. ALVINN uses a two-layer feedforward neural network while ROBIN relies on a radial basis function network (RBFN). Although current ANN-based methods have achieved great success in vision-based navigation, they have two major disadvantages: (1) Local minimum problem: The training of either multilayer perceptron or radial basis function network can get stuck at poor local minimums. (2) The flexibility problem: After the system has been trained in certain r...
ML
400733
View-independent Recognition of Hand Postures Since human hand is highly articulated and deformable, hand posture recognition is a challenging example in the research of view-independent object recognition. Due to the difficulties of the modelbased approach, the appearance-based learning approach is promising to handle large variation in visual inputs. However, the generalization of many proposed supervised learning methods to this problem often suffers from the insufficiency of labeled training data. This paper describes an approach to alleviate this difficulty by adding a large unlabeled training set. Combining supervised and unsupervised learning paradigms, a novel and powerful learning approach, the Discriminant-EM (D-EM) algorithm, is proposed in this paper to handle the case of small labeled training set. Experiments show that D-EM outperforms many other learning methods. Based on this approach, we implement a gesture interface to recognize a set o...
IR
lau99programming
Programming by Demonstration: An Inductive Learning Formulation Although Programming by Demonstration (PBD) has the potential to improve the productivity of unsophisticated users, previous PBD systems have used brittle, heuristic, domain-specific approaches to execution-trace generalization. In this paper we define two applicationindependent methods for performing generalization that are based on well-understood machine learning technology.  TGen vs uses version-space generalization, and  TGen foil is based on the FOIL inductive logic programming  algorithm. We analyze each method both theoretically and empirically, arguing that TGen vs has lower sample complexity, but TGen foil can learn a much more interesting class of programs.  Keywords  Programming by Demonstration, machine learning, inductive logic programming, version spaces  INTRODUCTION  Computer users are largely unable to customize massproduced applications to fit their individual tasks. This problem of end-user customization has been addressed in several ways.  ffl Adjustable preference...
ML
kruschwitz01exploiting
Exploiting Structure for Intelligent Web Search Together with the rapidly growing amount of online data we register an immense need for intelligent search engines that access a restricted amount of data as found in intranets or other limited domains. This sort of search engines must go beyond simple keyword indexing/matching, but they also have to be easily adaptable to new domains without huge costs. This paper presents a mechanism that addresses both of these points: first of all, the internal document structure is being used to extract concepts which impose a directorylike structure on the documents similar to those found in classified directories. Furthermore, this is done in an efficient way which is largely language independent and does not make assumptions about the document structure.
IR
knoblock00accurately
Accurately and Reliably Extracting Data from the Web: A Machine Learning Approach A critical problem in developing information agents for the Web is accessing data that is formatted for human use. We have developed a set of tools for extracting data from web sites and transforming it into a structured data format, such as XML. The resulting data can then be used to build new applications without having to deal with unstructured data. The advantages of our wrapping technology over previous work are the the ability to learn highly accurate extraction rules, to verify the wrapper to ensure that the correct data continues to be extracted, and to automatically adapt to changes in the sites from which the data is being extracted.  1 Introduction  There is a tremendous amount of information available on the Web, but much of this information is not in a form that can be easily used by other applications. There are hopes that XML will solve this problem, but XML is not yet in widespread use and even in the best case it will only address the problem within application domains...
IR
oard01clef
The CLEF 2003 Interactive Track The CLEF 2003 Interactive Track (iCLEF) was the third  year of a shared experiment design to compare strategies for cross-language  search assistance. Two kinds of experiments were performed: a) experiments  in Cross-Language Document Selection, where the user task is to  scan a ranked list of documents written in a foreign language, selecting  those which seem relevant to a given query. The aim here is to compare  di#erent translation strategies for an "indicative" purpose; and b)  Full Cross-Language Search experiments, where the user task is to maximize  the number of relevant documents that can be found in a foreignlanguage  collection with the help of an end-to-end cross-language search  system. Participating teams could choose to focus on any aspects of the  search task (e.g., query formulation, query translation and/or relevance  feedback). This paper describes the shared experiment design and briefly  summarizes the experiments run by the five teams that participated.
HCI
40032
Rotational Polygon Containment and Minimum Enclosure An algorithm and a robust floating point implementation is given for rotational polygon containment:given polygons P 1 ,P 2 ,P 3 ,...,P k and a container polygon C, find rotations and translations for the k polygons that place them into the container without overlapping. A version of the algorithm and implementation also solves rotational minimum enclosure: givenaclass C of container polygons, find a container C in C of minimum area for which containment has a solution. The minimum enclosure is approximate: it bounds the minimum area between (1-epsilon)A and A. Experiments indicate that finding the minimum enclosure is practical for k = 2, 3 but not larger unless optimality is sacrificed or angles ranges are limited (although these solutions can still be useful). Important applications for these algorithm to industrial problems are discussed. The paper also gives practical algorithms and numerical techniques for robustly calculating polygon set intersection, Minkowski sum, and range in...
DB
theilmann99disseminating
Disseminating Mobile Agents for Distributed Information Filtering An often claimed benefit of mobile agent technology is the reduction of communication cost. Especially the area of information filtering has been proposed for the application of mobile filter agents. However, an effective coordination of agents, which takes into account the current network conditions, is difficult to achieve.  This contribution analyses the situation that data distributed among various remote data servers has to be examined with mobile filter agents. We present an approach for coordinating the agents' employment, which minimizes communication costs. Validation studies on the possible cost savings for various constellations show that savings up to 90% can be achieved in the face of actual Internet conditions.  1. Introduction  An often claimed benefit of mobile agent technology is the reduction of communication cost, either for decreasing an application's latency or for reducing the load on a network. This prediction has been made especially for scenarios of information...
Agents
vilalta00quantification
A Quantification Of Distance-Bias Between Evaluation Metrics In Classification This paper provides a characterization of bias  for evaluation metrics in classification (e.g.,  Information Gain, Gini,   2  , etc.). Our characterization  provides a uniform representation  for all traditional evaluation metrics.  Such representation leads naturally to a measure  for the distance between the bias of  two evaluation metrics. We give a practical  value to our measure by observing if  the distance between the bias of two evaluation  metrics correlates with differences in  predictive accuracy when we compare two  versions of the same learning algorithm that  differ in the evaluation metric only. Experiments  on real-world domains show how  the expectations on accuracy differences generated  by the distance-bias measure correlate  with actual differences when the learning  algorithm is simple (e.g., search for the  best single-feature or the best single-rule).  The correlation, however, weakens with more  complex algorithms (e.g., learning decision  trees). Our results sh...
ML
ng98reconfiguring
On Reconfiguring Query Execution Plans in Distributed Object-Relational DBMS Massive database sizes and growing demands for decision support and data mining result in long-running queries in extensible Object-Relational DBMS, particularly in decision support and data warehousing analysis applications. Parallelization of query evaluation is often required for acceptable performance. Yet queries are frequently processed suboptimally due to (1) only coarse or inaccurate estimates of the query characteristics and database statistics available prior to query evaluation; (2) changes in system configuration and resource availability during query evaluation. In a distributed environment, dynamically reconfiguring query execution plans (QEPs), which adapts QEPs to the environment as well as the query characteristics, is a promising means to significantly improve query evaluation performance. Based on an operator classification, we propose an algorithm to coordinate the steps in a reconfiguration and introduce alternatives for execution context checkpointing and restorin...
DB
straccia01reasoning
Reasoning within Fuzzy Description Logics Description Logics (DLs) are suitable, well-known, logics for managing structured knowledge. They allow reasoning about individuals and well defined concepts, i.e. set of individuals with co#hfiP pro# erties. The experience in using DLs inapplicatio#& has sho wn that in many cases we wo#6H like to extend their capabilities. In particular, their use in the co# texto# Multimedia Info#mediafi Retrieval (MIR) leadsto the co# vincement that such DLssho#PF allo w the treatmento f the inherentimprecisio# in multimediao# ject co# tent representatio# and retrieval. In this paper we will present a fuzzyextensio#  ALC,co#  bining Zadeh's fuzzy lo#zy with a classical DL. In particular,co#rticu beco#FK fuzzy and, thus,reaso#HO6 ab o#fi impreciseco#recis is suppo#ppfi6 We will define its syntax, its semantics, describe its pro# erties and present a co#PHOSfi9 tpro#F&fi9KFS calculus for reasoning in it. 
ML
346939
Feature Selection in Web Applications By ROC Inflections and Powerset Pruning coetzee,compuman,lawrence,giles¥ A basic problem of information processing is selecting enough features to ensure that events are accurately represented for classification problems, while simultaneously minimizing storage and processing of irrelevant or marginally important features. To address this problem, feature selection procedures perform a search through the feature power set to find the smallest subset meeting performance requirements. Major restrictions of existing procedures are that they typically explicitly or implicitly assume a fixed operating point, and make limited use of the statistical structure of the feature power set. We present a method that combines the Neyman-Pearson design procedure on finite data, with the directed set structure of the Receiver Operating Curves on the feature subsets, to determine the maximal size of the feature subsets that can be ranked in a given problem. The search can then be restricted to the smaller subsets, resulting in significant reductions in computational complexity. Optimizing the overall Receiver Operating Curve also allows for end users to select different operating points and cost functions to optimize. The algorithm also produces a natural method of Boolean representation of the minimal feature combinations that best describe the data near a given operating point. These representations are especially appropriate when describing data using common text-related features useful on the web, such as thresholded TFIDF data. We show how to use these results to perform automatic Boolean query modification generation for distributed databases, such as niche metasearch engines. 1
IR
schmidt01xml
The XML Benchmark Project With standardization efforts of a query language for XML documents drawing to a close, researchers and users increasingly focus their attention on the database technology that has to deliver on the new challenges that the sheer amount of XML documents produced by applications pose to data management: validation, performance evaluation and optimization of XML query processors are the upcoming issues. Following a long tradition in database research, the XML Store Benchmark Project provides a framework to assess an XML database's abilities to cope with a broad spectrum of different queries, typically posed in real-world application scenarios. The benchmark is intended to help both implementors and users to compare XML databases independent of their own, specific application scenario. To this end, the benchmark o ers a set queries each of which is intended to challenge a particular primitive of the query processor or storage engine. The overall workload wepropose consists of a scalable document database and a concise, yet comprehensive set of queries, which covers the major aspects of query processing. The queries' challenges range from stressing the textual character of the document to data analysis queries, but include also typical ad-hoc queries. We complement our research with results obtained from running the benchmark on our XML database platform. They are intended to give a rst baseline, illustrating the state of the art.
DB
298827
Audio Signal Classification Audio signal classification (ASC) consists of extracting relevant features from a sound, and of using these features to identify into which of a set of classes the sound is most likely to fit. The feature extraction and grouping algorithms used can be quite diverse depending on the classification domain of the application. This paper presents background necessary to understand the general research domain of ASC, including signal processing, spectral analysis, psychoacoustics and auditory scene analysis. Also presented are the basic elements of classification systems. Perceptual and physical features are discussed, as well as clustering algorithms and analysis duration. Neural nets and hidden Markov models are discussed as they relate to ASC. These techniques are presented with an overview of the current state of the ASC research literature. 1 Introduction This paper will present a review of the state of the current research literature pertaining to audio signal classifiation (ASC). ...
AI
pitoura99scalable
Scalable Processing of Read-Only Transactions in Broadcast Push Recently, push-based delivery has attracted considerable attention as a means of disseminating information to large client populations in both wired and wireless settings. In this paper, we address the problem of ensuring the consistency and currency of client read-only transactions in the presence of updates. To this end, additional control information is broadcast. A suite of methods is proposed that vary in the complexity and volume of the control information transmitted and subsequently differ in response times, degrees of concurrency, and space and processing overheads. The proposed methods are combined with caching to improve query latency. The relative advantages of each method are demonstrated through both simulation results and qualitative arguments. Read-only transactions are processed locally at the client without contacting the server and thus the proposed approaches are scalable, i.e., their performance is independent of the number of clients.  1. Introduction  In traditio...
DB
2056
Interaction between Path and Type Constraints XML [7], which is emerging as an important standard for data exchange on the World-Wide Web, highlights the importance of semistructured data. Although the XML standard itself does not require any schema or type system, a number of proposals [6, 17, 19] have been developed that roughly correspond to data definition languages. These allow one to constrain the structure of XML data by imposing a schema on it. These and other proposals also advocate the need for integrity constraints, another form of constraints that should, for example, be capable of expressing inclusion constraints and inverse relationships. The latter have recently been studied as path constraints in the context of semistructured data [4, 9]. It is likely that future XML proposals will involve both forms of constraints, and it is therefore appropriate to understand the interaction between them. This paper investigates that interaction. In particular it studies constraint implication problems, which are important both i...
DB
shu98using
Using Constraint Satisfaction for View Update Translation View update is the problem of translating update requests against a view into update requests against the base data. In this paper, we present a novel approach to this problem in relational databases. Using conditional tables to represent relational views, we translate a view update problem into a disjunction of a number of constraint satisfaction problems (CSPs). Solutions to the CSPs correspond to possible translations of the view update. Semantic information to resolve ambiguity can be handled as additional constraints in the CSPs. This approach enables us to apply the rich results of the CSP research to analyze and solve an important problem in database management.
DB
bates98state
The State of the Art in Distributed and Dependable Computing This report is dedicated to the memory of Henrique Fonseca
HCI
arpinar00open
An Open Electronic Marketplace through Agent-based Workflows: MOPPET We propose an electronic marketplace architecture, called MOPPET, where the commerce processes in the marketplace are modeled as adaptable agent-based workflows. The higher level of abstraction provided by the workflow technology makes the customization of electronic commerce processes for different users possible. Agent-based implementation, on the other hand, provides for a highly reusable component-based workflow architecture as well as negotiation ability and the capability to adapt to dynamic changes in the environment. Agent communication is handled through Knowledge Query and Manipulation Language (KQML). A workflow-based architecture also makes it possible for complete modeling of electronic commerce processes by allowing involved parties to be able to invoke already existing applications or to define new tasks and to restructure the control and data flow among the tasks to create custom built process definitions. In the proposed architecture all data exchanges are realized through Extensible Markup Language (XML) providing uniformity, simplicity and a highly open and interoperable architecture. Metadata of activities are expressed through Resource Description Framework (RDF). Common Business Library (CBL) is used for achieving interoperability across business domains and domain specific Document Type Definitions (DTDs) are used for vertical industries. We provide our own specifications for missing DTDs to be replaced by the original specifications when they become available.
IR
502274
Interpretation of Shape-related Iconic Gestures in Virtual Environments The interpretation of iconic gestures in spatial domains is a promising idea to improve the communicative capabilities of human-computer interfaces. So far, approaches towards gesture recognition focused mainly on deictic and emblematic gestures. Iconics, viewed as iconic signs in the sense of Peirce, are different from deictics and emblems, for their relation to the referent is based on similarity. In the work reported here, the breakdown of the complex notion of similarity provides the key idea towards a computational model of gesture semantics for iconic gestures. Based on an empirical study,we describe first steps towards a recognition model for shape-related iconic gestures and its implementation in a prototype gesture recognition system. Observations are focused on spatial concepts and their relation to features of iconic gestural expressions. The recognition model is based on a graphmatching method which compares the decomposed geometrical structures of gesture and object.
HCI
sanchez98agora
Agora: Enhancing Group Awareness and Collaboration in Floristic Digital Libraries .  Digital libraries can be regarded as virtual spaces in which collaborative scholarly research can be conducted. Floristic digital libraries provide such collaboration spaces for scientists working on solutions for Earth's biodiversity problems. However, group awareness and collaboration are not easily achieved in an increasingly distributed environment such as the virtual space in which digital library users (particularly botanists and biologists) do their work. We describe an environment that enables group awareness, communication, and collaboration among users in a globally accessible floristic digital library. This is achieved by extending existing library facilities with recommendation and alerting services, as well as various communication interfaces.  Keywords: Recommendation services, group awareness, agents, floristic digital libraries.  1. Introduction  Digital libraries comprise highly complex and dynamic information spaces on top of which a variety of services are provide...
IR
wang01supporting
Supporting Workspace Awareness in Distance Learning Environments: Issues and Experiences in the Development of a Collaborative Learning System In recent years, we have witnessed an enormous growth in networks and related technologies. Course materials are increasingly published on web servers, and students are encouraged to access these at leisure. Distance learning via the WWW shifted the education paradigm from teacher-centered instruction to user-centered collaborative learning. Systems that allow users to learn collaboratively are increasingly interesting to scientific communities and learning organizations. We initially designed and prototyped a collaborative system to support collaborative learning over the Internet. A usability study of the first prototype revealed the importance of awareness information. Our review of three wellknown collaborative systems finds that such systems today also lack support for awareness information, especially workspace awareness. We then consider various types of awareness in collaborative learning situations and set out the design requirements of our system. From these requirements, we have designed and prototyped several awareness widgets for a typical collaborative tool: the shared electronic whiteboard. These widgets help learners maintain awareness of other learners' interactions with the shared workspace.
HCI
353348
A Formal Approach to Detecting Security Flaws in Object-Oriented Databases This paper adopts the method-based authorization model and assumes the following database management policies. Let (m, (c 1 ,c 2 , ...,c n )) be in an authorization for a user u.
DB
khudanpur99maximum
A Maximum Entropy Language Model Integrating N-Grams And Topic Dependencies For Conversational Speech Recognition A compact language model which incorporates local dependencies in the form of N-grams and long distance dependencies through dynamic topic conditional constraints is presented. These constraints are integrated using the maximum entropy principle. Issues in assigning a topic to a test utterance are investigated. Recognition results on the Switchboard corpus are presented showing that with a very small increase in the number of model parameters, reduction in word error rate and language model perplexity are achieved over trigram models. Some analysis follows, demonstrating that the gains are even larger on content-bearing words. The results are compared with those obtained by interpolating topicindependent and topic-specific N-gram models. The framework presented here extends easily to incorporate other forms of statistical dependencies such as syntactic word-pair relationships or hierarchical topic constraints. 1. INTRODUCTION Language modeling is a crucial component of systems that c...
IR
bellavista01how
How to Monitor and Control Resource Usage in Mobile Agent Systems The Mobile Agent technology has already shown its advantages, but at the same time has already remarked new problems currently limiting its diffusion in commercial environments. A key issue is to control the operations that foreign mobile agents are authorized to perform on hosting execution environments. It is necessary not only to rule the MA access to resources but also to control resource usage of admitted agents at execution time, for instance to protect against possible denial-of-service attacks. The paper presents a solution framework for the on-line monitoring and control of Java-based MA platforms. In particular, it describes the design and implementation of MAPI, an on-line monitoring component that we have integrated within the SOMA system. The paper shows how to use MAPI as the building block of a distributed monitoring tool that gives application- and kernel-level information about the state of mobile agents and their resource usage, thus enabling the enforcement of management policies on MA resource consumption.  1. 
Agents
mchugh99optimizing
Optimizing Branching Path Expressions Path expressions form the basis of most query languages for semistructured data and XML, specifying  traversals through graph-based data. We consider the query optimization problem for path expressions  that "branch," or specify traversals through two or more related subgraphs; such expressions are  common in nontrivial queries over XML data. Searching the entire space of query plans for branching  path expressions is generally infeasible, so we introduce several heuristic algorithms and postoptimizations  that generate query plans for branching path expressions. All of our algorithms have  been implemented in the Lore database system for XML, and we report experimental results over a variety  of database and query shapes. We compare optimization and execution times across our suite of  algorithms and post-optimizations, and for small queries we compare against the optimal plan produced  by an exhaustive search of the plan space.  1 Introduction  Wo r k i n semistructured data [Abi97, ...
DB
452812
Pedagogical Content Knowledge in a Tutorial Dialogue System to Support Self-Explanation We are engaged in a research project to create a tutorial dialogue system that helps students learn through self-explanation. Our current prototype is able to analyze students' general explanations of their problem-solving steps, stated in their own words, recognize the types of omissions that we often see in these explanations, and provide feedback. Our approach to architectural tradeoffs is to equip the system with a sophisticated NLU component but to keep dialogue management simple. The system has a knowledge-based NLU component, which performed with 81% accuracy in a preliminary evaluation study. The system's approach to dialogue management can be characterised as "classify and react". In each dialogue cycle, the system classifies the student input with respect to a hierarchy of explanation categories that represent common ways of stating complete or incomplete explanations of geometry rules. The system then provides feedback based on that classification. We consider what extensions are necessary or desirable in order to make the dialogues more robust.
HCI
renambot00cavestudy
CAVEStudy: an Infrastructure for Computational Steering in Virtual Reality Environments We present the CAVEStudy system that enables scientists to interactively steer a simulation from a virtual reality (VR) environment. No modification to the source code is necessary. CAVEStudy allows interactive and immersive analysis of a simulation running on a remote computer. Using a high-level description of the simulation, the system generates the communication layer (based on CAVERNSoft) needed to control the execution and to gather data at runtime. We describe three case-studies implemented with CAVEStudy: soccer simulation, diode laser simulation, and molecular dynamics.  1. Introduction  High-speed networks and high performance graphics open opportunities for completely new types of applications. As a result, the world of scientific computing is moving away from the batch-oriented management to interactive programs. Also, virtual reality (VR) systems are now commercially available, but so far scientists mainly use them for off-line visualization of data sets produced by a simu...
HCI
traum99speech
Speech Acts for Dialogue Agents this paper by the U.S. Army Research Office under contract/grant number  DAAH 04 95 10628 and the U.S. National Science Foundation under grant IRI9311988. Some of the work described above was developed in collaboration with James Allen and supported by ONR/DARPA under grant number N00014-92J -1512, by ONR under research grant number N00014-90-J-1811, and by NSF under grant number IRI-9003841.
Agents
amyot00extension
On the Extension of UML with Use Case Maps Concepts . Descriptions of reactive systems focus heavily on behavioral aspects,  often in terms of scenarios. To cope with the increasing complexity of services  provided by these systems, behavioral aspects need to be handled early in the  design process with flexible and concise notations as well as expressive concepts.  UML offers different notations and concepts that can help describe such  services. However, several necessary concepts appear to be absent from UML,  but present in the Use Case Map (UCM) scenario notation. In particular, Use  Case Maps allow scenarios to be mapped to different architectures composed of  various component types. The notation supports structured and incremental development  of complex scenarios at a high level of abstraction, as well as their  integration. UCMs specify variations of run-time behavior and scenario structures  through sub-maps "pluggable" into placeholders called stubs. This paper  presents how UCM concepts could be used to extend the semantics...
DB
mohan99repeating
Repeating History beyond ARIES In this paper, I describe first the background behind the development of the original ARIES recovery method, and its significant impact on the commercial world and the research community. Next, I provide a brief introduction to the various concurrency control and recovery methods in the ARIES family of algorithms. Subsequently, I discuss some of the recent developments affecting the transaction management area and what these mean for the future. In ARIES, the concept of repeating history turned out to be an important paradigm. As I examine where transaction management is headed in the world of the internet, I observe history repeating itself in the sense of requirements that used to be considered significant in the mainframe world (e.g., performance, availability and reliability) now becoming important requirements of the broader information technology community as well. 1. Introduction Transaction management is one of the most important functionalities provided by a...
DB
frank98generating
Generating Accurate Rule Sets Without Global Optimization The two dominant schemes for rule-learning, C4.5 and RIPPER, both operate in two stages. First they induce an initial rule set and then they refine it using a rather complex optimization stage that discards (C4.5) or adjusts (RIPPER) individual rules to make them work better together. In contrast, this paper shows how good rule sets can be learned one rule at a time, without any need for global optimization. We present an algorithm for inferring rules by repeatedly generating partial decision trees, thus combining the two major paradigms for rule generation—creating rules from decision trees and the separate-and-conquer rule-learning technique. The algorithm is straightforward and elegant: despite this, experiments on standard datasets show that it produces rule sets that are as accurate as and of similar size to those generated by C4.5, and more accurate than RIPPER’s. Moreover, it operates efficiently, and because it avoids postprocessing, does not suffer the extremely slow performance on pathological example sets for which the C4.5 method has been criticized.  
ML
kurimo99latent
Latent Semantic Indexing by Self-Organizing Map An important problem for the information retrieval from spoken documents is how to extract those relevant documents which are poorly decoded by the speech recognizer. In this paper we propose a stochastic index for the documents based on the Latent Semantic Analysis (LSA) of the decoded document contents. The original LSA approach uses Singular Value Decomposition to reduce the dimensionality of the documents. As an alternative, we propose a computationally more feasible solution using Random Mapping (RM) and Self-Organizing Maps (SOM). The motivation for clustering the documents by SOM is to reduce the effect of recognition errors and to extract new characteristic index terms. Experimental indexing results are presented using relevance judgments for the retrieval results of test queries and using a document perplexity defined in this paper to measure the power of the index models. 1. INTRODUCTION  In this paper we present methods for indexing speech data which has been automatically t...
IR
huget02desiderata
Desiderata for Agent Oriented Programming Languages Multiagent system designers need programming languages in order to develop agents and multiagent systems. Current approaches consist to use classical programming languages like C or C++ and above all Java which is the most preferred language by agent community thanks to its rich library of functions. The aim of Java is not to design multiagent systems so it does not encompass multiagent features. The aim of this paper is to present a set of characteristics which could be present in an agent-oriented programming language. This paper also describes what kind of multiagent systems could be developed with this set of characteristics.
Agents
jenkins00primitivebased
Primitive-Based Movement Classification for Humanoid Imitation . Motor control is a complex problem and imitation is a powerful mechanism for acquiring new motor  skills. In this paper, we describe perceptuo-motor primitives, a biologically-inspired notion for a basis set of  perceptual and motor routines. Primitives serve as a vocabulary for classifying and imitating observed human  movements, and are derived from the imitator's motor repertoire. We describe a model of imitation based  on such primitives and demonstrate the feasibility of the model in a constrained implementation. We present  approximate motion reconstruction generated from visually captured data of typically imitated tasks taken from  aerobics, dancing, and athletics.  1 Introduction  Imitation is a powerful mechanism for acquiring new skills. It involves an intricate interaction between perceptual and motor mechanisms, both of which are complex in themselves. Research into vision and motor control has explored the role of "subroutines", schemas [1], and other variants based on ...
ML
aha97learning
Learning to Refine Case Libraries: Initial Results . Conversational case-based reasoning (CBR) systems, which incrementally extract a query description through a user-directed conversation, are advertised for their ease of use. However, designing large case libraries that have good performance (i.e., precision and querying efficiency) is difficult. CBR vendors provide guidelines for designing these libraries manually, but the guidelines are difficult to apply. We describe an automated inductive approach that revises conversational case libraries to increase their conformance with design guidelines. Revision increased performance on three conversational case libraries. 1 Introduction  In the context of the ECML-97 Workshop entitled Case-Based Learning: Beyond Classification of Feature Vectors, this paper's contribution focuses on using machine learning methods to assist in the design of case libraries. These libraries are designed for solution retrieval rather than classification tasks, and each case might contain a unique solution. Cas...
ML
murray01specifying
Specifying Agents with UML in Robotic Soccer The use of agents and multiagent systems is widespread in computer  science nowadays. Thus the need for methods to specify agents in a clear  and simple manner arises.  In this paper we propose an approach to specifying agents with the  help of UML statecharts. Agents are specified on different levels of abstraction.  In addition a method for specifying multiagent plans with explicit  cooperation is shown.  As an example domain we chose robotic soccer, which lays the basis  of the annual RoboCup competitions. Robotic soccer is an ideal testbed  for research in the fields of robotics and multiagent systems. In the  RoboCup Simulation League the research focus is laid on agents and  multiagent systems, and we will demonstrate our approach by using examples  from this domain.  Keywords: Multiagent Systems, Unified Modeling Language (UML),  Specification, RoboCup, Robotic Soccer  1 
Agents
48156
Agents in Annotated Worlds Virtual worlds offer great potential as environments for education, entertainment, and collaborative work. Agents that function effectively in heterogeneous virtual spaces must have the ability to acquire new behaviors and useful semantic information from those contexts. The human-computer interaction literature discusses how to construct spaces and objects that provide "knowledge in the world" that aids human beings to perform these tasks. In this paper, we describe how to build comparable annotated environments containing explanations of the purpose and uses of spaces and activities that allow agents quickly to become intelligent actors in those spaces. Examples are provided from our application domain, believable agents acting as inhabitants and guides in a children's exploratory world. Keywords: believability, human-like qualities of synthetic agents, synthetic agents 1. Introduction Today's virtual environments present opportunities for simulation and interaction involving ma...
Agents
weber00unsupervised
Unsupervised Learning of Models for Recognition Abstract. We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars. 1 Introduction and Related Work We are interested in the problem of recognizing members of object classes, where we define an object class as a collection of objects which share characteristic features or parts that are visually similar and occur in similar spatial configurations. When building models for object classes of this type, one is faced with three problems (see Fig. 1).
ML
wiederhold95valueadded
Value-added Mediation in Large-Scale Information Systems Many information-processing tasks can be part of multiple customer applications, as summarizing  stock prices, integrating catolog information from several companies in the same line of business,  predicting the weather, and checking on transportation resources. We assign such sharable services  to an active middleware layer, interposed between clients and servers. We define domain-specific  mediator modules to populate this layer.  Such mediating services must be of value to the customers, so that it will benefit their client  applications to access mediators rather than the server sources directly. Several categories of value  can be considered: improvement in access and coverage, improvement of content, and delegation  of maintenance. We will define criteria for mediating modules: ownership by party who assumes  responsibility for the rseults of the services, domain-specificity to delimit the scope of such a responsibility,  and, of course, conformance with interface standards that ...
HCI
jansche01information
Information Extraction via Heuristics for a Movie Showtime Query System Semantic interpretation for limited-domain spoken dialogue systems often amounts to extracting information from utterances. For a system that provides movie showtime information, queries are classified along four dimensions: question type, and movie titles, towns and theaters that were mentioned. Simple heuristics suffice for constructing highly accurate classifiers for the latter three attributes; classifiers for the question type attribute are induced from data using features tailored to spoken language phenomena. Since separate classifiers are used for the four attributes, which are not independent, certain errors can be detected and corrected, thus increasing robustness. 1.
IR
ha98geometric
Geometric Foundations for Interval-Based Probabilities The need to reason with imprecise probabilities arises in a wealth of situations ranging from pooling of knowledge from multiple experts to abstraction-based probabilistic planning. Researchers have typically represented imprecise probabilities using intervals and have developed a wide array of different techniques to suit their particular requirements. In this paper we provide an analysis of some of the central issues in representing and reasoning with interval probabilities. At the focus of our analysis is the probability cross-product operator and its interval generalization, the cc-operator. We perform an extensive study of these operators relative to manipulation of sets of probability distributtions. This study provides insight into the sources of the strengths and weaknesses of various approaches to handling probability intervals. We demonstrate the application of our results to the problems of inference in interval Bayesian networks and projection and evaluation of abstract pro...
AI
handschuh02authoring
Authoring and Annotation of Web Pages in CREAM Richly interlinked, machine-understandable data constitute the basis for the Semantic Web.  We provide a framework, CREAM, that allows for creation of metadata. While the annotation  mode of CREAM allows to create metadata for existing web pages, the authoring mode lets  authors create metadata --- almost for free --- while putting together the content of a page.  As a particularity of our framework, CREAM allows to create relational metadata, i.e. metadata  that instantiate interrelated definitions of classes in a domain ontology rather than a comparatively  rigid template-like schema as Dublin Core. We discuss some of the requirements one  has to meet when developing such an ontology-based framework, e.g. the integration of a metadata  crawler, inference services, document management and a meta-ontology, and describe its  implementation, viz. Ont-O-Mat a component-based, ontology-driven Web page authoring and  annotation tool.
IR
235157
Face Detection Using Mixtures of Linear Subspaces We present two methods using mixtures of linear subspaces for face detection in gray level images. One method uses a mixture of factor analyzers to concurrently perform clustering and, within each cluster, perform local dimensionality reduction. The parameters of the mixture model are estimated using an EM algorithm. A face is detected if the probability of an input sample is above a predefined threshold. The other mixture of subspaces method uses Kohonen’s self-organizing map for clustering and Fisher Linear Discriminant to find the optimal projection for pattern classification, and a Gaussian distribution to model the class-conditional density function of the projected samples for each class. The parameters of the class-conditional density functions are maximum likelihood estimates and the decision rule is also based on maximum likelihood. A wide range of face images including ones in different poses, with different expressions and under different lighting conditions are used as the training set to capture the variations of human faces. Our methods have been tested on three sets of 225 images which contain 871 faces. Experimental results on the first two datasets show that our methods perform as well as the best methods in the literature, yet have fewer false detects. 1
ML
koller98probabilistic
Probabilistic Frame-Based Systems Two of the most important threads of work in knowledge representation today are frame-based representation systems (FRS's) and Bayesian networks (BNs). FRS's provide an excellent representation for the organizational structure of large complex domains, but their applicability is limited because of their inability to deal with uncertainty and noise. BNs provide an intuitive and coherent probabilistic representation of our uncertainty, but are very limited in their ability to handle complex structured domains. In this paper, we provide a language that cleanly integrates these approaches, preserving the advantages of both. Our approach allows us to provide natural and compact definitions of probability models for a class, in a way that is local to the class frame. These models can be instantiated for any set of interconnected instances, resulting in a coherent probability distribution over the instance properties. Our language also allows us to represent important types of uncertainty tha...
AI
barros97towards
Towards Real-Scale Business Transaction Workflow Modelling While the specification languages of workflow management systems focus on process execution semantics, the successful development of workflows relies on a fuller conceptualisation of business processing, including process semantics. For this, a wellspring of modelling techniques, paradigms and informal-formal method extensions which address the analysis of organisational processing structures (enterprise modelling) and communication (based on speech-act theory), is available. However, the characterisations - indeed the cognition - of workflows still appears coarse. In this paper, we provide the complementary, empirical insight of a real-scale business transaction workflow. The development of the workflow model follows a set of principles which we believe address workflow modelling suitability. Through the principles, advanced considerations including asynchronous as well as synchronous messaging, temporal constraints and a service-oriented perspective are motivated. By illust...
HCI
dimitrova02web
Web Genre Visualization Web users vary widely in terms of their expertise on the topics for which they search, the amount of detail they seek, etc. Unfortunately, today's one-size-fits-all Web search services do not cater to such individual preferences. For example, it is difficult to query for documents that give extensive detail but assume modest prior expertise. We describe how shallow text classification techniques can be used to sort the documents returned by Web search services according to genre dimensions such as level of expertise and amount of detail, and propose a simple visualization interface that helps users rapidly find appropriate documents. (Keywords: document genre; information retrieval, visualisation, text classification, shallow linguistic processing)  Motivation  Consider two users seeking information about Pearson' correlation coefficient. Alice is writing a data-analysis program and needs a web page to remind her of the equations. Bob, a teacher, wants to point his pupils to an overview that isn't bogged down in equations.
IR
horvitz99principles
Principles of Mixed-Initiative User Interfaces Recent debate has centered on the relative promise of focusing user-interface research on developing new  metaphors and tools that enhance users' abilities to directly manipulate objects versus directing effort toward developing interface agents that provide automation. In this paper, we review principles that show promise for allowing engineers to enhance human---computer interaction through an elegant coupling of automated services with direct manipulation. Key ideas will be highlighted in terms of the LookOut system for scheduling and meeting management.  Keywords  Intelligent agents, direct manipulation, user modeling, probability, decision theory, UI design  INTRODUCTION  There has been debate among researchers about where great opportunities lay for innovating in the realm of human--- computer interaction [10]. One group of researchers has expressed enthusiasm for the development and application of new kinds of automated services, often referred to as interface "agents." The effo...
HCI
edmond98achieving
Achieving Workflow Adaptability by means of Reflection Belief in the importance of business processes has triggered considerable interest in the workflow systems that automate these processes. However, of the two competing management philosophies that promulgate business processes, Business Process Reengineering proposes radical change, whereas Continuous Process Improvement places much greater emphasis on adaptability. The former school is somewhat discredited, whereas the latter school seems more likely to endure, thus making the flexibility and evolution of workflows an issue of increasing importance. In this paper, we present a programmable object-oriented metalevel framework which aims to reveal the processes of assembling and coordinating the tasks that make up business processes. This is achieved by isolating four key facets -- state, behaviour, location and coordination. In particular, we open up the general process of task coordination and specification, allowing for extensions in a planned way. By suitable manipulation of coordin...
DB
293286
Managing Historical Semistructured Data this article appeared in [6].
DB
chen01optimal
Optimal Anytime Search For Constrained Nonlinear Programming In this thesis, we study optimal anytime stochastic search algorithms (SSAs) for solving general constrained nonlinear programming problems (NLPs) in discrete, continuous and mixed-integer space. The algorithms are general in the sense that they do not assume differentiability or convexity of functions. Based on the search algorithms, we develop the theory of SSAs and propose optimal SSAs with iterative deepening in order to minimize their expected search time. Based on the optimal SSAs, we then develop optimal anytime SSAs that generate improved solutions as more search time is allowed. Our SSAs for solving general constrained NLPs are based on the theory of discrete con-strained optimization using Lagrange multipliers that shows the equivalence between the set of constrained local minima (CLMdn) and the set of discrete-neighborhood saddle points (SPdn). To implement this theory, we propose a general procedural framework for locating an SPdn. By incorporating genetic algorithms in the framework, we evaluate new constrained search algorithms: constrained genetic algorithm (CGA) and combined constrained simulated annealing and genetic algorithm (CSAGA).
AI
351802
Design and Implementation of the OLOG Deductive Object-Oriented Database Management System . OLOG is a novel deductive database system for advanced  intelligent information system applications. It directly supports eective  storage, ecient access and inference of large amount of persistent data  with complex structures. It provides a SQL-like data denition language  and data manipulation language, and a declarative rule-based query language.  It combines the best of the deductive, object-oriented, and objectrelational  approaches in a uniform framework. This paper describes the  design and implementation of the OLOG system.  1 Introduction  Deductive, object-oriented, and object-relational databases are three important extensions of the traditional relational database technology. Deductive databases stem from the integration of logic programming and relational databases. It oers representational and operational uniformity, reasoning capabilities, recursion, declarative querying, ecient secondary storage access, etc. However, deductive databases based on relational databas...
DB
38463
Formalising the Knowledge Content of Case Memory Systems Discussions of case-based reasoning often reflect an implicit assumption that a case memory system will become better informed, i.e. will increase in knowledge, as more cases are added to the case-base. This paper considers formalisations of this `knowledge content' which are a necessary preliminary to more rigourous analysis of the performance of case-based reasoning systems. In particular we are interested in modelling the learning aspects of case-based reasoning in order to study how the performance of a case-based reasoning system changes as it accumlates problem-solving experience. The current paper presents a `case-base semantics' which generalises recent formalisations of case-based classification. Within this framework, the paper explores various issues in assuring that these sematics are well-defined, and illustrates how the knowledge content of the case memory system can be seen to reside in both the chosen similarity measure and in the cases of the case-base. 1 Introduction ...
ML
469106
Time Series Classification by Boosting Interval Based Literals A supervised classification method for temporal series, even multivariate, is presented. It is based on boosting very simple classifiers: clauses with one literal in the body. The background predicates are based on temporal intervals. Two types of predicates are used: i) relative predicates, such as "increases" and "stays", and ii) region predicates, such as "always" and "sometime", which operate over regions in the dominion of the variable. Experiments on di#erent data sets, several of them obtained from the UCI repositories, show that the proposed method is highly competitive with previous approaches.  Keywords: time series classification, interval based literals, boosting, machine learning.  1 
DB
wachsmuth01lifelike
Lifelike Gesture Synthesis and Timing for Conversational Agents Besides the inclusion of gesture recognition devices as an intuitive input modality, the synthesis of lifelike gesture is finding growing attention in human-computer interface research. In particular, the generation of synthetic gesture in connection with text-to-speech systems is one of the goals for embodied conversational agents which have become a new paradigm for the study of gesture and for human-computer interface [1]. Embodied conversational agents are computer-generated characters that demonstrate similar properties as humans in face-to-face conversation, including the ability to produce and respond to verbal and nonverbal communication. They may represent the computer in an interaction with a human or represent their human users as &quot;avatars &quot; in a computational environment. In this context, this contribution focusses on an approach for synthesizing lifelike gestures for an articulated virtual agent, with particular emphasis on how to achieve temporal coordination with external information such as the signal generated by a text-to-speech system. The context of this research is the conception of an &quot;articulated communicator &quot; that conducts multimodal dialogue with a human partner in cooperating on a construction task. Gesture production and performance in humans is a complex and multi-stage process.
HCI
roman00legorb
LegORB and Ubiquitous CORBA The increasing popularity of ubiquitous computing and the new approaches for low-consumption, shortrange wireless connectivity will enable a future with hundreds of heterogeneous devices interconnected to achieve a common task. However, communication among those network enabled heterogeneous devices requires standard protocols and well defined interfaces.  While existing middleware architectures already offer standard mechanisms (DCOM, CORBA, JINI), they are, in most of the cases, not suitable for most of the heterogeneous devices. The resources required by those middleware solutions normally exceed the computational limits of the heterogeneous devices  We present in this paper a minimalist component-based Object Request Broker (ORB) that can be dynamically reconfigured and requires, for the smallest configuration, 6Kb of memory.  Introduction  The incoming ubiquitous computing trend allows the existence of collections of network-enabled devices attached to rooms, people and buildings....
HCI
sycara95using
Using Case-Based Reasoning to Acquire User Scheduling Preferences that Change over Time Production/Manufacturing scheduling typically involves the acquisition of user optimization preferences. The ill-structuredness of both the problem space and the desired objectives make practical scheduling problems difficult to formalize and costly to solve, especially when problem configurations and user optimization preferences change over time. This paper advocates an incremental revision framework for improving schedule quality and incorporating user dynamically changing preferences through Case-Based Reasoning. Our implemented system, called CABINS, records situation-dependent tradeoffs and consequences that result from schedule revision to guide schedule improvement. The preliminary experimental results show that CABINS is able to effectively capture both user static and dynamic preferences which are not known to the system and only exist implicitly in a extensional manner in the case base. 1 Introduction Scheduling deals with allocation of a limited set of resources to a nu...
ML
295535
The Adaptive Agent Architecture: Achieving FaultTolerance Using Persistent Broker Teams Brokers are used in many multi-agent systems for locating agents, for routing and sharing  information, for managing the system, and for legal purposes, as independent third parties.  However, these multi-agent systems can be incapacitated and rendered non-functional when the  brokers become inaccessible due to failures such as machine crashes, network breakdowns, and  process failures that can occur in any distributed software system.  We propose that the theory of teamwork can be used to create robust brokered architectures that  can recover from broker failures, and we present the Adaptive Agent Architecture (AAA) to show  the feasibility of this approach. The AAA brokers form a team with a joint commitment to serve  any agent that registers with the broker team as long as the agent remains registered with the  team. This commitment enables the brokers to substitute for each other when needed. A multiagent  system based on the AAA can continue to work despite broker failures as long...
Agents
201746
Incremental and Interactive Sequence Mining The discovery of frequent sequences in temporal databases is an important data mining problem. Most current work assumes that the database is static, and a database update requires rediscovering all the patterns by scanning the entire old and new database. In this paper, we propose novel techniques for maintaining sequences in the presence of a) database updates, and b) user interaction (e.g. modifying mining parameters). This is a very challenging task, since such updates can invalidate existing sequences or introduce new ones. In both the above scenarios, we avoid re-executing the algorithm on the entire dataset, thereby reducing execution time. Experimental results confirm that our approach results in substantial performance gains. 1 Introduction  Sequence mining is an important data mining task, where one attempts to discover frequent sequences over time, of attribute sets in large databases. This problem was originally motivated by applications in the retailing industry, including...
DB
275630
On the Learnability and Design of Output Codes for Multiclass Problems . Output coding is a general framework for solving multiclass categorization problems. Previous research on output codes has focused on building multiclass machines given predefined output codes. In this paper we discuss for the first time the problem of designing output codes for multiclass problems. For the design problem of discrete codes, which have been used extensively in previous works, we present mostly negative results. We then introduce the notion of continuous codes and cast the design problem of continuous codes as a constrained optimization problem. We describe three optimization problems corresponding to three different norms of the code matrix. Interestingly, for the l 2 norm our formalism results in a quadratic program whose dual does not depend on the length of the code. A special case of our formalism provides a multiclass scheme for building support vector machines which can be solved efficiently. We give a time and space efficient algorithm for solving the quadratic program. We describe preliminary experiments with synthetic data show that our algorithm is often two orders of magnitude faster than standard quadratic programming packages. We conclude with the generalization properties of the algorithm. Keywords: Multiclass categorization,output coding, SVM 1.
IR
lueg00information
Information Seeking as Socially Situated Activity this paper, we discuss implications of situatedness in its social and cultural meaning in the context of information seeking support. We proceed as follows. First, we discuss some of the varying meanings of the term \situated". Then, we outline how we interpret \accounting for situatedness" in the context of information seeking support. Finally, we discuss tools that implement aspects of what we consider important in this context.
HCI
yang98feature
Feature Subset Selection Using A Genetic Algorithm Practical pattern classification and knowledge discovery problems require selection of a subset of attributes or features (from a much larger set) to represent the patterns to be classified. This paper presents an approach to the multi-criteria optimization problem of feature subset selection using a genetic algorithm. Our experiments demonstrate the feasibility of this approach for feature subset selection in the automated design of neural networks for pattern classification and knowledge discovery. 1 Introduction Many practical pattern classification tasks (e.g., medical diagnosis) require learning of an appropriate classification function that assigns a given input pattern (typically represented using a vector of attribute or feature values) to one of a finite set of classes. The choice of features, attributes, or measurements used to represent patterns that are presented to a classifier affect (among other things): ffl The accuracy of the classification function that can be learn...
ML
rizzo97personalitydriven
Personality-Driven Social Behaviors in Believable Agents Agents are considered "believable" when they are viewed by an audience as endowed with thoughts, desires, and emotions, typical of different personalities. The paper describes our work in progress aimed at realizing believable agents that perform helping behaviors influenced by their own personalities; the latter are represented as different clusters of prioritized goals and preferences over plans for achieving the goals. The implementation is based on the integration of a statebased planner that serves as the reasoning tool for the agents and a situation-driven execution system. Introduction  "There is a notion in the Arts of `believable character '. It does not mean an honest or reliable character, but one that provides the illusion of life, and thus permits the audience's suspension of disbelief  1  . The idea of believability has long been studied and explored in literature, theater, film, radio, drama, and other media ". (Bates 1994) Believability therefore refers to a character's...
HCI
friedman01multivariate
Multivariate Information Bottleneck The Information bottleneck method is an unsupervised  non-parametric data organization technique.  Given a joint distribution P (A; B), this  method constructs a new variable T that extracts  partitions, or clusters, over the values of A that  are informative about B. The information bottleneck  has already been applied to document  classification, gene expression, neural code, and  spectral analysis. In this paper, we introduce  a general principled framework for multivariate  extensions of the information bottleneck method.  This allows us to consider multiple systems of  data partitions that are inter-related. Our approach  utilizes Bayesian networks for specifying  the systems of clusters and what information  each captures. We show that this construction  provides insight about bottleneck variations  and enables us to characterize solutions of these  variations. We also present a general framework  for iterative algorithms for constructing solutions,  and apply it to several examples.  
IR
snyders00confseek
ConfSeek - A Multi-user, Multi-Threaded Specialized Search Engine for Conferences The explosive growth of the World Wide Web - the latest estimates for its size are around 1,000,000,000 web-pages - has made critical the need to nd information more accurately than what the current generic search engines can deliver. This project implements a prototype specialized search engine that allows user to submit queries for conferences in a specic eld of interest, and returns the detailed information about those conferences (deadlines, etc.). It uses multiple existing search engines to provide better coverage of the information available on the Internet. It can interact with multiple users concurrently and makes use of multi-threading to achieve faster information processing.  The goal is to make this tool available to the scientic community to provide researchers with improved access to conference information. We discuss possible extensions (e.g. ranking of conferences according to their `quality', trip planning, etc.).  Contents  1 Introduction 2 2 Goals of this Project...
IR
chang99automatic
Automatic I/O Hint Generation through Speculative Execution Aggressive prefetching is an effective technique for reducing the execution times of disk-bound applications; that is, applications that manipulate data too large or too infrequently used to be found in file or disk caches. While automatic prefetching approaches based on static analysis or historical access patterns are effective for some workloads, they are not as effective as manually-driven (programmer-inserted) prefetching for applications with irregular or input-dependent access patterns. In this paper, we propose to exploit whatever processor cycles are left idle while an application is stalled on I/O by using these cycles to dynamically analyze the application and predict its future I/O accesses. Our approach is to speculatively pre-execute the application's code in order to discover and issue hints for its future read accesses. Coupled with an aggressive hint-driven prefetching system, this automatic approach could be applied to arbitrary applications, and should be particularl...
DB
502499
Aliasing on the World Wide Web: Prevalence and Performance Implications Aliasing occurs in Web transactions when requests containing different URLs elicit replies containing identical data payloads. Aliasing can cause cache misses, and there is reason to suspect that offthe -shelf Web authoring tools might increase aliasing on the Web. Existing research literature, however, says little about the prevalence of aliasing in user-initiated transactions or its impact on endto -end performance in large multi-level cache hierarchies.
IR
445103
Building Intelligent Systems For Mining Information Extraction Rules From Web Pages By Using Domain Knowledge Previous researches on automatic information extraction experienced difficulties in acquiring and representing useful domain knowledge and in coping with the structural heterogeneity among different information sources. As a result, many real-world information sources with complex document structures could not be correctly analyzed. In order to resolve these problems, this paper presents a method of building intelligent systems for mining information extraction rules from semi-structured Web pages by using domain knowledge. This system automatically generates a wrapper for each information source and performs information extraction and information integration by applying this wrapper to the corresponding source. Both the domain knowledge and the wrapper are represented by XML documents to increase flexibility and interoperability. By testing our prototype system on several real-estate information sites, we can claim that it creates the correct wrappers for most Web sources and consequently facilitates effective information extraction for heterogeneous information sources.  1. 
IR
453805
Using Models of Score Distributions in Information Retrieval Empirical modeling of a number of different text search engines shows that the score distributions on a per query basis may be fitted approximately using an exponential distribution for the set of nonrelevant documents and a normal distribution for the set of relevant documents. This model fits not only probabilistic search engines like INQUERY but also vector space search engines like SMART and also LSI search engines. The model also appears to be true of search engines operating on a number of different languages. This leads to the hypothesis that all 'good' text search engines operating on any language have similar characteristics. The question then arises as to whether the shape of the score distributions reflects some underlying model of language or the search process itself. We discuss how they arise given certain assumptions about word distributions in documents. We then show that given a query for which relevance information is not available, a mixture model consisting of an exponential and a normal distribution can be fitted to the score distribution. These distributions can be used to map the scores of a search engine to probabilities. This model has many possible applications. For example, the outputs of different search engines can be combined by averaging the probabilities (optimal if the search engines are independent) or by using the probabilities to select the best engine for each query. Results show that the technique performs as well as the best current combination techniques. A number of different IR tasks may benefit from score modeling including filtering, multi-lingual retrieval and relevance feedback. We also discuss possible future improvements to the process of score modeling. 1.
IR
norrie00extended
An Extended Entity-Relationship Approach to Data Management in Object-Oriented Systems Database programming in object-oriented systems can be supported by combining data  modelling and programming technologies such that a data model supports the management of  collections of objects where those objects are as specified by the underlying object-oriented programming  language. This approach is the basis of the object data management services (ODMS)  of the Comandos system. The ODMS data model provides constructs for the representation of  both entities and their relationships and further supports rich classification structures. To complement  the structural model, there is an operational model based on an algebra over collections  of objects.  1 Introduction  Object-oriented technologies are gaining in popularity as the basis for software development platforms. Meanwhile the family of entity-relationship data models retain their wide-spread use and popularity for conceptual modelling. How then can these two successful technologies be combined to support the development of ...
DB
bohlin00path
Path Planning Using Lazy PRM This paper describes a new approach to probabilistic roadmap planners (PRMs). The overall theme of the algorithm, called Lazy PRM, is to minimize the number of collision checks performed during planning and hence minimize the running time of the planner. Our algorithm builds a roadmap in the configuration space, whose nodes are the user-defined initial and goal configurations and a number of randomly generated nodes. Neighboring nodes are connected by edges representing paths between the nodes. In contrast with PRMs, our planner initially assumes that all nodes and edges in the roadmap are collision-free, and searches the roadmap at hand for a shortest path between the initial and the goal node. The nodes and edges along the path are then checked for collision. If a collision with the obstacles occurs, the corresponding nodes and edges are removed from the roadmap. Our planner either finds a new shortest path, or first updates the roadmap with new nodes and edges, and then searches for a shortest path. The above process is repeated until a collision-free path is returned.
AI
giorgini01multiagent
Multi-Agent Architectures as Organizational Structures A Multi-Agent System (MAS) is an organization of coordinated autonomous agents that interact in order to achieve particular, possible common goals. Considering real world organizations as an analogy, this paper proposes architectural styles for MAS which adopt concepts from organizational theories. The styles are modeled using the i* framework which o#ers the notions of actor, goal and actor dependency and specified in Formal Tropos. They are evaluated with respect to a set of software quality attributes, such as predictability or adaptability. In addition, we conduct a comparative study of organizational and conventional software architectures using the mobile robot control example from the Software Engineering literature. The research is conducted in the context of Tropos, a comprehensive software system development methodology.
Agents
grieser00unifying
A Unifying Approach to HTML Wrapper Representation and Learning . The number, the size, and the dynamics of Internet information  sources bears abundant evidence of the need for automation in information  extraction. This calls for representation formalisms that match  the World Wide Web reality and for learning approaches and learnability  results that apply to these formalisms.  The concept of elementary formal systems is appropriately generalized to  allow for the representation of wrapper classes which are relevant to the  description of Internet sources in HTML format. Related learning results  prove that those wrappers are automatically learnable from examples.  This is setting the stage for information extraction from the Internet by  exploitation of inductive learning techniques.  1 Motivation  Today's online access to millions or even billions of documents in the World Wide Web is a great challenge to research areas related to knowledge discovery and information extraction (IE). The general task of IE is to locate specific pieces of text i...
IR
455511
Probabilistic Default Reasoning with Conditional Constraints We present an approach to reasoning from statistical and subjective knowledge, which is based on a combination of probabilistic reasoning from conditional constraints with approaches to default reasoning from conditional knowledge bases. More precisely, we introduce the notions of -, lexicographic, and conditional entailment for conditional constraints, which are probabilistic generalizations of Pearl's entailment in system    , Lehmann's lexicographic entailment, and Geffner's conditional entailment, respectively. We show that the new formalisms have nice properties. In particular, they show a similar behavior as referenceclass reasoning in a number of uncontroversial examples. The new formalisms, however, also avoid many drawbacks of reference-class reasoning. More precisely, they can handle complex scenarios and even purely probabilistic subjective knowledge as input. Moreover, conclusions are drawn in a global way from all the available knowledge as a whole. We then show that the new formalisms also have nice general nonmonotonic properties. In detail, the new notions of -, lexicographic, and conditional entailment have similar properties as their classical counterparts. In particular, they all satisfy the rationality postulates proposed by Kraus, Lehmann, and Magidor, and they have some general irrelevance and direct inference properties. Moreover, the new notions of - and lexicographic entailment satisfy the property of rational monotonicity. Furthermore, the new notions of -, lexicographic, and conditional entailment are proper generalizations of both their classical counterparts and the classical notion of logical entailment for conditional constraints. Finally, we provide algorithms for reasoning under the new formalisms, and we analyze its computational com...
DB
sakama99updating
Updating Extended Logic Programs through Abduction . This paper introduces techniques for updating knowledge bases represented in extended logic programs. Three di#erent types of updates, view updates, theory updates, and inconsistency removal, are considered. We formulate these updates through abduction, and provide methods for computing them with update programs. An update program is an extended logic program which specifies changes on abductive hypotheses, then updates are computed by the U-minimal answer sets of an update program. The proposed technique provides a uniform framework for these di#erent types of updates, and each update is computed using existing procedures of logic programming. 1 Introduction A knowledge base must be updated when new information arrives. There are three cases in updating a knowledge base. The first one is that a knowledge base contains two di#erent kinds of knowledge --- variable knowledge and invariable knowledge. In this case, updates are permitted only on variable knowledge. Updates on the invari...
DB
shavlik98building
Building Intelligent Agents for Web-Based Tasks: A Theory-Refinement Approach We present and evaluate an infrastructure with which to rapidly and easily build intelligent software agents for Web-based tasks. Our design is centered around two basic functions: ScoreThisLink  and ScoreThisPage. If given highly accurate such functions, standard heuristic search would lead to efficient retrieval of useful information. Our approach allows users to tailor our system 's behavior by providing approximate advice about the above functions. This advice is mapped into neural network implementations of the two functions. Subsequent reinforcements from the Web (e.g., dead links) and any ratings of retrieved pages that the user wishes to provide are, respectively, used to refine the link- and page-scoring functions. Hence, our agent architecture provides an appealing middle ground between nonadaptive "agent" programming languages and systems that solely learn user preferences from the user's ratings of pages. We present a case study where we provide some simple advice and speci...
ML
koperski98mining
Mining Knowledge in Geographical Data this article, a short overview is provided to summarize recent studies on spatial data mining, including spatial data mining techniques, their strengths and weaknesses, how and when to apply them, and what are the challenges yet to be faced.
DB
bernstein00vision
A Vision for Management of Complex Models Many problems encountered when building applications of database systems involve the manipulation of models. By “model, ” we mean a complex structure that represents a design artifact, such as a relational schema, object-oriented interface, UML model, XML DTD, web-site schema, semantic network, complex document, or software configuration. Many uses of models involve managing changes in models and transformations of data from one model into another. These uses require an explicit representation of “mappings ” between models. We propose to make database systems easier to use for these applications by making “model ” and “model mapping ” first-class objects with special operations that simplify their use. We call this capability model management. In addition to making the case for model management, our main contribution is a sketch of a proposed data model. The data model consists of formal, object-oriented structures for representing models and model mappings, and of high-level algebraic operations on those structures, such as matching, differencing, merging, function application, selection, inversion and instantiation. We focus on structure and semantics, not implementation. 1
DB
509024
A Multidimentional Framework for the Evaluation of Multiagent System Methodologies Because of the great interest in using multiagent systems (MAS) in a wide variety of applications in recent years, agent-oriented methodologies and related modeling techniques have become a priority for the development of large scale agent-based systems. The work we present here belongs to the disciplines of Software Engineering and Distributed Artificial Intelligence. More specifically, we are interested in software engineering aspects involved in the development of multiagent systems (MAS). Several methodologies have been proposed for the development of MAS. For the most part, these methodologies remain incomplete: they are either an extension of object-oriented methodologies or an extension of knowledge-based methodologies. In addition, too little effort has gone into the standardization of MAS methodologies, platforms and environments. It seems obvious, therefore, that software engineering aspects of the development of MAS still remains an open field. The success of the agent paradigm requires systematic methodologies for the specification, analysis and design of "non toy" MAS applications. We present in this paper a new framework called MUCCMAS, which stands for MUltidimensional framework of Criteria for the Comparison of MAS methodologies, that enabled us to make a comparative analysis of existing main MAS methodologies.
Agents
funt98is
Is Machine Colour Constancy Good Enough? . This paper presents a negative result: current machine colour constancy algorithms are not good enough for colour-based object recognition. This result has surprised us since we have previously used the better of these algorithms successfully to correct the colour balance of images for display. Colour balancing has been the typical application of colour constancy, rarely has it been actually put to use in a computer vision system, so our goal was to show how well the various methods would do on an obvious machine colour vision task, namely, object recognition. Although all the colour constancy methods we tested proved insufficient for the task, we consider this an important finding in itself. In addition we present results showing the correlation between colour constancy performance and object recognition performance, and as one might expect, the better the colour constancy the better the recognition rate. 1 Introduction We set out to show that machine colour constancy had matured to...
ML
itti98model
A Model of Saliency-based Visual Attention for Rapid Scene Analysis A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail. Index terms: Visual attention, scene analysis, feature extraction, target detection, visual search. \Pi I. Introduction Primates have a remarkable ability to interpret complex scenes in real time, despite the limited speed of the neuronal hardware available for such tasks. Intermediate and higher visual processes appear to select a subset of the available sensory information before further processing [1], most likely to reduce the complexity of scene analysis [2]. This selection appears to be implemented in the ...
ML
162997
A Runtime System for Interactive Web Services Interactive web services are increasingly replacing traditional static web pages. Producing web services seems to require a tremendous amount of laborious lowlevel coding due to the primitive nature of CGI programming. We present ideas for an improved runtime system for interactive web services built on top of CGI running on virtually every combination of browser and HTTP/CGI server. The runtime system has been implemented and used extensively in <bigwig>, a tool for producing interactive web services.  Keywords: CGI, Interactive Web Service, Web Document Management, Runtime System, Session Model. 1 Introduction  An interactive web service consists of a global shared state (typically a database) and a number of distinct sessions that each contain some local private state and a sequential, imperative action. A web client may invoke an individual thread of one of the given session kinds. The execution of this thread may interact with the client and inspect or modify the global state. One...
DB
29745
Rigid and Articulated Motion Seen with an Uncalibrated Stereo Rig This paper establishes a link between uncalibrated stereo vision and the motion of rigid and articulated bodies. The variation in the projective reconstruction of a dynamic scene over time allows an uncalibrated stereo rig to be used as a faithful motion capturing device. We introduce an original theoretical framework -- projective kinematics -- which allows rigid and articulated motion to be represented within the transformation group of projective space. Corresponding projective velocities are defined in the tangent space. Most importantly, these projective motions inherit the Lie-group structure of the displacement group. These theoretical results lead immediately to nonmetric formulations of visual servoing, tracking, motion capturing and motion synthesis systems, that no longer require the metric geometry of a stereo camera or of the articulated body to be known. We report on such a nonmetric formulation of a visual servoing system and present simulated experimental results.  1 In...
AI
andersson99intelligent
Intelligent Agents -- A New Technology for Future Distributed Sensor Systems? This master thesis deals with intelligent agents and the possibility to use the intelligent agent technology in future distributed sensor systems. The term future distributed sensor system refers to a system based on several sensors that will be developed within a period of five to ten years. Since researchers have not agreed on a more precise definition of intelligent agents, we first examined what constitutes an intelligent agent and made a definition suited for our application domain. We used our definition as a base for investigating if and how intelligent agents can be used in future distributed sensor systems. We argue that it is not interesting to come up with a general agent definition applicable to every agent, instead one should make a foundation for a definition. When this is done we can decide on more specific features depending on the task the agent will perform and in what domain the agent will work in. Finally we conclude that it is possible to use the agent technology i...
Agents
breazeal98regulating
Regulating Human-Robot Interaction using "emotions", "drives" and facial expressions This paper presents a motivational system for an autonomous robot which is designed to regulate humanrobot interaction. The mode of social interaction is that of a caretaker-infant dyad where a human acts as the caretaker for the robot. The robot's motivational system is designed to generate an analogous interaction for a robot-human dyad as for an infantcaretaker dyad. An infant's emotions and drives play a very important role in generating meaningful interactions with the caretaker (Bullowa 1979). Similarly, the learning task for the robot is to apply various communication skills acquired during social exchanges to manipulate the caretaker such that its drives are satisfied. Toward this goal, the motivational system implements  drives, emotions, and facial expressions. The interaction is regulated specifically to promote a suitable learning environment. Although the details of the learning itself are beyond the scope of this paper, this work represents an important step toward realiz...
AI
376260
SQL Based Association Rule Mining using Commercial RDBMS (IBM DB2 UDB EEE) . Data mining is becoming increasingly important since the size of  databases grows even larger and the need to explore hidden rules from the  databases becomes widely recognized. Currently database systems are  dominated by relational database and the ability to perform data mining using  standard SQL queries will definitely ease implementation of data mining.  However the performance of SQL based data mining is known to fall behind  specialized implementation and expensive mining tools being on sale. In this  paper we present an evaluation of SQL based data mining on commercial  RDBMS (IBM DB2 UDB EEE). We examine some techniques to reduce I/O  cost by using View and Subquery. Those queries can be more than 6 times  faster than SETM SQL query reported previously. In addition, we have made  performance evaluation on parallel database environment and compared the  performance result with commercial data mining tool (IBM Intelligent Miner).  We prove that SQL based data mining can achie...
DB
205160
Analysis on a Mobile Agent Based Algorithm for Network Management Recent advance in the agent technology has brought in a new method for network routing, the ant routing algorithm. Although its effectiveness and efficiency have been demonstrated and reported in the literature, its properties have not yet been well studied. This paper will present some preliminary analysis of this algorithm in regard to its population growing property and jumping behavior. For both synchronized and asynchronized networks, we have shown that the expected number of agents in a node is no more than  (1 +max i  fj\Omega  i jg)km, where  j\Omega  i j is the number of neighbor hosts of the i  th  host; k is the number of agents generated per request and m is the average number of requests. It is shown that under a mild condition, for all p  (1 + max i  fj\Omega  i jg)km, the probability of the number of agents in a node exceeding p is less than  R 1  p  P(x)dx; where P(x) is a normal distributed function with mean and variance given by (1+max i  fj\Omega  i jg)km  and  (km)...
Agents
franconi00icom
The i.com Tool for Intelligent Conceptual Modelling In this paper we present i.com, a tool for intelligent conceptual modelling. i.com  allows for the specification of multiple EER diagrams and inter- and intra-schema constraints.  Complete logical reasoning is employed by the tool to verify the specification,  infer implicit facts, and manifest any inconsistencies.  1 Introduction  i.com is a tool supporting the conceptual design phase of an information system, and in particular of an integration information system -- such as a data warehouse. The tool is an evolution of part of the conceptual modelling demonstrators suite [Jarke et al., 2000] developed within the European ESPRIT Long Term Research Data Warehouse Quality (DWQ) project [Jarke et al., 1999] . i.com adopts an extended Entity-Relationship (EER) conceptual data model, enriched with multidimensional aggregations and interschema constraints. i.com is fully integrated with a very powerful description logics reasoning server which acts  as a background inference engine.  The co...
DB
zhang99situated
Situated Neuro-Fuzzy Control for Vision-Based Robot Localisation We introduce a neuro-fuzzy system for localising mobile robot solely based on raw vision data without relying on landmarks or artificial symbols. In an initial learning step the system is trained on the compressed input data so as to classify different situations and to associate appropriate behaviours to these situations. Input data may, for example, be generated by an omnidirectional vision system obviating the need for active cameras. At run time the compressed input data are fed into different B-spline fuzzy controllers which determine the correspondence between the actual situation and the situation they were trained for. The matching controller may then directly drive the actuators to realise the desired behaviour. The system thus realises a tight coupling between a very high-dimensional input parameter space and the robot actuators. It is completely free of any internal models such as maps of the environment, the algorithms are straightforward to implement and the computational ...
ML
zhang01efficient
Efficient Computation of Temporal Aggregates with Range Predicates A temporal aggregation query is an important but costly operation for applications that maintain timeevolving  data (data warehouses, temporal databases, etc.). Due to the large volume of such data, performance  improvements for temporal aggregation queries are critical. In this paper we examine techniques to compute  temporal aggregates that include key-range predicates (range temporal aggregates). In particular we concentrate  on SUM, COUNT and AVG aggregates. This problem is novel; to handle arbitrary key ranges, previous  methods would need to keep a separate index for every possible key range. We propose an approach based  on a new index structure called the Multiversion SB-Tree, which incorporates features from both the SB-Tree  and the Multiversion B-Tree, to handle arbitrary key-range temporal SUM, COUNT and AVG queries. We  analyze the performance of our approach and present experimental results that show its efficiency.  1 
DB
holmquist00play
The PLAY Research Group: Entertainment and Innovation in Sweden In a short time the research group PLAY has established an unorthodox but effective work style, where a creative approach to research in information technology is combined with a strong focus on achieving high-quality results. Being a young research group (both regarding the time it has existed and the average age of its members) has presented PLAY with both challenges and opportunities. We face the challenge of building a credible basis for research in the academic community, but also think that we have the opportunity to contribute innovative results to the research community and our industrial partners.  Keywords  HCI research groups, future HCI, European HCI, IT design  INTRODUCTION  How can one perform exciting and unorthodox research in information technology, while still assuring that results are useful and of good quality? How can a small group, consisting mostly of relatively inexperienced students, in a small country with very little traditions in groundbreaking IT research, ...
HCI
cadoli98survey
A Survey on Knowledge Compilation this paper we survey recent results in knowledge compilation of propositional knowledge bases. We first define and limit the scope of such a technique, then we survey exact and approximate knowledge compilation methods. We include a discussion of compilation for non-monotonic knowledge bases. Keywords: Knowledge Representation, Efficiency of Reasoning
AI
silverman01more
More Realistic Human Behavior Models for Agents in Virtual Worlds: Emotion, Stress, and Value Ontologies This paper focuses on challenges to improving the behavioral realism of computer generated agents and attempts to reflect the state of the art in human behavior modeling with particular attention to value  ontologies, emotion, and stress in game-theoretic settings. The goal is to help those interested in constructing more realistic software agents for use in simulations, in virtual reality environments, and in training and performance aiding settings such as on the web or in embedded applications. This paper pursues this goal by providing a framework for better integrating the theories and models contained in the diverse human behavior modeling literatures, such as those that straddle physiological, cognitive and emotive processes; individual differences; emergent group and crowd behavior; and (punctuated) equilibria in social settings. The framework is based on widely available ontologies of world values and how these  and physiological factors might be construed emotively into subjective expected utilities to guide the  reactions and deliberations of agents. For example what makes one set of opponent groups differ from another? This framework serves as an extension of Markov decision processes appropriate for iterative play in game-theoretic settings, with particular emphasis on agent capabilities for redefining drama and for finding meta-games to counter the human player. This article presents the derivation of the framework and some initial results and lessons learned about integrating behavioral models into interactive dramas and meta-games that stimulate (systemic) thought and training doctrine.  1) 
Agents
bohlen00temporal
Temporal Statement Modifiers A wide range of database applications manage time-varying data. Many temporal query languages have been proposed, each one the result of many carefully made yet subtly interacting design decisions. In this article we advocate a different approach to articulating a set of requirements, or desiderata, that directly imply the syntactic structure and core semantics of a temporal extension of an (arbitrary) nontemporal query language. These desiderata facilitate transitioning applications from a nontemporal query language and data model, which has received only scant attention thus far. The paper then introduces the notion of statement modifiers that provide a means of systematically adding temporal support to an existing query language. Statement modifiers apply to all query language statements, for example, queries, cursor definitions, integrity constraints, assertions, views, and data manipulation statements. We also provide a way to systematically add temporal support to an existing implementation. The result is a temporal query language syntax, semantics, and implementation that derives from first principles. We exemplify this approach by extending SQL-92 with statement modifiers. This extended language, termed ATSQL, is formally defined via a denotational-semantics-style mapping of
DB
ezeife01selecting
Selecting and Materializing Horizontally Partitioned Warehouse Views Data warehouse views typically store large aggregate tables based on a subset of dimension attributes of the main data warehouse fact table. Aggregate views can be stored as 2  n  subviews of a data cube with n attributes. Methods have been proposed for selecting only some of the data cube views to materialize in order to speed up query response time, accommodate storage space constraint and reduce warehouse maintenance cost. This paper proposes a method for selecting and materializing views, which selects and horizontally fragments a view, recomputes the size of the stored partitioned view while deciding further views to select. # 2001 Elsevier Science B.V. All rights reserved.  Keywords: Data warehouse; Views; Fragmentation; Performance benet  1. Introduction  Decision support systems (DSS) used by business executives require analyzing snapshots of departmental databases over several periods of time. Departmental databases of the same organization (e.g., a bank) may be stored on dier...
DB
parker98adaptive
Adaptive Heterogeneous Multi-Robot Teams This research addresses the problem of achieving fault tolerant cooperation within small- to medium-sized teams of heterogeneous mobile robots. We describe a novel behavior-based, fully distributed architecture, called ALLIANCE, that utilizes adaptive action selection to achieve fault tolerant cooperative control in robot missions involving loosely coupled tasks. The robots in this architecture possess a variety of high-level functions that they can perform during a mission, and must at all times select an appropriate action based on the requirements of the mission, the activities of other robots, the current environmental conditions, and their own internal states. Since such cooperative teams often work in dynamic and unpredictable environments, the software architecture allows the team members to respond robustly and reliably to unexpected environmental changes and modi cations in the robot team that may occur due to mechanical failure, the learning of new skills, or the addition or removal of robots from the team by human intervention. After presenting ALLIANCE, we describe in detail our experimental results of an implementation of this architecture on a team of physical mobile robots performing a cooperative box pushing demonstration. These experiments illustrate the ability ofALLIANCE to achieve adaptive, fault-tolerant cooperative control amidst dynamic changes in the capabilities of the robot team.
AI
critchlow00datafoundry
DataFoundry: Information Management for Scientific Data Data warehouses and data marts have been successfully applied to a multitude of commercial business applications. They have proven to be invaluable tools by integrating information from distributed, heterogeneous sources and summarizing this data for use throughout the enterprise. Although the need for information dissemination is as vital in science as in business, working warehouses in this community are scarce because traditional warehousing techniques don't transfer to scientific environments. There are two primary reasons for this difficulty. First, schema integration is more difficult for scientific databases than for business sources, because of the complexity of the concepts and the associated relationships. While this difference has not yet been fully explored, it is an important consideration when determining how to integrate autonomous sources. Second, scientific data sources have highly dynamic data representations (schemata). When a data source participating in a warehouse...
DB
liu99deductive
Deductive Database Languages: Problems and Solutions this paper, we discuss these problems from four different aspects: complex values, object orientation, higher-orderness, and updates. In each case, we examine four typical languages that address the corresponding issues.
DB
herbrich99regression
Regression Models for Ordinal Data: A Machine Learning Approach In contrast to the standard machine learning tasks of classification and metric regression we investigate the problem of predicting variables of ordinal scale, a setting referred to as ordinal regression. The task of ordinal regression arises frequently in the social sciences and in information retrieval where human preferences play a major role. Also many multi--class problems are really problems of ordinal regression due to an ordering of the classes. Although the problem is rather novel to the Machine Learning Community it has been widely considered in Statistics before. All the statistical methods rely on a probability model of a latent (unobserved) variable and on the condition of stochastic ordering. In this paper we develop a distribution independent formulation of the problem and give uniform bounds for our risk functional. The main difference to classification is the restriction that the mapping of objects to ranks must be transitive and asymmetric. Combining our theoretical framework with results from measurement theory we present an approach that is based on a mapping from objects to scalar utility values and thus guarantees transitivity and asymmetry. Applying the principle of Structural Risk Minimization as employed in Support Vector Machines we derive a new learning algorithm based on large margin rank boundaries for the task of ordinal regression. Our method is easily extended to nonlinear utility functions. We give experimental results for an Information Retrieval task of learning the order of documents with respect to an initial query. Moreover, we show that our algorithm outperforms more naive approaches to ordinal regression such as Support Vector Classification and Support Vector Regression in the case of more than two ranks.
IR
druin01designing
Designing a Digital Library for Young Children: An Intergenerational Partnership As more information resources become accessible using computers, our digital interfaces to those resources need to be appropriate for all people. However when it comes to digital libraries, the interfaces have typically been designed for older children or adults. Therefore, we have begun to develop a digital library interface developmentally appropriate for young children (ages 5-10 years old). Our prototype system we now call "QueryKids" offers a graphical interface for querying, browsing and reviewing search results. This paper describes our motivation for the research, the design partnership we established between children and adults, our design process, the technology outcomes of our current work, and the lessons we have  learned.  Keywords  Children, digital libraries, information retrieval design techniques, education applications, participatory design, cooperative inquiry, intergenerational design team, zoomable user interfaces (ZUIs).  THE NEED FOR RESEARCH  A growing body of k...
IR
337032
A Sound Algorithm for Region-Based Image Retrieval Using an Index Region-based image retrieval systems aim to improve the effectiveness of content-based search by decomposing each image into a set of "homogeneous" regions. Thus, similarity between images is assessed by computing similarity between pairs of regions and then combining the results at the image level. In this paper we propose the first provably sound algorithm for performing region-based similarity search when regions are accessed through an index. Experimental results demonstrate the effectiveness of our approach, as also compared to alternative retrieval strategies. 1. Introduction  Many real world applications, in the field of medicine, weather prediction, and communications, to name a few, require efficient access to image databases based on content. To this end, the goal of content-based image retrieval (CBIR) systems is to define a set of properties (features) able to effectively characterize the content of images and then to use such features during retrieval. Users accessing a CB...
ML
150449
A Wearable Spatial Conferencing Space Wearable computers provide constant access to computing and communications resources. In this paper we describe how the computing power of wearables can be used to provide spatialized 3D graphics and audio cues to aid communication. The result is a wearable augmented reality communication space with audio enabled avatars of the remote collaborators surrounding the user. The user can use natural head motions to attend to the remote collaborators, can communicate freely while being aware of other side conversations and can move through the communication space. In this way the conferencing space can support dozens of simultaneous users. Informal user studies suggest that wearable communication spaces may offer several advantages, both through the increase in the amount of information it is possible to access and the naturalness of the interface.  1: Introduction  One of the broad trends emerging in human-computer interaction is the increasing portability of computing and communication fac...
HCI
89078
Confluence of Computer Vision and Interactive Graphics for Augmented Reality . Augmented reality #AR# is a technology in which a user's view of the real world  is enhanced or augmented with additional information generated from a computer model.  Using AR technology, users can interact with a combination of real and virtual objects in  a natural way. This paradigm constitutes the core of a very promising new technology for  many applications. However, before it can be applied successfully, AR has to ful#ll very strong  requirements including precise calibration, registration and tracking of sensors and objects in  the scene, as well as a detailed overall understanding of the scene.  At ECRCwe see computer vision and image processing technology play an increasing role  in acquiring appropriate sensor and scene models. To balance robustness with automation,  weintegrate automatic image analysis with both interactive user assistance and input from  magnetic trackers and CAD-models. Also, in order to meet the requirements of the emerging  global information society...
HCI
carreras01hybrid
Hybrid Coordination of Reinforcement Learning-based Behaviors for AUV Control This paper proposes a Hybrid Coordination method for Behavior-based Control Architectures. The hybrid method takes in advantages of the robustness and modularity in competitive approaches as well as optimized trajectories in cooperative ones. This paper will demonstrate the feasibility of this hybrid method with a 3D-navigation application to an Autonomous Underwater Vehicle (AUV). The behaviors were learnt online by means of Reinforcement Learning. Q(l)- learning was used extending the one-step learning of the popular Q-learning to n-steps. Realistic simulations were carried out. Results showed the good performance of the hybrid method on behavior coordination as well as on increasing and improving behavior learning.
ML
87928
Authoritative Sources in a Hyperlinked Environment The link structure of a hypermedia environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. Versions of this principle have been studied in the hypertext research community and (in a context predating hypermedia) through journal citation analysis in the field of bibliometrics. But for the problem of searching in hyperlinked environments such as the World Wide Web, it is clear from the prevalent techniques that the information inherent in the links has yet to be fully exploited. In this work we develop a new method for automatically extracting certain types of information about a hypermedia environment from its link structure, and we report on experiments that demonstrate its effectiveness for a variety of search problems on the www.  The central problem we consider is that of determining the relative "authority" of pages in such environments. This issue is central to a number of basic hypertext search t...
IR
267537
Hybrid Neural Systems   This chapter provides an introduction to the eld of hybrid  neural systems. Hybrid neural systems are computational systems which  are based mainly on articial neural networks but also allow a symbolic  interpretation or interaction with symbolic components. In this overview,  we will describe recent results of hybrid neural systems. We will give  a brief overview of the main methods used, outline the work that is  presented here, and provide additional references. We will also highlight  some important general issues and trends.  
ML
186486
Design Issues for Mixed-Initiative Agent Systems This paper addresses the effect of mixed-initiative systems on multiagent systems design. A mixed-initiative system is one in which humans interact directly with software agents in a collaborative approach to problem solving. There are two main levels at which multiagent systems are designed: the domain level and the individual agent level. At the domain level, there are few unique challenges to mixedinitiative system design. However, at the individual agent level, the agent itself must be designed to interact with the human and the agent system, integrating the two into a single system. Introduction  Much of the current research related to intelligent agents has focused on the capabilities and structure of individual agents. However, in order to solve complex problems, these agents must work cooperatively with other agents in a heterogeneous environment. This is the domain of  Multiagent Systems. In multiagent systems, we are interested in the coordinated behavior of a system of indiv...
Agents
bettini99symbolic
Symbolic Representation of User-defined Time Granularities In the recent literature on time representation, an effort has been made to characterize the notion of time granularity and the relationships between granularities, in order to have a common framework for their specification, and to allow the interoperability of systems adopting different time granularities. This paper considers the mathematical characterization of finite and periodical time granularities, and it identifies a user-friendly symbolic formalism which captures exactly that class of granularities. This is achieved by a formal analysis of the expressiveness of well-known symbolic representation formalisms. 1. Introduction There is a wide agreement in the AI and database community on the requirement for a data/knowledge representation system of supporting standard as well as user-defined time granularities. Examples of standard time granularities are days, weeks, months, while user defined granularities may include businessweeks, trading-days, working-shifts, school-terms, wi...
DB
147521
Query Optimization in the Presence of Limited Access Patterns 1 Introduction The goal of a query optimizer of a database system is to translate a declarative query expressed on a logical schema into an imperative query execution plan that accesses the physical storage of the data, and applies a sequence of relational operators. In building query execution plans, traditional relational query optimizers try to find the most efficient method for accessing the necessary data. When possible, a query optimizer will use auxiliary data structures such as an index on a file in order to efficiently retrieve a certain set of tuples in a relation. However, when such structures do not exist or are not useful for the given query, the alternative of scanning the entire relation always exists. The existence of the fall back option to perform a complete scan is an important assumption in traditional query optimization. Several recent query processing applications have the common characteristic that it is not always possible to perform complete scans on the data. Instead, the query optimization problem is complicated by the fact that there are only limited access patterns to the data. One such
DB
namee01proposal
A Proposal for an Agent Architecture for Proactive Persistent Non Player Characters In the past, games development has been driven by the need to achieve more realistic graphics.
Agents
lawrence99text
Text and Image Metasearch on the Web As the Web continues to increase in size, the relative coverage of Web search engines is decreasing, and search tools that combine the results of multiple search engines are becoming more valuable. This paper provides details of the text and image metasearch functions of the Inquirus search engine developed at the NEC Research Institute. For text metasearch, we describe features including the use of link information in metasearch, and provide statistics on the usage and performance of Inquirus and the Web search engines. For image metasearch, Inquirus queries multiple image search engines on the Web, downloads the actual images, and creates image thumbnails for display to the user. Inquirus handles image search engines that return direct links to images, and engines that return links to HTML pages. For the engines that return HTML pages, Inquirus analyzes the text on the pages in order to predict which images are most likely to correspond to the query. The individual image search engin...
IR
agrawal00athena
Athena: Mining-based Interactive Management of Text Databases Abstract. We describe Athena: a system for creating, exploiting, and maintaining a hierarchy of textual documents through interactive miningbased operations. Requirements of any such system include speed and minimal end-user e ort. Athena satis es these requirements through linear-time classi cation and clustering engines which are applied interactively to speed the development of accurate models. Naive Bayes classi ers are recognized to be among the best for classifying text. We show that our specialization of the Naive Bayes classi er is considerably more accurate (7 to 29 % absolute increase in accuracy) than a standard implementation. Our enhancements include using Lidstone's law of succession instead of Laplace's law, under-weighting long documents, and over-weighting author and subject. We also present a new interactive clustering algorithm, C-Evolve, for topic discovery. C-Evolve rst nds highly accurate cluster digests (partial clusters), gets user feedback to merge and correct these digests, and then uses the classi cation algorithm to complete the partitioning of the data. By allowing this interactivity in the clustering process, C-Evolve achieves considerably higher clustering accuracy (10 to 20 % absolute increase in our experiments) than the popular K-Means and agglomerative clustering methods. 1
IR
muhlenbein99convergence
Convergence Theory and Applications of the Factorized Distribution Algorithm The paper investigates the optimization of additively decomposable functions (ADF) by a new evolutionary algorithm called Factorized Distribution Algorithm (FDA). FDA is based on a factorization of the distribution to generate search points. First separable ADFs are considered. These are mapped to generalized linear functions with metavariables defined for multiple alleles. The mapping transforms FDA into an Univariate Marginal Frequency Algorithm (UMDA). For UMDA the exact equation for the response to selection is computed under the assumption of proportionate selection. For truncation selection an approximate equation for the time to convergence is used, derived from an analysis of the OneMax function. FDA is also numerically investigated for non separable functions. The time to convergence is very similar to separable ADFs. FDA outperforms the genetic algorithm with recombination of strings by far.  Keywords  response to selection, Fisher's Theorem, additively decomposable functions...
ML
251052
Control Law Design for Haptic Interfaces to Virtual Reality The goal of control law design for haptic displays is to provide a safe and stable user interface while maximizing the operator's sense of kinesthetic immersion in a virtual environment. This paper outlines a control design approach which stabilizes a haptic interface when coupled to a broad class of human operators and virtual environments. Two-port absolute stability criteria are used to develop explicit control law design bounds for two different haptic display implementations: impedance display and admittance display. The strengths and weaknesses of each approach are illustrated through numerical and experimental results for a three degree-offreedom device. The example highlights the ability of the proposed design procedure to handle some of the more difficult problems in control law synthesis for haptics, including structural flexibility and non-collocation of sensors and actuators. The authors are with the Department of Electrical Engineering University of Washington, Box 352500 Seattle, WA 98195-2500  2  I. 
HCI
laviola99flex
Flex And Pinch: A Case Study Of Whole Hand Input Design For Virtual Environment Interaction We present a discussion of design issues involving whole hand input in virtual environments. In many cases, whole hand input devices limit the types of interaction that the user can perform in the virtual world due to the nature of the device. One possible approach to alleviate these limitations is to provide hybrid input devices which enable the user to combine information generated from two different whole hand input devices. In this paper, we describe our Pinch Glove like input device which is used as a tool to augment bend-sensing gloves for object manipulation and menu selection as well as a method to test and evaluate different hand postures and gestures that could not be developed with a single whole hand device. KEYWORDS: Human-Computer Interaction, Virtual Environments, 3D Graphics Applications, Conductive Cloth, Flex and Pinch Input INTRODUCTION There have been a number of different approaches for interacting in virtual environments. In general, these approaches have attem...
HCI
deligiannidis02dlove
DLoVe: Using Constraints to Allow Parallel Processing in Multi-User Virtual Reality In this paper, we introduce DLoVe, a new paradigm for designing and implementing distributed and nondistributed virtual reality applications, using one-way constraints. DLoVe allows programs written in its framework to be executed on multiple computers for  improved performance. It also allows easy specification and implementation of multi-user interfaces. DLoVe hides all the networking aspects of message passing among the machines in the distributed environment and performs the needed network optimizations. As a result, a user of DLoVe does not need to understand parallel and distributed programming to use the system; he or she  needs only be able to use the serial version of the user  interface description language. Parallelizing the computation is performed by DLoVe, without modifying the interface description.
HCI
casati01improving
Improving Business Process Quality through Exception Understanding, Prediction, and Prevention Business process automation technologies are  being increasingly used by many companies to  improve the efficiency of both internal processes  as well as of e-services offered to customers. In  order to satisfy customers and employees,  business processes need to be executed with a  high and predictable quality. In particular, it is  crucial for organizations to meet the Service  Level Agreements (SLAs) stipulated with the  customers and to foresee as early as possible the  risk of missing SLAs, in order to set the right  expectations and to allow for corrective actions.  In this paper we focus on a critical issue in  business process quality: that of analyzing,  predicting and preventing the occurrence of  exceptions, i.e., of deviations from the desired or  acceptable behavior. We characterize the  problem and propose a solution, based on data  warehousing and mining techniques. We then  describe the architecture and implementation of a  tool suite that enables exception analysis,  prediction, and prevention. Finally, we show  experimental results obtained by using the tool  suite to analyze internal HP processes.  1. 
DB
panzarasa02formalizing
Formalizing Collaborative Decision-making and Practical Reasoning in Multi-agent Systems In this paper, we present an abstract formal model of decision-making in a social setting that covers all aspects of the process, from recognition of a potential for cooperation through to joint decision. In a multi-agent environment, where self-motivated autonomous agents try to pursue their own goals, a joint decision cannot be taken for granted. In order to decide effectively, agents need the ability to (a) represent and maintain a model of their own mental attitudes, (b) reason about other agents' mental attitudes, and (c) influence other agents' mental states. Social mental shaping is advocated as a general mechanism for attempting to have an impact on agents' mental states in order to increase their cooperativeness towards a joint decision. Our approach is to specify a novel, high-level architecture for collaborative decision-making in which the mentalistic notions of belief, desire, goal, intention, preference and commitment play a central role in guiding the individual agent's and the group's decision-making behaviour. We identify preconditions that must be fulfilled before collaborative decision-making can commence and prescribe how cooperating agents should behave, in terms of their own decision-making apparatus and their interactions with others, when the decision-making process is progressing satisfactorily. The model is formalized through a new, many-sorted, multi-modal logic.
Agents
heinze99plan
Plan Recognition in Military Simulation: Incorporating Machine Learning with Intelligent Agents A view of plan recognition shaped by both  operational and computational requirements is  presented. Operational requirements governing  the level of fidelity and nature of the reasoning  process combine with computational requirements  including performance speed and software  engineering effort to constrain the types  of solutions available to the software developer.  By adopting machine learning to provide  spatio-temporal recognition of environmental  events and relationships, an agent can be provided  with a mechanism for mental state recognition  qualitatively different from previous research.  An architecture for integrating machine  learning into a BDI agent is suggested and the  results from the development of a prototype  provide proof-of-concept.  1 Introduction  This paper proposes machine-learning as a tool to assist in the construction of agents capable of plan recognition. This paper focuses on the beliefs-desires-intentions (BDI) class of agents. These agents have been ...
Agents
paulson00inductive
The Inductive Approach to Verifying Cryptographic Protocols Informal arguments that cryptographic protocols are secure can be made rigorous using inductive definitions. The approach is based on ordinary predicate calculus and copes with infinite-state systems. Proofs are generated using Isabelle/HOL. The human e#ort required to analyze a protocol can be as little as a week or two, yielding a proof script that takes a few minutes to run.  Protocols are inductively defined as sets of traces. A trace is a list of communication events, perhaps comprising many interleaved protocol runs. Protocol descriptions incorporate attacks and accidental losses. The model spy knows some private keys and can forge messages using components decrypted from previous tra#c. Three protocols are analyzed below: OtwayRees (which uses shared-key encryption), Needham-Schroeder (which uses public-key encryption), and a recursive protocol [9] (which is of variable length).  One can prove that event ev always precedes event ev # or that property  P holds provided X remains ...
ML
slonim01power
The Power of Word Clusters for Text Classification The recently introduced Information Bottleneck method [21] provides an information theoretic framework, for extracting features of one variable, that are relevant for the values of another variable. Several previous works already suggested applying this method for document clustering, gene expression data analysis, spectral analysis and more. In this work we present a novel implementation of this method for supervised text classification. Specifically, we apply the information bottleneck method to find word-clusters that preserve the information about document categories and use these clusters as features for classification. Previous work [1] used a similar clustering procedure to show that word-clusters can significantly reduce the feature space dimensionality, with only a minor change in classification accuracy. In this work we present similar results and go further to show that when the training sample is small word clusters can yield significant improvement in classification accuracy (up to  ¢¡¤£) over the performance using the words directly. 1
IR
lawrence98searching
Searching the world wide web The coverage and recency of the major World Wide Web search engines was analyzed, yielding some surprising results. The coverage of any one engine is significantly limited: No single engine indexes more than about one-third of the “indexable Web, ” the coverage of the six engines investigated varies by an order of magnitude, and combining the results of the six engines yields about 3.5 times as many documents on average as compared with the results from only one engine. Analysis of the overlap between pairs of engines gives an estimated lower bound on the size of the indexable Web of 320 million pages. The Internet has grown rapidly since its inception in December 1969 (1) and is anticipated to expand 1000 % over the next few years (2). The amount of scientific information and the number of electronic journals on the Internet continue to increase [about 1000 journals as of 1996 (2, 3)]. The Internet and the World Wide Web (the Web) represent significant advancements for the retrieval and dissemination of scientific and other literature and for the advancement of education (2, 4). With the introduction of full-text search engines such as AltaVista (www.
IR
305534
An Open Framework for Distributed Multimedia Retrieval This article describes a framework for distributed multimedia retrieval which permits the  connection of compliant user interfaces with a variety of multimedia retrieval engines via an  open communication protocol, MRML (Multi Media Retrieval Markup Language). It allows  the choice of image collection, feature set and query algorithm during run{time, permitting  multiple users to query a system adapted to their needs, using the query paradigm adapted  to their problem such as query by example (QBE), browsing queries, or query by annotation.  User interaction is implemented over several levels and in diverse ways. Relevance feedback  is implemented using positive and negative example images that can be used for a  best{match QBE query. In contrast, browsing methods try to approach the searched image  by giving overviews of the entire collection and by successive renement. In addition to these  query methods, Long term o line learning is implemented. It allows feature preferences per  ...
ML
441373
DB-Prism: Integrated Data Warehouses and Knowledge Networks for Bank Controlling DB-Prism is an integrated data warehouse system
DB
norman97motivationbased
Motivation-Based Direction of Planning Attention in Agents With Goal Autonomy The action of an agent with goal autonomy will be driven by goals generated with reference to its own beliefs and desires. This ability is essential for agents that are required to act in their own interests in a domain that is not entirely predictable. At any time, the situation may warrant the generation of new goals. However, it is not always the case that changes in the domain that lead to the generation of a goal are detected immediately before the goal should be pursued. Action may not be appropriate for some time. Furthermore, an agent may be influenced by goals that tend to recur periodically, or at particular times of the day or week for example. Such goals serve to motivate an agent towards interacting with other agents or processes with certain types of predictable behaviour patterns. This thesis provides a model of a goal autonomous agent that may generate goals in response to unexpected changes in its domain or cyclically through automatic processes. An important effect of...
Agents
lin01efficiently
Efficiently Computing Weighted Proximity Relationships in Spatial Databases Spatial data mining recently emerges from a number of real applications, such as real-estate marketing, urban planning, weather forecasting, medical image analysis, road traffic accident analysis, etc. It demands for efficient solutions for many new, expensive, and complicated problems. In this paper, we investigate the problem of evaluating the top k distinguished "features" for a "cluster" based on weighted proximity relationships between the cluster and features. We measure proximity in an average fashion to address possible nonuniform data distribution in a cluster. Combining a standard multi-step paradigm with new lower and upper proximity bounds, we presented an efficient algorithm to solve the problem. The algorithm is implemented in several different modes. Our experiment results not only give a comparison among them but also illustrate the efficiency of the algorithm.
DB
camacho00travelplan
TravelPlan: A MultiAgent System to Solve Web Electronic Travel Problems This paper presents TravelPlan, a multiagent architecture to co-operative work between different elements (human and/or software) whose main goal is to recommend useful solutions in the electronic tourism domain to system users. The system uses different types of intelligent autonomous agents whose main characteristics are cooperation, negotiation, learning, planning and knowledge sharing. The information used by the intelligent agents is heterogeneous and geographically distributed. The main information source of the system is Internet (the web). Other information sources are distributed knowledge bases in the own system.. The process to obtain, filter, and store the information is performed automatically by agents. This information is translated into a homogeneous format for high-level reasoning in order to obtain different partial solutions. Partial solutions are reconstructed into a general solution (or solutions) to be presented to the user. The system will recommend different solution...
Agents
4664
Computation of the Semantics of Autoepistemic Belief Theories Recently, one of the authors introduced a simple and yet powerful non-monotonic knowledge representation framework, called the Autoepistemic Logic of Beliefs, AEB. Theories in AEB are called autoepistemic belief theories. Every belief theory  T has been shown to have the least static expansion T which is computed by iterating a natural monotonic belief closure operator \Psi T starting from T . This way, the least static expansion T of any belief theory provides its natural non-monotonic semantics which is called the static semantics.  It is easy to see that if a belief theory T is finite then the construction of its least static expansion T stops after countably many iterations. However, a somewhat surprising result obtained in this paper shows that the least static expansion of any finite belief theory T is in fact obtained by means of a single iteration of the belief closure operator \Psi T (although this requires T to be of a special form, we also show that T can be always put in th...
AI
9105
Partitioning-Based Clustering for Web Document Categorization Clustering techniques have been used by manyintelligent software agents in order to retrieve, lter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related web documents to automatically formulate queries and search for other similar documents on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to de ne a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classi cation. Many of these traditional algorithms, however, falter when the dimensionality of the feature space becomes high relative to the size of the document space. In this paper, we introduce two new clustering algorithms that can e ectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques, which are based on generalizations of graph partitioning, do not require pre-speci ed ad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiments on real Web data using various feature selection heuristics, and compare our clustering schemes to standard distance-based techniques, such ashierarchical agglomeration clustering, and Bayesian classi cation methods, such as AutoClass.
IR
318212
Towards Active Logic Programming In this paper we present the new logic programming language DALI, aimed at defining agents and agent systems. A main design objective for DALI has been that of introducing in a declarative fashion all the essential features, while keeping the language as close as possible to the syntax and semantics of the plain Horn--clause language. Special atoms and rules have been introduced, for representing: external events, to which the agent is able to respond (reactivity); actions (reactivity and proactivity); internal events (previous conclusions which can trigger further activity); past and present events (to be aware of what has happened). An extended resolution is provided, so that a DALI agent is able to answer queries like in the plain Horn--clause language, but is also able to cope with the different kinds of events, and exhibit a (rational) reactive and proactive behaviour. 1 Introduction In this paper we address the issue of defining a logic programming language for reac...
Agents
bakker01lart
LART: flexible, low-power building blocks for wearable computers To ease the implementation of different wearable computers, we developed a low-power processor board (named LART) with a rich set of interfaces. The LART supports dynamic voltage scaling, so performance (and power consumption) can be scaled to match demands: 59-221 MHz, 106-640 mW. High-end wearables can be configured from multiple LARTs operating in parallel; alternatively, FPGA boards can be used for dedicated data-processing, which reduces power consumption significantly.
HCI
16755
Case-based Learning for Knowledge-based Design Support . We present a general approach to combine methods of interactive knowledge acquisition with methods for machine learning. The approach has been developed in order to deliver knowledge required by support-systems for design-tasks. Learning rests upon a knowledge representation scheme for cases that distinguishes between knowledge needed for subgoaling and knowledge needed for design. We employ traces, i.e., protocols of the user's actions when tackling design-tasks as the initial input for incremental knowledge acquisition. This allows to learn task structures to be used for subgoaling and case-bases plus similarity relations applicable to particular case-bases. 1 INTRODUCTION Integrating incremental learning into a knowledge-based systems seems to be a promising way to lessen the burden of knowledge elicitation to system development [9]. The goal of this paper is to point out how learning can be used in an interactive design-support system that uses Cbr [8] as the main problem solvin...
ML
grasso99augmenting
Augmenting Recommender Systems by Embedding Interfaces into Office Practices Automated collaborative filtering systems collect evaluations from users of the quality and relevance of stored information items, such as scientific papers, books, and movies. A number of users need to give evaluations for the systems to be able to produce statistically high quality predictions of an item's interest. Promoting the creation of a rich meta-layer of evaluations is essential for these systems, but several important issues remain to be resolved.  The work presented here first analyses the issues around the collection of recommendations, then proposes a set of design principles for improving and automating the collection of recommendations, and finally presents how these principles have been implemented in a real usage setting.  1. Systems to alleviate the information  overload  `Information overload' may be an abused term but it is an increasingly apt description of our current experience in dealing with information. The increase in communication channels and publishing me...
IR
508816
Enabling Ad-Hoc Collaboration through Schedule Learning and Prediction The transferal of the desktop interface to the world at large is not the goal of ubiquitous computing. Rather, ubiquitous computing strives to increase the responsiveness of the world at large to the individual. A large part of this responsiveness is improved communication with other individuals. In this paper we describe a system that can enable ad--hoc collaboration between several people by creating a model of the daily schedules of individuals and by performing predictions based on this model. Using GPS data we learn to distinguish locations and track the times that these locations are visited. In addition, we use Markov models to predict which locations might be visited next based on the user's previous behavior.
HCI
kanungo99omnipage
OmniPage vs. Sakhr: Paired Model Evaluation of Two Arabic OCR Products Characterizing the performance of Optical Character Recognition (OCR) systems is crucial for monitoring technical progress, predicting OCR performance, providing scientific explanations for the system behavior and identifying open problems. While research has been done in the past to compare performances of two or more OCR systems, all assume that the accuracies achieved on individual documents in a dataset are independent when, in fact, they are not. In this paper we show that accuracies reported on any dataset are correlated and invoke the appropriate statistical technique --- the paired model --- to compare the accuracies of two recognition systems. Theoretically we show that this method provides tighter confidence intervals than methods used in OCR and computer vision literature. We also propose a new visualization method, which we call the accuracy scatter plot, for providing a visual summary of performance results. This method summarizes the accuracy comparisons on the entire cor...
IR
carvalho00genetic
A Genetic Algorithm-Based Solution for the Problem of Small Disjuncts . In essence, small disjuncts are rules covering a small number of  examples. Hence, these rules are usually error-prone, which contributes to a  decrease in predictive accuracy. The problem is particularly serious because,  although each small disjuncts covers few examples, the set of small disjuncts  can cover a large number of examples. This paper proposes a solution to the  problem of discovering accurate small-disjunct rules based on genetic algorithms.  The basic idea of our method is to use a hybrid decision tree / genetic  algorithm approach for classification. More precisely, examples belonging to  large disjuncts are classified by rules produced by a decision-tree algorithm,  while examples belonging to small disjuncts are classified by a new genetic  algorithm, particularly designed for discovering small-disjunct rules.  1 Introduction  In the context of the well-known classification task of data mining, the discovered knowledge is often expressed as a set of IF-THEN predict...
ML
pouzol01from
From Declarative Signatures to Misuse IDS In many existing misuse intrusion detection systems, intrusion  signatures are very close to the detection algorithms. As a consequence,  they contain too many cumbersome details. Recent work have  proposed declarative signature languages that raise the level of abstraction  when writing signatures. However, these languages do not always  come with operational support. In this article, we show how to transform  such declarative signatures into operational ones. This process points  out several technical details which must be considered with care when  performing the translation by hand, but which can be systematically  handled.
AI
300668
Building a Large Location Table to Find Replicas of Physics Objects The problem of building a large location table for physics objects occurs within a number of planned physics data management systems, like those that control reclustering and wide-area replication. To satisfy their e#ciency goals, these systems have to make local or remote replicas of individual physics objects, which contain raw or reconstructed data for a single event, rather than replicas of large run or ntuple files. This replication implies the use of a table to resolve the logical, location independent object descriptor into a physical location where an object replica can be found. For modern physics experiments the table needs to scale to at least some 10  10  objects. We argue that such a table can be e#ciently implemented by limiting the freedom of lookup operations, and by exploiting some specific properties of the physics data model. One specific viable implementation is discussed.  Key words: Object location table, object-oriented databases, object clustering,  object re-cl...
DB
cleary00generating
Generating a Topically Focused VirtualReality Internet Surveys highlight that Internet users are frequently frustrated by failing to locate useful information, and by difficulty in browsing anarchically linked web-structures. We present a new Internet browsing application (called VR-net) that addresses these problems. It first identifies semantic domains consisting of tightly interconnected web-page groupings. The second part populates a 3D virtual world with these information sources, representing all relevant pages plus appropriate structural relations. Users can then easily browse through around a semantically focused virtual library. 1 Introduction  The Internet is a probably the most significant global information resource ever created, allowing access to an almost unlimited amount of information. In this paper we describe two inter-related difficulties suffered by Internet users, and their combined influence on web use. We then introduce an integrated "search and browse" solution tool that directly tackles both issues. We also examin...
IR
wu00adaptive
An Adaptive Self-Organizing Color Segmentation Algorithm with Application to Robust Real-time Human Hand Localization In Proc. Asian Conf. on Computer Vision, Taiwan, 2000  This paper describes an adaptive self-organizing color segmentation algorithm and a transductive learning algorithm used to localize human hand in video sequences. The color distribution at each time frame is approximated by the proposed 1-D self-organizing map (SOM), in which schemes of growing, pruning and merging are facilitated to find an appropriate number of color cluster automatically. Due to the dynamic backgrounds and changing lighting conditions, the distribution of color over time may not be stationary. An algorithm of SOM transduction is proposed to learn the nonstationary color distribution in HSI color space by combining supervised and unsupervised learning paradigms. Color cue and motion cue are integrated in the localization system, in which motion cue is employed to focus the attention of the system. This approach is also applied to other tasks such as human face tracking and color indexing. Our localization system...
HCI
bergmark02collection
Collection Synthesis The invention of the hyperlink and the HTTP transmission protocol caused an amazing new structure to appear on the Internet -- the World Wide Web. With the Web, there came spiders, robots, and Web crawlers, which go from one link to the next checking Web health, ferreting out information and resources, and imposing organization on the huge collection of information (and dross) residing on the net. This paper reports on the use of one such crawler to synthesize document collections on various topics in science, mathematics, engineering and technology. Such collections could be part of a digital library.
IR
32986
Min-Wise Independent Permutations We define and study the notion of min-wise independent families of permutations. We say that  F #  S n is min-wise independent if for any set X  #  [n] and any x  #  X, when # is chosen at random in  F  we have Pr(min{#(X)} = #(x)) = 1  |X|  . In other words we require that all the elements of any fixed set X have an equal chance to become the minimum element of the image of X under #.  Our research was motivated by the fact that such a family (under some relaxations) is essential to the algorithm used in practice by the AltaVista web index software to detect and filter near-duplicate documents. However, in the course of our investigation we have discovered interesting and challenging theoretical questions related to this concept -- we present the solutions to some of them and we list the rest as open problems.  # Digital SRC, 130 Lytton Avenue, Palo Alto, CA 94301, USA. E-mail: broder@pa.dec.com.  + Computer Science Department, Stanford University, CA 94305, USA. E-mail: moses@cs.stan...
AI
delgado99multiagent
A Multiagent Architecture For Fuzzy Modeling In this paper a hybrid learning system that combines different fuzzy modeling techniques  is being investigated. In order to implement the different methods, we propose  the use of intelligent agents, which collaborate by means of a multiagent architecture.
Agents
28223
Probabilistic Deduction with Conditional Constraints over Basic Events We study the problem of probabilistic deduction with conditional constraints over basic events. We show that globally complete probabilistic deduction with conditional constraints over basic events is NP-hard. We then concentrate on the special case of probabilistic deduction in conditional constraint trees. We elaborate very efficient techniques for globally complete probabilistic deduction. In detail, for conditional constraint trees with point probabilities, we present a local approach to globally complete probabilistic deduction, which runs in linear time in the size of the conditional constraint trees. For conditional constraint trees with interval probabilities, we show that globally complete probabilistic deduction can be done in a global approach by solving nonlinear programs. We show how these nonlinear programs can be transformed into equivalent linear programs, which are solvable in polynomial time in the size of the conditional constraint trees. 1. Introduction  Dealing wit...
AI
360831
Exploring Brick-Based Navigation and Composition in an Augmented Reality . BUILD-IT is a planning tool based on computer vision technology,  supporting complex planning and composition tasks. A group of people, seated  around a table, interact with objects in a virtual scene using real bricks. A plan  view of the scene is projected onto the table, where object manipulation takes  place. A perspective view is projected on the wall. The views are set by virtual  cameras, having spatial attributes like shift, rotation and zoom. However, planar  interaction with bricks provides only position and rotation information. Object  height control is equally constrained by planar interaction. The aim of this paper  is to suggest methods and tools bridging the gap between planar interaction and  three-dimensional control. To control camera attributes, active objects, with intelligent  behaviour are introduced. To control object height, several real and  virtual tools are suggested. Some of the solutions are based on metaphors, like  window, sliding-ruler and floor.  1 I...
HCI
202938
On2broker: Semantic-Based Access to Information Sources at the WWW On2broker provides brokering services to improve access to heterogeneous, distributed and semistructured information sources as they are presented in the World Wide Web. It relies on the use of ontologies to make explicit the semantics of web pages. In the paper we will discuss the general architecture and main components of On2broker and provide some application scenarios.  1. Introduction  In the paper we describe a tool environment called On2broker  1  that processes information sources and content descriptions in HTML, XML, and RDF and that provides intelligent information retrieval, query answering and maintenance support. Central for our approach is the use of ontologies to describe background knowledge and to make explicit the semantics of web documents. Ontologies have been developed in the area of knowledge-based systems for structuring and reusing large bodies of knowledge (cf. CYC [Lenat, 1995], (KA)2 [Benjamins et al., 1998]). Ontologies are consensual and formal specificat...
DB
damiani97semantic
Semantic Approaches to Structuring and Querying Web Sites In order to pose effective queries to Web sites, some form of site data model must be implicitly or explicitly shared by users. Many approaches try to compensate for the lack of such a common model by considering the hypertextual structure of Web sites; unfortunately, this structure has usually little to do with data semantics. In this paper a different technique is proposed that allows for both navigational and logical/conceptual description of Web sites. The data model is based on WG-log, a query language based on the graph-oriented database model of GOOD [Gys94] and G-log [Par95], which allows the description of data manipulation primitives via (sets of) graph(s). The WG-log description of a Web site schema is lexically based on standard hypermedia design languages, thus allowing for easy schema generation by current hypermedia authoring environments. The use of WG-log for queries allows graphic query construction with respect to both the navigational and the logical parts of schema...
IR
popescul00automatic
Automatic Labeling of Document Clusters Automatically labeling document clusters with words which indicate their topics is difficult to do well. The most commonly used method, labeling with the most frequent words in the clusters, ends up using many words that are virtually void of descriptive power even after traditional stop words are removed. Another method, labeling with the most predictive words, often includes rather obscure words. We present two methods of labeling document clusters motivated by the model that words are generated by a hierarchy of mixture components of varying generality. The first method assumes existence of a document hierarchy (manually constructed or resulting from a hierarchical clustering algorithm) and uses a 2 test of significance to detect different word usage across categories in the hierarchy. The second method selects words which both occur frequently in a cluster and effectively discriminate the given cluster from the other clusters. We compare these methods on abstracts of documents sel...
IR
vdovjak01rdf
RDF Based Architecture for Semantic Integration of Heterogeneous Information Sources . The proposed integration architecture aims at exploiting data semantics in order to provide a coherent and meaningful (with respect to a given conceptual model) view of the integrated heterogeneous information sources. The architecture is split into five separate layers to assure modularization, providing description, requirements, and interfaces for each. It favors the lazy retrieval paradigm over the data warehousing approach. The novelty of the architecture lies in the combination of semantic and on-demand driven retrieval. This line of attack offers several advantages but brings also challenges, both of which we discuss with respect to RDF, the architecture's underlying model. 1 Introduction, Background, and Related Work  With the vast expansion of the World Wide Web during the last few years the integration of heterogeneous information sources has become a hot topic. A solution to this integration problem allows for the design of applications that provide a uniform access to dat...
DB
kodratoff00comparing
Comparing Machine Learning and Knowledge Discovery in DataBases: An Application to Knowledge Discovery in Texts INTRODUCTION  KDD is better known by the oversimplified name of Data Mining (DM). Actually, most academics are rather interested by DM which develops methods for extracting knowledge from a given set of data. Industrialists and experts should be more interested in KDD which comprises the whole process of data selection, data cleaning, transfer to a DM technique, applying the DM technique, validating the results of the DM technique, and finally interpreting them for the user. In general, this process is a cycle that improves under the criticism of the expert.  Machine Learning (ML) and KDD have in common a very strong link : they both acknowledge the importance of induction as a normal way of thinking, while other scientific fields are reluctant to accept it, to say the least. We shall first explore this common point. We believe that this reluctance relies on a misuse of apparent contradictions inside the theory of confirmation, that is we shall revisit Hempel paradox in order t
IR
granlund01patternsupported
A Pattern-Supported Approach to the User Interface Design Process Patterns describe generic solutions to common problems in context. Originating from the world of architecture, patterns have been used mostly in object-oriented programming and data analysis. The goal of HCI patterns is to create an inventory of solutions to help designers (and usability engineers) to resolve UI development problems that are common, difficult and frequently encountered. In this paper, we present our pattern-supported approach to user interface design in the context of information visualization. Using a concrete example from the telecommunications domain, we will focus on a task/subtask pattern to illustrate how knowledge about a task and an appropriate interaction design solution can be captured and communicated.  1 
HCI
friedman99learning
Learning Probabilistic Relational Models A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with &quot;flat &quot; data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning — the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases. 1
DB
kemme00new
A New Approach to Developing and Implementing Eager Database Replication Protocols Database replication is traditionally seen as a way to increase the availability and performance of distributed databases. Although a large number of protocols providing data consistency and fault-tolerance have been proposed, few of these ideas have ever been used in commercial products due to their complexity and performance implications. Instead, current products allow inconsistencies and often resort to centralized approaches which eliminates some of the advantages of replication. As an alternative, we propose a suite of replication protocols that addresses the main problems related to database replication. On the one hand, our protocols maintain data consistency and the same transactional semantics found in centralized systems. On the other hand, they provide flexibility and reasonable performance. To do so, our protocols take advantage of the rich semantics of group communication primitives and the relaxed isolation guarantees provided by most databases. This allows us to eliminate the possibility of deadlocks, reduce the message overhead and increase performance. A detailed simulation study shows the feasibility of the approach and the flexibility with which different types of bottlenecks can be circumvented.
DB
gotoh00language
Language Model Adaptation .15> attempt to exploit longer distance dependencies.  -- infer some notion of `topic' from text.  -- compute topic dependent probability. 8th ELSNET summer school 2  Language Model Adaptation 26 July 2000 ' & $ %  Adaptive Language Modelling  Stage 1: automatic derivation of topic information from text.  ffl loose definition of document:  a unit of spoken (or written) data of a certain length that contains some topic(s), or content(s).  ffl topic of a document (= long distance or document-wide statistics.  ffl information retrieval (IR): `bag-of-words' model based on a histogram of weighted unigram frequencies.  Stage 2: combination of global and topic-dependent text statistics.  ffl mixture.  ffl maximum entropy modelling. (ref) Jelinek (1997). ' & $ %  Mixtur
IR
callan99automatic
Automatic Discovery of Language Models for Text Databases The proliferation of text databases within large organizations and on the Internet makes it difficult for a person to know which databases to search. Given language models  that describe the contents of each database, a database selection  algorithm such as GlOSS can provide assistance by automatically selecting appropriate databases for an information need. Current practice is that each database provides its language model upon request, but this cooperative  approach has important limitations. This paper demonstrates that cooperation is not required. Instead, the database selection service can construct its own language models by sampling database contents via the normal process of running queries and retrieving documents. Although random sampling is not possible, it can be approximated with carefully selected queries. This sampling  approach avoids the limitations that characterize the cooperative approach, and also enables additional capabilities. Experimental results demonstrate th...
IR
drummond97using
Using a Case Base of Surfaces to Speed-Up Reinforcement Learning . This paper demonstrates the exploitation of certain vision processing techniques to index into a case base of surfaces. The surfaces are the result of reinforcement learning and represent the optimum choice of actions to achieve some goal from anywhere in the state space. This paper shows how strong features that occur in the interaction of the system with its environment can be detected early in the learning process. Such features allow the system to identify when an identical, or very similar, task has been solved previously and to retrieve the relevant surface. This results in an orders of magnitude increase in learning rate. 1 Introduction  One important research issue for case based learning is its combination with other learning methods. As Aamodt and Plaza [1] point out, generally the machine learning community aims to produce "a coherent framework, where each learning method fulfills a specific and distinct role in the system." This paper discusses one such approach, combinin...
ML
cacheda01superimposing
Superimposing Codes Representing Hierarchical Information in Web directories In this article we describe how superimposed coding can be used  to represent hierarchical information, which is especially useful in categorized information retrieval systems (for example, Web  directories). Superimposed coding have been widely used in  signature files in a rigid manner, but our approach is more flexible and powerful. The categorization is based on a directed acyclic  graph and each document is assigned to one or more nodes, using  superimposed coding we represent the categorization information  of each document in a signature. In this paper we explain the  superimposed coding theory and how this coding technique can  be applied to more flexible environments. Furthermore, we realize an exhaustive analysis of the important factors that have  repercussions on the performance of the system. Finally we  expose the conclusions obtained from this article.
IR
bhalotia02keyword
Keyword Searching and Browsing in Databases using BANKS With the growth of the Web, there has been a rapid increase in the number of users who need to access online databases without having a detailed knowledge of the schema or of query languages; even relatively simple query languages designed for non-experts are too complicated for them. We describe BANKS, a system which enables keyword-based search on relational databases, together with data and schema browsing. BANKS enables users to extract information in a simple manner without any knowledge of the schema or any need for writing complex queries. A user can get information by typing a few keywords, following hyperlinks, and interacting with controls on the displayed results. BANKS models tuples as nodes in a graph, connected by links induced by foreign key and other relationships. Answers to a query are modeled as rooted trees connecting tuples that match individual keywords in the query. Answers are ranked using a notion of proximity coupled with a notion of prestige of nodes based on inlinks, similar to techniques developed for Web search. We present an efficient heuristic algorithm for finding and ranking query results. 1.
IR
timm01from
From Corporate Memories to Supply Web Memory Modern production has discovered knowledge as  an additional factor of production and a new trend  of research, development and implementation of  corporate memory systems is arising. The global  economy leads to tighter corporation relations between  enterprises. Therefore the knowledge of one  product does not exist in a single company but  within participating companies respective the supply  chain. A modern product centered knowledge  management has to face the difficult task of the integration  of distributed knowledge sources.  This contribution states our interest in research on  the integration of corporate memory. In a first step  we are focusing on single products leading to supply  chain memories. Further research and development  will lead to supply web memory.  1 
Agents
ghani01combining
Combining Labeled and Unlabeled Data for Text Classification With a Large Number of Categories A major concern with supervised learning techniques for text classification  is that they often require a large number of labeled examples to learn  accurately. One way to reduce the amount of labeled data required is to  develop algorithms that can learn effectively from a small number of labeled  examples augmented with a large number of unlabeled examples.  In this paper, we develop a framework to incorporate unlabeled data in  the Error-Correcting Output Coding (ECOC) setup by decomposing multiclass  problems into multiple binary problems and then use Co-Training  to learn the individual binary classification problems. We show that our  method is especially useful for classification tasks involving a large number  of categories where Co-training doesn't perform very well by itself  and when combined with ECOC, outperforms several other algorithms  that combine labeled and unlabeled data for text classification.  1 
IR
3489
Mixed Depth Representations for Dialog Processing We describe our work on developing a general purpose tutoring system that will allow students to practice their decision-making skills in a number of domains. The tutoring system, B2, supports mixed-initiative natural language interaction. The natural language processing and knowledge representation components are also general purpose|which leads to a tradeo between the limitations of super cial processing and syntactic representations and the di culty of deeper methods and conceptual representations. Students ' utterances may be short and ambiguous, requiring extensive reasoning about the domain or the discourse model to fully resolve. However, full disambiguation is rarely necessary. Our solution is to use a mixed-depth representation, one that encodes syntactic and conceptual information in the same structure. As a result, we can use the same representation framework to produce a detailed representation of requests (which tend to be well-speci ed) and to produce a partial representation of questions (which tend to require more inference about the context). Moreover, the representations use the same knowledge representation framework that is used to reason about discourse processing and domain information|so that the system can reason with (and about) the utterances, if necessary.
AI
michaud99representation
Representation of behavioral history forl earningin nonstationary conditions A robot having to operate in nonstationary conditions needs to learn how to modify its control policy to adapt to the changing dynamics of the environment. Using the behavior-based approach to manage the interactions between the robot and its environment, we propose a method that models these interactions and adapts the selection of behaviors according to the history of behavior use. The learning and the use of this &quot;Interaction Model &quot; are validated using a vision- and sonar-based Pioneer I robot in the context of a multi-robot foraging task. Results show the effectiveness of the approach in taking advantage of any regularities experienced in the world, leading to fast and adaptable specialization for the learning robot. 1
AI
muslea01hierarchical
Hierarchical Wrapper Induction for Semistructured Information Sources With the tremendous amount of information that becomes available on the Web on a daily basis, the abilitytoquickly develop information agents has become a crucial problem. A vital componentofanyWeb-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex documentinto a series of simpler extraction tasks. We introduce an inductive algorithm, stalker, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results showthatstalker requires up to two orders of magnitude fewer examples than other algorithms. Furthermore,    can wrap information sources that could not be wrapped by existing inductivetechniques.
IR
weng98visionguided
Vision-Guided Navigation Using SHOSLIF . This paper presents an unconventional approach to vision-guided autonomous navigation. The system recalls information about scenes and navigational experience using content-based retrieval from a visual database. To achieve a high applicability to various road types, we do not impose a priori scene features, such as road edges, that the system must use. But rather, the system automatically derives features from images during supervised learning. To accomplish this, the system uses principle component analysis and linear discriminant analysis to automatically derive the most expressive features (MEF) for scene reconstruction or the most discriminating features (MDF) for scene classi cation. These features best describe or classify the population of the scenes and approximate complex decision regions using piecewise linear boundaries up to a desired accuracy. A new self-organizing scheme called recursive partition tree (RPT) is used for automatic construction of a vision-and-control da...
ML
428001
Learning and Tracking Human Motion Using Functional Analysis We present a method for the modeling and tracking of human motion using a sequence of 2D video images. Our analysis is divided in two parts: statistical learning and Bayesian tracking. First, we estimate a statistical model of typical activities from a large set of 3D human motion data. For this purpose, the human body is represented as a set of articulated cylinders and the evolution of a particular joint angle is described by a time-series. Specifically, we consider periodic motion such as “walking ” in this work, and we develop a new set of tools that allows for the automatic segmentation of the training data into a sequence of identical “motion cycles”. Then we compute the mean and the principal components of these cycles using a new algorithm to account for missing information and to enforce smooth transitions between different cycles. The learned temporal model provides a prior probability distribution over human motions which is used for tracking. We adopt a Bayesian perspective and approximate the posterior distribution of the body parameters using a particle filter. The resulting algorithm is able to track human subjects in monocular video sequences and to recover their 3D motion in complex unknown environments. 1
HCI
450055
Engineering Mobile-agent Applications via Context-dependent Coordination The design and development of Internet applications can take advantage of a paradigm based on autonomous and mobile agents. However, mobility introduces peculiar coordination problems in agent-based Internet applications. First, it suggests the exploitation of an infrastructure based on a multiplicity of local interaction spaces. Second, it may require coordination activities to be adapted both to the characteristics of the execution environment where they occur and to the needs of the application to which the coordinating agents belong. In this context, this paper introduces the concept of context-dependent coordination based on programmable interaction spaces. On the one hand, interaction spaces associated to different execution environments may be independently programmed so as to lead to differentiated, environment-dependent, behaviors. On the other hand, agents can program the interaction spaces of the visited execution environments to obtain an application-dependent behavior of the interaction spaces themselves. Several examples show how an infrastructure enforcing context-dependent coordination can be effectively exploited to simplify and make more modular the design of Internet applications based on mobile agents. In addition, the MARS coordination infrastructure is presented as an example of a system in which the concept of context-dependent coordination has found a clean and efficient implementation.
Agents
38408
Continuous Categories For a Mobile Robot Autonomous agents make frequent use of knowledge in the form of categories --- categories of objects, human gestures, web pages, and so on. This paper describes a way for agents to learn such categories for themselves through interaction with the environment. In particular, the learning algorithm transforms raw sensor readings into clusters of time series that have predictive value to the agent. We address several issues related to the use of an uninterpreted sensory apparatus and show specific examples where a Pioneer 1 mobile robot interacts with objects in a cluttered laboratory setting.  Introduction  "There is nothing more basic than categorization to our thought, perception, action, and speech" (Lakoff 1987). For autonomous agents, categories often appear as abstractions of raw sensor readings that provide a means for recognizing circumstances and predicting effects of actions. For example, such categories play an important role for a mobile robot that navigates around obstacles ...
AI
wolter00spatiotemporal
Spatio-temporal representation and reasoning based on RCC-8 this paper is to introduce a hierarchy of languages intended for qualitative spatio-temporal representation and reasoning, provide these languages with topological temporal semantics, construct effective reasoning algorithms, and estimate their computational complexity.
DB
joachims01statistical
A Statistical Learning Model of Text Classification for Support Vector Machines This paper develops a theoretical learning model of text classification for Support Vector Machines (SVMs). It connects the statistical properties of text-classification tasks with the generalization performance of a SVM in a quantitative way. Unlike conventional approaches to learning text classifiers, which rely primarily on empirical evidence, this model explains why and when SVMs perform well for text classification. In particular, it addresses the following questions: Why can support vector machines handle the large feature spaces in text classification effectively? How is this related to the statistical properties of text? What are sufficient conditions for applying SVMs to text-classification problems successfully?
IR
vaneijk01generalised
Generalised Object-Oriented Concepts for Inter-Agent Communication . In this paper, we describe a framework to program open societies of concurrently operating agents. The agents maintain a subjective theory about their environment and interact with each other via a communication mechanism suited for the exchange of information, which is a generalisation of the traditional rendez-vous communication mechanism from the object-oriented programming paradigm. Moreover, following object-oriented programming, agents are grouped into agent classes according to their particular characteristics; viz. the program that governs their behaviour, the language they employ to represent information and most interestingly the questions they can be asked to answer. We give and operational model of the programming language in terms of a transition system for the formal derivation of computations of multi-agent programs. 1 Introduction The field of multi-agent systems is a rapidly growing research area. Although in this field there is no real consensus on what ...
Agents
bergamaschi98semantic
A Semantic Approach to Information Integration: the MOMIS project this paper, we propose a semantic approach to the integration of heterogeneous information. The approach follows the semantic paradigm, in that conceptual schemata of an involved source are considered, and a common data model (ODM I 3) and language (ODL I 3) are adopted to describe sharable information. ODM I 3 and ODL I 3 are defined as a subset of the corresponding ODMG-93 [13] ODM and ODL. A Description Logics ocdl (object description language with constraints [6]) is used as a
DB
460350
The Intelligent Surfer: Probabilistic Combination of Link and Content Information in PageRank The PageRank algorithm, used in the Google search engine, greatly  improves the results of Web search by taking into account the link  structure of the Web. PageRank assigns to a page a score proportional  to the number of times a random surfer would visit that page,  if it surfed indefinitely from page to page, following all outlinks  from a page with equal probability. We propose to improve PageRank  by using a more intelligent surfer, one that is guided by a  probabilistic model of the relevance of a page to a query. Efficient  execution of our algorithm at query time is made possible by precomputing  at crawl time (and thus once for all queries) the necessary  terms. Experiments on two large subsets of the Web indicate  that our algorithm significantly outperforms PageRank in the (human  -rated) quality of the pages returned, while remaining efficient  enough to be used in today's large search engines.
IR
samtani98recent
Recent Advances and Research Problems in Data Warehousing . In the recent years, the database community has witnessed the emergence of a new technology, namely data warehousing. A data warehouse is a global repository that stores pre-processed queries on data which resides in multiple, possibly heterogeneous, operational or legacy sources. The information stored in the data warehouse can be easily and efficiently accessed for making effective decisions. The On-Line Analytical Processing (OLAP) tools access data from the data warehouse for complex data analysis, such as multidimensional data analysis, and decision support activities. Current research has lead to new developments in all aspects of data warehousing, however, there are still a number of problems that need to be solved for making data warehousing effective. In this paper, we discuss recent developments in data warehouse modelling, view maintenance, and parallel query processing. A number of technical issues for exploratory research are presented and possible solutions are discusse...
DB
198731
Learning Environmental Features for Pose Estimation We present a method for learning a set of environmental features which are useful for pose estimation. The landmark learning mechanism is designed to be applicable to a wide range of environments, and generalized for di#erent sensing modilities. In the context of computer vision, each landmark is detected as a local extremum of a measure of distinctiveness and represented by an appearance-based encoding which is exploited for matching. The set of obtained landmarks can be parameterized and then evaluated in terms of their utility for the task at hand. The method is used to motivate a general approach to task-oriented sensor fusion. We present experimental evidence that demonstrates the utility of the method. 1 Introduction In this paper, we develop an approach to sensorbased robot localization by learning a set of recognizable features in the robot's environment. In particular, we consider the problem of learning a set of image-domain landmarks from a set of di#erent views of a scene. ...
Agents
508492
SCANMail: a voicemail interface that makes speech browsable, readable and searchable Increasing amounts of public, corporate, and private speech data are now available on-line. These are limited in their usefulness, however, by the lack of tools to permit their browsing and search. The goal of our research is to provide tools to overcome the inherent difficulties of speech access, by supporting visual scanning, search, and information extraction. We describe a novel principle for the design of UIs to speech data: What You See Is Almost What You Hear (WYSIAWYH). In WYSIAWYH, automatic speech recognition (ASR) generates a transcript of the speech data. The transcript is then used as a visual analogue to that underlying data. A graphical user interface allows users to visually scan, read, annotate and search these transcripts. Users can also use the transcript to access and play specific regions of the underlying message. We first summarize previous studies of voicemail usage that motivated the WYSIAWYH principle, and describe a voicemail UI, SCANMail, that embodies WYSIAWYH. We report on a laboratory experiment and an 18 user, two month field trial evaluation. SCANMail outperformed a state of the art voicemail system on core voicemail tasks. This was attributable to SCANMail's support for visual scanning, search and information extraction. While the ASR transcripts contain errors, they nevertheless improve the efficiency of voicemail processing. Transcripts either provide enough information for users to extract key points or to navigate to important regions of the underlying speech, which they can then play directly.
IR
ambroszkiewicz98team
Team Formation by Self-Interested Mobile Agents . A process of team formation by autonomous agents in a distributed  environment is presented. Since the environment is distributed,  there are serious problems with communication and consistent decision  making inside a team. To deal with these problems, the standard technique  of token passing in a computer network is applied. The passing  cycle of the token serves as the communication route. It assures consistent  decision making inside the team maintaining its organizational  integrity. On the other hand it constitutes a component of the plan of  the cooperative work performed by a complete team. Two algorithms for  team formation are given. The first one is based on simple self-interested  agents that still can be viewed as reactive agents (see [14]) although augmented  with knowledge, goal, and cooperation mechanisms. The second  one is based on sophisticated self-interested agents. Moreover, the algorithm  based on fully cooperative agents, which is an adaptation of the  static ...
Agents
bolotov99clausal
A Clausal Resolution Method for CTL Branching-Time Temporal Logic In this paper we extend our clausal resolution method for linear time temporal logics to a branching-time framework. Thus, we propose an efficient deductive method useful in a variety of applications requiring an expressive branching-time temporal logic in AI. The branching-time temporal logic considered is Computation Tree Logic (CTL), often regarded as the simplest useful logic of this class. The key elements of the resolution method, namely the normal form, the concept of step resolution and a novel temporal resolution rule, are introduced and justified with respect to this logic. A completeness argument is provided, together with some examples of the use of the temporal resolution method. Finally, we consider future work, in particular the extension of the method yet further, to Extended CTL (ECTL), which is CTL extended with fairness operators, and CTL    , the most powerful logic of this class. We will also outline possible implementation of the approach by adapting techniques de...
DB
middleton02exploiting
Exploiting Synergy Between Ontologies and Recommender Systems Recommender systems learn about user preferences over time, automatically finding things of similar interest. This reduces the burden of creating explicit queries. Recommender systems do, however, suffer from cold-start problems where no initial information is available early on upon which to base recommendations. Semantic knowledge structures, such as ontologies, can provide valuable domain knowledge and user information. However, acquiring such knowledge and keeping it up to date is not a trivial task and user interests are particularly difficult to acquire and maintain. This paper investigates the synergy between a web-based research paper recommender system and an ontology containing information automatically extracted from departmental databases available on the web. The ontology is used to address the recommender systems cold-start problem. The recommender system addresses the ontology’s interest-acquisition problem. An empirical evaluation of this approach is conducted and the performance of the integrated systems measured.
IR
smyth01data
Data Mining At The Interface Of Computer Science And Statistics This chapter is written for computer scientists, engineers, mathematicians, and scientists who wish to gain a better understanding of the role of statistical thinking in modern data mining. Data mining has attracted considerable attention both in the research and commercial arenas in recent years, involving the application of a variety of techniques from both computer science and statistics. The chapter discusses how computer scientists and statisticians approach data from different but complementary viewpoints and highlights the fundamental differences between statistical and computational views of data mining. In doing so we review the historical importance of statistical contributions to machine learning and data mining, including neural networks, graphical models, and flexible predictive modeling. The primary conclusion is that closer integration of computational methods with statistical thinking is likely to become increasingly important in data mining applications.  Keywords: Data mining, statistics, pattern recognition, transaction data, correlation. 1. 
ML
510100
On the Robustness of some Cryptographic Protocols for Mobile Agent Protection Mobile agent security is still a young discipline and most naturally,  the focus up to the time of writing was on inventing new cryptographic protocols  for securing various aspects of mobile agents. However, past experience shows  that protocols can be flawed, and flaws in protocols can remain unnoticed for a  long period of time. The game of breaking and fixing protocols is a necessary  evolutionary process that leads to a better understanding of the underlying problems  and ultimately to more robust and secure systems. Although, to the best of  our knowledge, little work has been published on breaking protocols for mobile  agents, it is inconceivable that the multitude of protocols proposed so far are all  flawless. As it turns out, the opposite is true. We identify flaws in protocols proposed  by Corradi et al., Karjoth et al., and Karnik et al., including protocols based  on secure co-processors.
Agents
yu01indexing
Indexing the Distance: An Efficient Method to KNN Processing In this paper, we present an efficient method, called iDistance, for K-nearest neighbor (KNN) search in a high-dimensional space. iDistance partitions the data and selects a reference point for each partition. The data in each cluster are transformed into a single dimensional space based on their similarity with respect to a reference point. This allows the points to be indexed using a B + -tree structure and KNN search be performed using one-dimensional range search. The choice of partition and reference point provides the iDistance technique with degrees of freedom most other techniques do not have. We describe how appropriate choices here can effectively adapt the index structure to the data distribution. We conducted extensive experiments to evaluate the iDistance technique, and report results demonstrating its effectiveness.
DB
sim00to
To Boldly Go: Bayesian Exploration for Mobile Robots This work addresses the problem of robot exploration. That is, the task of automatically learning a map of the environment which is useful for mobile robot navigation and localization. The exploration mechanism is intended to be applicable to an arbitrary environment, and is independent of the particular representation of the world. We take an information-theoretic approach and avoid the use of arbitrary heuristics. Preliminary results are presented and we discuss future directions for investigation.
ML
132458
Equal Time for Data on the Internet with WebSemantics . Many collections of scientific data in particular disciplines are available today around the world. Much of this data conforms to some agreed upon standard for data exchange, i.e., a standard schema and its semantics. However, sharing this data among a global community of users is still difficult because of a lack of standards for the following necessary functions: (i) data providers need a standard for describing or publishing available sources of data; (ii) data administrators need a standard for discovering the published data and (iii) users need a standard for accessing this discovered data. This paper describes a prototype implementation of a system, WebSemantics, that accomplishes the above tasks. We describe an architecture and protocols for the publication, discovery and access to scientific data. We define a language for discovering sources and querying the data in these sources, and we provide a formal semantics for this language. 1 Introduction  Recently, many standardized...
IR
381995
A Practical Approach for Recovery of Evicted Variables SRC’s charter is to advance the state of the art in computer systems by doing basic and applied research in support of our company’s business objectives. Our interests and projects span scalable systems (including hardware, networking, distributed systems, and programming-language technology), the Internet (including the Web, e-commerce, and information retrieval), and human/computer interaction (including user-interface technology, computer-based appliances, and mobile computing). SRC was established in 1984 by Digital Equipment Corporation. We test the value of our ideas by building hardware and software prototypes and assessing their utility in realistic settings. Interesting systems are too complex to be evaluated solely in the abstract; practical use enables us to investigate their properties in depth. This experience is useful in the short term in refining our designs and invaluable in the long term in advancing our knowledge. Most of the major advances in information systems have come through this approach, including personal computing, distributed systems, and the Internet. We also perform complementary work of a more mathematical character. Some of
HCI
19461
Intelligent Diagnosis Systems This paper examines and compares several different approaches to the design of Intelligent Systems for Diagnosis and Advising applications. These include expert systems or knowledge-based systems, case-based reasoning systems, truth (or reason) maintenance systems, statistical pattern classification systems, decision trees, and artificial neural networks (or connectionist systems). The key aspects of each approach are demonstrated through the design of a system for a simple automobile fault diagnosis task. The paper also discusses the domain characteristics and design and performance requirements that influence the choice of a specific technique (or a combination of techniques) for a given application. 1 Introduction  The last few decades have seen a proliferation of intelligent systems for diagnosis, advising, forecasting, and related applications (Dean et al., 1995; Durkin, 1994; Ginsberg, 1993; Luger & Stubblefield, 1993; Puppe, 1993; Rich & Knight, 1991; Russell & Norvig, 1995; Ste...
ML
smith00conversation
Conversation Trees and Threaded Chats Chat programs and instant messaging services are increasingly popular among Internet users. However, basic issues with the interfaces and data structures of most forms of chat limit their utility for use in formal interactions (like group meetings) and decision-making tasks. In this paper, we discuss Threaded Text Chat, a program designed to address some of the deficiencies of current chat programs. Standard forms of chat introduce ambiguity into interaction in a number of ways, most profoundly by rupturing connections between turns and replies. Threaded Chat presents a solution to this problem by actively supporting the basic turn-taking structure of human conversation. While the solution introduces interface design challenges of its own, usability studies show that users' patterns of interaction in Threaded Chat are equally effective, but different (and possibly more efficient) than standard chat programs.  Keywords  Chat programs, turn-taking, conversation, computer mediated communi...
HCI
srinivasan02target
Target Seeking Crawlers and their Topical Performance Topic driven crawlers can complement search engines by targeting relevant portions of the Web. A topic driven crawler must exploit the information available about the topic and its underlying context. In this paper we extend our previous research on the design and evaluation of topic driven crawlers by comparing seven different crawlers on a harder problem, namely, seeking highly relevant target pages. We find that exploration is an important aspect of a crawling strategy. We also study how the performance of crawler strategies depends on a number of topical characteristics based on notions of topic generality, cohesiveness, and authoritativeness. Our results reveal that topic generality is an obstacle for most crawlers, that three crawlers tend to perform better when the target pages are clustered together, and that two of these also display better performance when topic targets are highly authoritative.
IR
hohl99nexus
Nexus - An Open Global Infrastructure for Spatial-Aware Applications Due to the lack of a generic platform for location- and spatial-aware systems, many basic services have to be reimplemented in each application that uses spatial-awareness. A cooperation among different applications is also difficult to achieve without a common platform. In this paper we present a platform that solves these problems. It provides an infrastructure that is based on computer models of regions of the physical world, which are augmented by virtual objects. We show how virtual objects make the integration of existing information systems and services in spatial-aware systems easier. Furthermore, our platform supports interactions between the computer models and the real world and integrates single models in a global "Augmented World".  Contents  1 Introduction 3 2 General Idea 4  2.1 Augmented Areas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Augmented World . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3 Example Scenario 6 4 Require...
HCI
brazier99compositional
Compositional Design and Reuse of a Generic Agent Model This paper introduces a formally specified design of a compositional generic agent model (GAM). This  agent model abstracts from specific application domains; it provides a unified formal definition of a model  for weak agenthood. It can be (re)used as a template or pattern for a large variety of agent types and  application domain types. The model was designed on the basis of experiences in a number of application  domains. The compositional development method DESIRE was used to design the agent model GAM at a  conceptual and logical level. It serves as a unified, precisely defined coneptual structure which can be  refined by specialisation and instantiation to a large variety of other, more specific agents. To illustrate  reuse of this agent model, specialisation and instantiation to model co-operative information gathering  agents is described in depth. Moreover, it is shown how GAM can be used to describe in a unified and  hence more comparable manner a large number of agent architectures from the literature.
Agents
532267
A Comparison of Decision Making Criteria and Optimization Methods for Active Robotic Sensing This work presents a comparison of decision making criteria  and optimization methods for active sensing in robotics. Active sensing  incorporates the following aspects: (i) where to position sensors, and  (ii) how to make decisions for next actions, in order to maximize information  gain and minimize costs. We concentrate on the second aspect:  "Where should the robot move at the next time step?". Pros and cons of  the most often used statistical decision making strategies are discussed.
ML
surmann00five
A Five Layer Sensor Architecture for Autonomous Robots in Indoor Environments Autonomous mobile service robots for transportation tasks in indoor environments e.g. multistory buildings, have to act in normal dynamic environments but with a huge number of components. The robots total repertoire of skills is high according to the complexity of the building and its respective task. Difficult tasks can only be achieved on the base by immediate sensing of the environment. This paper describes a five layer sensor architecture with an integrated world model for multistory buildings. In contrast to grid based approaches we use a feature based approach. The sensor architecture as well as the evaluation modules of the sensor data are based on natural landmarks. The key features of the sensor architecture are reuseability, modularity and portability to other multistory buildings as well as extendibility with different sensors.  Keywords---robot architectures, sensor architectures, collision avoidance and sensor-based control, robot sensing and data fusion, behavior-based robotics  I. 
ML
andre99integrating
Integrating Models of Personality and Emotions into Lifelike Characters . A growing number of research projects in academia and industry  have recently started to develop lifelike agents as a new metaphor for highly  personalised human-machine communication. A strong argument in favour of  using such characters in the interface is the fact that they make humancomputer  interaction more enjoyable and allow for communication styles  common in human-human dialogue. In this paper we discuss three ongoing  projects that use personality and emotions to address different aspects of the  affective agent-user interface: (a) Puppet uses affect to teach children how the  different emotional states can change or modify a character's behaviour, and  how physical and verbal actions in social interactions can induce emotions in  others; (b) the Inhabited Market Place uses affect to tailor the roles of actors in  a virtual market place; and (c) Presence uses affect to enhance the believability  of a virtual character, and produce a more natural conversational manner.  Int...
HCI
coellocoello99updated
An Updated Survey of Evolutionary Multiobjective Optimization Techniques: State of the Art and Future Trends This paper reviews some of the most popular evolutionary multiobjective optimization techniques currently reported in the literature, indicating some of their main applications, their advantages, disadvantages, and degree of aplicability. Finally, some of the most promising areas of future research are briefly discussed. 1 Introduction  Since the pioneering work of Rosenberg in the late 1960s regarding the possibility of using genetic-based search to deal with multiple objectives [1], this new area of research (now called Evolutionary Multi-Objective Optimization, or EMOO for short) has grown considerably as indicates the notable increment (mainly in the last 15 years) of technical papers in international conferences and peer-reviewed journals, special sessions in international conferences and interest groups in the Internet  1  . Multiobjective optimization is with no doubt a very important research topic both for scientists and engineers, not only because of the multiobjective nature...
ML
czajkowski98jres
JRes: A Resource Accounting Interface for Java With the spread of the Internet the computing model on server systems is undergoing several important changes. Recent research ideas concerning dynamic operating system extensibility are finding their way into the commercial domain, resulting in designs of extensible databases and Web servers. In addition, both ordinary users and service providers must deal with untrusted downloadable executable code of unknown origin and intentions.  Across the board, Java has emerged as the language of choice for Internet-oriented software. We argue that, in order to realize its full potential in applications dealing with untrusted code, Java needs a flexible resource accounting interface. The design and prototype implementation of such an interface --- JRes --- is presented in this paper. The interface allows to account for heap memory, CPU time, and network resources consumed by individual threads or groups of threads. JRes allows limits to be set on resources available to threads and it can invoke...
IR
lacorte99agent
An Agent Based Framework for Mobile Users User mobility together with an easy access to distributed resources is one of the greatest challenge to be faced in the future years. At the same time, agent technology is seen as a very promising approach to deal with distributed computing and user mobility. In this paper an agent-based strategy for support of mobile users is presented. It is based on a mobile agent platform developed at the University of Catania, which has been enhanced in order to allow the user to access network services in a mobile environment. Main functionalities and architecure of the above platform are described.  1 Introduction  The quick expansion of wireless communication technologies and of portable computing devices, has made mobile computing more and more important. The user wishes to access the information he/she needs at any moment, independently of the place where he/she is. The ever increasing computing power available in notebooks, makes them a valid working tool for the user who needs to move from ...
Agents
swindells00system
System Lag Tests for Augmented and Virtual Environments We describe a simple technique for accurately calibrating the temporal lag in augmented and virtual environments within the Enhanced Virtual Hand Lab (EVHL), a  collection of hardware and software to support research on goal-directed human hand motion. Lag is the sum of various delays in the data pipeline associated with sensing, processing, and displaying information from the physical world to produce an augmented or virtual world. Our main calibration technique uses a modified phonograph turntable to provide easily tracked periodic motion, reminiscent of the pendulum-based calibration technique of Liang, Shaw and Green. Measurements show a three-frame (50 ms) lag for the EVHL. A second technique, which uses a  specialized analog sensor that is part of the EVHL, provides a "closed loop" calibration capable of sub-frame accuracy. Knowing the lag to sub-frame accuracy enables a predictive tracking scheme to compensate for the end-toend  lag in the data pipeline. We describe both techniques and the EVHL environment in which they are used.
HCI
camacho01mapweb
MAPWEB: Cooperation between Planning Agents and Web Agents This paper presents MAPWeb (MultiAgent Planning in the Web), a multiagent system for cooperative work among dierent intelligent software agents whose main goal is to solve user planning problems using the information stored in the World Wide Web (Web). MAPWeb is made of a heterogeneous mixture of intelligent agents whose main characteristics are cooperation, reasoning, and knowledge sharing. The architecture of MAPWeb uses four types of agents:UserAgents that are the bridge between the users and the system; ControlAgents (Manager and Coach Agents) that are responsible to manage the rest of agents; PlannerAgents  that are able to solve planning problems; and nally WebAgents whose aim is to retrieve, represent and share information obtained from the Web. MAPWeb solves planning problems by means of cooperation between PlannerAgents and WebAgents. Instead of trying the PlannerAgent to solve the whole planning problem, the PlannerAgent focuses on a less restricted (and therefore easier to solve) problem (what we call an abstract problem) and cooperates with the WebAgents to validate and complete abstract solutions. In order for cooperation to take place, a common language and data structures have also been dened.  Categories and Subject Descriptors  H.3.5 [Online Information Services]: Data sharing, Webbased services; I.2 [Articial Intelligence]; I.2.6 [Learning]:  Knowledge acquisition; I.2.8 [Problem Solving]: Planning; I.2.11 [Distributed Articial Intelligence]: Intelligent agents, Multi-Agent Systems, Web agents  Keywords  Information System, Agent Architecture, Multi-Agent Systems, Web Agents, Intelligent Agents, Planning.  1. 
Agents
269982
Audio-Visual Speaker Detection using Dynamic Bayesian Networks The development of human-computer interfaces poses a challenging problem: actions and intentions of different users have to be inferred from sequences of noisy and ambiguous sensory data. Temporal fusion of multiple sensors can be efficiently formulated using dynamic Bayesian networks (DBNs). DBN framework allows the power of statistical inference and learning to be combined with contextual knowledge of the problem. We demonstrate the use of DBNs in tackling the problem of audio/visual speaker detection. "Off-the-shelf" visual and audio sensors (face, skin, texture, mouth motion, and silence detectors) are optimally fused along with contextual information in a DBN architecture that infers instances when an individual is speaking. Results obtained in the setup of an actual human-machine interaction system (Genie Casino Kiosk) demonstrate superiority of our approach over that of static, context-free fusion architecture.  1. Introduction  Advanced human--computer interfaces increasingly r...
AI
cohen01learning
Learning to Match and Cluster Entity Names Introduction Information retrieval is, in large part, the study of methods for assessing the similarity of pairs of documents. Document similarity metrics have been used for many tasks including ad hoc document retrieval, text classification [YC1994], and summarization [GC1998,SSMB1997]. Another problem area in which similarity metrics are central is record linkage (e.g., [KA1985]), where one wishes to determine if two database records taken from different source databases refer to the same entity. For instance, one might wish to determine if two database records from two different hospitals, each containing a patient's name, address, and insurance information, refer to the same person; as another example, one might wish to determine if two bibliography records, each containing a paper title, list of authors, and journal name, refer to the same publication. In both of these examples (and in many other practical cases) most of the record fields
IR
parsons99approach
An approach to using degrees of belief in BDI agents : The past few years have seen a rise in the popularity of the use of mentalistic attitudes such as beliefs, desires and intentions to describe intelligent agents. Many of the models which formalise such attitudes do not admit degrees of belief, desire and intention. We see this as an understandable simplification, but as a simplification which means that the resulting systems cannot take account of much of the useful information which helps to guide human reasoning about the world. This paper starts to develop a more sophisticated system based upon an existing formal model of these mental attributes.  1 Introduction  In the past few years there has been a lot of attention given to building formal models of autonomous software agents; pieces of software which operate to some extent independently of human intervention and which therefore may be considered to have their own goals and the ability to determine how to achieve those goals. Many of these formal models are based on the use of ...
Agents
huget02model
Model Checking Agent UML Protocol Diagrams Agents in multiagent systems use protocols in order to exchange messages and to coordinate together. Since agents and objects are not exactly the same, designers do not use directly communication protocols used in distributed systems but a new type called interaction protocols encompassing agent features such as richer messages and the ability to cooperate and to coordinate. Obviously, designers consider formal description techniques used for communication protocols. New graphical modeling languages based on UML appeared several years ago. Agent UML is certainly the best known. Until now, no validation is given for Agent UML. The aim of this paper is to present how to model check Agent UML protocol diagrams.
Agents
389181
A Fast Multi-Dimensional Algorithm for Drawing Large Graphs We present a novel hierarchical force-directed method for drawing large graphs. The algorithm produces a graph embedding in an Euclidean space E of any dimension. A two or three dimensional drawing of the graph is then obtained by projecting a higher-dimensional embedding into a two or three dimensional subspace of E. Projecting high-dimensional drawings onto two or three dimensions often results in drawings that are "smoother" and more symmetric. Among the other notable features of our approach are the utilization of a maximal independent set filtration of the set of vertices of a graph, a fast energy function minimization strategy, e#cient memory management, and an intelligent initial placement of vertices. Our implementation of the algorithm can draw graphs with tens of thousands of vertices using a negligible amount of memory in less than one minute on a mid-range PC. 1 Introduction  Graphs are common in many applications, from data structures to networks, from software engineering...
AI
494121
Some Considerations about Embodied Agents   As computers are being more and more part of our world we feel the urgent need of proper user interface to interact with. The use of command lines typed on a keyboard are more and more obsolete, specially as computers are receiving so much attention from a large audience. The metaphor of face-to-face communication applied to human-computer interaction is finding a lot of attraction. Humans are used since they are born to communicate with others. Seeing faces, interpreting their expressions, understanding speech are all part of our development and growth. But face-to-face conversation is very complex as it involved a huge number of factors. We speak with our voice, but also with our hand, eye, face and body. Our gesture modifies, emphasizes, contradicts what we say by words. The production of speech and nonverbal behaviors work in parallel and not in antithesis. They seem to be two different forms (voice and body gestures) of the same process (speech). They add info
Agents
11099
ifile: An Application of Machine Learning to E-Mail Filtering The rise of the World Wide Web and the ever-increasing amounts of machine-readable text has caused text classification to become a important aspect of machine learning. One specific application that has the potential to affect almost every user of the Internet is e-mail filtering. The WorldTalk Corporation estimates that over 60 million business people use e-mail [6]. Many more use e-mail purely on a personal basis and the pool of e-mail users is growing daily. And yet, automated techniques for learning to filter e-mail have yet to significantly affect the e-mail market. Here, I attack problems that plague practical e-mail ltering and suggest solutions that will bring us closer to the acceptance of using automated classification techniques to filter personal e-mail. I also present a filtering system, ifile, that is both effective and efficient, and which has been adapted to a popular e-mail client. Results are presented from a number of experiments and show that a system such as ifile could become a...
DB
marmasse99commotion
comMotion: a context-aware communication system How many times have you gone to the grocery store but left the grocery list on the refrigerator door? Wouldn't it be more efficient to have a reminder to buy groceries and the shopping list delivered to you when you were in the vicinity of the store?  We live in a world in which the information overload is part of our daily life. Many of us receive large quantities of email or voice mail messages. Yet many of these messages are relevant only in a particular context. We can use a system of reminders to keep up with all we have to do, but these reminders are often relevant only to a specific location. If reminders, to-do lists, messages and other information were delivered in the most timely and relevant context, part of the overload would be reduced. This paper describes comMotion, a context-aware communication system for a mobile or wearable computing platform.  Keywords  Mobile, ubiquitous and wearable computing; locationaware, context-aware applications.  
HCI
346149
Rewriting Logic as a Metalogical Framework A metalogical framework is a logic with an associated methodology that is used to represent other logics and to reason about their metalogical properties. We propose that logical frameworks can be good metalogical frameworks when their logics support reflective reasoning and their theories always have initial models. We present a concrete realization of this idea in rewriting logic. Theories in rewriting logic always have initial models and this logic supports reflective reasoning. This implies that inductive reasoning is valid when proving properties about the initial models of theories in rewriting logic, and that we can use reflection to reason at the metalevel about these properties. In fact, we can uniformly reflect induction principles for proving metatheorems about rewriting logic theories and their parameterized extensions. We show that this reflective methodology provides an effective framework for di erent, non-trivial, kinds of formal metatheoretic reasoning; one can, for examp...
ML
talim01controlling
Controlling the Robots of Web Search Engines Robots are deployed by a Web search engine for collecting information from different Web servers in order to maintain the currency of its data base of Web pages. In this paper, we investigate the number of robots to be used by a search engine so as to maximize the currency of the data base without putting an unnecessary load on the network. We use a queueing model to represent the system. The arrivals to the queueing system are Web pages brought by the robots; service corresponds to the indexing of these pages. The objective is to find the number of robots, and thus the arrival rate of the queueing system, such that the indexing queue is neither starved nor saturated. For this, we consider a finite-buffer queueing system and define the cost function to be minimized as a weighted sum of the loss rate and the fraction of time that the system is empty. Both static and dynamic policies are considered. In the static setting the number of robots is held fixed; in the dynamic setting robots may be re...
IR
bertelsen99augmenting
Augmenting reality in mobile substrates - On the design of computer support for process control The paper investigates augmented reality as a perspective on the design of computer support for process control in a distributed environment. Based on empirical studies of work in a wastewater treatment plant, three technical approaches on augmented reality --- augmenting the user; the object of work; and the environment --- are examined in terms of a collection of design scenarios .We conclude that these approaches when used, as metaphors rather than a consistent theoretical framework, may inform design of mobile support for process control work.  Keywords: Augmented reality, process control, mobile computing, human-computer interaction, participatory design, workplace studies.  BRT Keywords: AB, FA, FC, GA, HD Introduction  Advanced technical (process) settings, such as modern wastewater treatment plants, are characterised by being highly distributed and dynamic. A possible strategy for supporting work in such settings is through the introduction of mobile technology. In this paper, ...
HCI
stanoi00reverse
Reverse Nearest Neighbor Queries for Dynamic Databases In this paper we propose an algorithm for answering reverse nearest neighbor (RNN) queries, a problem formulated only recently. This class of queries is strongly related to that of nearest neighbor (NN) queries, although the two are not necessarily complementary. Unlike nearest neighbor queries, RNN queries find the set of database points that have the query point as the nearest neighbor. There is no other proposal we are aware of, that provides an algorithmic approach to answer RNN queries. The earlier approach for RNN queries ([KM99]) is based on the pre-computation of neighborhood information that is organized in terms of auxiliary data structures. It can be argued that the precomputation of the RNN information for all points in the database can be too restrictive. In the case of dynamic databases, insert and update operations are expensive and can lead to modifications of large parts of the auxiliary data structures. Also, answers to RNN queries for a set of data points depend on t...
DB
norrie02webbased
Web-Based Integration of Printed and Digital Information The affordances of paper have ensured its retention as a key information medium, in spite of dramatic increases in the use of digital technologies for information storage, processing and delivery. Recent developments in paper, printing and wand technologies may lead to the widespread use of digitally augmented paper in the near future, thereby enabling the paper and digital worlds to be linked together. We are interested in using these technologies to achieve a true integration of printed and digital information sources such that users may browse freely back and forth between paper and digital resources. We present web-based server technologies that support this integration by allowing users to dynamically link areas of printed documents to objects of an application database. The server component is implemented using the eXtensible Information Management Architecture (XIMA) and is independent of the particular paper, printing and reader technologies used to realise the digitally augmented paper. The framework presented manages semantic information about application objects, documents, users, links and client devices and it supports universal client access.
IR
johns01how
How Emotions and Personality Effect the Utility of Alternative Decisions: A Terrorist Target Selection Case Study The role of emotion modeling in the development of computerized agents has long been unclear. This is  partially due to instability in the philosophical issues of the problem as psychologists struggle to build models for their  own purposes, and partially due to the often-wide gap between these theories and that which can be implemented by an  agent author. This paper describes an effort to use emotion models in part as a deep model of utility for use in decision  theoretic agents. This allows for the creation of simulated forces capable of balancing a great deal of competing goals,  and in doing so they behave, for better or for worse, in a more realistic manner.
Agents
kang01qrtdb
qRTDB: Qos-Sensitive Real-Time Database Introduction  Recently the demand for real-time database services is exploding. The applications requiring such services include sensor data fusion, decision support, web information service, e-commerce, online trading, and dataintensive smart space applications. Furthermore, the information system is being globalized due to the fast growth of the Internet. Despite the importance and wide applicability, the performance and predictability of a database system    \Gamma the core component of global information systems \Gamma are relatively limited compared to the other real-time systems such as real-time operating systems. It can not be easily replicated due to the consistency problem. In addition, the database system has relatively low predictability compared to other real-time systems due to data dependence of the transaction execution, data and resource conflicts, dynamic paging and I/O, and transaction aborts and the resulting rollbacks and restarts [36]. Because of the limited perfo
DB
391515
Learning Comprehensible Conceptual User Models for User Adaptive Meta Web Search In course of the OySTER project our goal is to induce conceptual user models that allow for a transparent query refinement and information filtering in the domain of Www meta--search. User models which describe a user's interest with respect to an underlying ontology allow for a manual user model editing process and also pose a well defined problem for a conceptual inductive learning task. OySTER is a research prototype that is currently being developed at the university of Osnabruck. Introduction User Modeling and Machine Learning. User models represent assumptions about a user. User modeling systems infer user models from user interaction, store user models and induce new assumptions by reasoning about the models. These models are used within the system in order to adapt to the user. Furthermore, these models shall be accessible to the user --- they should be both understandable and manually modifyable. Incorporating machine learning into this framework often leads to intertwine...
IR
picco99lime
LIME: Linda Meets Mobility Lime is a system designed to assist in the rapid development of dependable mobile applications over both wired and ad hoc networks. Mobile agents reside on mobile hosts and all communication takes place via transiently shared tuple spaces distributed across the mobile hosts. The decoupled style of computing characterizing the Linda model is extended to the mobile environment. At the application level, both agents and hosts perceive movement as a sudden change of context. The set of tuples accessible by a particular agent residing on a given host is altered transparently in response to changes in the connectivity pattern among the mobile hosts. In this paper we present the key design concepts behind the Lime system. 1 INTRODUCTION Today's users demand ubiquitous network access independent of their physical location. This style of computation, often referred to as mobile computing, is enabled by rapid advances in the wireless communication technology. The networking scenarios enabled ...
Agents
aroyo02layered
A Layered Approach towards Domain Authoring Support This paper presents an approach to authoring support for Web courseware based on a layered  ontological paradigm. The ontology-based layers in the courseware authoring architecture  serve as a basis for formal semantics and reasoning support in performing generic authoring  tasks. This approach represents an extension of our knowledge classification and indexing  mechanism from a previously developed system, AIMS, aimed at supporting students while  completing learning tasks in a Web-based learning/training environment. We propose the addition  of two vertical layers in the system architecture, Author assisting layer and Operational  layer, with the role of facilitating the creation of the ontological layers (Course ontology and  Domain ontology) and of the educational metadata layer. Here we focus on the domain ontology  creation process, together with the support that the additional layers can provide within  this process. We exemplify our method by presenting a set of generic tasks related to conceptbased  domain authoring and their ontological support
IR
kwok01scaling
Scaling Question Answering to the Web The wealth of information on the web makes it an attractive resource for seeking quick answers to simple, factual questions such as “who was the first American in space? ” or “what is the second tallest mountain in the world? ” Yet today’s most advanced web search services (e.g., Google and AskJeeves) make it surprisingly tedious to locate answers to such questions. In this paper, we extend question-answering techniques, first studied in the information retrieval literature, to the web and experimentally evaluate their performance. First we introduce MULDER, which we believe to be the first general-purpose, fully-automated question-answering system available on the web. Second, we describe MULDER’s architecture, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall. Finally, we compare MULDER’s performance to that of Google and AskJeeves on questions drawn from the TREC-8 question answering track. We find that MULDER’s recall is more than a factor of three higher than that of AskJeeves. In addition, we find that Google requires 6.6 times as much user effort to achieve the same level of recall as MULDER.
IR
kim00heterogeneous
Heterogeneous Multimedia Database Selection on the Web Multimedia databases on the Web have two distinct characteristics, autonomy and heterogeneity. They are established and maintained independently and queries are processed depending on their own schemes. In this paper, we investigate the problem of the database selection from a number of multimedia databases dispersed on the Web. In the multimedia objects retrieval from distributed sites, it is crucial that the metaserver has the capability to find objects, globally similar to a given query object, from different multimedia databases with different local similarity measures. We propose a novel selection algorithm to determine candidate databases that contain more objects relevant to the query than other databases. The selection of databases is based on the number of relevant objects at each local database that is estimated by using a few sample objects and the histogram information. An extensive experiment on a large number of real data sets demonstrates that our proposed method perform...
IR
abonyi02datadriven
Data-Driven Generation of Compact, Accurate, and Linguistically-Sound Fuzzy Classifiers Based on a Decision-Tree Initialization The data-driven identification of fuzzy rule-based classifiers for high-dimensional problems is addressed. A binary decision-tree-based initialization of fuzzy classifiers is proposed for the selection of the relevant features and e#ective initial partitioning of the input domains of the fuzzy system. Fuzzy classifiers have more flexible decision boundaries than decision trees (DTs) and can therefore be more parsimonious. Hence, the decision tree initialized fuzzy classifier is reduced in an iterative scheme by means of similarity-driven rule-reduction. To improve classification performance of the reduced fuzzy system, a genetic algorithm with a multi-objective criterion searching for both redundancy and accuracy is applied. The proposed approach is studied for (i) an artificial problem, (ii) the Wisconsin Breast Cancer classification Preprint submitted to Elsevier Science 19 May 2002 problem, and (iii) a summary of results is given for a set of well-known classification problems available from the Internet: Iris, Ionospehere, Glass, Pima, and Wine data.
AI
karadkar00evolution
Evolution of the Walden's Paths Authoring Tools : Changing user skills, available infrastructure, and work practices have caused many  differences in the authoring support provided by the Walden's Paths project since its  conception. In this paper we trace these changes and the transition from the earlier authoring  tools that supported an integrated authoring process, to the more recent tools designed to work  with the Web applications that teachers have become accustomed to.  1. Introduction  Hypertext has come a long way from being found only in research systems to being a part of our everyday lives in the form of the World-Wide Web (WWW or the Web). We use the Web for browsing academic information, for furthering business interests, for entertainment and a variety of other purposes. There is an immense amount of information on the Web that can be used for a variety of reasons. Web-based information can be harnessed to supplement classroom teaching for K-12 students. K-12 teachers can use Web-based information in the curriculum t...
IR
flank98layered
A Layered Approach To Nlp-Based Information Retrieval A layered approach to information retrieval permits the inclusion of multiple search engines as well as multiple databases, with a natural language layer to convert English queries for use by the various search engines. The NLP layer incorporates morphological analysis, noun phrase syntax, and semantic expansion based on WordNet.  1 Introduction  This paper describes a layered approach to information retrieval, and the natural language component that is a major element in that approach. The layered approach, packaged as Intermezzo  TM  , was deployed in a pre-product form at a government site. The NLP component has been installed, with a proprietary IR engine, PhotoFile, (Flank, Martin, Balogh and Rothey, 1995), (Flank, Garfield, and Norkin, 1995), at several commercial sites, including Picture Network International (PNI), Simon and Schuster, and John Deere. Intermezzo employs an abstraction layer to permit simultaneous querying of multiple databases. A user enters a query into a clien...
IR
yu99finding
Finding the Most Similar Documents across Multiple Text Databases In this paper, we present a methodology for finding the n most similar documents across multiple text databases for any given query and for any positive integer n. This methodology consists of two steps. First, databases are ranked in a certain order. Next, documents are retrieved from the databases according to the order and in a particular way. If the databases containing the n most similar documents for a given query can be ranked ahead of other databases, the methodology will guarantee the retrieval of the n most similar documents for the query. A statistical method is provided to identify databases, each of which is estimated to contain at least one of the n most similar documents. Then, a number of strategies is presented to retrieve documents from the identified databases. Experimental results are given to illustrate the relative performance of different strategies. 1 Introduction The Internet has become a vast information source in recent years and can be considered as the w...
IR
japan98learning
Learning to Perceive the World as Articulated: An Approach for Hierarchical Learning in Sensory-Motor Systems This paper describes how agents can learn an internal model of the world  structurally by focusing on the problem of behavior-based articulation. We develop  an on-line learning scheme -- the so-called mixture of recurrent neural  net (RNN) experts -- in which a set of RNN modules becomes self-organized  as experts on multiple levels in order to account for the different categories  of sensory-motor flow which the robot experiences. Autonomous switching of  activated modules in the lower level actually represents the articulation of the  sensory-motor flow. In the meanwhile, a set of RNNs in the higher level competes  to learn the sequences of module switching in the lower level, by which articulation  at a further more abstract level can be achieved. The proposed scheme  was examined through simulation experiments involving the navigation learning  problem. Our dynamical systems analysis clarified the mechanism of the articulation;  the possible correspondence between the articulation...
AI
marques01integrating
Integrating Mobile Agents into Off-the-Shelf Web Servers: The M&M Approach The mobile agent paradigm provides a new approach for developing distributed systems. During the last two years, we have been working on a project that tries to overcome some of the limitations found in terms of programmability and usability of the mobile agent paradigm in real applications. In the M&M framework there are no agent platforms. Instead applications become agent-enabled by using simple JavaBeans components. In our approach the agents arrive and departure directly from the applications, interacting with them from the inside.
Agents
435716
Flexible Queries to Semi-structured Datasources: the WG-log Approach A line of research is presented aimed at specifying both logical and navigational aspects of semi-structured data sources such as Web sites through the unifying notion of schema. Gracefully supporting schemata that are huge or subject to change, the WG-Log language allows for a uniform representation of queries and views, the latter expressing customized access structures to site information. A survey of related work and some directions for future research involving fuzzy query techniques are also outlined. 1 Introduction and Motivations Modern network-oriented information systems often have to deal with data that are semi-structured, i.e. lack the strict, regular, and complete structure required by traditional database management systems (see [Abi97] and [Suc97] for a survey on semi-structured data and related research). Information is semi-structured also when the structure of data varies w.r.t. time, rather than w.r.t. space: even if data is fairly well structured, such struc...
IR
garland00learning
Learning Task Models for Collagen For an application-independent collaborative tool, a  key step is to develop a detailed task model for a particular  domain. This is a time consuming and dicult  task, and seems to require a fairly advanced knowledge  of AI representations for plans, goals, and recipes.  This paper discusses some preliminary ideas for making  it easier to construct and evolve task models, either  through interaction with a human domain expert,  through machine learning, or in a mixed-initiative system.  Introduction  An important trend in recent work on human-computer interaction and user modeling has been to view humancomputer interaction as a kind of collaboration. In this approach, the human user and the computer, often personied as an \agent," coordinate their actions toward achieving shared goals. Collagen is an application-independent collaboration manager based on the SharedPlan theory of task-oriented collaborative discourse (Grosz & Sidner, 1990; Rich & Sidner, 1998; Lesh, Rich, & Sidner, 199...
HCI
krishnamurthy01dynamic
A Dynamic Replica Selection Algorithm for Tolerating Timing Faults Server replication is commonly used to improve the fault tolerance and response time of distributed services. An important problem when executing time-critical applications in a replicated environment is that of preventing timing failures by dynamically selecting the replicas that can satisfy a client's timing requirement, even when the quality of service is degraded due to replica failures and excess load on the server. In this paper, we describe the approach we have used to solve this problem in AQuA, a CORBA-based middleware that transparently replicates objects across a local area network. The approach we use estimates a replica's response time distribution based on performance measurements regularly broadcast by the replica. An online model uses these measurements to predict the probability with which a replica can prevent a timing failure for a client. A selection algorithm then uses this prediction to choose a subset of replicas that can together meet the client's timing constraints with at least the probability requested by the client. We conclude with experimental results based on our implementation.
IR
162738
Using Knowledge Containers to Model a Framework for Learning Adaptation Knowledge In this paper we present a framework for learning adaptation knowledge which knowledge light approaches for case-based reasoning (CBR) systems. Knowledge light means that these approaches use already acquired knowledge inside the CBR system. Therefor we describe the sources of knowledge inside a CBR system along the different knowledge containers. After that we present our framework in terms of these knowledge containers. Further we apply our framework in a case study to one knowledge light approach for learning adaptation knowledge. After that we point on some issues which should be addressed during the design or the use of such algorithms for learning adaptation knowledge. From our point of view many of these issues should be the topic of further research. Finally we close with a short discussion and an outlook to further work.
ML
haritsa00realtime
Real-Time Index Concurrency Control Real-time database systems are expected to rely heavily on indexes to speed up data access and thereby help more transactions meet their deadlines. Accordingly, highperformance index concurrency control (ICC) protocols are required to prevent contention for the index from becoming a bottleneck. In this paper, we develop real-time variants of a representative set of classical B-tree ICC protocols and, using a detailed simulation model, compare their performance for real-time transactions with firm deadlines. We also present and evaluate a new real-time ICC protocol called GUARD-link that augments the classical B-link protocol with a feedback-based admission control mechanism. Both point and range queries, as well as the undos of the index actions of aborted transactions are included in the scope of our study. The performance metrics used in evaluating the ICC protocols are the percentage of transactions that miss their deadlines and the fairness with respect to transaction type and size...
DB
dwyer99flow
Flow Analysis for Verifying Specifications of Concurrent and Distributed Software This paper presents FLAVERS, a finite state verification approach that analyzes whether concurrent or sequential programs satisfy user-defined correctness properties. In contrast to other finite-state verification techniques, FLAVERS is based on algorithms with low-order polynomial bounds on the running time. FLAVERS achieves this efficiency at the cost of precision. Users, however, can improve the precision of the results by selectively and judiciously incorporating additional semantic information into the analysis problem. The FLAVERS analysis approach has been implemented for programs written in Ada. We report on an empirical study of the performance of applying the FLAVERS/Ada tool set to a collection of multi-tasking Ada programs. This study indicates that sufficient precision for proving program properties can be achieved and that the cost for such analysis grows as a low-order polynomial in the size of the program. 1 Introduction  The application of distributed and concurrent pr...
DB
mcbreen01evaluating
Evaluating Humanoid Synthetic Agents in E-Retail Applications This paper presents three experiments designed to empirically evaluate humanoid synthetic agents in electronic retail applications. Firstly, human-like agents were evaluated in a single e-retail application, a home furnishings service. The second experiment explored application dependency effects by evaluating the same human-like agents in a different e-retail application, a personalized CD service. The third experiment evaluated the effectiveness of a range of humanoid cartoon-like agents. Participants eavesdropped on spoken dialogues between a "customer" and each of the agents, which played the role of conversational sales assistants. Results showed participants expected a high level of realistic human-like verbal and nonverbal communicative behavior from the human-like agents. Overall ratings of the agents showed no significant application dependency: Two different groups of participants rated the human-like agents in similar ways in a different application. Further results showed participants have a preference for three--dimensional (3-D) rather than two--dimensional (2-D) cartoon-like agents and have a desire to interact with fully embodied agents.  I. 
Agents
zhang99towards
Towards Transparent Control of Large and Complex Systems System Identi cation. Unlike with Markovian Decision Processes, some systems' output depends not only on the current state, but also the previous input/output. As a training data set for nonlinear system identi cation, the Box-Jenkins gas furnace data [BJ70] is often studied and compared. The furnace input is the gas ow rate x(t), the output y(t) is the CO 2 concentration. At least 10 candidate inputs are considered: x(t 6); x(t 5); : : : ; x(t 1); y(t 1); : : : ; y(t 4). If all of them are used, building a fuzzy controller means to solve a 10-input-1-output problem. If each input is de ned by 5 linguistic terms, this would result in a fuzzy rule system of about 10 million rules. The modelling and prediction o
IR
458790
Audio Driven Facial Animation For Audio-Visual Reality In this paper, we demonstrate a morphing based automated audio driven facial animation system. Based on an incoming audio stream, a face image is animated with full lip synchronization and expression. An animation sequence using optical flow between visemes is constructed, given an incoming audio stream and still pictures of a face speaking different visemes. Rules are formulated based on coarticulation and the duration of a viseme to control the continuity in terms of shape and extent of lip opening. In addition to this new viseme-expression combinations are synthesized to be able to generate animations with new facial expressions. Finally various applications of this system are discussed in the context of creating audio-visual reality.  1. 
HCI
dubois00augmented
Augmented Reality: Which Augmentation for Which Reality? In this paper, we first present a brief review of approaches used for studying and designing Augmented Reality (AR) systems. The variety of approaches and definitions in AR requires classification. We define two intrinsic characteristics of AR systems, task focus and nature of augmentation. Based on these two characteristics, we identify four classes of AR systems. In addition our OP-a-S notation provides a complementary characterization method based on interaction. Using OP-a-S, an AR system is modeled as a set of components that communicate with each other. One crucial type of OP-a-S component is the Adapter that establishes a bridge between the real world and the virtual world. By defining a classification scheme, we aim at providing a better understanding of the paradigm of AR and at laying the foundations of future design principles according to the class of systems. Keywords Classification. Interaction characterization. INTRODUCTION One of the recent design goals in Human Com...
HCI
284772
Meeting Plan Recognition Requirements for Real-Time Air-Mission Simulations In this paper, the potential synergy between instancebased pattern recognition and means-end (possible world) reasoning is explored, for supporting plan recognition in multi-aeroplane air-mission simulations. A combination of graph matching, induction, probabilistic principles and dynamic programming are applied to traces of aeroplane behaviour during flight manoeuvres. These satisfy the real-time constraints of the simulation. This enables the agents to recognise what other agents are doing and to abstract about their activity, at the instrumentation level. A means-end-reasoning model is then used to deliberate about and invoke standard operating procedures, based on recognised activity. The reasoning model constrains the recognition process by framing queries according to what a pilot would expect during the execution of the current plan(s). Results from experiments involving the dMARS procedural reasoning system and the CLARET pattern matching and induction system are described for ...
Agents
455346
Accelerating Reinforcement Learning through the Discovery of Useful Subgoals An ability to adjust to changing environments and unforeseen circumstances is likely to be an important component of a successful autonomous space robot. This paper shows how to augment reinforcement learning algorithms with a method for automatically discovering certain types of subgoals online. By creating useful new subgoals while learning, the agent is able to accelerate learning on a current task and to transfer its expertise to related tasks through the reuse of its ability to attain subgoals. Subgoals are created based on commonalities across multiple paths to a solution. We cast the task of finding these commonalities as a multiple-instance learning problem and use the concept of diverse density to find solutions. We introduced this approach in [10] and here we present additional results for a simulated mobile robot task.  1 
ML
park98alternative
Alternative Correctness Criteria for Multiversion Concurrency Control and a Locking Protocol via Freezing Concurrency control protocols based on multiversions have been used in some commercial transaction processing systems in order to provide the serializable executions of transactions. In the existing protocols, transactions are allowed to read only the most recent version of each data item in order to ensure the correct execution of transactions. However, this feature is not desirable in some advanced database systems which have more requirements such as timing or security constraints besides serializability. In this paper, we propose a new correctness criteria, called F-serializability, for multiversion concurrency control protocols. It is the extended definition of `1-serial' and relaxes the condition so that a protocol provides not only the most recent version but also the previous one to transactions, if necessary. We prove that whenever a multiversion schedule is F-serializable, the schedule is also one-copy serializable. This is the first contribution of our paper. Next, we propos...
DB
seymore99learning
Learning Hidden Markov Model Structure for Information Extraction Statistical machine learning techniques, while well proven in fields such as speech recognition, are just beginning to be applied to the information extraction domain. We explore the use of hidden Markov models for information extraction tasks, specifically focusing on how to learn model structure from data and how to make the best use of labeled and unlabeled data. We show that a manually-constructed model that contains multiple states per extraction field outperforms a model with one state per field, and discuss strategies for learning the model structure automatically from data. We also demonstrate that the use of distantly-labeled data to set model parameters provides a significant improvement in extraction accuracy. Our models are applied to the task of extracting important fields from the headers of computer science research papers, and achieve an extraction accuracy of 92.9%.  Introduction  Hidden Markov modeling is a powerful statistical machine learning technique that is just ...
ML
201909
Agent Aided Aircraft Maintenance Aircraft maintenance is performed bymechanics who are required,  for non-standard discrepancies, to consult expert engineers for repair  instructions and approval. In addition to their own experience, these  engineers rely on external information sources, which are often inadequately  indexed and geographically dispersed. The timely retrieval  of this distributed information is essencial to the engineers' abilityto  devise and recommend repair procedures in response to the mechanics  ' requests. This problem domain is well suited for a multi-agent  system: it consists of distributed multi-modal information whichis  needed bymultiple users with diverse preferences. In this paper, we  describe an implementation of such a system, using the RETSINA  multi-agentarchitecture. Such an implementation reinforces the importance  of multi-agent systems, and in particular the usefulness of  the RETSINA infrastructure as a basis for the construction of such  systems.  i  1 Introduction  Agent aided...
Agents
ghani00data
Data Mining on Symbolic Knowledge Extracted from the Web Information extractors and classifiers operating on unrestricted, unstructured texts are an errorful source of large amounts of potentially useful information, especially when combined with a crawler which automatically augments the knowledge base from the world-wide web. At the same time, there is much structured information on the World Wide Web. Wrapping the web-sites which provide this kind of information provide us with a second source of information; possibly less up-to-date, but reliable as facts. We give a case study of combining information from these two kinds of sources in the context of learning facts about companies. We provide results of association rules, propositional and relational learning, which demonstrate that data-mining can help us improve our extractors, and that using information from two kinds of sources improves the reliability of data-mined rules. 1. INTRODUCTION The World Wide Web has become a significant source of information. Most of this computer-retri...
IR
sloman99building
Building Cognitively Rich Agents Using The Sim Agent Toolkit Synthetic agents with varying degrees of intelligence and autonomy are being designed in many research laboratories. The motivations include military training simulations, games and entertainments, educational software, digital personal assistants, software agents managing Internet transactions or purely scientific curiosity. Different approaches are being explored, including, at one extreme, research on the interactions between agents, and at the other extreme research on processes within agents. The first approach focuses on forms of communication, requirements for consistent collaboration, planning of coordinated behaviours to achieve collaborative goals, extensions to logics of action and belief for multiple agents, and types of emergent phenomena when many agents interact, for instance taking routing decisions on a telecommunications network. The second approach focuses on the internal architecture of individual agents required for social interaction, collaborative behaviours, complex decision making, learning, and emergent phenomena within complex agents. Agents with complex internal structure may, for example, combine perception, motive generation, planning, plan execution, execution monitoring, and even emotional reactions.
Agents
51036
Stratum Approaches to Temporal DBMS Implementation database interfaces, legacy systems. Previous approaches to implementing temporal DBMSs have assumed that a temporal DBMS must be built from scratch, employing an integrated architecture and using new temporal implementation techniques such as temporal indexes and join algorithms. However, this is a very large and time-consuming task. This paper explores approaches to implementing a temporal DBMS as a stratum on top of an existing non-temporal DBMS, rendering implementation more feasible by reusing much of the functionality of the underlying conventional DBMS. More specifically, the paper introduces three stratum meta-architectures, each with several specific architectures. Based on a new set of evaluation criteria, advantages and disadvantages of the specific architectures are identified. The paper also classifies all existing temporal DBMS implementations according to the specific architectures they employ. It is concluded that a stratum architecture is the best short, medium, and perhaps even longterm, approach to implementing a temporal DBMS. 1
DB
menczer01evaluating
Evaluating Topic-Driven Web Crawlers Due to limited bandwidth, storage, and computational resources, and to the dynamic nature of the Web, search engines cannot index every Web page, and even the covered portion of the Web cannot be monitored continuously for changes. Therefore it is essential to develop effective crawling strategies to prioritize the pages to be indexed. The issue is even more important for topic-specific search engines, where crawlers must make additional decisions based on the relevance of visited pages. However, it is difficult to evaluate alternative crawling strategies because relevant sets are unknown and the search space is changing. We propose three different methods to evaluate crawling strategies. We apply the proposed metrics to compare three topic-driven crawling algorithms based on similarity ranking, link analysis, and adaptive agents.
IR
ahamad99scalable
Scalable Consistency Protocols for Distributed Services AbstractÐA common way to address scalability requirements of distributed services is to employ server replication and client caching of objects that encapsulate the service state. The performance of such a system could depend very much on the protocol implemented by the system to maintain consistency among object copies. We explore scalable consistency protocols that never require synchronization and communication between all nodes that have copies of related objects. We achieve this by developing a novel approach called local consistency (LC). LC based protocols can provide increased flexibility and efficiency by allowing nodes control over how and when they become aware of updates to cached objects. We develop two protocols for implementing strong consistency using this approach and demonstrate that they scale better than a traditional invalidation based consistency protocol along the system load and geographic distribution dimensions of scale. Index TermsÐScalable services, distributed objects, replication, caching, consistency protocols. 1
DB
35804
Query Optimization for Selections using Bitmaps Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous  and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.  1 Introduction  Bitmap indexing has become a promising technique for query processing in DWs. Variations of bitmap indexes include bit-sliced indexes [14, 3], encoded bitmap indexes (EBI) [18], bitmapped join indexes [13], range-based bitmap indexes [20], and others[16]. For query operations, such as selections, aggregates, and joins, query evaluation algorithms using bitmaps have been proposed in ...
DB
graef00construction
Construction of Adaptive Web-Applications from Reusable Components . The Web has become a ubiquitous environment for application
HCI
ghani00using
Using Error-Correcting Codes For Text Classification This paper explores in detail the use of Error  Correcting Output Coding (ECOC) for learning  text classifiers. We show that the accuracy of a  Naive Bayes Classifier over text classification  tasks can be significantly improved by taking  advantage of the error-correcting properties of  the code. We also explore the use of different  kinds of codes, namely Error-Correcting Codes,  Random Codes, and Domain and Data-specific  codes and give experimental results for each of  them. The ECOC method scales well to large  data sets with a large number of classes.  Experiments on a real-world data set show a  reduction in classification error by up to 66%  over the traditional Naive Bayes Classifier. We  also compare our empirical results to semitheoretical  results and find that the two closely  agree.  1. Introduction  Text Classification is the problem of grouping text documents into classes or categories. For the purpose of this paper, we define classification as categorizing documents in...
IR
puliafito00using
Using Mobile Agents to implement flexible Network Management strategies Due to their intrinsic complexity, computer and communication systems require increasingly  more sophisticated management starategies to be adopted in order to guarantee adequate levels  of performance and reliability. The centralized paradigm adopted by the SNMP is appropriate  in several network management applications, but the quick expansion of networks has posed the  problem of its scalability, as well as for any other centralized model. Mobile agents represent a challenging  approach to provide advanced network management functionalities, due to the possibility  to easily implement a decentralized and active monitoring of the system. In this paper we discuss  how to take advantage of this technology and identify some reference scenario where mobile agents  represent a very promising approach. We also describe a prototype implementation based on our  mobile agent platform called MAP and show how it is possible to take advantages from using the  features it provides.  Keywords: net...
Agents
hattori99socialware
Socialware: Multiagent Systems for Supporting Network Communities ing with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org.  Community B Community A Community C  Community agent A1 Community agent B1 Community agent B2 Personal unit 1 Personal unit 2 Personal unit 5 Personal unit 6 Personal unit 3 Personal unit 7 Personal unit 4 User 5 to join User 4 to leave Figure 1: A general architecture of socialware as a multiagent system Socialware as Multiagent Systems   There are several characteristics specific to network communities, which make a multiagent architecture attractive to use. The first characteristic is that the participants of a network community are widely distributed and the number of potential participants is large. Hence, no solid, centralized, or monolithic system would be adequate: A distributed system would be required, in which perso...
Agents
dragoni00updating
Updating Mental States from Communication . In order to perform effective communication agents must be able to foresee  the effects of their utterances on the addressee's mental state. In this paper we investigate  on the update of the mental state of an hearer agent as a consequence of the utterance  performed by a speaker agent. Given an agent communication language with a STRIPSlike  semantics, we propose a set of criteria that allow to bind the speaker's mental state  to its uttering of a certain sentence. On the basis of these criteria, we give an abductive  procedure that the hearer can adopt to partially recognize the speaker's mental state that  led to a specific utterance. This procedure can be adopted by the hearer to update its own  mental state and its image of the speaker's mental state.  1 Introduction  In multi-agent systems, communication is necessary for the agents to cooperate and coordinate their activities or simply to avoid interfering with one another. If agents are not designed with embedded pre-compiled...
Agents
cruz99preserving
Preserving Contextual Navigation in Hypermedia Querying Navigating query results is a highly volatile task, usually requiring much effort from the users, while not providing a firm reference point for further queries or refinement. Many users accessing traditional search engines are confronted with lengthy pages of hypertext links, through which relevant information must be found. Accessing each link can bring a user closer or farther from the information they seek; in either case, the context between the query results and the current web page can be lost. Delaunay  MM  addresses these issues by creating virtual documents through which all query results are displayed in a meaningful and organized fashion; and that reference the originating web page, thus providing semantic browsing and hypermedia navigation without loss of context.  Keywords: Digital Library Querying, Hypermedia Querying, Context Preservation  Introduction  With an increase in the number of users daily, the World Wide Web has become one of the most indispensable technologie...
IR
124833
Supporting Dialogue Inferencing in Conversational Case-Based Reasoning . Dialogue inferencing is the knowledge-intensive process of inferring aspects of a user's problem from its partial description. Conversational case-based reasoning (CCBR) systems, which interactively and incrementally elicit a user's problem description, suffer from poor retrieval efficiency (i.e., they prompt the user with questions that the user has already implicitly answered) unless they perform dialogue inferencing. The standard method for dialogue inferencing in CCBR systems requires library designers to supply explicit inferencing rules. This approach is problematic (e.g., maintenance is difficult). We introduce an alternative approach in which the CCBR system guides the library designer in building a domain model. This model and the partial problem description are then given to a query retrieval system (PARKA-DB) to infer any implied answers during a conversation. In an initial empirical evaluation in the NaCoDAE CCBR tool, our approach improved retrieval efficiency without sa...
AI
reitmayr01wearable
A Wearable 3D Augmented Reality Workspace This poster describes our work to build a wearable Augmented Reality system that supports true stereoscopic 3D graphics. Through a pen and pad interface well known 2D user interfaces can be presented to the user, whereas the tracking of the pen allows to use direct interaction with virtual objects. The system is assembled from off-the-shelf hardware components and serves as a basic test bed for user interface experiments related to collaboration between stationary and mobile AR users.  1. 
HCI
4579
An Architecture and Object Model for Distributed Object-Oriented Real-Time Databases The confluence of computers, communications, and databases is quickly creating a distributed database where many applications require real-time access to both temporally accurate and multimedia data. This is particularly true in military and intelligence applications, but these required features are needed in many commercial applications as well. We are developing a distributed database, called BeeHive, which could offer features along different types of requirements: real-time, fault-tolerance, security, and quality-of service for audio and video. Support of these features and potential trade-offs between them could provide a significant improvement in performance and functionality over current distributed database and object management systems. In this paper, we present a high level design for BeeHive architecture and sketch the design of the BeeHive Object Model (BOM) which extends object-oriented data models by incorporating time and other features into objects, resulting in a high...
DB
luck01conceptual
A Conceptual Framework for Agent Definition and Development The use of agents of many different kinds in a variety of fields of computer science and  artificial intelligence is increasing rapidly and is due, in part, to their wide applicability. The  richness of the agent metaphor that leads to many different uses of the term is, however, both a  strength and a weakness: its strength lies in the fact that it can be applied in very many different  ways in many situations for different purposes; the weakness is that the term agent is now used so  frequently that there is no commonly accepted notion of what it is that constitutes an agent. This  paper addresses this issue by applying formal methods to provide a defining framework for agent  systems. The Z specification language is used to provide an accessible and unified formal account  of agent systems, allowing us to escape from the terminological chaos that surrounds agents. In  particular, the framework precisely and unambiguously provides meanings for common concepts  and terms, enables alternative models of particular classes of system to be described within it, and  provides a foundation for subsequent development of increasingly more refined concepts.   
Agents
288424
Principles for a Usability-Oriented Pattern Language High-level usability principles (e.g. "Familiarity") are difficult to apply to specific projects, and style guides providing more detailed instructions are often misinterpreted and inaccessible. An approach to usability based on design patterns enables designers to learn how certain recurring problems can be solved according to high-level principles. This paper summarises a review of the desirable properties advocated by five popular style guides, and discusses how this list has been modified to provide an underlying philosophy which is appropriate for a usability-oriented pattern language. A sample pattern which exemplifies this philosophy, involving iteration through selectable objects, is described. KEYWORDS Usability engineering, Design techniques, Style guides.  1. Introduction  There has been considerable discussion about how to reconcile the gaps between software engineering (SE) and human-computer interaction (HCI). One of the primary ways to smoothly integrate the disciplines ...
HCI
inoue01controlling
Controlling Speculative Computation in Multi-Agent Environments In this paper, we propose a multi-agent system which performs speculative computation under incomplete communication environments. In a master-slave style multi-agent system with speculative computation, a master agent asks queries to slave agents in problem solving, and proceeds computation with default answers when answers from slave agents are delayed. We rst provide a semantics for speculative computation using default logic. Then, in the proposed system, we use the consequence- nding procedure SOL written in the Java language to perform data-driven deductive reasoning. The use of a consequence- nding procedure is convenient for updating agents' beliefs according to situation changes in the world. In our system, slave agents can change their answers frequently, yet a master agent can avoid duplicate computation. As long as actual answers from slave agents do not con- ict with any previously encountered situation, the obtained conclusions are never recomputed. We applied the proposed system to the meeting-room reservation problem to see the usefulness of the framework. 1
Agents
845
Learning to Catch: Applying Nearest Neighbor Algorithms to Dynamic Control Tasks Steven L. Salzberg  1  and David W. Aha  2 1 Introduction  Dynamic control problems are the subject of much research in machine learning (e.g., Selfridge, Sutton, & Barto, 1985; Sammut, 1990; Sutton, 1990). Some of these studies investigated the applicability of various k-nearest neighbor methods (Dasarathy, 1990) to solve these tasks by modifying control strategies based on previously gained experience (e.g., Connell & Utgoff, 1987; Atkeson, 1989; Moore, 1990; 1991). However, these previous studies did not highlight the fact that small changes in the design of these algorithms drastically alter their learning behavior. This paper describes a preliminary study that investigates this issue in the context of a difficult dynamic control task: learning to catch a ball moving in a three-dimensional space, an important problem in robotics research (Geng et al., 1991). Our thesis in this paper is that agents can improve substantially at physical tasks by storing experiences without explicitly...
ML
332262
Ensembles of Classifiers Based on Approximate Reducts . A problem of improving rough set based expert systems by  modifying a notion of reduct is discussed. A notion of approximate reduct  is introduced, as well as some proposals of quality measure for such a  reduct. A complete classifying system based on approximate reducts is  presented and discussed. It is proved that a problem of nding optimal set  of classifying agents based on approximate reducts is NP-hard; a genetic  algorithm is used to nd the suboptimal set. Experimental results show,  that the classifying system is eective and relatively fast.  1 Introduction  Rough set expert systems base on the notion of reduct [11], [12], a minimal subset of attributes which is sucient to discern between objects with dierent decision values. A set of short reducts can be used to generate rules [2]. A problem of short reducts generation is NP-hard, but an approximate algorithm (like the genetic one described in [13], [8] and implemented successfully { see [10]) can be used to obtain redu...
AI
baek01cost
Cost Effective Mobile Agent Planning for Distributed Information Retrieval The number of agents' and the execution time are two significant performance factors' in mobile agent planning. Fewer agents' cause lower network traffic and consume less bandwidth. Regardless of the number of agents' used, the execution time for a task must be kept minimal, which means that use of the minimal number of agents' must not impact on the execution time unfavorably. As the population of the mobile agent application domain grows', the importance of these two factors' also increases.
Agents
nottelmann01learning
Learning probabilistic Datalog rules for information classification and transformation Probabilistic Datalog is a combination of classical Datalog (i.e., function-free Horn clause predicate logic) with probability theory. Therefore, probabilistic weights may be attached to both facts and rules. But it is often impossible to assign exact rule weights or even to construct the rules themselves. Instead of specifying them manually, learning algorithms can be used to learn both rules and weights. In practice, these algorithms are very slow because they need a large example set and have to test a high number of rules. We apply a number of extensions to these algorithms in order to improve efficiency. Several applications demonstrate the power of learning probabilistic Datalog rules, showing that learning rules is suitable for low dimensional problems (e.g., schema mapping) but inappropriate for higher dimensions like e.g. in text classification.
IR
hazelhurst98binary
Binary Decision Diagram Representations Of Firewall And Router Access Lists Network firewalls and routers can use a rule database to decide which packets will be allowed from one network onto another. By filtering packets the firewalls and routers can improve security and performance -- by excluding packets which may pose a security risk to a network or are not relevant to it. However, as the size of the rule list increases, it becomes difficult to maintain and validate the rules, and the cost of rule lookup may add significantly to latency. Ordered binary decision diagrams (BDDs) -- a compact method of representing and manipulating boolean expressions -- are a potential method of representing the rules. This paper explores how BDDs can be used to develop methods that help analysis of rules to validate them and changes to them, to improve performance, and facilitate hardware support.  1 Introduction  The growth of network and internet communication creates several challenges for network design. Two important issues are security and performance. As the volume o...
DB
274436
Collision Avoidance and Resolution Multiple Access for Multichannel Wireless Networks The CARMA-NTG protocol is presented and analyzed. CARMA-NTG dynamically divides the channel into cycles of variable length; each cycle consists of a contention period and a group-transmission period. During the contention period, a station with one or more packets to send competes for the right to be added to the group of stations allowed to transmit data without collisions; this is done using a collision resolution splitting algorithm based on a request-to-send/clear-to-send (RTS/CTS) message exchange with non-persistent carrier sensing. CARMA-NTG ensures that one station is added to the group transmission period if one or more stations send requests to be added in the previous contention period. The group-transmission period is a variable-length train of packets, which are transmitted by stations that have been added to the group by successfully completing an RTS/CTS message exchange in previous contention periods. As long as a station maintains its position in the group, it is able to transmit data packets without collision. An upper bound is derived for the average costs of obtaining the first success in the splitting algorithm. This bound is then applied to the computation of the average channel utilization in a fully connected network with a large number of stations. These results indicate that collision resolution is a powerful mechanism in combination with floor acquisition and group allocation multiple access. 1
HCI
chang98data
Data Resource Selection in Distributed Visual Information Systems With the advances in multimedia databases and the popularization of the Internet, it is now possible to access large image and video repositories distributed throughout the world. One of the challenging problems in such an access is how the information in the respective databases can be summarized to enable an intelligent selection of relevant database sites based on visual queries. This paper presents an approach to solve this problem based on image content-based indexing of a metadatabase at a query distribution server. The metadatabase records a summary of the visual content of the images in each database through image templates and statistical features characterizing the similarity distributions of the images. The selection of the databases is done by searching the metadatabase using a ranking algorithm that uses query similarity to a template and the features of the databases associated with the template. Two selection approaches, termed mean-based and histogram-based approaches, ...
IR
300852
Centroid-Based Document Classification: Analysis & Experimental Results . In this paper we present a simple linear-time centroid-based document  classification algorithm, that despite its simplicity and robust performance,  has not been extensively studied and analyzed. Our experiments show that this  centroid-based classifier consistently and substantially outperforms other algorithms  such as Naive Bayesian, k-nearest-neighbors, and C4.5, on a wide range  of datasets. Our analysis shows that the similarity measure used by the centroidbased  scheme allows it to classify a new document based on how closely its behavior  matches the behavior of the documents belonging to different classes. This  matching allows it to dynamically adjust for classes with different densities and  accounts for dependencies between the terms in the different classes.  1 Introduction  We have seen a tremendous growth in the volume of online text documents available on the Internet, digital libraries, news sources, and company-wide intranets. It has been forecasted that these docu...
IR
jantke94nonstandard
Nonstandard Concepts of Similarity in Case-Based Reasoning Introduction  The present paper is aimed at propagating new concepts of similarity more flexible and expressive than those underlying most case-based reasoning approaches today. So, it mainly deals with criticizing approaches in use, with motivating and introducing new notions and notations, and with first steps towards future applications. The investigations at hand originate from the author's work in learning theory. In exploring the relationship between inductive learning and case-based learning within a quite formal setting (cf. [Jan92b]), it turned out that both areas almost coincide, if sufficiently flexible similarity concepts are taken into acount. This provides some formal arguments for the necessity of non-symmetric similarity measures. Encouraged by these first results, the author tried to investigate more structured learning problems from the view point of case-based reasoning. It turned out that an appropriate handling requires formalisms allowing similarity concep
ML
warshaw99rulebased
Rule-Based Query Optimization, Revisited We present an overview and initial performance assessment of a rule-based query optimizer written in VenusDB. VenusDB is an active-database rule language embedded in C++. Following the developments in extensible database query optimizers, first in rule-based form followed by optimizers written as object-oriented programs, the VenusDB optimizer avails the advantages of both. To date, development of rule-based query optimizers have included the definition and implementation of custom rule languages. Thus, extensibility required detailed understanding and often further development of the underlying search mechanism of the rule system. Objectoriented query optimizers appear to have achieved their goals with respect to a clear organization and encapsulation  of an optimizer's elements. They do not, however, provide for the concise, declarative expression of domain specific heuristics. Our experience demonstrates that a rule-based query optimizer developed in VenusDB can be well structured, ...
DB
tzouramanis00multiversion
Multiversion Linear Quadtree for Spatio-Temporal Data Research in spatio-temporal databases has largely focused on extensions of access methods for the proper handling of time changing spatial information. In this paper, we present the Multiversion Linear Quadtree (MVLQ), a spatio-temporal access method based on Multiversion B-trees (MVBT) [2], embedding ideas from Linear Region Quadtrees [4]. More specifically, instead of storing independent numerical data having a different transaction-time each, for every consecutive image we store a group of codewords that share the same transaction-time, whereas each codeword represents a spatial subregion. Thus, the new structure may be used as an index mechanism for storing and accessing evolving raster images. We also conducted a thorough experimentation using sequences of real and synthetic raster images. In particular, we examined the time performance of temporal window queries, and provide results for a variety of parameter settings.
DB
nie01mining
Mining Source Coverage Statistics for Data Integration Recent work in data integration has shown the importance of statistical information about the coverage and overlap of sources for efficient query processing. Despite this recognition there are no effective approaches for learning the needed statistics. The key challenge in learning such statistics is keeping the number of needed statistics low enough to have the storage and learning costs manageable. Naive approaches can become infeasible very quickly. In this paper we present a set of connected techniques that estimate the coverage and overlap statistics while keeping the needed statistics tightly under control. Our approach uses a hierarchical classification of the queries, and threshold based variants of familiar data mining techniques to dynamically decide the level of resolution at which to learn the statistics. We describe the details of our method, and present preliminary experimental results showing the feasibility of the approach.  1. 
DB
458848
Bringing Robustness to End-User Programming In some cases, end-user programming allows the design of stand-alone applications. But none of the existing approaches is concerned by safety aspects of programming. Heavy techniques exist to develop safe applications, particularly in non-interactive domains. They involve software engineering techniques, and sometimes, formal methods. All these techniques are very far from end-users. Our idea is to let this part to experts, and to connect end-user programming onto this safe conventional development. Starting from an existing functional core, we built an interactive end-user programming environment called GenBuild, which allows designing interactive stand-alone applications. GenBuild allows the verification of some properties that are a first step towards the development of safe end-user programming.  1. 
HCI
sakakibara94learning
Learning Languages by Collecting Cases and Tuning Parameters . We investigate the problem of case-based learning of formal languages. Case-based reasoning and learning is a currently booming area of artificial intelligence. The formal framework for case-based learning of languages has recently been developed by [JL93] in an inductive inference manner. In this paper, we first show that any indexed class of recursive languages in which finiteness is decidable is case-based representable, but many classes of languages including the class of all regular languages are not case-based learnable with a fixed universal similarity measure, even if both positive and negative examples are presented. Next we consider a framework of case-based learning where the learning algorithm is allowed to learn similarity measures, too. To avoid trivial encoding tricks, we carefully examine to what extent the similarity measure is going to be learned. Then by allowing only to learn a few parameters in the similarity measures, we show that any indexed class of recursive ...
ML
scassellati98building
Building Behaviors Developmentally: A New Formalism This paper advocates a developmental approach to building complex interactive behaviors for robotic systems. A developmental methodology is advantageous because it provides a structured decomposition of complex tasks, because it facilitates learning, and because it allows for a gradual increase in task complexity. The developmental approach provides a structured means both of dividing a task among research units, as well as a metric for evaluating the progress of the task. Initial work with developmental modeling has also hinted that these skill decompositions may make the overall task easier to accomplish through the re-use of knowledge gained from developmental precursors. We report here on two projects of building behaviors developmentally for a humanoid robot. In the first project, the robot learned to reach to a visual target by following a developmental progression similar to those observed in infants. The second project outlines a proposal for building social skills developmenta...
AI
grimm01systems
Systems Directions for Pervasive Computing Pervasive computing, with its focus on users and their tasks rather than on computing devices and technology, provides an attractive vision for the future of computing. But, while hardware and networking infrastructure to realize this vision are becoming a reality, precious few applications run in this infrastructure. We believe that this lack of applications stems largely from the fact that it is currently too hard to design, build, and deploy applications in the pervasive computing space. In this paper, we argue that existing approaches to distributed computing are flawed along three axes when applied to pervasive computing; we sketch out alternatives that are better suited for this space. First, application data and functionality need to be kept separate, so that they can evolve gracefully in a global computing infrastructure. Second, applications need to be able to acquire any resource they need at any time, so that they can continuously provide their services in a highly dynamic environment. Third, pervasive computing requires a common system platform, allowing applications to be run across the range of devices and to be automatically distributed and installed. 1.
Agents
azuma99challenge
The Challenge of Making Augmented Reality Work Outdoors this paper, along with others we have yet to imagine. Acknowledgments
HCI
455692
SI-Designer: an Integration Framework for E-Commerce Electronic commerce lets people purchase goods  and exchange information on business transactions  on-line. Therefore one of the main challenges for  the designers of the e-commerce infrastructures is  the information sharing, retrieving data located in  different sources thus obtaining an integrated view  to overcome any contradiction or redundancy. Virtual  Catalogs synthesize this approach as they are  conceived as instruments to dynamically retrieve  information from multiple catalogs and present  product data in a unified manner, without directly  storing product data from catalogs.  In this paper we propose SI-Designer, a support  tool for the integration of data from structured and  semi-structured data sources, developed within the  MOMIS (Mediator environment for Multiple Information  Sources) project.  1 
DB
roy99coastal
Coastal Navigation - Mobile Robot Navigation with Uncertainty in Dynamic Environments Ships often use the coasts of continents for navigation in the absence of better tools such as GPS, since being close to land allows sailors to determine with high accuracy where they are. Similarly for mobile robots, in many environments global and accurate localization is not always feasible. Environments can lack features, and dynamic obstacles such as people can confuse and block sensors. In this paper, we demonstrate a technique for generating trajectories that take into account both the information content of the environment, and the density of the people in the environment. These trajectories reduce the average positional certainty as the robot moves, reducing the likelihood the robot will become lost at any point. Our method was successfully implemented and used by the mobile robot Minerva, a museum tourguide robot, for a 2 week period in the Smithsonian National Museum of American History. 1 Introduction  One essential component of any operational mobile robot system is the ab...
AI
364207
The KRAFT Architecture for Knowledge Fusion and Transformation This paper describes the KRAFT architecture which supports the fusion of knowledge  from multiple, distributed, heterogeneous sources. The architecture uses  constraints as a common knowledge interchange format, expressed against a common  ontology. Knowledge held in local sources can be tranformed into the common  constraint language, and fused with knowledge from other sources. The  fused knowledge is then used to solve some problem or deliver some information  to a user. Problem-solving in KRAFT typically exploits pre-existing constraint  solvers. KRAFT uses an open and flexible agent architecture in which knowledge  sources, knowledge fusing entities, and users are all represented by independent  KRAFT agents, communicating using a messaging protocol. Facilitator agents  perform matchmaking and brokerage services between the various kinds of agent.  KRAFT is being applied to an example application in the domain of network data  services design.  1 Introduction and Motivation  Most ...
Agents
20543
Nonmonotonic Inheritance in Object-Oriented Deductive Database Languages Deductive object-oriented frameworks integrate logic rules and inheritance. There, specific problems arise: Due to the combination of deduction and inheritance, (a) deduction can take place depending on inherited facts, thus raising indirect conflicts, and (b) also the class hierarchy and-membership is subject to deduction. From this point of view, we investigate the application of the extension semantics of Default Logic to deductive object-oriented database languages. By restricting the problem to Horn programs and a special type of defaults tailored to the semantics of inheritance, a forwardchaining construction of extensions is possible. This construction is compared with a solution as implemented in the F-Logic system Florid which is based on a combination of classical deductive fixpoints and an inheritance-trigger mechanism. The paper is a condensed version of [MK98]. 1 Introduction  In deductive object-oriented database languages, a class hierarchy and nonmonotonic inheritance i...
DB
baldoni00intentionguided
Intention-guided Web Sites: A New Perspective on Adaptation Recent years witnessed a rapid growth of multimedia technologies for offering information  and services on the internet. One of the many problems that are to be faced in this context is the great  variety of possible users and the consequent need to adapt both the presentation of information and the  interaction to the specific user's characteristics.
Agents
138121
An Algorithm to Evaluate Quantified Boolean Formulae and its Experimental Evaluation The high computational complexity of advanced reasoning tasks such as reasoning about knowledge and planning calls for efficient and reliable algorithms for reasoning problems harder than NP. In this paper we propose Evaluate, an algorithm for evaluating Quantified Boolean Formulae, a language that extends propositional logic in a way such that many advanced forms of propositional reasoning, e.g., circumscription, can be easily formulated as evaluation of a QBF. Algorithms for evaluation of QBFs are suitable for the experimental analysis on a wide range of complexity classes, a property not easily found in other formalisms.  Evaluate is based on a generalization of the Davis-Putnam procedure for SAT, and is guaranteed to work in polynomial space. Before presenting the algorithm, we discuss several abstract properties of QBFs that we singled out to make it more efficient. We also discuss various options that were investigated about heuristics and data structures, and report the main res...
AI
kopp00planning
Planning and Motion Control in Lifelike Gesture: A Refined Approach In this paper an operational model for the automatic generation of lifelike gestures of an anthropomorphic virtual agent is described. The biologically motivated approach to controlling the movements of a highly articulated figure provides a transformation of spatiotemporal gesture specifications into an analog representation of the movement from which the animations are directly rendered. To this end, knowledge-based computer animation techniques are combined with appropriate methods for trajectory formation and articulated figure animation.  1. Introduction  The inclusion of nonverbal modalities into the communicative behaviors of virtual agents has moved into focus of human-computer interface researchers. Humans are more likely to consider computer-generated figures lifelike when appropriate nonverbal behaviors are displayed in addition to speech. This enables the evocation of social communicative attributions to the artificial agent, which are supposed to be advantageous for a natu...
Agents
rauber01integrating
Integrating Automatic Genre Analysis into Digital Libraries With the number and types of documents in digital library systems increasing, tools for automatically organizing and presenting the content have to be found. While many approaches focus on topic-based organization and structuring, hardly any system incorporates automatic structural analysis and representation. Yet, genre information (unconsciously) forms one of the most distinguishing features in conventional libraries and in information searches. In this paper we present an approach to automatically analyze the structure of documents and to integrate this information into an automatically created content-based organization. In the resulting visualization, documents on similar topics, yet representing dierent genres, are depicted as books in diering colors. This representation supports users intuitively in locating relevant information presented in a relevant form.  Keywords  Genre Analysis, Self-Organizing Map (SOM), SOMLib, Document Clustering, Visualization, Metaphor Graphics  1. 
IR
374383
Localisation using Automatically Selected Landmarks from Panoramic Images The use of visual landmarks for robot localisation is a promising field. It is apparent that the success of localisation by visual landmarks depends on the landmarks chosen. Good landmarks are those which remain reliable over time and through changes in position and orientation. This paper describes a system which learns places by automatically selecting landmarks from panoramic images and uses them for localisation tasks. An adaption of the biologically inspired Turn Back and Look behaviour is used to evaluate potential landmarks. Normalised correlation is used to overcome the a#ects of changes in illumination in the environment. Results from real robot experiments are reported, showing successful localisation from up to one meter away from the learnt position. 1 Introduction Visual localisation is one of the key problems in making successful autonomous robots. Vision as a sensor is the richest source of information about a mobile agent's environment and as such con...
Agents
ragsdale00adaptation
Adaptation Techniques for Intrusion Detection and Intrusion Response Systems This paper examines techniques for providing adaptation in intrusion detection and intrusion response systems. As attacks on computer systems are becoming increasingly numerous and sophisticated, there is a growing need for intrusion detection and response systems to dynamically adapt to better detect and respond to attacks. The Adaptive Hierarchical Agentbased Intrusion Detection System (AHA! IDS) provides detection adaptation by adjusting the amount of system resources devoted to the task of detecting intrusive activities. This is accomplished by dynamically invoking new combinations of lower level detection agents in response to changing circumstances and by adjusting the confidence associated with these lower-level agents. The Adaptive Agentbased Intrusion Response System (AAIRS) provides response adaptation by weighting those responses that have been successful in the past over those techniques that have not been as successful. As a result, the more successful responses are used...
AI
bertino01measuring
Measuring the Structural Similarity among XML Documents and DTDs Sources of XML documents are proliferating on the Web and documents are more and more frequently exchanged among sources. At the same time, there is an increasing need of exploiting database tools to manage this kind of data. An important novelty of XML is that information on document structures is available on the Web together with the document contents. This information can be used to improve document handling and to achieve more effective and efficient searches on documents. However, in such an heterogeneous environment as the Web, it is not reasonable to assume that XML documents that enter a source always conform to a predefined DTD present in the source.
IR
314563
Scheduling Queries on Tape-resident Data Advances in storage technology have made near-line tertiary storage a viable alternative for  database and data warehouse systems. Tertiary storage systems are employed in cases where secondary  storage can not satisfy the data handling requirements or tertiary storage is more cost effective  option. Tertiary storage devices have traditionally been used as archival storage. The new application  domains require on-demand retrieval of data from these devices. This paper investigates issues in  optimizing I/O time for a query whose data resides on automated tertiary storage containing multiple  storage devices. We model the problem as a limited storage parallel two-machine flow-shop scheduling  problem with additional constraints. Given a query, we establish an upper bound on the number  of storage devices for an optimal I/O schedule and provide experimental proof for it. For queries  that access small amounts of data from multiple media, we derive an optimal schedule analytically.  For q...
DB
knowles99evolutionary
Evolutionary Approaches to Off-Line Routing in Backbone Communications Networks Off-line routing in backbone communications networks is an important combinatorial optimisation problem. It has three main uses: first, off-line routing provides reference benchmark results for dynamic (on-line) routing strategies. Second, and more interestingly, off-line routing is becoming more and more investigated and employed in its own right as a way of quickly finding significantly improved routings for live networks which can then be imposed on the network to offer a net improvement in quality of service. Third, it can be used in networks where bandwidth may be booked in advance. In this paper we introduce and investigate a number of heuristic techniques applicable to the routing problem for use in stochastic, iterative search. Results are presented which indicate that these heuristics significantly improve the search for solutions, particularly when on-line performance is considered. We also investigate how computation time can be further reduced by the use of delta-evaluation...
ML
132577
Optimizing Object Queries Using an Effective Calculus Object-oriented databases (OODBs) provide powerful data abstractions and modeling facilities but they generally lack a suitable framework for query processing and optimization. One of the key factors for OODB systems to successfully compete with relational systems as well as to meet the performance requirements of many non-traditional applications is the development of an effective query optimizer. We propose an effective framework with a solid theoretical basis for optimizing OODB query languages. Our calculus, called the monoid comprehension calculus, captures most features of ODMG OQL and is a good basis for expressing various optimization algorithms concisely. This paper concentrates on query unnesting, an optimization that, even though improves performance considerably, is not treated properly (if at all) by most OODB systems. Our framework generalizes many unnesting techniques proposed recently in the literature and is capable of removing any form of query nesting using a very si...
DB
rao97unified
A Unified View of Plans as Recipes Plans as recipes or abstract structures, as well as plans as mental attitudes that guide an agent in its planning process has been enthusiastically embraced by both philosophers and AI practitioners. They play a central role in a class of rational agents, called Belief-Desire-Intention (BDI) agents. This dual view of plans can not only be used for efficient planning, but can also be used for recognizing the plans of other agents, coordinating one's actions and achieving joint intentions with other members of a larger collective or team, and finally recognizing the collective plans and intentions of other teams. In this paper, we start with a simple notion of execution plans and discuss its operational semantics. We progressively extend this notion of plans to recognition plans, joint execution plans, and joint recognition plans. The primary contribution of this paper is in providing an integrated view of plans that facilitate individual an collective planning and recognition.   1 Int...
Agents
arlein99making
Making LDAP Active with the LTAP Gateway: Case Study in Providing Telecom Integration and Enhanced Services LDAP (Lightweight Directory Access Protocol) directories are being rapidly deployed on the Web. They are currently used to store data like white pages information, user profiles, and network device descriptions. These directories offer a number of advantages over current database technology in that they provide better support for heterogeneity and scalability. However, they lack some basic database functionality (e.g., triggers, transactions) that is crucial for Directory Enabled Networking (DEN) tasks like provisioning network services, allocating resources, reporting, managing end-to-end security, and offering mobile users customized features that follow them. In order to address these limitations while keeping the simplicity and performance features of LDAP directories, unbundled and portable solutions are needed. In this paper we discuss LDAP limitations we faced while building an LDAP meta-directory that integrates data from legacy telecom systems, and how LTAP (Lightweight Trigger Access Process), a portable gateway that adds active functionality to LDAP directories, overcomes these limitations.
DB
aksoy98textural
Textural Features for Image Database Retrieval This paper presents two feature extraction methods and two decision methods to retrieve images having some section in them that is like the user input image. The features used are variances of gray level co-occurrences and line-angle-ratio statistics constituted by a 2-D histogram of angles between two intersecting lines and ratio of mean gray levels inside and outside the regions spanned by those angles. The decision method involves associating with any pair of images either the class “relevant” or “irrelevant”. A Gaussian classifier and nearest neighbor classifier are used. A protocol that translates a frame throughout every image to automatically define for any pair of images whether they are in the relevance class or the irrelevance class is discussed. Experiments on a database of 300 gray scale images with 9,600 groundtruth image pairs showed that the classifier assigned 80 % of the image pairs we were sure were relevant, to the relevance class correctly. The actual retrieval accuracy is greater than this lower bound of 80%. 
DB
carkacioglu02learning
Learning Similarity Space In this study, we suggest a method to adapt an image retrieval system into a configurable one. Basically, original feature space of a content-based retrieval system is nonlinearly transformed into a new space, where the distance between the feature vectors is adjusted by learning. The transformation is realized by Artificial Neural Network architecture. A cost function is defined for learning and optimized by simulated annealing method. Experiments are done on the texture image retrieval system, which use Gabor Filter features. The results indicate that configured image retrieval system is significantly better than the original system. 1.
AI
bruno01stholes
STHoles: A Multidimensional Workload-Aware Histogram Attributes of a relation are not typically independent. Multidimensional histograms can be an effective tool for accurate multiattribute query selectivity estimation. In this paper, we introduce STHoles,a\workload-aware" histogram that allows bucket nesting to capture data regions with reasonably uniform tuple density. STHoles histograms are built without examining the data sets, but rather by just analyzing query results. Buckets are allocated where needed the most as indicated by the workload, which leads to accurate query selectivity estimations. Our extensive experiments demonstrate that STHoles histograms consistently produce good selectivity estimates across synthetic and real-world data sets and across query workloads, and, in many cases, outperform the best multidimensional histogram techniques that require access to and processing of the full data sets during histogram construction. 1.
DB
phan01new
A New TWIST on Mobile Computing: Two-Way Interactive Session Transfer The ubiquitous use of computer resources for daily productivity is a goal that presently remains unrealised. We believe that the convergence of desktop and mobile applications into a seamless computing experience will provide a strong motivation for future "anytime, anywhere computing." In this paper we describe this convergence as the capability to perform the handoff of application sessions across heterogeneous platforms using the network as a conduit. In addition to discussing the architecture and protocols to facilitate this capability, in this paper we also provide a taxonomy for describing a variety of different session handoff schemes. In particular, we have identified an important Two-Way Interactive Session Transfer (TWIST) behaviour for communication between heterogeneous clients and servers. To demonstrate our concepts, we have implemented the handoff capability with TWIST semantics into a real-world application that serves as a teaching tool for radiology clinicians. From experimental data we will show that the handoff mechanism incurs little delay to transfer large dataladen sessions.  1 
HCI
accot97beyond
Beyond Fitts' Law: Models for Trajectory-Based HCI Tasks Trajectory-based interactions, such as navigating through nested-menus, drawing curves, and moving in 3D worlds, are becoming common tasks in modern computer interfaces. Users' performances in these tasks cannot be successfully modeled with Fitts' law as it has been applied to pointing tasks. Therefore we explore the possible existence of robust regularities in trajectory-based tasks. We used "steering through tunnels" as our experimental paradigm to represent such tasks, and found that a simple "steering law" indeed exists. The paper presents the motivation, analysis, a series of four experiments, and the applications of the steering law.
HCI
mchugh98indexing
Indexing Semistructured Data This paper describes techniques for building and exploiting indexes on semistructured data: data that may not have a fixed schema and that may be irregular or incomplete. We first present a general framework for indexing values in the presence of automatic type coercion. Then based on  Lore, a DBMS for semistructured data, we introduce four types of indexes and illustrate how they are used during query processing. Our techniques and indexing structures are fully implemented and integrated into the Lore prototype. 1 Introduction  We call data that is irregular or that exhibits type and structural heterogeneity semistructured,  since it may not conform to a rigid, predefined schema. Such data arises frequently on the Web, or when integrating information from heterogeneous sources. In general, semistructured data can be neither stored nor queried in relational or object-oriented database management systems easily and efficiently. We are developing Lore  1  , a database management system d...
DB
ordonez00discovering
Discovering Interesting Association Rules in Medical Data We are presently exploring the idea of discovering association rules in medical data. There are several technical aspects which make this problem challenging. In our case medical data sets are small, but have high dimensionality. Information content is rich: there exist numerical, categorical, time and even image attributes. Data records are generally noisy. We explain how to map medical data to a transaction format suitable for mining rules. The combinatorial nature of association rules matches our needs, but current algorithms are unsuitable for our purpose. We thereby introduce an improved algorithm to discover association rules in medical data which incorporates several important constraints. Some interesting results obtained by our program are discussed and we explain how the program parameters were set. We believe many of the problems we come across are likely to appear in other domains.  1 Introduction  Data Mining is an active research area. One of the most popular approaches t...
AI
471747
MARIAN: Flexible Interoperability for Federated Digital Libraries Federated digital libraries are composed of distributed autonomous (heterogeneous) information services but provide users with a transparent, integrated view of collected information – respecting different information sources’ autonomy. In this paper we discuss a federated system for the Networked Digital Library of Theses and Dissertations (NDLTD), an international consortium of universities, libraries, and other supporting institutions focused on electronic theses and dissertations (ETDs). The NDLTD has so far allowed its members considerable autonomy, though agreements are developing on metadata standards and on support of the Open Archives initiative that eventually will promote greater homogeneity. At present, federation requires dealing flexibly with differences among systems, ontologies, and data formats. Our solution involves adapting MARIAN, an objectoriented digital library retrieval system developed with support by NLM and NSF, to serve as mediation middleware for the federated NDLTD collection. Components of the solution include: 1) the use of several harvesting techniques; 2) an architecture based on object-oriented ontologies of search modules and metadata; 3) diversity within the harvested data joined to a single collection view for the user; and 4) an integrated framework for addressing such questions as data quality, information compression, and flexible search. The system can handle very large dynamic collections. An adaptable relationship between the collection view and harvested data facilitates adding new sites to the federation and adapting to changes in existing sites. MARIAN’s modular architecture and powerful and flexible data model work together to build an effective integrated solution within a simple uniform framework. We present both the general design of the system and operational details of a preliminary federated collection involving several thousand ETDs in four different formats and two languages from USA and Europe.
IR
starner98visual
Visual Contextual Awareness in Wearable Computing Small, body-mounted video cameras enable a different style of wearable computing interface. As processing power increases, a wearable computer can spend more time observing its user to provide serendipitous information, manage interruptions and tasks, and predict future needs without being directly commanded by the user. This paper introduces an assistant for playing the real-space game Patrol. This assistant tracks the wearer's location and current task through computer vision techniques and without off-body infrastructure. In addition, this paper continues augmented reality research, started in 1995, for binding virtual data to physical locations. 1. Introduction  For most computer systems, even virtual reality systems, sensing techniques are a means of getting input directly from the user. However, wearable computers offer a unique opportunity to re-direct sensing technology towards recovering more general user context. Wearable computers have the potential to "see" as the user sees...
HCI
elio00task
Task Models, Intentions, and Agent Conversation Policies . It is possible to define conversation policies, such as communication  or dialogue protocols, that are based strictly on what messages and,  respectively, what performatives may follow each other. While such an  approach has many practical applications, such protocols support only "local  coherence" in a conversation. Lengthy message exchanges require some  infrastructure to lend them "global coherence." Recognition of agent intentions  about the joint task is essential for this global coherence, but there are further  mechanisms needed to ensure that both local and global coherence are jointly  maintained. This paper presents a general yet practical approach to designing,  managing, and engineering agents that can do simple run-time intention  recognition without creating complex multi-state protocols. In this approach we  promote developing abstract task models and designing conversation policies  in terms of such models. An implemented agent assistant based on these ideas is  brie...
Agents
devedzic01knowledge
Knowledge Modeling - State of the Art A major characteristic of developments in the broad field of Artificial Intelligence (AI) during the 1990s has been an increasing integration of AI with other disciplines. A number of other computer science fields and technologies have been used in developing intelligent systems, starting from traditional information systems and databases, to modern distributed systems and the Internet. This paper surveys knowledge modeling techniques that have received most attention in recent years among developers of intelligent systems, AI practitioners and researchers. The techniques are described from two perspectives, theoretical and practical. Hence the first part of the paper presents major theoretical and architectural concepts, design approaches, and research issues. The second part discusses several practical systems, applications, and ongoing projects that use and implement the techniques described in the first part. Finally, the paper briefly covers some of the most recent results in the fields of intelligent manufacturing systems, intelligent tutoring systems, and ontologies.  2 1. 
Agents
450405
Coverage Problems in Wireless Ad-hoc Sensor Networks Wireless ad-hoc sensor networks have recently emerged as a premier research topic. They have great long term economic potential, ability to transform our lives, and pose many new system building challenges. Sensor networks also pose a number of new conceptual and optimization problems. Some, such as location, deployment, and tracking, are fundamental issues, in that many applications rely on them for needed information.  In this paper, we address one of the fundamental problems, namely coverage. Coverage in general, answers the questions about quality of service (surveillance) that can be provided by a particular sensor network. We first define the coverage problem from several points of view including deterministic, statistical, worst and best case, and present examples in each domain. By combining computational geometry and graph theoretic techniques, specifically the Voronoi diagram and graph search algorithms, we establish the main highlight of the paper - optimal polynomial time worst and average case algorithm for coverage calculation. We also present comprehensive experimental results and discuss future research directions related to coverage in sensor networks.  I. 
HCI
111340
Run-time Detection in Parallel and Distributed Systems: An Application to Safety-Critical Applications As systems are becoming more complex, there is increasing interest in their runtime analysis, understanding their dynamic behavior and possibly controling it as well. This paper describes complex distributed and parallel applications that use run-time analyses to attain scalability improvements with respect to the amount and complexity of the data transmitted, transformed, and shared among different application components. Such improvements are derived from using database techniques when manipulating data streams. Namely, by imposing a relational model on a data stream, filters and constraints on the stream may be expressed in the form of database queries evaluated against the data events comprising the stream. Streams may then be filtered using runtime optimization techniques derived from query optimization methods. This paper also presents a tool, termed Cnet, which offers (1) means for the dynamic creation of queries and their application to distributed data streams, (2) permits the...
DB
vanhoof98specialising
Specialising The Other Way Around In this paper, we present a program transformation based on bottom up evaluation of logic programs. We explain that using this technique, programs can be "specialised" w.r.t. a set of unit clauses instead of a query. Moreover, examples indicate that excellent specialisation can be obtained when this bottom up transformation is combined with a more traditional top down approach, resulting in conceptually cleaner techniques requiring a less complicated control than one overall approach.  1 
DB
kaburlasos00learning
Learning and Decision-Making in the Framework of Fuzzy Lattices A novel theoretical framework is delineated for supervised and unsupervised learning. It is called framework of fuzzy lattices, or FLframework for short, and it suggests mathematically sound tools for dealing separately and/or jointly with disparate types of data including vectors of numbers, fuzzy sets, symbols, etc. Specific schemes are proposed for clustering and classification having the capacity to deal with both missing and don't care data values; the schemes in question can be implemented as neural networks. The proposed learning schemes are employed here for pattern recognition on seven data sets including benchmark data sets, and the results are compared with those ones by various learning techniques from the literature. Finally, aiming at a mutual cross-fertilization, the FL-framework is associated with established theories for learning and/or decision-making including probability theory, fuzzy set theory, Bayesian decision-making, theory of evidence, and adaptive resonance t...
ML
tresp98description
A Description Logic for Vague Knowledge This work introduces the concept language ALCFM which is an extension of ALC to many-valued logics. ALCFM allows to express vague concepts, e.g. more or less enlarged or very small. To realize this extension to many-valued logics, the classical notions of satisfiability and subsumption had to be modified appropriately. For example, ALCFM -concepts are no longer either satisfiable or unsatisfiable, but they are satisfiable to a certain degree. The main contribution of this paper is a sound and complete method for computing the degree of subsumption between two  ALCFM -concepts. 1 Introduction  This work takes its motivation from the occurrence of vague concept descriptions in different application areas. Often, application--inherent information is characterized by a very high degree of vagueness. Appropriate information systems must be able to process this kind of data. So far, there are no systems that really solve the corresponding problems due to the lack of powerful basic methods. A...
AI
531549
Towards Robust Teams with Many Agents Agents in deployed multi-agent systems monitor other agents to coordinate and collaborate robustly. However, as the number of agents monitored is scaled up, two key challenges arise: (i) the number of monitoring hypotheses to be considered can grow exponentially in the number of agents; and (ii) agents become physically and logically unconnected (unobservable) to their peers. This paper examines these challenges in teams of cooperating agents, focusing on a monitoring task that is of particular importance to robust teamwork: detecting disagreements among team-members. We present YOYO, a highly scalable disagreement-detection algorithm which guarantees sound detection in time linear in the number of agents despite the exponential number of hypotheses. In addition, we present new upper bounds about the number of agents that must be monitored in a team to guarantee disagreement detection. Both YOYO and the new bounds are explored analytically and empirically in thousands of monitoring problems, scaled to thousands of agents.
Agents
weiss98intelligent
Intelligent Telecommunication Technologies anage telecommunication networks. Building such applications involved acquiring valuable telecommunication knowledge from human experts and then applying this knowledge, typically by embedding it in an expert system. This knowledge acquisition process is so time-consuming that it is referred to as the "knowledge acquisition bottleneck". Data mining techniques are now being applied to industrial applications to break this bottleneck, by replacing the manual knowledge acquisition process with automated knowledge discovery. Telecommunication networks, which routinely generate tremendous amounts of data, are ideal candidates for data mining [1]. This section will describe expert system and data mining technologies and how they are evolving to solve complex industrial problems. 1.1 Expert Systems Expert systems are programs which represent and apply factual knowledge of specific areas of expertise to solve problems [2]. Expert systems have been applied extensively within
ML
marsella00interactive
Interactive Pedagogical Drama This paper describes an agent-based approach to realizing interactive pedagogical drama. Characters choose their actions autonomously, while director and cinematographer agents manage the action and its presentation in order to maintain story structure, achieve pedagogical goals, and present the dynamic story to as to achieve the best dramatic effect. Artistic standards must be maintained while permitting substantial variability in story scenario. To achieve these objectives, scripted dialog is deconstructed into elements that are portrayed by agents with emotion models. Learners influence how the drama unfolds by controlling the intentions of one or more characters, who then behave in accordance with those intentions. Interactions between characters create opportunities to move the story in pedagogically useful directions, which the automated director exploits. This approach is realized in the multimedia title Carmen's Bright IDEAS, an interactive health intervention designed to impro...
Agents
dubois99classification
Classification Space for Augmented Surgery, an Augmented Reality Case Study One of the recent design goals in Human Computer Interaction has been to extend the sensorymotor capabilities of computer systems to combine the real and the virtual in order to assist the user in his environment. Such systems are called Augmented Reality (AR). Although AR systems are becoming more prevalent we still do not have a clear understanding of this interaction paradigm. In this paper we propose OPAS as a generic framework for classifying existing AR systems. Computer Assisted Medical Interventions (CAMI), for which the added value of AR has been demonstrated by experience, are discussed in light of OPAS. We illustrate OPAS using our system, CASPER (Computer ASsisted PERicardial puncture), a CAMI system which assists in surgical procedures (pericardial punctures). KEYWORDS: Augmented Surgery, CAMI, Augmented Reality, Classification Space  1. INTRODUCTION  The term "Augmented Reality" (AR) appears in the literature usually in conjunction with the term "Virtual Reality" (VR). Th...
HCI
moukas96amalthaea
Amalthaea: Information Discovery and Filtering using a Multiagent Evolving Ecosystem Agents are semi-intelligent programs that assist the user in performing repetitive  and time-consuming tasks. Information discovery and information filtering are  a suitable domain for applying agent technology. Ideas drawn from the field of  autonomous agents and artificial life are combined in the creation of an evolving  ecosystem composed of competing and cooperating agents. A co-evolution model  of information filtering agents that adapt to the various user's interests and information  discovery agents that monitor and adapt to the various on-line information  sources is analyzed. Results from a number of experiments are presented and discussed.  Keywords: Agents, Information Filtering, Evolution, World-Wide-Web  1 Introduction  The exponential increase of computer systems that are interconnected in on-line networks has resulted in a corresponding exponential increase in the amount of information available on-line. This information is distributed among heterogeneous sources and is...
IR
187693
Leveraging Mediator Cost Models with Heterogeneous Data Sources : Distributed systems require declarative access to diverse data sources of information. One approach to solving this heterogeneous distributed database problem is based on mediator architectures. In these architectures, mediators accept queries from users, process them with respect to wrappers, and return answers. Wrapper provide access to underlying data sources. To efficiently process queries, the mediator must optimize the plan used for processing the query. In classical databases, cost-estimate based query optimization is an effective method for optimization. In a heterogeneous distributed databases, cost-estimate based query optimization is difficult to achieve because the underlying data sources do not export cost information. This paper describes a new method that permits the wrapper programmer to export cost estimates (cost estimate formulas and statistics). For the wrapper programmer to describe all cost estimates may be impossible due to lack of information or burdensome due...
DB
43511
The Logic Programming Paradigm  
DB
hammer99information
The Information Integration Wizard (IWiz) Project  data.  To illustrate the need for integration of heterogeneous data sources, suppose we have a scenario where consumers want to purchase computer-related products from one of the many e-commerce sites. However, before making the purchase they would like to gather all the relevant, available information in order to help them in their decision making process. For example, consumers may want to access product information on available desktops, laptops, software, and other accessories and check availability and pricing information. In addition, users may also want to access other online sources for related background information such as consumer reports, press releases etc. This situation is depicted in Figure 1.  Typically, each source uses different tools and data modeling techniques to create and manage their data. This means, the same concept, for example, the entity software, may be described by a different term and different set of attributes in different sources (e.g
DB
gallwitz98erlangen
The Erlangen Spoken Dialogue System EVAR: A State-of-the-Art Information Retrieval System In this paper, we present an overview of the spoken dialogue system EVAR that was developed at the University of Erlangen. In January 1994, it became accessible over telephone line and could answer inquiries in the German language about German InterCity train connections. It has since been continuously improved and extended, including some unique features, such as the processing of out--of--vocabulary words and a flexible dialogue strategy that adapts to the quality of the recognition of the user input. In fact, several different versions of the system have emerged, i.e. a subway information system, train and flight information systems in different languages, and an integrated multilingual and multifunctional system which covers German and 3 additional languages in parallel. Current research focuses on the introduction of stochastic models into the semantic analysis, on the direct integration of prosodic information into the word recognition process, on the detection of user emotion, a...
AI
gellersen00adding
Adding Some Smartness to Devices and Everyday Things In mobile computing, context-awareness indicates the ability of a system to obtain and use information on aspects of the system environment. To implement contextawareness, mobile system components have to be augmented with the ability to capture aspects of their environment. Recent work has mostly considered locationawareness, and hence augmentation of mobile artifacts with locality. In this paper we discuss augmentation of mobile artifacts with diverse sets of sensors and perception techniques for awareness of context beyond location. We report experience from two projects, one on augmentation of mobile phones with awareness technologies, and the other on embedding of awareness technology in everyday non-digital artifacts.
HCI
tandler01software
Software Infrastructure for Ubiquitous Computing Environments: Supporting Synchronous Collaboration with Heterogeneous Devices In ubiquitous computing environments, multiple users work with a wide range of different devices. In many cases, users interact and collaborate using multiple heterogeneous devices at the same time. The configuration of the devices should be able to change frequently due to a highly dynamic, flexible and mobile nature of new work practices. This produces new requirements for the architecture of an appropriate software infrastructure. In this paper, an architecture designed to meet these requirements is proposed. To test its applicability, this architecture was used as the basis for the implementation of BEACH, the software infrastructure of i-LAND (the ubiquitous computing environment at GMD-IPSI). It provides the functionality for synchronous cooperation and interaction with roomware components, i.e. room elements with integrated information technology. In conclusion, our experiences with the current implementation are presented.
HCI
santos99dynamic
Dynamic User Model Construction with Bayesian Networks for Intelligent Information Queries The complexity of current software applications is overwhelming users. The need exists for intelligent interface agents to address the problems of increasing taskload that is overwhelming the human user. Interface agents could help alleviate user taskload by extracting and analyzing relevant information, and providing information abstractions of that information, and providing timely, beneficial assistance to users. Central to providing assistance to a user is the issue of correctly determining the user's intent. The Clavin project is to build an intelligent natural language query information management system. Clavin must maintain a dynamic user model of the relevant concepts in the user inquiries as they relate to the information sources. The primary goal of Clavin is to autonomously react to changes in user intent as well as the information sources, by dynamically constructing the appropriate queries relative to the changes identified. In this paper, we discuss the...
Agents
533789
RF*IPF: A Weighting Scheme for Multimedia Information Retrieval Region-based approach has become a popular research trend in the field of multimedia database retrieval. In this paper, we present the Region Frequency and Inverse Picture Frequency (RF*IPF) weighting, a measure developed to unify region-based multimedia retrieval systems with textbased information retrieval systems. The weighting measure gives the highest weight to regions that occur often in a small number of images in the database. These regions are considered discriminators. With this weighting measure, we can blend image retrieval techniques with TF*IDFbased text retrieval techniques for large-scale Web applications. The RF*IPF weighting has been implemented as a part of our experimental SIMPLIcity image retrieval system and tested on a database of about 200,000 general-purpose images. Experiments have shown that this technique is effective in discriminating images of different semantics. Additionally, the overall similarity approach enables a simple querying interface for multimedia information retrieval systems. 1 
IR
puliafito00map
MAP: Design and Implementation of a Mobile Agents Platform The recent development of telecommunication networks has contributed to the success of applications such as information retrieval and electronic commerce, as well as all the services that take advantage of communication in distributed systems. In this area, the emerging technology of mobile agents aroused considerable interest. Mobile agents are applications that can move through the network for carrying out a given task on behalf of the user. In this work we present a platform (called MAP (Mobile Agents Platform)) for the development and the management of mobile agents. The language used both for developing the platform and for carrying out the agents is Java. The platform gives the user all the basic tools needed for creating some applications based on the use of agents. It enables us to create, run, suspend, resume, deactivate, reactivate local agents, to stop their execution, to make them communicate each other and migrate.  Keywords: mobile agents, distributed computing, Java, net...
Agents
bettini01klava
Klava: a Java Framework for Distributed and Mobile Applications Highly distributed networks have now become a common infrastructure for a new  kind of wide-area distributed applications whose key design principle is network awareness,  namely the ability of dealing with dynamic changes of the network environment.  Network-aware computing has called for new programming languages that exploit the  mobility paradigm as a basic interaction mechanism. In this paper we present the architecture  of Klava, an experimental Java framework for distributed applications and  code mobility. We explain how Klava implements code mobility by relying on Java  and show a few distributed applications that exploit mobile code and are programmed  in Klava.  Keywords: Code Mobility, Distributed Applications, Network Awareness, Language and Middleware Implementation, Tuple Spaces, Java.  1 
Agents
27645
Optimizing Large Join Queries in Mediation Systems In data integration systems, queries posed to a mediator need to be translated into a sequence of queries to the underlying data sources. In a heterogeneous environment, with sources of diverse and limited query capabilities, not all the translations are feasible. In this paper, we study the problem of finding feasible and efficient query plans for mediator systems. We consider conjunctive queries on mediators and model the source capabilities through attribute-binding adornments. We use a simple cost model that focuses on the major costs in mediation systems, those involved with sending queries to sources and getting answers back. Under this metric, we develop two algorithms for source query sequencing -- one based on a simple greedy strategy and another based on a partitioning scheme. The first algorithm produces optimal plans in some scenarios, and we show a linear bound on its worst case performance when it misses optimal plans. The second algorithm generates optimal plans in more scenarios, while having no bound on the margin by which it misses the optimal plans. We also report on the results of the experiments that study the performance of the two algorithms. 
DB
prodromidis99comparative
A Comparative Evaluation of Meta-Learning Strategies over Large and Distributed Data Sets There has been considerable interest recently in various approaches to scaling up machine learning systems to large and distributed data sets. We have been studying approaches based upon the parallel application of multiple learning programs at distributed sites, followed by a meta-learning stage to combine the multiple models in a principled fashion. In this paper, we empirically determine the "best" data partitioning scheme for a selected data set to compose "appropriatelysized " subsets and we evaluate and compare three di#erent strategies, Voting, Stacking and Stacking with Correspondence Analysis (SCANN) for combining classification models trained over these subsets. We seek to find ways to e#ciently scale up to large data sets while maintaining or improving predictive performance measured by the error rate, a cost model, and the TP-FP spread.  Keywords: classification, multiple models, meta-learning, stacking, voting, correspondence analysis, data partitioning Email address of co...
ML
chirkova00linearly
Linearly Bounded Reformulations of Conjunctive Databases (Extended Abstract) Database reformulation is the process of rewriting the data  and rules of a deductive database in a functionally equivalent manner.
DB
210
A Bayesian Computer Vision System for Modeling Human Interactions AbstractÐWe describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task [1]. The system is particularly concerned with detecting when interactions between people occur and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach [2]. We propose and compare two different state-based learning architectures, namely, HMMs and CHMMs for modeling behaviors and interactions. The CHMM model is shown to work much more efficiently and accurately. Finally, to deal with the problem of limited training data, a synthetic ªAlife-styleº training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training. Index TermsÐVisual surveillance, people detection, tracking, human behavior recognition, Hidden Markov Models. 1
ML
fjeld99exploring
Exploring Brick-Based Camera Control Introduction  BUILD-IT is a planning tool based on computer vision technology, with a capacity for complex planning and composition tasks (Rauterberg et al. 1997). The system enables users, grouped around a table, to interact in a virtual scene, using real bricks to select and manipulate objects in the scene (Fig. 1, left). A plan view of the scene is projected onto the table. A perspective view of the scene, called side view, is projected on the wall. The plan view contains a storage space with originals, allowing users to select new objects. Object selection is done by putting a brick at the object position. Once selected, objects can be positioned, rotated and fixed by simple brick manipulation (Fig. 1, right). They are de-selected, and stay put, when the brick is covered or removed. Objects brought back to the storage space are deleted.  Figure 1: BUILD-IT offers a plan view for combined action and perception, and a<F1
HCI
schmidt00implicit
Implicit Human Computer Interaction Through Context this paper we concentrate on the last case, knowing that in most scenarios a combination of all four cases is the way of choice
HCI
amato99probabilistic
Probabilistic Roadmap Methods are Embarrassingly Parallel In this paper we report on our experience parallelizing probabilistic roadmap motion planning methods (prms). We show that significant, scalable speedups can be obtained with relatively little effort on the part of the developer. Our experience is not limited to prms, however. In particular, we outline general techniques for parallelizing types of computations commonly performed in motion planning algorithms, and identify potential difficulties that might be faced in other efforts to parallelize sequential motion planning methods. 1 Introduction  Automatic motion planning has application in many areas such as robotics, virtual reality systems, and computer-aided design. Although many different motion planning methods have been proposed, most are not used in practice since they are computationally infeasible except for some restricted cases, e.g., when the robot has very few degrees of freedom (dof) [12, 16]. Indeed, there is strong evidence that any complete planner (one that is guaran...
AI
xu98neural
A Neural Network Model for Monotonic Diagnostic Problem Solving The task of diagnosis is to find a hypothesis that best explains a set of manifestations (observations). Generally, it is computationally expensive to find a hypothesis because the number of the potential hypotheses is exponentially large. Recently, many efforts have been made to find parallel processing methods to solve the above difficulty. In this paper, we propose a neural network model for diagnostic problem solving where a diagnostic problem is treated as a combinatorial optimisation problem. One feature of the model is that the causal network is directly used as the network. Another feature is that the errors between the observations and the current activations of manifestation nodes are used to guide the network computing for finding optimal diagnostic hypotheses.   1 Introduction  For a set of manifestations(observations), the diagnostic inference is to find the most plausible faults or disorders which can explain why the manifestations are present. In general, an individual d...
ML
66053
Secure Object Spaces - A coordination model for Agents Coordination languages based on shared data spaces are well suited to programming parallel applications composed of cooperating software components. Secure Object Spaces (SOS) extend this model to support composition of independently developed, mutually suspicious, software components. SOS provides flexible and efficient security facilities based on a cryptographic re-interpretation of object types. We discuss the implementation of secure object spaces in the context of a Java-based mobile agent system. 1 Introduction  Coordination languages based on shared data spaces have been around for over fifteen years. Oftentimes, researchers have advocated their use for structuring distributed and concurrent systems. This because the mode of communication provided by these languages, sometimes called generative communication, is  anonymous --- processes interact by reading and writing entries in a shared space without having to know their interlocutor(s); and is uncoupled --- processes are not ...
Agents
sharma96speechgesture
Speech/Gesture Interface to a Visual Computing Environment for Molecular Biologists In recent years there has been tremendous progress in 3D, immersive display and virtual reality (VR) technologies. Scientific visualization of data is one of many applications that has benefited from this progress. To fully exploit the potential of these applications in the new environment there is a need for "natural" interfaces that allow the manipulation of such displays without burdensome attachments. This paper describes the use of visual hand gesture analysis enhanced with speech recognition for developing a bimodal gesture/speech interface for controlling a 3-D display. The interface augments an existing application, VMD, which is a VR visual computing environment for molecular biologists. The free hand gestures are used for manipulating the 3-D graphical display together with a set of speech commands. We concentrate on the visual gesture analysis techniques used in developing this interface. The dual modality of gesture/speech is found to greatly aid the interaction capability....
HCI
456004
Accessing Information and Services on the DAML-Enabled Web The DARPA Agent Markup Language (DAML) program aims to allow one to mark up web pages to indicate the meaning of their content; it is intended that the results delivered by a DAML-enabled browser will more closely match the intentions of the user than is possible with today's syntactically oriented search engines.  In this paper we present our vision of a DAML-enabled search architecture. We present a set of queries of increasing complexity that should be answered efficiently in a Semantic Web. We describe several scenarios illustrating how queries are processed, identifying the main software components necessary to facilitate the search. We examine the issue of inference in search, and we address how to characterize procedures and services in DAML, enabling a DAML query language to find web sites with specified capabilities.  Key Words: Semantic Web, DAML, inference, Web services, process modeling.  1. 
IR
bruni01two
Two Algebraic Process Semantics for Contextual Nets Abstract We show that the so-called ‘Petri nets are monoids ’ approach initiated by Meseguer and Montanari can be extended from ordinary place/transition Petri nets to contextual nets by considering suitable nonfree monoids of places. The algebraic characterizations of net concurrent computations we provide cover both the collective and the individual token philosophy, uniformly along the two interpretations, and coincide with the classical proposals for place/transition Petri nets in the absence of read-arcs.
DB
mudgal00bilateral
Bilateral Negotiation with Incomplete and Uncertain Information: a Decision-Theoretic Approach using a Model of the Opponent . The application of software agents to e-commerce has made a radical  change in the way businesses and consumer to consumer transactions take  place. Agent negotiation is an important aspect of e-commerce to bring  satisfactory agreement in business transactions. We approach e-commerce and  negotiation in the context of a distributed multiagent peer help system, I-Help,  supporting students in a university course. Personal agents keep models of  student preferences and negotiate on their behalf to acquire resources (help)  from other agents. We model negotiation among personal agents by means of  influence diagram, a decision theoretic tool. To cope with the uncertainty  inherent in a dynamic market with self-interested participants, the agents create  models of their opponents during negotiation, which help them predict better  their opponents' actions. We carried out experiments comparing the proposed  negotiation mechanism with influence diagram, one using in addition a model  of t...
Agents
moerland00line
An On-Line EM Algorithm Applied to Kernel PCA Kernel principal component analysis (Pca) is a recent method for non-linear feature extraction. Applying kernel Pca to a data set with N patterns requires storing and nding the eigenvectors of a N N kernel matrix. This paper describes how an Expectation-Maximization (Em) algorithm for standard Pca can be adapted to kernel Pca without having to store the kernel matrix. Experimental results are given where Em for kernel Pca extracts up to 512 non-linear features from a data set with 15,000 examples. The extracted features lead to good performance when used as pre-processed data for a linear classifier. A novel on-line Em algorithm for Pca is presented and shown to further speed up the learning process.
AI
rahwan00virtual
Virtual Enterprise Design - BDI Agents vs. Objects Current research identifying architectures for a virtual enterprise has moved from information modelling to role modelling. Thus, a high level of autonomy results from the distribution of responsibilities, capabilities, and knowledge among different business units in the virtual enterprise at the design stage. Current trends tend towards using object-oriented technology as an effective abstract system design and implementation methodology. We argue that applying the software agent paradigm to the virtual enterprise provides various advantages on both the design and operational levels. We further show that the Belief Desire Intention agent architecture has additional abilities of mapping real world business unit autonomy and interaction. We also introduce the Belief Desire Intention agent paradigm capability of facilitating highly flexible (agile) enterprise design and implementation. 1
Agents
455655
Discovering Unexpected Information from Your Competitors' Web Sites Ever since the beginning of the Web, finding useful information from the Web has been an important problem. Existing approaches include keyword-based search, wrapper-based information extraction, Web query and user preferences. These approaches essentially find information that matches the user's explicit specifications. This paper argues that this is insufficient. There is another type of information that is also of great interest, i.e., unexpected information, which is unanticipated by the user. Finding unexpected information is useful in many applications. For example, it is useful for a company to find unexpected information about its competitors, e.g., unexpected services and products that its competitors offer. With this information, the company can learn from its competitors and/or design counter measures to improve its competitiveness. Since the number of pages of a typical commercial site is very large and there are also many relevant sites (competitors), it is very difficult for a human user to view each page to discover the unexpected information. Automated assistance is needed. In this paper, we propose a number of methods to help the user find various types of unexpected information from his/her competitors' Web sites. Experiment results show that these techniques are very useful in practice and also efficient.  Keywords  Information interestingness, Web comparison, Web mining.  1. 
IR
maynard01named
Named Entity Recognition from Diverse Text Types Current research in Information Extraction  tends to be focused on application-specific systems  tailored to a particular domain. The Muse  system is a multi-purpose Named Entity recognition  system which aims to reduce the need for  costly and time-consuming adaptation of systems  to new applications, with its capability for  processing texts from widely di#ering domains  and genres. Although the system is still under  development, preliminary results are encouraging,  showing little degradation when processing  texts of lower quality or of unusual types. The  system currently averages 93% precision and  95% recall across a variety of text types.
IR
timm01enterprise
Enterprise Agents Solving Conflicts: The cobac Approach . Autonomous agents seem to be a  promising approach for application in computermediated  supply webs. Supporting the management and  integration of the planning, scheduling, and controlling  processes they can be used as "enterprise delegates".  However, this leads to a problem with common  autonomous agent architectures, as they are not  designed to model complex decision behavior of entire  companies.  In this paper we present an innovative approach to  model Intelligent Agents in the context of the new agent  architecture "Enterprise Agents", which is using  conflicts of interests explicitly to determine agent's  behavior and discuss its application to eBusiness.  Keywords. Agent Control, Conflict Management,  Adaptation, BDI-Architecture, Enterprise Agents  1. 
Agents
533894
Evaluation of Recommender Algorithms for an Internet Information Broker Based Association rules are a widely used technique to generate recommendations in commercial and research recommender systems. Since more and more Web sites, especially of retailers, offer automatic recommender services using Web usage mining, evaluation of recommender algorithms becomes increasingly important. In this paper we first present a framework for the evaluation of different aspects of recommender systems based on the process of discovering knowledge in databases of Fayyad et al. and then we focus on the comparison of the performance of two recommender algorithms based on frequent itemsets. The first recommender algorithm uses association rules, and the other recommender algorithm is based on the repeat-buying theory known from marketing research. For the evaluation we concentrated on how well the patterns extracted from usage data match the concept of useful recommendations of users. We use 6 month of usage data from an educational Internet information broker and compare useful recommendations identified by users from the target group of the broker with the results of the recommender algorithms. The results of the evaluation presented in this paper suggest that frequent itemsets from purchase histories match the concept of useful recommendations expressed by users with satisfactory accuracy (higher than 70%) and precision (between 60% and 90%). Also the evaluation suggests that both algorithms studied in the paper perform similar on real-world data if they are tuned properly.
HCI
gouveia98end
End of First Year Report displays of information (graphs, plots, etc.) are a recent invention at around 1750-1800 [15]. Andrews defines Information Visualisation as the visual presentation of information spaces and structures to facilitate their rapid assimilation and understanding [149]. In the same document, the authors give a collection of Information Visualisation pointers (references also available at http://www.iicm.edu/hci/ivis). A more complete on-line document for Information Visualisation, from Andrews is available at http://www.iicm.edu.hci/ivis/node2.htm. A report on three-dimensional Information Visualisation is given by Young, also available on-line, at http://www.dur.ac.uk/~dcs3py/pages/work/documents/litsurvey /IV-Survey/ [150]. This report gives a visualisation techniques enumeration and a survey of research visualisation systems. Two other Web resources for Information Visualisation are Olive (Online of Information Visualisation Environments - http://otal.umd.edu/Olive/), and the CS348 course...
HCI
wu00selfsupervised
Self-Supervised Learning for Visual Tracking and Recognition of Human Hand Due to the large variation and richness of visual inputs, statistical learning gets more and more concerned in the practice of visual processing such as visual tracking and recognition. Statistical models can be trained from a large set of training data. However, in many cases, since it is not trivial to obtain a large labeled and representative training data set, it would be difficult to obtain a satisfactory generalization. Another difficulty is how to automatically select good features for representation. By combining both labeled and unlabeled training data, this paper proposes a new learning paradigm, selfsupervised learning, to investigate the issues of learning bootstrapping and model transduction. Inductive learning and transductive learning are the two main cases of self-supervised learning, in which the proposed algorithm, Discriminant-EM (D-EM), is a specific learning technique. Vision-based gesture...
IR
wisneski98ambient
Ambient Displays: Turning Architectural Space into an Interface between People and Digital Information . We envision that the architectural space we inhabit will be a new form of interface between humans and online digital information. This paper discusses Ambient Displays: a new approach to interfacing people with online digital information. Ambient Displays present information within a space through subtle changes in light, sound, and movement, which can be processed in the background of awareness. We describe the design and implementation of two example Ambient Displays, the ambientROOM and Ambient Fixtures. Additionally, we discuss applications of Ambient Displays and propose theories of design of such interfaces based on our initial experiences. 1 INTRODUCTION Ambient \Am"bi*ent\, a. Surrounding, encircling, encompassing, and environing. -Oxford English Dictionary Display \Dis*play"\, n. An opening or unfolding; exhibition; manifestation. -Webster's Revised Unabridged Dictionary (1913) Nature is filled with subtle, beautiful and expressive ambient displays that engage ...
HCI
filho98gathering
Gathering User Interface Design Requirements for Social Computing Design for cooperation is a challenge. As designers we note that as we are moving towards the final years of this century, several areas have achieved significant breakthroughs. Among them, it is easy to perceive that areas of Computing and Telecommunications have had an impact of paramount importance to society as a whole. These technologies have allowed an increasing integration of research fields, people of various backgrounds and abilities as well as made the interaction of different cultures possible. As a result, we have been living in the Internet era with a very large number of Web sites which can be visited, queried and played with. That constitutes what we call social computing. Application examples are: digital libraries, health care information systems, Physics collaboratories, and Web-based entertainments like interactive Web games. Within this context, we are concerned with the user interface design requirements gathering for such systems. In that sense, we present a prot...
HCI
soderland01building
Building a Machine Learning Based Text Understanding System Text understanding systems are approaching the  point of being a practical technology as long as the  system is trained for a narrowly defined domain.  Machine learning and statistical approaches can  minimize the effort involved in adapting a text  understanding system to a new domain.  This paper presents a system whose goal is deep  understanding, limited only by the necessity of  designing a formal representation of the target  concepts relevant to the domain. This system is an  advance over previous machine learning based  systems because of its richer output representation,  and an advance over equally expressive text  understanding systems because of its more  extensive use of machine learning.  1 Information Extraction from Free Text  A variety of systems have been developed in recent years that extract information from text. None of them attempts general-purpose understanding, but instead focus on narrowly defined information needs. A domain is defined as a collection of docum...
IR
markatos99caching
On Caching Search Engine Results In this paper we explore the problem of Caching of Search Engine Query Results in order to reduce the computing and I/O requirements needed to support the functionality of a search engine of the world-wide web. Based on traces from search engines we show that there is significant locality in the queries asked, that is, 20-30% of the queries have been previously submitted by the same or a different user. Using trace-driven simulation we show that medium-sized caches can hold most of the frequently-submitted queries. Finally, we propose and evaluate a new cache replacement algorithm named LRU-2S, that takes into account both the frequency and the recency of access to a page when making a replacement decision.
DB
wooldridge98tableaubased
A Tableau-Based Proof Method for Temporal Logics of Knowledge and Belief . In this paper we define two logics, KLn and BLn , and present tableau-based decision procedures for both. KLn is a temporal logic of knowledge. Thus, in addition to the usual connectives of linear discrete temporal logic, it contains a set of unary modal connectives for representing the knowledge  possessed by agents. The logic BLn is somewhat similar; it is a temporal logic that contains connectives for representing the beliefs of agents. In addition to a complete formal definition of the two logics and their decision procedures, the paper includes a brief review of their applications in AI and mainstream computer science, correctness proofs for the decision procedures, a number of worked examples illustrating the decision procedures, and some pointers to further work.  KEYWORDS:Temporal logics of knowledge and belief, theorem proving, tableau. 1 Introduction  This paper presents two logics, called KLn and BLn respectively, and gives tableau-based decision procedures for both. The l...
AI
ginsberg99gib
GIB: Steps Toward an Expert-Level Bridge-Playing Program This paper describes Goren In a Box (gib), the first bridge-playing program to approach the level of a human expert. We give a basic overview of the algorithms used, describe their strengths and weaknesses, and present the results of experiments comparing gib to both human opponents and earlier programs. Introduction Of all the classic games of skill, only card games and Go have yet to see the appearance of serious computer challengers. In Go, this appears to be because the game is fundamentally one of pattern recognition as opposed to search; the brute-force techniques that have been so successful in the development of chess-playing programs have failed almost utterly to deal with Go's huge branching factor. Indeed, the arguably strongest Go program in the world was beaten by Janice Kim in the AAAI-97 Hall of Champions after Kim had given the program a monumental 25 stone handicap. Card games appear to be different. Perhaps because they are games of imperfect information, or perhaps...
ML
thompson99active
Active Learning for Natural Language Parsing and Information Extraction In natural language acquisition, it is difficult to gather the annotated data needed for supervised learning; however, unannotated data is fairly plentiful. Active learning methods attempt to select for annotation and training only the most informative examples, and therefore are potentially very useful in natural language applications. However, existing results for active learning have only considered standard classification tasks. To reduce annotation effort while maintaining accuracy, we apply active learning to two non-classification tasks in natural language processing: semantic parsing and information extraction. We show that active learning can significantly reduce the number of annotated examples required to achieve a given level of performance for these complex tasks. Keywords: active learning, natural language learning, learning for parsing, learning for information extraction Email address of contact author: cthomp@csli.stanford.edu Phone number of contact author: (650)8...
ML
shapiro99snep
SNePS: A Logic for Natural Language Understanding and Commonsense Reasoning The use of logic for knowledge representation and reasoning systems is controversial. There are, indeed, several ways that standard First Order Predicate Logic is inappropriate for modelling natural language understanding and commonsense reasoning. However, a more appropriate logic can be designed. This chapter presents several aspects of such a logic.  1 Introduction  My colleagues, students, and I have been engaged in a long-term project to build a natural language using intelligent agent. While our approach to natural language understanding (NLU) and commonsense reasoning (CSR) has been logic-based, we have thought that the logics developed for metamathematics, e.g. (Kleene, 1950), are not the best ones for our purpose. Instead, we have designed new logics, better suited for NLU and CSR. The current version of these logics constitutes the formal language and inference mechanism of the knowledge representation/reasoning (KRR) system, SNePS 2.4 (Shapiro and The SNePS Implementation Gr...
AI
vandertorre98diagnosis
Diagnosis and Decision Making in Normative Reasoning . Diagnosis theory reasons about incomplete knowledge and only considers the past. It distinguishes between violations and non-violations. Qualitative decision theory reasons about decision variables and considers the future. It distinguishes between fulfilled goals and unfulfilled goals. In this paper we formalize normative diagnoses and decisions in the special purpose formalism  DIO(DE)  2  as well as in extensions of the preference-based deontic logic PDL. The DIagnostic and  DEcision-theoretic framework for DEontic reasoning DIO(DE)  2  formalizes reasoning about violations and fulfillments, and is used to characterize the distinction between normative diagnosis theory and (qualitative) decision theory. The extension of the preference-based deontic logic PDL shows how normative diagnostic and decision-theoretic reasoning -- i.e. reasoning about violations and fulfillments -- can be formalized as an extension of deontic reasoning. 1. Introduction  In the AI and Law literature it is...
AI
jennings00autonomous
Autonomous Agents For Business Process Management : Traditional approaches to managing business processes are often inadequate for large-scale, organisation -wide, dynamic settings. However since Internet and Intranet technologies have become widespread, an increasing number of business processes exhibit these properties. Therefore a new approach is needed. To this end, we describe the motivation, conceptualisation, design and implementation of a novel agent-based business process management system. The key advance of our system is that responsibility for enacting various components of the business process is delegated to a number of autonomous problem solving agents. To enact their role, these agents typically interact and negotiate with other agents in order to coordinate their actions and to buy in the services they require. This approach leads to a system that is significantly more agile and robust than its traditional counterparts. To help demonstrate these benefits, a companion paper describes the application of our system to a ...
Agents
hightower01realtime
Real-Time Error in Location Modeling for Ubiquitous Computing No matter which technologies or techniques a ubiquitous location system  uses, its measurements will have some amount of quantifiable error.
HCI
liu00learning
Learning the Face Space - Representation and Recognition This paper advances an integrated learning and evolutionary computation methodology for approaching the task of learning the face space. The methodology is geared to provide a framework whereby enhanced and robust face coding and classification schemes can be derived and evaluated using both machine and human benchmark studies. In particular we take an interdisciplinary approach, drawing from the accumulated and vast knowledge of both the computer vision and psychology communities, and describe how evolutionary computation and statistical learning can engage in mutually beneficial relationships in order to define an exemplar (Absolute)-Based Coding (ABC) multidimensional face space representation for successfully coping with changing population (face) types, and to leverage past experience for incremental face space definition.  1. Introduction  Among the most challenging tasks for visual form (`shape') analysis and object recognition are understanding how people process and recognize ...
ML
sturm00firstorder
First-order expressivity for S5-models: modal vs. two-sorted languages this paper we are going to prove some results on the expressive power of the standard first-order modal language by comparing it with its extensional counterpart. We thereby restrict our attention to the case where the modal language is interpreted on S5-models. Moreover, we decided to deal exclusively with constant domain models, that is with models in which the domains of all worlds are the same. It is worth mentioning, however, that our method can be applied to logics based on varying domain models as well. Before we describe the results of this paper in more detail, we hasten to add that there exists some work done by other authors to which our results are related. In [8, 9, 10] Fine proves, among other things, a number of preservation results for modal first-order formulas which are relevant for certain philosophical distinctions. For instance, he provides a semantical characterization of de dicto formulas (within S5) and investigates conditions under which de re formulas are eliminable in certain extensions of S5 (see also [6, 18]). In [3] it was shown 2  that a remarkable portion of classical model theory can be transferred to the domain of modal logic. More closely related to our work are [14] and [15]. They discuss a number of formulas from the two-sorted language that are not expressible in the modal language.
DB
pal02web
Web Mining in Soft Computing Framework: Relevance, State of the Art and Future Directions This paper summarizes the different characteristics of web data, the basic components of web mining and its different types, and their current states of the art. The reason for considering web mining, a separate field from data mining, is explained. The limitations of some of the existing web mining methods and tools are enunciated, and the significance of soft computing (comprising fuzzy logic (FL), artificial neural networks (ANNs), genetic algorithms (GAs), and rough sets (RSs) highlighted. A survey of the existing literature on "soft web mining" is provided along with the commercially available systems. The prospective areas of web mining where the application of soft computing needs immediate attention are outlined with justification. Scope for future research in developing "soft web mining" systems is explained. An extensive bibliography is also provided.
ML
filliat00active
Active Perception and Map Learning for Robot Navigation This paper describes a simulated on-line mapping system for robot navigation. This system allows the autonomous creation of topological maps enhanced with metrical information provided by internal (odometry) and external (vision and sonars) sensors. Within such maps, the robot's position is represented and calculated probabilistically according to algorithms that are inspired by Hidden Markov Models. The visual system is very simple and does not allow reliable recognition of speci c places but, used jointly with odometry, sonar recordings and an active perception system, it allows reliable localization even when the robot starts exploring its environment, and when it is passively translated from one place to another. Advantages and drawbacks of the current system are discussed, together with means to remediate the latter.
ML
69807
Regularizing AdaBoost Boosting methods maximize a hard classification margin and are known as powerful techniques that do not exhibit overfitting for low noise cases. Also for noisy data boosting will try to enforce a hard margin and thereby give too much weight to outliers, which then leads to the dilemma of non-smooth fits and overfitting. Therefore we propose three algorithms to allow for soft margin classification by introducing regularization with slack variables into the boosting concept: (1) AdaBoost reg and regularized versions of (2) linear and (3) quadratic programming AdaBoost. Experiments show the usefulness of the proposed algorithms in comparison to another soft margin classifier: the support vector machine.  1 Introduction  Boosting and other ensemble methods have been used with success in several applications, e. g. OCR [12, 7]. For low noise cases several lines of explanation have been proposed as candidates for explaining the well functioning of boosting methods. (a) Breiman proposed that ...
ML
28307
The Case Against Accuracy Estimation for Comparing Induction Algorithms We analyze critically the use of classification accuracy to compare classifiers on natural data sets, providing a thorough investigation using ROC analysis, standard machine learning algorithms, and standard benchmark data sets. The results raise serious concerns about the use of accuracy for comparing classifiers and draw into question the conclusions that can be drawn from such studies. In the course of the presentation, we describe and demonstrate what we believe to be the proper use of ROC analysis for comparative studies in machine learning research. We argue that this methodology is preferable both for making practical choices and for drawing scientific conclusions. 1 INTRODUCTION  Substantial research has been devoted to the development and analysis of algorithms for building classifiers, and a necessary part of this research involves comparing induction algorithms. A common methodology for such evaluations is to perform statistical comparisons of the accuracies of learned class...
ML
531713
Integrating Peer-to-Peer Networking and Computing in the Agentscape Framework The combination of peer-to-peer networking and agentbased computing seems to be a perfect match. Agents are cooperative and communication oriented, while peerto -peer networks typically support distributed systems in which all nodes have equal roles and responsibilities. AgentScape is a framework designed to support large-scale multi-agent systems. Pole extends this framework with peerto -peer computing. This combination facilitates the development and deployment of new agent-based peer-to-peer applications and services.
Agents
273596
Experimental Results with Real-Time Scheduling Using DECAF DECAF [6] is a software toolkit for the rapid design, development, and execution of "intelligent" agents to achieve solutions in complex software systems. From a research community perspective, DECAF provides a modular platform for evaluating and disseminating results in agent architectures, including communication, planning, scheduling, execution monitoring, coordination, diagnosis, and learning. This paper will describe a methodology and results for building and evaluating execution schedules of agent actions in the DECAF architecture. A brief description of DECAF is provided describing how the modular design allows for testing of multiple scheduling algorithms. A description of the types of agents that were used to demonstrate three particular capabilities of DECAF; scalability, parallelism and the threaded nature of the architecture. Lastly, experiments using different scheduling algorithms were utilized to develop an experimental platform for future research in agent scheduling. F...
Agents
starner98realtime
Real-Time American Sign Language Recognition Using Desk and Wearable Computer Based Video Hidden Markov models (HMM's) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe two experiments that demonstrate a realtime HMM-based system for recognizing sentence level American Sign Language (ASL) without explicitly modeling the fingers. The first experiment tracks hands wearing colored gloves and attains a word accuracy of 99%. The second experiment tracks hands without gloves and attains a word accuracy of 92%. Both experiments have a 40 word lexicon.  1 Introduction  While there are many different types of gestures, the most structured sets belong to the sign languages. In sign language, each gesture already has assigned meaning, and strong rules of context and grammar may be applied to make recognition tractable. To date, most work on sign language recognition has employed expensi...
HCI
bertino99conceptual
A Conceptual Annotation Approach to Indexing in a Web-Based Information System All the specialists have agreed that the possibility of adding to multimedia WWW objects some sort of `conceptual ' annotations describing their information content would greatly contribute to solve the problem of their `intelligent ' indexing and retrieval. We propose to associate with the Web objects not the final conceptual annotation, but a simple natural language (NL) caption, in the form of short texts representing a general, neutral description of their informational content. The NL caption will then be converted into a conceptual annotation in NKRL (Narrative Knowledge Representation Language), making use of an automatic translation system like those we have implemented in the context of recent European projects. 1 Introduction Many applications enabled by the WWW, such as distance learning, electronic commerce, information gathering and filtering, have strong need for tools supporting the effective retrieval of information. It is today well recognized that an effective retriev...
IR
okamoto02supporting
Supporting Cross-Cultural Communication with a Large-Screen System As opportunities for international collaboration and crosscultural communication among people from heterogeneous cultures increase, the importance of electronic communication support is increasing. To support cross-cultural communication, we believe it is necessary to offer environments in which participants enjoy conversations, which allow them to share one anothers background and profile visually. We believe that the following three functions are important: (1) showing topics based on participants profiles and cultural background; (2) life-sized, large-screen interface; and, (3) displaying objects which show feelings of identify. In this paper, we discuss the implementation and the empirical evaluation of two systems that were designed to support cross-cultural communication in the real world or between remote locations. From the empirical evaluation of these systems, we conclude that these systems add new functionality to support conversation contents, which may be especially useful in a cross-cultural context where language skills are an issue, and this type of environment may be especially useful in a pre-collaboration context.
HCI
4495
Structure and Performance of Decision Support Algorithms on Active Disks Growth and usage trends for large decision support databases indicate that there is a need for architectures that scale the processing power as the dataset grows. These trends indicate that the processing demand for large decision support databases is growing faster than the improvement in performance of commodity processors. To meet this need, several researchers have recently proposed Active Disk/IDISK architectures which integrate substantial processing power and memory into disk units. In this paper, we examine the utility of Active Disks for decision support databases. We try to answer the following questions. First, is it possible to restructure algorithms for common decision support tasks to utilize Active Disks? Second, how does the performance of Active Disks compare with that of traditional servers for these tasks? Finally, how would Active Disks be integrated into the software architecture of decision support databases? 1 Introduction  Growth and usage trends for large decis...
DB
506324
Algorithms for Temporal Query Operators in XML Databases The contents of an XML database or XML/Web data warehouse is seldom static. New documents  are created, documents are deleted, and more important: documents are updated. In many  cases, we want to be able to search in historical versions, retrieve documents valid at a certain  time, query changes to documents, etc. This can be supported by extending the system with temporal  database features. In this paper we describe the new query operators needed in order to  support an XML query language which supports temporal operations. We also describe the algorithms  which can make efficient implementation of these query operators possible.  Keywords: XML, temporal databases, query processing  1 
DB
slonim00document
Document Clustering using Word Clusters via the Information Bottleneck Method We present a novel implementation of the recently introduced information bottleneck method for unsupervised document clustering. Given a joint empirical distribution of words and documents, p(x; y), we first cluster the words, Y , so that the obtained word clusters, Y_hat , maximally preserve the information on the documents. The resulting joint distribution, p(X; Y_hat ), contains most of the original information about the documents, I(X; Y_hat ) ~= I(X;Y ), but it is much less sparse and noisy. Using the same procedure we then cluster the documents, X , so that the information about the word-clusters is preserved. Thus, we first find word-clusters that capture most of the mutual information about the set of documents, and then find document clusters, that preserve the information about the word clusters. We tested this procedure over several document collections based on subsets taken from the standard 20Newsgroups corpus. The results were assessed by calculating the correlation between the document clusters and the correct labels for these documents. Finding from our experiments show that this double clustering procedure, which uses the information bottleneck method, yields significantly superior performance compared to other common document distributional clustering algorithms. Moreover, the double clustering procedure improves all the distributional clustering methods examined here.
IR
bruni00algebraic
Algebraic Models for Contextual Nets We extend the algebraic approach of Meseguer and Montanari from ordinary place/transition Petri nets to contextual nets, covering both the collective and the individual token philosophy uniformly along the two interpretations of net behaviors.
DB
206485
An Ejection Chain Approach for the Generalized Assignment Problem this paper, we propose an ejection chain approach under the framework of tabu search (TS) for the generalized assignment problem (GAP), which is known to be NP-hard (Sahni and Gonzalez 1976). GAP seeks a minimum cost assignment of n jobs to m agents subject to a resource constraint for each agent. Among various heuristic algorithms developed for GAP are: a combination of the greedy method and local search by Martello and Toth (1981, 1990); a tabu search and simulated annealing approach by Osman (1995); a genetic algorithm by Chu and Beasley (1997); VDS methods by Amini and Racer (1995) and Racer and Amini (1994); a tabu search approach by Laguna et al. (1995) (which is proposed for a generalization of GAP); a set partitioning heuristic by Cattrysse et al. (1994); a relaxation heuristic by Lorena and Narciso (1996); a GRASP and MAX-MIN ant system combined with local search and tabu search by Lourenco and Serra (1998); a linear relaxation heuristic by Trick (1992); and so on. Many exact algorithms have also been proposed (e.g., Nauss 2003, Savelsbergh 1997). A simpler version of an ejection chain approach has also been proposed for the GAP in Laguna et al. (1995). Our ejection chain is based on the idea described in Glover (1997)
ML
setiono99mapping
On Mapping Decision Trees and Neural Networks There exist several methods for transforming decision trees to neural networks. These methods typically construct the networks by directly mapping decision nodes or rules to the neural units. As a result, the networks constructed are often larger than necessary. This paper describes a pruning-based method for mapping decision trees to neural networks, which can compress the network by removing unimportant and redundant units and connections. In addition, equivalent decision trees extracted from the pruned networks are simpler than those induced by well-known algorithms such as ID3 and C4.5. Keywords: decision trees, neural networks, pruning. 1 Introduction Decision trees have been widely used for nonparametric pattern classification tasks which involve several pattern classes and a large number of features. Given an input pattern, the tree classifier performs the comparisons stipulated in each decision node of the tree, and then branches to either the left or the right subtree based on...
ML
eidenbenz00atomi
Atomi - Automated Reconstruction Of Topographic Objects From Aerial Images Using Vectorized Map Information The project ATOMI is a co-operation between the Federal Office of Topography (L+T) and ETH Zurich. The aim of ATOMI is to update vector data of road centerlines and building roof outlines from 1:25,000 maps, fitting it to the real landscape, improve the planimetric accuracy to 1m and derive height information (one representative height for each building) with 1-2 m accuracy. This update should be achieved by using image analysis techniques developed at ETH Zurich and digital aerial imagery. The whole procedure should be implemented as a stand-alone software package, able to import and export data as used at L+T. It should be quasi operational, fast, and the most important reliable. We do not aim at full automation (ca. 80% completeness is a plausible target). The paper will present in detail the aims, input data, strategy and general methods used in ATOMI. We will also present an overview of the results achieved up to now, and problems faced in building and road reconstruction. More de...
DB
kuo00web
Web Document Classification based on Hyperlinks and Document Semantics . Besides the basic content, a web document also contains a set  of hyperlinks pointing to other related documents. Hyperlinks in a document  provide much information about its relation with other web documents.  By analyzing hyperlinks in documents, inter-relationship among  documents can be identified. In this paper, we will propose an algorithm  to classify web documents into subsets based on hyperlinks in documents  and their content. Representativedocuments will also be identified in  each subset based on the proposed similarity definition With the representative  document, searching for related documents can be achieved.  1 Introduction  The WWW is growing through a decentralized process, and documents in the web are lack of logical organization. Besides, the enormous number of web documents make the manipulation and further operation on web documents difficult. Although the size of web document set is large, we need not analyze all the web documents as a whole. Web documents can...
IR
beetz98causal
Causal Models of Mobile Service Robot Behavior Temporal projection, the process of predicting what will happen when a robot executes its plan, is essential for autonomous service robots to successfully plan their missions. This paper describes a causal model of the behavior exhibited by the mobile robot Rhino  when running concurrent reactive plans for performing office delivery jobs. The model represents aspects of robot behavior that cannot be represented by most action models used in AI planning: it represents the temporal structure of continuous control processes, several modes of their interferences, and various kinds of uncertainty. This enhanced expressiveness enables xfrm  (McD92; BM94), a robot planning system, to predict, and therefore forestall, various kinds of behavior flaws including missed deadlines whilst exploiting incidental opportunities. The proposed causal model is experimentally validated using the robot and its simulator.  Introduction  Temporal projection, the process of predicting what will happen when a ro...
AI
schapire98boosting
Boosting and Rocchio Applied to Text Filtering We discuss two learning algorithms for text filtering: modified Rocchio and a boosting algorithm called AdaBoost. We show how both algorithms can be adapted to maximize any general utility matrix that associates cost (or gain) for each pair of machine prediction and correct label. We first show that AdaBoost significantly outperforms another highly effective text filtering algorithm. We then compare AdaBoost and Rocchio over three large text filtering tasks. Overall both algorithms are comparable and are quite effective. AdaBoost produces better classifiers than Rocchio when the training collection contains a very large number of relevant documents. However, on these tasks, Rocchio runs much faster than AdaBoost. 1 Introduction  With the explosion in the amount of information available electronically, information filtering systems that automatically send articles of potential interest to a user are becoming increasingly important. If users indicate their interests to a filtering system...
IR
arisha99impact
Impact: A Platform for Collaborating Agents twork. The Impact server provides the infrastructure upon which different Impact agents can interact. To avoid a performance bottleneck, multiple copies of the server can be replicated and scattered across the network. Impact agents A set of data objects can be represented in a wide variety of ways. When building an application, we'd like to select a data structure that supports the application operations that are the most frequently executed, the most critical, or both. So, any definition of an agent must support such flexible choice of data structures, and agentization must let us extend arbitrary data representations. In Impact, an agent consists of any body of software code whatsoever, with the associated wrapper. Figure 2 shows such an agent's architecture. The software code. The agent's code consists of two parts: . a set of data structures (or data types) manipulated by the agent. For example, if we are building a database agen
Agents
kaljuvee01efficient
Efficient Web Form Entry on PDAs We propose a design for displaying and manipulating HTML forms on small PDA screens. The form input widgets are not shown until the user is ready to fill them in. At that point, only one widget is shown at a time. The form is summarized on the screen by displaying just the text labels that prompt the user for each widget's information. The challenge of this design is to automatically find the match between each text label in a form, and the input widget for which it is the prompt. We developed eight algorithms for performing such label-widget matches. Some of the algorithms are based on n-gram comparisons, while others are based on common form layout conventions. We applied a combination of these algorithms to 100 simple HTML forms with an average of four input fields per form. These experiments achieved a 95% matching accuracy. We developed a scheme that combines all algorithms into a matching system. This system did well even on complex forms, achieving 80% accuracy in our experiments involving 330 input fields spread over 48 complex forms. 
IR
terveen99constructing
Constructing, Organizing, and Visualizing Collections of Topically Related Web Resources For many purposes, the Web page is too small a unit of interaction and analysis. Web sites are structured multimedia documents consisting of many pages, and users often are interested in obtaining and evaluating entire collections of topically related sites. Once such a collection is obtained, users face the challenge of exploring, comprehending, and organizing the items. We report four innovations that address these user needs. • We replaced the web page with the web site as the basic unit of interaction and analysis. • We defined a new information structure, the clan graph, that groups together sets of related sites. • We augment the representation of a site with a site profile, information about site structure and content that helps inform user evaluation of a site. • We invented a new graph visualization, the auditorium visualization, that reveals important structural and content properties of sites within a clan graph. Detailed analysis and user studies document the utility of this approach. The clan graph construction algorithm tends to filter out irrelevant sites and discover additional relevant items. The auditorium visualization, augmented with drill down capabilities to explore site profile data, helps users to find high-quality sites as well as sites that serve a particular function.
HCI
steel02finding
Finding Counterexamples to Inductive Conjectures We present an implementation of a method for £nding counterexamples to universally quanti£ed inductive conjectures in £rst-order logic. Our method uses the proof by consistency strategy to guide a search for a counterexample and a standard £rst-order theorem prover to perform a concurrent check for inconsistency. We explain brie¤y the theory behind the method, describe our implementation, and evaluate results achieved on a variety of incorrect conjectures from various sources. Some work in progress is also presented: we are applying the method to the veri£cation of cryptographic security protocols. In this context, a counterexample to a security property can indicate an attack on the protocol, and our method extracts the trace of messages exchanged in order to effect the attack. This application demonstrates the advantages of the method, in that quite complex side conditions decide whether a particular sequence of messages is possible. Using a theorem prover provides a natural way of dealing with this. Some early results are presented and we discuss future work. 1
ML
maloof99machine
A Machine Learning Researcher's Foray into Recidivism Prediction We discuss an application of machine learning to recidivism prediction. Our initial results motivate the  need for a methodology for technique selection for applications that involve unequal but unknown error  costs, a skewed data set, or both. Evaluation methodologies traditionally used in machine learning are  inadequate for analyzing performance in these situations, although they arise frequently when addressing  real-world problems. After discussing the problem of recidivism prediction and the particulars of our  data set, we present experimental results that motivate the need to evaluate learning algorithm over a  range of error costs. We then describe Receiver Operating Characteristic (ROC) analysis, which has been  used extensively in signal detection theory for decades but has only recently begun to filter into machine  learning research. With this new perspective, we revisit the recidivism prediction task and present results  that contradict those obtained using a traditional ...
ML
kononenko95induction
Induction of decision trees using RELIEFF In the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies between them. Greedy search prevents current inductive machine learning algorithms to detect significant dependencies between the attributes. Recently, Kira and Rendell developed the RELIEF algorithm for estimating the quality of attributes that is able to detect dependencies between attributes. We show strong relation between RELIEF's estimates and impurity functions, that are usually used for heuristic guidance of inductive learning algorithms. We propose to use RELIEFF, an extended version of RELIEF, instead of myopic impurity functions. We have reimplemented Assistant, a system for top down induction of decision trees, using RELIEFF as an estimator of attributes at each selection step. The algorithm is tested on several artificial and several real world problems. Results show the advantage of the presented approach to inductive lea...
ML
488171
Situated Computing: A Paradigm to Enhance the Mobile User's Interaction When people interact with computers, they have to pay attention for things that are not related to the situation of the problems because the interfaces are not contextualized to their working environment. Hence it is sometimes hard to integrate computers as embedded tools, which facilitate users to accomplish their objectives easily in the working life. Situated computing is a new paradigm for mobile computer users based on their physical context and activities carried out in the workspace. It defines the infrastructure how the situated interaction can be provided using applications. In this chapter we first describe a model called situation metaphor to design interaction between the user and mobile computers as the basis for the situated computing. Thereafter, a framework called Situated Information Filing and Filtering (SIFF) is presented as the foundation for situated application development. In general a three-stages schema is given considerting the top stage for situated applications. Four applications based on the SIFF are also presented to demonstrate the enhancement of mobile user's interaction that can be achieved.
HCI
zhao00sfs
SFS Based View Synthesis for Robust Face Recognition Sensitivity to variations in pose is a challenging problem in face recognition using appearance-based methods. More specifically, the appearance of a face changes dramatically when viewing and/or lighting directions change. Various approaches have been proposed to solve this difficult problem. They can be broadly divided into three classes: 1) multiple image based methods where multiple images of various poses per person are available, 2) hybrid methods where multiple example images are available during learning but only one database image per person is available during recognition, and 3) single image based methods where no example based learning is carried out. In this paper, we present a method that comes under class 3. This method based on shape-from-shading (SFS) improves the performance of a face recognition system in handling variations due to pose and illumination via image synthesis. 1 Introduction Face recognition has become one of the most active areas of research in image...
ML
riley00behavior
On Behavior Classification in Adversarial Environments In order for robotic systems to be successful in domains with other agents possibly interfering with the accomplishing of goals, the agents must be able to adapt to the opponents' behavior. The more quickly the agents can respond to a new situation, the better they will perform. We present an approach to doing adaptation which relies on classification of the current adversary into predefined adversary classes. For feature extraction, we present a windowing technique to abstract useful but not overly complicated features. In order to take into account the spatial locality of topological differences, we use a previously developed similarity metric. The feature extraction and classification steps are fully implemented in the domain of simulated robotic soccer, and experimental results are presented.
ML
jonsson01automated
Automated State Abstraction for Options using the U-Tree Algorithm Learning a complex task can be significantly facilitated by defining a  hierarchy of subtasks. An agent can learn to choose between various  temporally abstract actions, each solving an assigned subtask, to accomplish  the overall task. In this paper, we study hierarchical learning using  the framework of options. We argue that to take full advantage of hierarchical  structure, one should perform option-specific state abstraction,  and that if this is to scale to larger tasks, state abstraction should be automated.  We adapt McCallum's U-Tree algorithm to automatically build  option-specific representations of the state feature space, and we illustrate  the resulting algorithm using a simple hierarchical task. Results  suggest that automated option-specific state abstraction is an attractive  approach to making hierarchical learning systems more effective.  1 Introduction  Researchers in the field of reinforcement learning have recently focused considerable attention on temporally abst...
ML
306507
A Behaviour-based Approach to Position Selection for Simulated Soccer Agents Selecting an optimal position for each soccer robot to move to in a robot football game is a challenging and complex task since the environment and robot motion are so dynamic and unpredictable. This paper provides an overview of behaviour-based position selection schemes used by Essex Wizards'99 simulated soccer team, a third place in RoboCup'99 simulator league at Stockholm. The focus concentrates on how each position selection behaviour is selected for individual robot agents. The factors that need to be considered and the architecture used to implement such position selection are also described. Finally the team performance at RoboCup'99 is examined, and future extensions are proposed.  1. Introduction  The Robot World Cup initiative (RoboCup) is an attempt to foster AI and intelligent robotics research by providing a uniform task, the game of soccer [4]. Some of the software technologies include design principles of autonomous agents, multi-agent collaboration, strategy acquisitio...
Agents
escudero00portability
On the Portability and Tuning of Supervised Word Sense Disambiguation Systems This report describes a set of experiments carried out to explore the portability of  alternative supervised Word Sense Disambiguation algorithms. The aim of the work  is threefold: firstly, studying the performance of these algorithms when tested on a  different corpus from that they were trained on; secondly, exploring their ability to  tune to new domains, and thirdly, demonstrating empirically that the LazyBoosting  algorithm outperforms state-of-the-art supervised WSD algorithms in both previous  situations.  Keywords: Word Sense Disambiguation, Machine Learning, Natural Language  Processing, Portability and Tuning of NLP systems.  1 Introduction  Word Sense Disambiguation (WSD) is the problem of assigning the appropriate meaning (sense) to a given word in a text or discourse where this meaning is distinguishable from other senses potentially attributable to that word. As an example, table 1 shows the definition of two senses of the word age  1  and an example sentence  2  for eac...
ML
gomes02versus
Versus: A Web Repository this paper we consider a Web application (or simply an application), as a Versus client with the ability of executing a task through parallel data processing. Therefore each application should be composed by a group of independent threads
DB
whittaker00jotmail
Jotmail: A Voicemail Interface That Enables You to See What Was Said stevew/julia/urs @ research.att.com Voicemail is a pervasive, but under-researched tool for workplace communication. Despite potential advantages of voicemail over email, current phone-based voicemail UIs are highly problematic for users. We present a novel, Web-based, voicemail interface, Jotmail. The design was based on data from several studies of voicemail tasks and user strategies. The GUI has two main elements: (a) personal annotations that serve as a visual analogue to underlying speech; (b) automatically derived message header information. We evaluated Jotmail in an 8-week field trial, where people used it as their only means for accessing voicemail. Jotmail was successful in supporting most key voicemail tasks, although users ' electronic annotation and archiving behaviors were different from our initial predictions. Our results argue for the utility of a combination of annotation based indexing and automatically derived information, as a general technique for accessing speech archives.
IR
386167
Views in a Large Scale XML Repository We are interested in maintaining and querying views in a huge and highly heterogeneous XML  repository (Web scale). In this context, views are very large and there is no apparent limitation  to their size. This raises interesting problems that we address in the paper: (i) how to distribute  views over several machines without having a negative impact on the query translation process; (ii)  how to quickly select the relevant part of a view given a query; (iii) how to minimize the cost of  communicating potentially large queries to the machines where they will be evaluated.  1 Introduction  We believe that XML will soon take an important and increasing share of the data published on the Web. This represents a major opportunity to, at last, provide an intelligent access to this amazing source of information. With that goal in mind, the Xyleme [18] project is building a warehouse which will store and provide sophisticate database-like services over all the XML documents of the Web. Notably...
DB
cicirello01wasp
Wasp Nests for Self-Configurable Factories Agent-based approaches to manufacturing scheduling and control have gained increasing attention in recent years. Such approaches are attractive because they offer increased robustness against the unpredictability of factory operations. But the specification of local coordination policies that give rise to efficient global performance and effectively adapt to changing circumstances remains an interesting challenge. In this paper, we introduce a new approach to this coordination problem, drawing on various aspects of a computational model of how wasp colonies coordinate individual activities and allocate tasks to meet the collective needs of the nest. We focus specifically on the problem of configuring machines in a factory to best satisfy (potentially changing) product demands over time. Our system models the set of jobs queued in front of any given machine as a wasp nest, wherein wasp-like agents interact to form a social hierarchy and prioritize the jobs that they represent. Other was...
Agents
valencia98algebraic
Algebraic Topology for Knowledge Representation in Analogy Solving . We propose a computational model for analogy solving based on a topological formalism of representation. The source and the target analogs are represented as simplexes and the analogy solving is modeled as a topological deformation of these simplexes along a polygonal chain and according to some constraints. We apply this framework to the resolution of IQ-tests typically presented as "given A, B and C, find D such that A is to B what C is to D". 1 Introduction  In this paper, we present a topological framework for knowledge representation based on the concept of simplicial complex. We present then the ESQIMO system which is the application of this framework to an analogy solving problem. The underlying idea developed here is that spatial relationships and more precisely topological relationships such as neighbor, border, dimension, obstruction, deformation, separabitily, path, etc, enable the building and structuration of knowledge representation. More precisely, we explore the possi...
AI
20379
Imitation and Mechanisms of Joint Attention: A Developmental Structure for Building Social Skills on a Humanoid Robot Abstract. Adults are extremely adept at recognizing social cues, such as eye direction or pointing gestures, that establish the basis of joint attention. These skills serve as the developmental basis for more complex forms of metaphor and analogy by allowing an infant to ground shared experiences and by assisting in the development of more complex communication skills. In this chapter, we review some of the evidence for the developmental course of these joint attention skills from developmental psychology, from disorders of social development such as autism, and from the evolutionary development of these social skills. We also describe an on-going research program aimed at testing existing models of joint attention development by building a human-like robot which communicates naturally with humans using joint attention. Our group has constructed an upper-torso humanoid robot, called Cog, in part to investigate how to build intelligent robotic systems by following a developmental progression of skills similar to that observed in human development. Just as a child learns social skills and conventions through interactions with its parents, our robot will learn to interact with people using natural social communication. We further consider the critical role that imitation plays in bootstrapping a system from simple visual behaviors to more complex social skills. We will present data from a face and eye finding system that serves as the basis of this developmental chain, and an example of how this system can imitate the head movements of an individual. 1
AI
sun01individual
Individual Action and Collective Function: from Sociology to Multi-Agent Learning > How do we characterize the process and the dynamics of co-learning, conceptually, mathematically, or computationally?   How do social structures and relations interact with co-learning of multiple agents?  1  And so on.  A key question, however, is as follows, which deserves some discussion here. Adam Smith (1976) put it this way:  He generally, indeed, neither intends to promote the public interest, nor knows how much  he is promoting it..... He intends only his own gain, and he is led by an invisible hand to  promote an end which was not part of his intention.  This paradox have been troubling sociologists and economists for many decades, and now computer scientists and psychologists as well. The issue may be formulated as the apparent gap between the individual intention in deciding his/her own action and the (possibly largely unintended) social function of his/her action. For example, how may self-interested action benet social welfare? Or,
ML
craven98learning
Learning to Extract Symbolic Knowledge from the World Wide Web The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more e ective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach istodevelop a trainable information extraction system that takes two inputs. The rst is an ontology that de nes the classes (e.g., Company, Person, Employee, Product) and relations (e.g., Employed.By, Produced.By) ofinterest when creating the knowledge base. The second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system that has created a knowledge base describing university people, courses, and research projects.
IR
esteban01using
Using linear classifiers in the integration of user modeling and text content analysis in the personalization of a Web-based Spanish News Service Nowadays many newspapers and news agencies offer personalized  information access services and, moreover, there is a growing interest in the  improvement of these services. In this paper we present a methodology useful  to improve the intelligent personalization of news services and the way it has  been applied to a Spanish relevant newspaper: ABC. Our methodology  integrates textual content analysis tasks and machine learning techniques to  achieve an elaborated user model, which represents separately short-term needs  and long-term multi-topic interests. The characterization of a user's interests  includes his preferences about structure (newspaper sections), content and  information delivery. A wide coverage and non-specific-domain classification  of topics and a personal set of keywords allow the user to define his preferences  about content. Machine learning techniques are used to obtain an initial  representation of each category of the topic classification. Finally, we introduce  some details about the Mercurio system, which is being used to implement this  methodology for ABC. We describe our experience and an evaluation of the  system in comparison with other commercial systems.
IR
liu98logical
Logical Semantics and Language for Databases with Partial and Complete Tuples and Sets (Extended Abstract) )  Mengchi Liu  Department of Computer Science University of Regina, Regina, Saskatchewan, Canada S4S 0A2 Email: mliu@cs.uregina.ca Abstract  We discuss the semantics of complex object databases with both partial and complete tuples and sets. We redefine the notion of database to reflect the existence of partial and complete tuples and sets and study how to integrate partial information about tuples and sets spread in the database and check consistency in the meantime. We also present a deductive language RLOG  II  for complex objects with null/unknown and inconsistent values based on Relationlog. The main novel feature of the language is that it is the only one that supports the null extended nested relational algebra operations directly and more importantly recursively. This work provides a firm logical foundation for nested relational and complex object databases that have both partial and complete tuples and sets and solves an open problem of supporting recursion with generic null/...
DB
falk99bubblebadge
The BubbleBadge: A Wearable Public Display We are exploring the design space of wearable computers by designing "public" wearable computer displays. This paper describes our first prototype, the BubbleBadge. By effectively turning the wearer's private display "inside out", the BubbleBadge transforms the wearable computing concept by making digital information public rather than private. User tests showed that the device introduces a new way to interact with information-providing devices, suggesting that it would be valuable to explore the concept further. Keywords Wearable computers, interaction technology, public displays INTRODUCTION A wearable computer is defined as a continuously running, augmenting and mediating computational device [2]. Wearable computers are usually highly private, since both input and output is controlled and seen only by the user, who is effectively "hiding" behind a hand-held keyboard and a head-mounted display. But while wearable computing can be a powerful tool for the single user, there is usuall...
HCI
convey01data
Data Integration Services Introduction  With the prevalence of the network technology and the Internet, access to data independent of its physical storage location has become highly facilitated. This further has enabled users to access a multitude of data sources that are related in some way and to combine the returned data to come up with useful information which is not physically stored in a single place. For instance, a person who has the intension of buying a car can query several car dealer web sites and then compare the results. He can further query a data source which provides information about car reviews to help his decision about the cars he liked. As another example, imagine a company which has several branches in di#erent cities. Each branch has its own local database recording its sales. Whenever global decisions about the company have to be made, each branch database must be queried and the results must be combined. On the other hand, contacting data sources individually and then combining
DB
liu99rol
ROL2: Towards a Real Deductive Object-Oriented Database Language ROL is a strongly typed deductive object-oriented database language. It integrates  many important features of deductive databases and object-oriented databases. However,  it is only a structurally object-oriented language. In this paper, we present our  extension of ROL called ROL2. Most importantly, ROL2 supports behaviorally objectoriented  features such as rule-based methods and encapsulation so that it is a now real  deductive object-oriented database language. It supports in a rule-based framework  nearly all important object-oriented features such as object identity, complex objects,  typing, information hiding, rule-based methods, encapsulation of such methods, overloading,  late binding, polymorphism, class hierarchies, multiple structural and behavioral  inheritance with overriding, blocking, and conict handling. It is so far the only  deductive system that supports all these features in a pure rule-based framework.  Keywords: object-oriented databases, deductive databases, ...
DB
taylor99virtual
Virtual Keyboards This paper describes a novel scheme for vision-based human computer interaction in  which traditional input and output devices, monitors, keyboards and mice, are replaced with  augmented reality displays, projection systems and cameras. User input is accomplished by  projecting an image of the interface onto a flat surface in the scene which is monitored with  a video camera. The scheme hinges on the observation that the relationship between the  three surfaces of interest, the work surface, the virtual keyboard and the image obtained by  the camera, can be characterized by projective transformations of RP  2  . This observation  leads to a fast and accurate online calibration algorithm.  The basic advantage of the vision based interaction techniques proposed in this paper  is that they do not involve mechanical input devices such as keyboards, mice and touch  screens. There are no moving parts and no wires to connect to the interface surface. By  avoiding a physical instantiation of t...
HCI
fink99adaptable
Adaptable and Adaptive Information Provision for All Users, Including Disabled and Elderly People Due to the tremendously increasing popularity of the World-Wide Web, hypermedia is going to be the leading online information medium for some years to come and will most likely become the standard gateway for citizens to the "information highway". Already today, visitors of web sites are generally heterogeneous and have different needs, and this is likely to increase in the future. The aim of the AVANTI project is to cater hypermedia information to these individual needs by adapting the content and the presentation of web pages to each individual user. The special needs of elderly and disabled users are also partly considered. A model of the characteristics of user groups, individual users and usage environments, and a domain model are exploited in the adaptation process. One aim of this research is to verify that adaptation and user modeling techniques that were hitherto mostly used for catering interactive software systems to able-bodied users also prove useful for adaptation to users with special needs. Another original aspect is the development of a network-wide user modeling server that can concurrently accommodate the user modeling needs of several applications and several instances of an application within a distributed computing environment.
HCI
390217
Autonomous Robot that Uses Symbol Recognition and Artificial Emotion to Attend the AAAI Conference This paper describes our approach in designing an autonomous  robot for the AAAI Mobile Robot Challenge,  making the robot attend the National Conference on AI.  The goal was to do a simplified version of the whole  task, by integrating methodologies developed in various  research projects conducted in our laboratory. Original  contributions are the use of a symbol recognition technique  to make the robot read signs, artificial emotion for  expressing the state of the robot in the accomplishment  of its goals, a touch screen for human-robot interaction,  and a charging station for allowing the robot to recharge  when necessary. All of these aspects are influenced by  the different steps to be followed by the robot attendee  to complete the task from start-to-end.  Introduction  LABORIUS is a young research laboratory interested in designing autonomous systems that can assist human in real life tasks. To do so, robots need some sort of "social intelligence ", giving them the ability to ...
Agents
shanmugasundaram99efficient
Efficient Concurrency Control for Broadcast Environments A crucial consideration in environments where data is broadcast to clients is the low bandwidth available for clients to communicate with servers. Advanced applications in such environments do need to read data that is mutually consistent aswell as current. However, given the asymmetric communication capabilities and the needs of clients in mobile environments, traditional serializability-based approaches are too restrictive, unnecessary, and impractical. We thus propose the use of a weaker correctness criterion called update consistency and outline mechanisms based on this criterion that ensure (1) the mutual consistency of data maintained by the server and read by clients, and (2) the currency of data read by clients. Using these mechanisms, clients can obtain data that is current and mutually consistent "off the air", i.e., without contacting the server to, say, obtain locks. Experimental results show a substantial reduction in response times as compared to existing (serializability-based) approaches. A further attractive feature of the approach is that if caching is possible at a client, weaker forms of currency can be obtained while still satisfying the mutual consistency of data.  
DB
533620
Ensemble Learning for Intrusion Detection in Computer Networks The security of computer networks plays a strategic role in modern computer systems. In order to enforce high protection levels against threats, a number of software tools are currently developed. Intrusion Detection Systems aim at detecting intruder who eluded the "first line" protection. In this paper, a pattern recognition approach to network intrusion detection based on ensemble learning paradigms is proposed. The potentialities of such an approach for data fusion and some open issues are outlined.
AI
schattenberg00planning
Planning Agents in James Abstract — Testing is an obligatory step in developing multi-agent systems. For testing multi-agent systems in virtual, dynamic environments, simulation systems are required that support a modular, declarative construction of experimental frames, that facilitate the embeddence of a variety of agent architectures, and that allow an efficient parallel, distributed execution. We introduce the system James (A Java-Based Agent Modeling Environment for Simulation). In James agents and their dynamic environment are modeled as reflective, time triggered state automata. Its possibilities to compose experimental frames based on predefined components, to express temporal interdependencies, to capture the phenomenon of pro-activeness and reflectivity of agents are illuminated by experiments with planning agents. The underlying planning system is a general purpose system, about which no empirical results exist besides traditional static benchmark tests. We analyze the interplay between heuristics for selecting goals, viewing range, commitment strategies, explorativeness, and trust in the persistence of the world and uncover properties of the agent, the planning engine and the chosen test scenario: Tileworld. I.
Agents
447337
A Multi-Agent Architecture for Intelligent Tutoring One of the most interesting realm among those ones  brought up to success by the development of the Internet is Distance Learning. A key issue in such a field is the development of systems for supporting Tutoring activities. This paper is concerned with the presentation of an innovative architecture for Intelligent Tutoring  which make use of Software Agents. The way in which the knowledge is represented and stored is discussed together with the ability of our system to manage individual learning paths for different users. The rationale for using Agents is presented and the implementation of the system is discussed.  1. 
AI
schweiss99architecture
An Architecture to Guide Crowds Using a Rule-Based Behavior System This paper describes a Client/Server architecture to combine the  control of human agents performing "intelligent actions" (guided by a Rule-Based Behavior System -- RBBS) with the management  of autonomous crowds which perform pre-programmed actions.  Our main goal being ability to model crowds formed by a large  number of agents (e.g. 1000), we have used pre-programmed  actions and basic behaviors. In addition, RBBS provides the user with an interface for real-time behavior control of some groups of  the crowd. This paper presents how the Server application deals  with virtual human agent's behaviors using a rule-based system.  Keywords  Multi-agent co-ordination and collaboration, agent architectures,  network agents, real-time performance, synthetic agents, rulebased  system, human crowds' model.  1. INTRODUCTION  Virtual humans grouped together to form crowds populating virtual worlds allow a more intuitive feeling of presence. However, the crowd is not only needed to create an at...
Agents
pelikan99parameterless
Parameter-less Genetic Algorithm: A Worst-case Time and Space Complexity Analysis In this paper, the worst-case analysis of the time and space complexity of the parameter-less genetic algorithm versus the genetic algorithm with an optimal population size is provided and the results of the analysis are discussed. Since the assumptions in order for the analysis to be correct are very weak, the result is applicable to a wide range of problems. Various configurations of the parameter-less genetic algorithm are considered and the results of their time and space complexity are compared. 1 Introduction  A parameter-less genetic algorithm (Harik & Lobo, 1999) is an alternative to a common trialand -error method of tweaking the values of the parameters of the genetic algorithm in order to find a set-up to accurately and reliably solve a given problem. The algorithm manages a number of independent runs of the genetic algorithm with different population sizes with the remaining parameters set to fixed values according to the theory of genetic algorithms' control maps introduce...
ML
mcroy95repair
The Repair of Speech Act Misunderstandings by Abductive Inference this paper, we have concentrated on the repair of mis-understanding. Our colleagues Heeman and Edmonds have looked at the repair of non-understanding. The difference between the two situations is that in the former, the agent derives exactly one interpretation of an utterance and hence is initially unaware of any problem; in the latter, the agent derives either more than one interpretation, with no way to choose between them, or no interpretation at all, and so the problem is immediately apparent. Heeman and Edmonds looked in particular at cases in which a referring expression uttered by one conversant was not understood by the other (Heeman and Hirst 1995; Edmonds 1994; Hirst et. al. 1994). Clark and his colleagues (Clark and Wilkes-Gibbs 1986; Clark 1993) have shown that in such situations, conversants will collaborate on repairing the problem by, in effect, negotiating a reconstruction or elaboration of the referring expression. Heeman and Edmonds model this with a plan recognition and generation system that can recognize faulty plans and try to repair them. Thus (as in our own model) two copies of the system can converse with each other, negotiating referents of referring expressions that are not understood by trying to recognize the referring plans of the other, repairing them where necessary, and presenting the new referring plan to the other for approval.
Agents
fox99sentinel
SENTINEL: A Multiple Engine Information Retrieval and Visualization System We describe a prototype Information Retrieval system, SENTINEL, under development at Harris Corporation's Information Systems Division. SENTINEL is a fusion of multiple information retrieval technologies, integrating n-grams, a vector space model, and a neural network training rule. One of the primary advantages of SENTINEL is its 3-dimenstional visualization capability that is based fully upon the mathematical representation of information within SENTINEL. This 3-dimensional visualization capability provides users with an intuitive understanding, with relevance feedback/query refinement techniques that can be better utilized, resulting in higher retrieval accuracy (precision).
DB
queloz99foreign
Foreign Event Handlers to Maintain Information Consistency and System Adequacy this paper is to describe novel applications of Mobile Code technology which have not appeared yet but should be feasible with our current knowledge of the domain. These new applications contradict the often-made observation that Mobile Code is just another technique that does not really bring much more possibilities than existing technologies for distributed applications. There is a whole class of problems that have not received much attention yet and that are not well managed by current environments. These are the problems of maintaining consistency of dynamic information and maintaining systems in adequacy with the ever changing requirements of customers. Our motivation is that, besides the quantitative improvements that most people expect from using Mobile Code, there is also a qualitative benefit which is even more important but not universally recognized now: Mobile Code allows communication with less conventions than message passing [5, 3]. Processes interconnected by Mobile Code still have to agree on high level encoding and synchronization primitives but these agreements are only a fraction of what is necessary to communicate. Many context dependent aspects can be encapsulated inside Mobile Code and changed when the context changes. Encapsulation has the same benefits here as in other software engineering domains: it reduces the dependency between components, thus reducing the number of modifications that we must make to software in order to adapt it to new requirements. For this reason we think that it is the best way to cope with systems that are distributed, hence not manageable by a single person or organization; that are dynamic, because the information they contain must change when the world itself changes; and that are evolving since the users discover n...
Agents
wilhelm99introducing
Introducing Trusted Third Parties to the Mobile Agent Paradigm Abstract. The mobile agent paradigm gains ever more acceptance for the creation of distributed applications, particularly in the domain of electronic commerce. In such applications, a mobile agent roams the global Internet in search of services for its owner. One of the problems with this approach is that malicious service providers on the agent's itinerary can access con dential information contained in the agent or tamper with the agent. In this article we identify trust as a major issue in this context and propose a pessimistic approach to trust that tries to prevent malicious behaviour rather than correcting it. The approach relies on a trusted and tamper-resistant hardware device that provides the mobile agent with the means to protect itself. Finally, weshow that the approach is not limited to protecting the mobile agents of a user but can also be extended to protect the mobile agents of a trusted third party inorder to take fulladvantage of the mobile agent paradigm. 1
Agents
wersing00competitive
A Competitive Layer Model for Feature Binding and Sensory Segmentation We present a recurrent neural network for feature binding and sensory segmentation,  the competitive layer model (CLM). The CLM uses topographically structured  competitive and cooperative interactions in a layered network to partition a set of input  features into salient groups. The dynamics is formulated within a standard additive recurrent  network with linear threshold neurons. Contextual relations among features are  coded by pairwise compatibilities which define an energy function to be minimized by  the neural dynamics. Due to the usage of dynamical winner-take-all circuits the model  gains more flexible response properties than spin models of segmentation by exploiting  amplitude information in the grouping process. We prove analytic results on the convergence  and stable attractors of the CLM, which generalize earlier results on winner-take-all networks, and incorporate deterministic annealing for robustness against local  minima. The piecewise linear dynamics of the CLM allows a linear eigensubspace  analysis which we use to analyze the dynamics of binding in conjunction with annealing.  For the example of contour detection we show how the CLM can integrate  figure-ground segmentation and grouping into a unified model.  
AI
vincent01agent
An Agent Infrastructure to Build and Evaluate Multi-Agent Systems: The Java Agent Framework and Multi-Agent System Simulator . In this paper, we describe our agent framework and address
Agents
arai01multiagent
Multiagent Systems Specification by UML Statecharts Aiming at Intelligent Manufacturing Multiagent systems are a promising new paradigm in computing,  which are contributing to various fields. Many theories and technologies  have been developed in order to design and specify multiagent systems,  however, no standard procedure is used at present. Industrial applications  often have a complex structure and need plenty of working  resources. They require a standard specification method as well. As the  standard method to design and specify software systems, we believe that  one of the key words is simplicity for their wide acceptance. In this paper,  we propose a method to specify multiagent systems, namely with UML  statecharts. We use them for specifying almost all aspects of multiagent  systems, because we think that it is an advantage to keep everything in  one type of diagram. We apply
Agents
florescu99performance
A Performance Evaluation of Alternative Mapping Schemes for Storing XML Data in a Relational Database XML is emerging as one of the dominant data formats for data processing on the Internet. To query  XML data, query languages likeXQL, Lorel, XML-QL, or XML-GL have been proposed. In this paper,  we study how XML data can be stored and queried using a standard relational database system. For this  purpose, we present alternative mapping schemes to store XML data in a relational database and discuss  how XML-QL queries can be translated into SQL queries for every mapping scheme. We present the  results of comprehensive performance experiments that analyze the tradeo#s of the alternative mapping  schemes in terms of database size, query performance and update performance. While our discussion  is focussed on XML and XML-QL, the results of this paper are relevant for most semi-structured data  models and most query languages for semi-structured data.  1 Introduction  It has become clear that not all applications are met by the relational, object-relational, or object-oriented data models. ...
DB
115390
Aspects of Interface Agents: Avatar, Assistant and Actor This paper introduces the interface agent research project being carryied out at ATR Media Integration & Communications Research Labs. We are interested in virtual interface agents to support human creative activities by mediating between human users and computer cyberspace. In this paper, we categorize interface agents into three types; avatars, assistants and actors based on their functionality and discuss a design framework of agents and related applications. We present the current status and objectives for the following topics; design of an asynchronous-hierarchical agent architecture, a character locomotion design tool, applications in a virtualized museum and a group discussion environment in which virtual agents would inhabit and act.  1 Introduction  A "virtual interface agent," or interface agent, is defined in this paper as an autonomous agent which mediates between a human user and computer cyberspace. An interface agent differs from an ordinary interface since it is expecte...
Agents
527578
Text-Based Content Search and Retrieval in ad hoc P2P Communities We consider the problem of content search and retrieval in peer-to-peer (P2P) communities. P2P computing is a potentially powerful model for information sharing between ad hoc groups' of users because of its' low cost of entry and natural model for resource scaling with community size. As P2P communities grow in size, however, locating information distributed across the large number of peers becomes problematic. We present a distributed text-based content search and retrieval algorithm to address this' problem. Our algorithm is' based on a state-of-the-art text-based document ranking algorithm: the vector-space model instantiated with the TFxlDF ranking rule. A naive application of TFxlDF wouM require each peer in a community to collect an inverted index of the entire community. This' is' costly both in terms of bandwidth and storage. Instea & we show how TFxlDF can be approximated given compact summaries of peers' local inverted indexes. We make three contributions: (a) we show how the TFxlDF rule can be adapted to use the index summaries, (b) we provide a heuristic for adaptively determining the set of peers that shouM be contacted for a query, and (c) we show that our algorithm tracks' TFxlDF's performance very closely, regardless of how documents' are distributed throughout the community. Furthermore, our algorithm preserves the main flavor of TFxlDF by retrieving close to the same set of documents for any given query.
IR
85838
Improving Data Driven Wordclass Tagging by System Combination In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.  Introduction  In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations. Traditionally, these models were categorized as either rule-based/symbolic or corpusbased /probabilistic. Recent ...
AI
133494
Saccadic Search with Gabor features applied to Eye Detection and Real-Time Head Tracking The Gabor decomposition is a ubiquitous tool in computer vision. Nevertheless, it is generally considered computationally demanding for active vision applications. We suggest an attention-driven approach to feature detection inspired by the human saccadic system. A dramatic speedup is achieved by computing the Gabor decomposition only on the points of a sparse retinotopic grid. An off-line eye detection application and a real-time head localisation and tracking system are presented. The real-time system features a novel eyeball-mounted camera designed to simulate the dynamic performance of the human eye and is, to the best of our knowledge, the first example of active vision system based on the Gabor decomposition.
ML
nahm00mutually
A Mutually Beneficial Integration of Data Mining and Information Extraction Text mining concerns applying data mining techniques to unstructured  text. Information extraction (IE) is a form of shallow  text understanding that locates specific pieces of data  in natural language documents, transforming unstructured  text into a structured database. This paper describes a system  called DISCOTEX, that combines IE and data mining  methodologies to perform text mining as well as improve  the performance of the underlying extraction system. Rules  mined from a database extracted from a corpus of texts are  used to predict additional information to extract from future  documents, thereby improving the recall of IE. Encouraging  results are presented on applying these techniques to a corpus  of computer job announcement postings from an Internet  newsgroup.  
ML
sloman02how
How Many Separately Evolved Emotional Beasties Live Within Us? A problem which bedevils the study of emotions, and the study of consciousness, is that we assume a shared understanding of many everyday concepts, such as `emotion', `feeling', `pleasure', `pain', `desire', `awareness', etc. Unfortunately, these concepts are inherently very complex, ill-defined, and used with different meanings by different people. Moreover this goes unnoticed, so that people think they understand what they are referring to even when their understanding is very unclear. Consequently there is much discussion that is inherently vague, often at cross-purposes, and with apparent disagreements that arise out of people unwittingly talking about different things. We need a framework which explains how there can be all the diverse phenomena that different people refer to when they talk about emotions and other affective states and processes. The conjecture on which this paper is based is that adult humans have a type of information-processing architecture, with components whi...
HCI
197022
Summarizing Text Documents: Sentence Selection and Evaluation Metrics Human-quality text summarization systems are difficult to design, and even more difficult to evaluate, in part because documents can differ along several dimensions, such as length, writing style and lexical usage. Nevertheless, certain cues can often help suggest the selection of sentences for inclusion in a summary. This paper presents our analysis of news-article summaries generated by sentence selection. Sentences are ranked for potential inclusion in the summary using a weighted combination of statistical and linguistic features. The statistical features were adapted from standard IR methods. The potential linguistic ones were derived from an analysis of news-wire summaries. Toevaluate these features we use a normalized version of precision-recall curves, with a baseline of random sentence selection, as well as analyze the properties of such a baseline. We illustrate our discussions with empirical results showing the importance of corpus-dependent baseline summarization standards, compression ratios and carefully crafted long queries.
IR
labrou01standardizing
Standardizing Agent Communication An Agent Communication Language (ACL) is a collection of  speech-act-like message types, with agreed-upon semantics, which facilitate  the knowledge and information exchange between software agents.
Agents
oviatt99ten
Ten Myths of Multimodal Interaction  
HCI
22358
Maintaining Temporal Views Over Non-Temporal Information Sources For Data Warehousing An important use of data warehousing is to provide temporal views over the history of source  data that may itself be non-temporal. While recent work in view maintenance is applicable to  data warehousing, only non-temporal views have been considered. In this paper, we introduce  a framework for maintaining temporal views over non-temporal information sources in a data  warehousing environment. We describe an architecture for the temporal data warehouse that  automatically maintains temporal views over non-temporal source relations, and allows users  to ask temporal queries using these views. Because of the dimension of time, a materialized  temporal view may need to be updated not only when source relations change, but also as time  advances. We present incremental techniques to maintain temporal views for both cases, and  outline the implementation of our approach in the WHIPS warehousing prototype at Stanford.  1 Introduction  A data warehouse is a repository for efficient querying ...
DB
bilgic97system
Systems Management In Concurrent Engineering Using Intelligent Software Agents Intelligent software agents are used in frameworks where large number of experts need to interact in a project concurrently as in the projects taken up by the aerospace industry. We describe one such framework and discuss an intelligent software agent to manage the systems design in such an environment. What makes the problem interesting is the existence of other intelligent agents in the framework that are responsible for various other tasks as well as other human users. Our Systems Design Management Agent (SDMA) uses its own domain knowledge to interact with the other agents and recommend strategies and policies. We take one example task of systems design management, risk management, and discuss how it is performed by the SDMA in detail. 1 Introduction Using intelligent software agents within the concurrent engineering paradigm has received attention from several research groups. The idea is to respond to the increased information and coordination demands of concurrent engineering p...
Agents
fox98active
Active Markov Localization for Mobile Robots Localization is the problem of determining the position of a mobile robot from sensor data. Most existing localization approaches are passive, i.e., they do not exploit the opportunity to control the robot's effectors during localization. This paper proposes an active localization approach. The approach is based on Markov localization and provides rational criteria for (1) setting the robot's motion direction (exploration), and (2) determining the pointing direction of the sensors so as to most efficiently localize the robot. Furthermore, it is able to deal with noisy sensors and approximative world models. The appropriateness of our approach is demonstrated empirically using a mobile robot in a structured office environment. Key words: Robot Position Estimation, Autonomous Service Robots 1 Introduction To navigate reliably in indoor environments, a mobile robot must know where it is. Over the last few years, there has been a tremendous scientific interest in algorithms for estimating ...
ML
bartoli01three
Three New Algorithms for Projective Bundle Adjustment with Minimum Parameters Bundle adjustment is a technique used to compute the maximum likelihood estimate of structure and motion from image feature correspondences. It practice, large non-linear systems have to be solved, most of the time using an iterative optimization process starting from a sub-optimal solution obtained by using linear methods. The behaviour, in terms of convergence, and the computational cost of this process depend on the parameterization used to represent the problem, i.e. of structure and motion.
AI
30237
Non-Supervised Sensory-Motor Agents Learning This text discusses a proposal for creation and destruction of neurons based on the sensory-motor activity. This model, called sensory-motor schema, is used to define a sensory-motor agent as a collection of activity schemata. The activity schema permits a useful distribution of neurons in a conceptual space, creating concepts based on action and sensation. Such approach is inspired in the theory of the Swiss psychologist and epistemologist Jean Piaget, and intends to make explicit the account of the processes of continuous interaction between sensory-motor agents and their environments when agents are producing cognitive structures. 1. Introduction  The notion of an autonomous agent plays a central role in contemporaneous research on Artificial Intelligence [3]. Cognitive agents are based on symbolic processing mechanisms. Reactive agents are based on alternative computational mechanisms like neural networks, analogic processing, etc. The alternative approach using autonomous agents b...
Agents
374208
Using Context as a Crystal Ball: Rewards and Pitfalls This paper discusses some of the potential rewards and pitfalls that can await designers wishing to incorporate context-awareness [Schilit,94][Brown,97] into interactive systems. Many of the issues are described in anecdotal form, based on our experiences developing and evaluating the context-aware GUIDE system [Cheverst,99][Cheverst,00].
HCI
46858
Simultaneous Proxy Evaluation The Simultaneous Proxy Evaluation (SPE) architecture is designed to evaluate multiple web proxies in parallel using object requests which are duplicated and passed to each proxy. The SPE architecture reduces problems of unrealistic test environments, dated and/or inappropriate workloads, and is additionally applicable to contentbased prefetching proxies. It is intended to measure byte and object hit rates, client-perceived latencies, and cache consistency. We characterize a space of proxy evaluation methodologies and place this architecture within it. 1 Introduction This paper presents a new architecture for the evaluation of proxy caches. Initially, it grew out of research in techniques for prefetching in web caches. In particular, we found that existing mechanisms for the evaluation of proxy caches were not well suited to prefetching systems. Objective evaluation is paramount to all research, whether applied or academic. Since this is certainly relevant when exploring various approac...
DB
bjrk00powerview
POWERVIEW - Using information links and information views to navigate and visualize information on small displays . PowerView is a PDA application designed to support people with  situational information, primarily during conversations and meetings with other  people. PowerView was designed to address a number of issues in interface  design concerning both information visualization and interaction on small,  mobile devices. In terms of information visualization, the system was required to  provide the user with a single integrated information system that enabled quick  access to related information once an object of interest had been selected. In  terms of interaction, the system was required to enable easy and efficient  information retrieval, including single-handed use of the device. These problems  were addressed by introducing Information Links and Information Views. An  evaluation of the application against the standard application suite bundle of the  PDA, a Casio Cassiopeia E-11, proved the interfaces equivalent in usability even  though the PowerView application uses a novel interface par...
HCI
453199
Learning Markov Processes this article, we restrict our attention to discrete time dynamical systems.) Typically we do not know the exact dynamics of the system, so instead we consider a probabilistic state transition function: P (X t+1 jX t ). Such a probabilistic formulation will be particularly useful when we try to learn the model from data. The state space,  might be discrete (nite) or continuous (innite). For example, we might just try to predict the probability that a stock goes up or down, in which case  = f"; #g; more ambitiously, we might try to predict its expected value, in which case  = IR. In general, the state is a vector of state variables, which we can partition into three kinds: input variables (ones which we can control), output variables (ones which we can observe), and hidden or latent variables (internal variables which we cannot directly control or observe); we shall denote these by U t , Y t and X t  respectively. See Figure 1. In this article, we shall consider how to learn models of this kind. We start by considering the special case in    To be published in The Encyclopedia of Cognitive Science, Macmillan, 2002 1  X1 X2 X3 Y1 Y2 Y3 U1 U2  . . . Figure 1: A generic discrete-time dynamical system represented as a dynamic Bayesian network (DBN) (see BAYESIAN BELIEF NETWORKS for a denition). U t is the input, X t is the hidden state, and Y t is the output. Shaded nodes are observed, clear nodes are hidden. Square nodes are xed inputs (controls), round nodes are random variables. Notice how what we see, Y t , may depend on the actions that we take, U t : this can be used to model active perception. X1 X2 X3 X4  . . . X1 X2 X2 X3 X3 X4  Figure 2: Converting a second order Markov model (top) into a rst order Markov model (bottom). which all variables are observed (i.e....
ML
pitoura99exploiting
Exploiting Versions for Handling Updates in Broadcast Disks Recently, broadcasting has attracted considerable attention as a means of disseminating information to large client populations in both wired and wireless settings. In this paper, we exploit versions to increase the concurrency of client transactions in the presence of updates. We consider three alternative mediums for storing versions: (a) the air: older versions are broadcast along with current data, (b) the client's local cache: older versions are maintained in cache, and (c) a local database or warehouse at the client: part of the server's database is maintained at the client in the form of a multiversion materialized view. The proposed techniques are scalable in that they provide consistency without any direct communication from clients to the server. Performance results show that the overhead of maintaining versions can be kept low, while providing a considerable increase in concurrency.   1 Introduction  While traditionally data are delivered from servers to clients on demand, a...
DB
444362
Knowledge-based Wrapper Generation by Using XML Information extraction is the process of recognizing  the particular fragments of a document that constitute  its core semantic content. However, most  previous information extraction systems were not  effective for real-world information sources due to  difficulties in acquiring and representing useful domain  knowledge and in dealing with structural heterogeneity  inherent in different sources.  In order to resolve these problems, this paper  proposes a scheme of knowledge-based wrapper  generation for semi-structured and labeled documents.  The implementation of an agent-oriented information  extraction system, XTROS, is described.  In contrast with previous wrapper learning agents,  XTROS represents both the domain knowledge and  the wrappers by XML documents to increase modularity,  flexibility, and interoperability among multiple  parties. XTROS also facilitates simpler implementation  of the wrapper generator by exploiting  XML parsers and interpreters. XTROS shows  good performance on several Web sites in the domain  of real estates, and it is expected to be easily  adaptable to different domains by plugging in appropriate  XML-based domain knowledge.  1 
IR
nrvg99persistent
The Persistent Cache: Improving OID Indexing in Temporal Object-Oriented Database Systems In a temporal OODB, an OID index (OIDX) is needed to map from OID to the physical location of the object. In a transaction time temporal OODB, the OIDX should also index the object versions. In this case, the index entries, which we call object descriptors (OD), also include the commit timestamp of the transaction that created the object version. The OIDX in a non-temporal OODB only needs to be updated when an object is created, but in a temporal OODB, the OIDX have to be updated every time an object is updated. We have in a previous study shown that this can be a potential bottleneck, and in this report, we present the Persistent Cache (PCache), a novel approach which reduces the index update and lookup costs in temporal OODBs. In this report, we develop a cost model for the PCache, and use this to show that the use of a PCache can reduce the average access cost to only a fraction of the cost when not using the PCache. Even though the primary context of this report is OID indexing in ...
DB
calvanese01description
Description Logics for Information Integration Information integration is the problem of combining the data residing at different, heterogeneous sources, and providing the user with a unified view of these data, called mediated schema. The mediated schema is therefore a reconciled view of the information, which can be queried by the user. It is the task of the system to free the user from the knowledge on where data are, and how data are structured at the sources.
DB
ware00reaching
Reaching for Objects in VR Displays: Lag and Frame Rate This article reports the results from three experimental studies of reaching behavior in a head-coupled stereo display system with a hand-tracking subsystem for object selection. It is found that lag in the head-tracking system is relatively unimportant in predicting performance, whereas lag in the hand-tracking system is critical. The effect of hand lag can be modeled by means of a variation on Fitts ’ Law with the measured system lag introduced as a multiplicative variable to the Fitts ’ Law index of difilculty. This means that relatively small lags can cause considerable degradation in performance if the targets are small. Another finding is that errors are higher for movement in and out of the screen, as compared to movements in the plane of the screen, and there is a small (10’%) time penalty for movement in the Z direction in all three experiments. Low frame rates cause a degradation in performance; however, this can be attributed to the lag which is caused by low frame rates, particularly if double buffering is used combined with early sampling of the hand-tracking device.
HCI
89384
Scalability In Formal Concept Analysis This paper presents the results of experiments carried out with a set of 4,000 medical discharge summaries in which were recognised 1,962 attributes from the Unified Medical Language System (UMLS). In this domain, the objects are medical documents (4,000) and the attributes are UMLS terms extracted from the documents (1,962). When Formal Concept Analysis is used to iteratively analyse and visualize this data, complexity and scalability become critically important. Although the amount of data used in this experiment is small compared with the size of primary memory in modern computers, the results are still important since the probability distributions which determine the efficiencies are likely to remain stable as the size of the data is increased. Our work presents two outcomes, firstly we present a methodology for exploring knowledge in text documents using Formal Concept Analysis by employing conceptual scales created as the result of direct manipulation of a line diagram. The conceptual scales lead to small derived purified contexts that are represented using nested line diagrams. Secondly, we present an algorithm for the fast determination of purified contexts from a compressed representation of the large formal context. Our work draws on existing encoding and compression techniques to show how rudimentary data analysis can lead to substantial efficiency improvements to knowledge visualisation. c fl 1993 Blackwell Publishers, 238 Main Street, Cambridge, MA 02142, USA, and 108 Cowley Road, Oxford, OX4 1JF, UK. SCALABILITY IN FORMAL CONCEPT ANALYSIS 3
AI
cho00synchronizing
Synchronizing a database to Improve Freshness In this paper we study how to refresh a local copy of an autonomous data source to maintain the copy up-to-date. As the size of the data grows, it becomes more difficult to maintain the copy "fresh," making it crucial to synchronize the copy effectively. We define two freshness metrics, change models of the underlying data, and synchronization policies. We analytically study how effective the various policies are. We also experimentally verify our analysis, based on data collected from 270 web sites for more than 4 months, and we show that our new policy improves the "freshness" very significantly compared to current policies in use.
DB
demeer00programmable
Programmable Agents for Flexible QoS Management in IP Networks Network programmability seems to be a promising solution to network management and quality of service (QoS) control. Software mobile-agents technology is boosting the evolution toward application-level control of network functionalities. Code may be deployed in the network dynamically and on demand for the benefit of applications or application classes. Agents support a dynamic distribution of control and management functions across networks, thus increasing flexibility and efficiency. We propose to use mobile-agent technology to overcome some of the problems inherent in current Internet technology. We focus our attention to QoS monitoring, being locally significant in network subdomains, and realize a QoS management strategy in response to variations of user, customer of application requirements, and of the network state. We describe our experience and the results obtained from our testbed, where software agents are instantiated, executed, migrated, and suspended in order to implement flexible QoS management in IP networks.
Agents
yin00knowledgebased
A Knowledge-Based Approach for Designing Intelligent Team Training Systems This paper presents a knowledge approach to designing team training systems using intelligent agents. We envision a computer-based training system in which teams are trained by putting them through scenarios, which allow them to practice their team skills. There are two important roles that intelligent agents can play; these are virtual team members, and tutors. To carry out these functions, these agents must be equipped with an understanding of the task domain, the team structure, the selected decision-making process and their beliefs about other team members' mental states. Even though existing agent teamwork models incorporate many of the elements listed above, they have not focused on analyzing information needs of team members to support proactive agent interactions. To encode the team knowledge, we have developed a representation language, based on the BDI model, called MALLET. A Petri Net model of an individual agent's plans and information needs can be derived from the role des...
Agents
thrun98framework
A Framework for Programming Embedded Systems: Initial Design and Results This paper describes CES, a proto-type of a new programming language for robots and other embedded systems, equipped with sensors and actuators. CES contains two new ideas, currently not found in other programming languages: support of computing with uncertain information, and support of adaptation and teaching as a means of programming. These innovations facilitate the rapid development of software for embedded systems, as demonstrated by two mobile robot applications. This research is sponsoredin part by DARPA via AFMSC (contract number F04701-97-C-0022), TACOM (contract number DAAE07-98-C-L032), and Rome Labs (contract number F30602-98-2-0137). The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing official policies or endorsements, either expressed or implied, of DARPA, AFMSC, TACOM, Rome Labs, or the United States Government.  Keywords: Artificial intelligence, embedded system, machine learning, mobil...
ML
kaur98improving
Improving Interaction with Virtual Environments Introduction  Virtual environments (VEs) provide a computer-based interface to a real-life or abstract space, using 3D graphics and 3D interaction techniques. VEs represent a novel interface style which offers new possibilities and challenges to human-computer interface design. However, studies of the design of VEs (Kaur et al., 1996) show that designers lack a coherent approach to design, especially interaction design. Designers appear to be pre-occupied with difficult technical issues and think little about supporting user interaction. However, major interaction problems have been found with current VEs, such as disorientation, perceptual misjudgements and difficulty finding and understanding available interactions (McGovern, 1993; COVEN, 1997). These common problems have been known to result in user frustration and a low usability and acceptability for the VE (Kaur et al., 1996; Miller 1994). Guidance is needed on interaction design for VEs to avoid such usability problems. 
HCI
437003
Formal Specification and Design of Mobile Systems Termination detection, a classical problem in distributed computing, is revisited in the new setting provided by the emerging mobile computing technology. A simple solution tailored for use in ad hoc networks is employed as a vehicle for demonstrating the applicability of formal requirements and design strategies to the new field of mobile computing. The approach is based on well understood techniques in specification refinement, but the methodology is tailored to mobile applications and helps designers address novel concerns such as the mobility of hosts, transient interactions, and specific coordination constructs. The proof logic and programming notation of Mobile UNITY provide the intellectual tools required to carry out this task.  
Agents
155240
Multi-Agent Reinforcement Learning: Weighting and Partitioning This paper addresses weighting and partitioning in complex reinforcement learning tasks, with the aim of facilitating learning. The paper presents some ideas regarding weighting of multiple agents and extends them into partitioning an input/state space into multiple regions with differential weighting in these regions, to exploit differential characteristics of regions and differential characteristics of agents to reduce the learning complexity of agents (and their function approximators) and thus to facilitate the learning overall. It analyzes, in reinforcement learning tasks, different ways of partitioning a task and using agents selectively based on partitioning. Based on the analysis, some heuristic methods are described and experimentally tested. We find that some off-line heuristic methods performed the best, significantly better than single-agent models.  Keywords: weighting, averaging, neural networks, partitioning, gating, reinforcement learning,   1 Introduction  Multiple ag...
Agents
caputo01from
From Markov Random Fields to Associative Memories and Back: Spin-Glass Markov Random Fields this paper we propose a fully connected energy function for Markov Random Field (MRF) modeling which is inspired by Spin-Glass Theory (SGT). Two major tasks in MRF modeling are how to define the neighborhood system for irregular sites and how to choose the energy function for a proper encoding of constraints. The proposed energy function offers two major advantages that makes it possible to avoid MRF modeling problems in the case of irregular sites. First, full connectivity makes the neighborhood definition irrelevant, and second, the energy function is defined independently of the considered application. A basic assumption in SGT is the infinite dimension of the configuration space in which the energy is defined; the choice of a particular energy function, which depends on the scalar product between configurations, allows us to use a kernel function in the energy formulation; this solves the problem of high dimensionality and makes it possible to use SGT results in an MRF framework. We call this new model Spin Glass- - Markov Random Field (SG-MRF). Experiments on textures and objects database show the correctness and effectiveness of the proposed model
AI
137638
HICAP: An Interactive Case-Based Planning Architecture and its Application to Noncombatant Evacuation Operations This paper describes HICAP (Hierarchical Interactive Case-based Architecture for Planning), a general purpose planning architecture that we have developed and applied to assist military commanders and their staff with planning NEOs (Noncombatant Evacuation Operations). HICAP integrates a hierarchical task editor, HTE, with a conversational case-based planning tool, NaCoDAE/HTN. In this application, HTE maintains an agenda of tactical planning tasks that, according to the guidelines indicated by military doctrine, must be addressed in a NEO plan. It also supports several bookkeeping tasks, which are crucial for large-scale planning tasks that differ greatly among different NEO operations. Military planning personnel select a task to decompose from HTE and then use NaCoDAE/HTN to interactively refine it into an operational plan by selecting and applying cases, which represent task decompositions from previous NEO operations. Thus, HICAP helps commanders by using previous experience to fo...
AI
329899
Learning to Construct Knowledge Bases from the World Wide Web The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs. The first is an ontology that defines the classes (e.g., company, person, employee, product) andrelations (e.g., employed by, produced by) of interest when creating the knowledge base. The second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This article describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system that has created a knowledge base describing university people, courses, and research projects.
IR
fleuret99graded
Graded Learning for Object Detection Our goal is to detect all instances of a generic object class, such as a face, in greyscale scenes. The design of the algorithm is motivated by computational efficiency. The search is coarse-to-fine in both the exploration of poses and the representation of the object class. Starting from training examples, we recursively learn a hierarchy of spatial arrangements of edge fragments, graded by their size (sparsity). The arrangements have no a priori semantic or geometric interpretation. Instead, they are selected to be "decomposable": Each can be split into two correlated subarrangements, each of which can be further divided, etc. As a result, the probability of an arrangement of size k appearing on an object instance decays slowly with k. We demonstrate this both theoretically and in experiments in which detection means finding a sufficient number of arrangements of various sizes. 1 Introduction  Starting with a training set of examples of a generic object class (e.g., "face"), our goal...
ML
chung01dynamic
Dynamic Update Cube for Range-Sum Queries A range-sum query is very popular and becomes  important in finding trends and in discovering  relationships between attributes in diverse  database applications. It sums over the selected  cells of an OLAP data cube where target cells are  decided by the specified query ranges. The direct  method to access the data cube itself forces too  many cells to be accessed, therefore it incurs a  severe overhead. The response time is very  crucial for OLAP applications which need  interactions with users. In the recent dynamic  enterprise environment, data elements in the  cube are frequently changed. The response time  is affected in such an environment by the update  cost as well as the search cost of the cube.  In this paper, we propose an efficient algorithm  to reduce the update cost significantly while  maintaining reasonable search efficiency, by  using an index structure called the -tree. In  addition, we propose a hybrid method to provide  either an approximate result or a precise one to  reduce the overall cost of queries. It is useful for  various applications that need a quick approximate  answer rather than an accurate one, such as  decision support systems.  1. 
DB
schroeder01fuzzy
Fuzzy Argumentation and Extended Logic Programming We define a fuzzy version of extended logic programming under well-founded semantics with explicit negation. We develop a fuzzy fixpoint argumentation semantics and an efficient, top-down, goal-directed proof procedure. We show that the procedure is sound and complete with respect to the argumentation semantics.
ML
509926
Concepts and Architecture of a Security-Centric Mobile Agent Server Mobile software agents are software components that are able to move in a network. They are often considered as an attractive technology in electronic commerce applications. Although security concerns prevail. In this paper we describe the architecture and concepts of the SeMoA server -- a runtime environment for Java-based mobile agents. Its architecture has a focus on security and easy extendability, and offers a framework for transparent content inspection of agents by means of filters. We implemented filters that handle agent signing and authentication as well as selective encryption of agent contents. Filters are applied transparently such that agents need not be aware of the security services provided by the server.
Agents
sandholm99algorithms
Algorithms for Optimizing Leveled Commitment Contracts In automated negotiation systems consisting of self-interested agents, contracts have traditionally been binding. Leveled commitment contracts—i.e. contracts where each party can decommit by paying a predetermined penalty were recently shown to improve Pareto efficiency even if agents rationally decommit in Nash equilibrium using inflated thresholds on how good their outside offers must be before they decommit. This paper operationalizes the four leveled commitment contracting protocols by presenting algorithms for using them. Algorithms are presented for computing the Nash equilibrium decommitting thresholds and decommitting probabilities given the contract price and the penalties. Existence and uniqueness of the equilibrium are analyzed. Algorithms are also presented for optimizing the contract itself (price and penalties). Existence and uniqueness of the optimum are analyzed. Using the algorithms we offer a contract optimization service on the web as part of ('Mediator, our next generation electronic commerce server. Finally, the algorithms are generalized to contracts involving more than two agents. 1
Agents
352769
Query Processing in Relationlog . Relationlog is a persistent deductive database system that  supports eective storage, ecient access and inference of large amounts  of data with complex structures. In this paper, we describe query processing  in the Relationlog system. In particular, we illustrate the extended  semi-naive and magic-set techniques used in Relationlog.  1 Introduction  During the past decades, the nested relational and complex object models [1, 5, 11, 12, 15, 18, 21, 22] were developed to extend the applicability of the traditional relational model to more complex, non-business applications such as CAD, image processing and text retrieval [2].  Another important direction of intense research has been in using a logic programming based language Datalog [8, 23] as a database query language. Such a language provides a simple and natural way to express queries on a relational database and is more expressive than the traditional relational languages.  In the past several years, there have been some eorts...
DB
li98adome
ADOME: An Advanced Object Modelling Environment ADOME, ADvanced Object Modeling Environment, an approach to integrating data and knowledge management based on object-oriented technology, is presented. Next generation information systems will require more flexible data modelling capabilities than those provided by current object-oriented DBMSs. In particular, integration of data and knowledge management capabilities will become increasingly important. In this context, ADOME provides versatile role facilities that serve as "dynamic binders" between data objects and production rules, thereby facilitating flexible data and knowledge management integration. A prototype that implements this mechanism and the associated operators has been constructed on top of a commercial object-oriented DBMS and a rule base system.  Index Terms: Object modeling, knowledge semantics, dynamic roles, object-oriented databases, nextgeneration information systems 1 Introduction  Increasingly, organizations require more intelligent information management. In o...
DB
98937
A Web Agent for the Maintenance of a Database of Academic Contacts this paper to present all details of the confidence rules for Academia, however these are fully discussed in (Magnanelli 1997). It is important to point out that we consider the information in the Web not only as free to use but also as true and updated. The agent is unable to detect that information is wrong in the case that the correct information is not available. 6 User Interaction
DB
442794
On splitting and Cloning Agents Embedded with cloning mechanisms, an agent can balance its own loads by discharging computing tasks to its clones when it is over-loaded. In addition, it's more reasonable to transfer the smarter, smaller clones of an agent rather than the bulky agent itself in mobile computing. In this paper, a simple BDI agent model is formally established. Using this model, the semantics of constructing new agents by inheritance and self-identifying behavior of existing agents are precisely de  ned. Four kinds of cloning mechanisms are identi  ed, the properties of each cloning mechanism and the relationships in between are studied, and some implementation issues are also discussed.
Agents
535484
Information Retrieval on the World Wide Web and Active Logic: A Survey and Problem Definition As more information becomes available on the World Wide Web (there are currently over 4 billion pages covering most areas of human endeavor), it becomes more difficult to provide effective search tools for information access. Today, people access web information through two main kinds of search interfaces: Browsers (clicking and following hyperlinks) and Query Engines (queries in the form of a set of keywords showing the topic of interest). The first process is tentative and time consuming and the second may not satisfy the user because of many inaccurate and irrelevant results. Better support is needed for expressing one's information need and returning high quality search results by web search tools. There appears to be a need for systems that do reasoning under uncertainty and are flexible enough to recover from the contradictions, inconsistencies, and irregularities that such reasoning involves.
IR
344548
DyDa: Dynamic Data Warehouse Maintenance in a Fully Concurrent Environment Data warehouse is an emerging technology to support high-level decision making by gathering data from several distributed information sources into one repository. In dynamic environments, data warehouses must be maintained in order to stay consistent with the underlying sources. Recently proposed view maintenance algorithms tackle the problem of data warehouse maintenance under concurrent source data updates.While the view synchronization is to handle non-concurrent source schema changes. However, the concurrency between interleaved schema changes and data updates still remain unexplored problems.  In this paper, we propose a solution framework called DyDa that successfully addresses this problem. The DyDa framework detects concurrent schema changes by a broken query scheme and conicting concurrent data updates by a local timestamp scheme. A fundamental idea of the DyDa framework is the development of a two-layered architecture that separates the concerns for concurrent data updates and concurrent schema changes handling without imposing any restrictions on the sourse update transactions. At the lower level of the framework, it employs a local compensation algorithm to handle concurrent data updates, and a metadata name mapping strategy to handle concurrent source rename operations. At the higher level, it addresses the problem of concurrent source drop operations. For the latter problem, we design a strategy for the detection and correction of such concurrency and nd an executable plan for the aected updates. We then develop a new view adaption algorithm, called Batch-VA for execution of such plan to incrementally adapt the view. Put together, these algorithms are the rst to provide a complete solution to data warehouse management in a fully concurrent environment....
DB
brandt01antisocial
Antisocial Agents and Vickrey Auctions In recent years auctions have become more and more important in  the field of multiagent systems as useful mechanisms for resource allocation and  task assignment. In many cases the Vickrey (second-price sealed-bid) auction  is used as a protocol that prescribes how the individual agents have to interact  in order to come to an agreement. We show that the Vickrey auction, despite  its theoretical benefits, is inappropriate if "antisocial" agents participate in the  auction process. More specifically, an antisocial attitude for economic agents that  makes reducing the profit of competitors their main goal besides maximizing their  own profit is introduced. Under this novel condition, agents need to deviate from  the dominant truth-telling strategy. This paper presents a strategy for bidders in  repeated Vickrey auctions who are intending to inflict losses to fellow agents in  order to be more successful, not in absolute measures, but relatively to the group  of bidders. The strategy is evaluated in a simple task allocation scenario.
Agents
540663
Practical Guidelines for the Readability of IT-architecture Diagrams This paper presents the work done to establish guidelines for the creation of readable IT-architecture diagrams and gives some examples of guidelines and some examples of improved diagrams. These guidelines are meant to assist practicing IT-architects in preparing the diagrams to communicate their architectures to the various stakeholders. Diagramming has always been important in information technology (IT), but the recent interest in ITarchitecture, the widespread use of software and developments in electronic communication, make it necessary to again look at the rt of making diagrams'for this particular class and its users. The guidelines indicate how various visual attributes, like hierarchy, layout, color, form, graphics, etc. can contribute to the readability of IT-architecture diagrams. The emphasis is on the outward appearance of diagrams. Some additional support is given for the thinking/reasoning processes while designing or using a set of diagrams and an attempt is made to arrive at a rationale of these guidelines. An evaluation process has been performed with three groups of practicing IT-architects. The outcome of this evaluation is presented. This work is part of a more comprehensive research project on "Visualisation of IT- architecture".
HCI
papaioannou01manufacturing
Manufacturing Systems Integration and Agility: Can Mobile Agents Help? Mobile code is being championed as a solution to a plethora of software problems. This paper investigates whether Mobile Agents and Mobile Objects support improved system integration and agility in the manufacturing domain.  We describe two systems built to support the Sales Order Process of a distributed manufacturing enterprise, using IBM's Aglets Software Development Kit. The Sales Order Process model and the requirements for agility used as the basis for these implementations are derived from data collected in an industrial case study.  Both systems are evaluated using the Goal/Question/Metric methodology. Two new metrics for Semantic Alignment and Change Capability are presented and used to evaluate each system with respect to the degree of system agility supported.  The work described provides evidence that both Mobile Agent and Mobile Object systems have inherent properties that can be used to build agile distributed systems. Further, Mobile Agents with their additional autonomy...
Agents
zavrel00information
Information Extraction by Text Classification: Corpus Mining for Features This paper describes a method for building an Information Extraction (IE) system using standard text classification machine learning techniques, and datamining for complex features on a large corpus of example texts that are only superficially annotated. We have successfully used this method to build an IE system (Textractor) for job advertisements. 1. Introduction For rapid development of an Information Extraction system in a large new domain, the usual methods of semicorpusbased hand-crafting of extraction rules are often simply too laborious. Therefore one must turn to the use of machine learning techniques and try to induce the knowledge needed for extraction from annotated training samples. Techniques for the induction of extraction rules are e.g. described by (Freitag, 1998
IR
487737
Rough Set Theory: A Data Mining Tool for Semiconductor Manufacturing The growing volume of information poses interesting challenges and calls for tools that discover properties of data. Data mining has emerged as a discipline that contributes tools for data analysis, discovery of new knowledge, and autonomous decisionmaking. In this paper, the basic concepts of rough set theory and other aspects of data mining are introduced. The rough set theory offers a viable approach for extraction of decision rules from data sets. The extracted rules can be used for making predictions in the semiconductor industry and other applications. This contrasts other approaches such as regression analysis and neural networks where a single model is built. One of the goals of data mining is to extract meaningful knowledge. The power, generality, accuracy, and longevity of decision rules can be increased by the application of concepts from systems engineering and evolutionary computation introduced in this paper. A new rule-structuring algorithm is proposed. The concepts presented in the paper are illustrated with examples.
ML
gunn98support
Support Vector Machines for Classification and Regression The problem of empirical data modelling is germane to many engineering applications. In empirical data modelling a process of induction is used to build up a model of the system, from which it is hoped to deduce responses of the system that have yet to be observed. Ultimately the quantity and quality of the observations govern the performance of this empirical model. By its observational nature data obtained is finite and sampled; typically this sampling is non-uniform and due to the high dimensional nature of the problem the data will form only a sparse distribution in the input space. Consequently the problem is nearly always ill posed (Poggio et al., 1985) in the sense of Hadamard (Hadamard, 1923). Traditional neural network approaches have suffered difficulties with generalisation, producing models that can overfit the data. This is a consequence of the optimisation algorithms used for parameter selection and the statistical measures used to select the ’best’ model. The foundations of Support Vector Machines (SVM) have been developed by Vapnik (1995) and are gaining popularity due to many attractive features, and promising empirical performance. The formulation embodies the Structural Risk Minimisation (SRM) principle, which has been shown to be superior, (Gunn et al., 1997), to traditional Empirical Risk Minimisation (ERM) principle, employed by conventional neural networks. SRM minimises an upper bound on the expected risk, as opposed to ERM that minimises the error on the training data. It is this difference which equips SVM with a greater ability to generalise, which is the goal in statistical learning. SVMs were developed to solve the classification problem, but recently they have been extended to the domain of regression problems (Vapnik et al., 1997). In the literature the terminology for SVMs can be slightly confusing. The term SVM is typically used to describe classification with support vector methods and support vector regression is used to describe regression with support vector methods. In this report the term SVM will refer to both classification and regression methods, and the terms Support Vector Classification (SVC) and Support Vector Regression (SVR) will be used for specification. This section continues with a brief introduction to the structural risk
ML
finney02learning
Learning with Deictic Representation Most reinforcement learning methods operate on propositional representations of the world state. Such representations are often intractably large and generalize poorly. Using a deictic representation is believed to be a viable alternative: they promise generalization while allowing the use of existing reinforcement-learning methods. Yet, there are few experiments on learning with deictic representations reported in the literature. In this paper we explore the effectiveness of two forms of deictic representation and a naïve propositional representation in a simple blocks-world domain. We find, empirically, that the deictic representations actually worsen performance. We conclude with a discussion of possible causes of these results and strategies for more effective learning in domains with objects. 1
ML
pazzani00representation
Representation of Electronic Mail Filtering Profiles: A User Study Electronic mail offers the promise of rapid communication of essential information. However, electronic mail is also used to send unwanted messages. A variety of approaches can learn a profile of a user's interests for filtering mail. Here, we report on a usability study that investigates what types of profiles people would be willing to use to filter mail.  Keywords  Mail Filtering; User Studies  1. INTRODUCTION  While electronic mail offers the promise of rapid communication of essential information, it also facilitates transmission of unwanted messages such as advertisements, solicitations, light bulb jokes, chain letters, urban legends, etc. Software that automatically sorts mail into categories (e.g., junk, talk announcements, homework questions) would help automate the process of sorting through mail to prioritize messages or suggest actions (such as deleting junk mail or forwarding urgent messages to a handheld device). Such software maintains a profile of the user's interests. ...
IR
murthy99context
Context Filters for Document-Based Information Filtering In this paper we propose a keyPhrase-sense disambiguation methodology called "context filters" for use in keyPhrase based information filtering systems. A context filter finds whether an input keyPhrase has occurred in the required context. Context filters consider various factors of ambiguity. Some of these factors are special to information filtering and they are handled in a structured fashion. The proposed context filters are very comprehensibile. Context filters consider varieties of contexts which are not considered in existing word-sense disambiguation methods but these are all needed for information filtering. The ideas on context filters that we report in this paper form important elements of an Instructible Information Filtering Agent that we are developing. 1. Introduction  Information filtering is the process of separating out irrelevant documents from relevant ones. Its importance has motivated several researchers to develop software agents such as SIFT, InfoScan, iAgent, ...
IR
54866
A Machine Learning Approach to Building Domain-Specific Search Engines Domain-specific search engines are becoming  increasingly popular because they offer increased  accuracy and extra features not possible  with general, Web-wide search engines.  Unfortunately, they are also difficult and timeconsuming  to maintain. This paper proposes  the use of machine learning techniques to  greatly automate the creation and maintenance  of domain-specific search engines. We describe  new research in reinforcement learning, text  classification and information extraction that  enables efficient spidering, populates topic hierarchies,  and identifies informative text segments.  Using these techniques, we have built  a demonstration system: a search engine for  computer science research papers available at  www.cora.justresearch.com.  1 Introduction  As the amount of information on the World Wide Web grows, it becomes increasingly difficult to find just what wewant. While general-purpose search engines suchas  AltaVista and HotBot offer high coverage, they often provi...
ML
rom00gaia
Gaia: Enabling Active Spaces Ubiquitous computing promotes physical spaces with hundreds of specialized embedded devices that increase our productivity, alleviate some specific everyday tasks and provide new ways of interacting with the computational environment. Personal computers lose the focus of attention due to the fact that the computational environment is spread across the physical space. Therefore, the users' view of the computational environment is finally extended beyond the physical limits of the computer. Physical spaces become computer systems, or in other terms, Active Spaces. However, these Active Spaces require novel system software capable of seamlessly coordinating their hidden complexity. Our goal is to extend the model provided by current computer systems to allow interaction with physical spaces and their contained entities (physical and virtual) by means of a single abstraction called Active Space.  1. Introduction  Ubiquitous computing promotes the proliferation of embedded devices specializ...
HCI
420411
Providing an Embedded Software Environment for Wireless PDAs . The use of wireless Pdas is foreseen to outrun the one of Pcs in the near future. However, for this to actually happen, adequate software environments must be devised in order to allow the execution of various types of applications. This paper introduces the base features of such an environment, which is a customizable Jvm-based middleware. In particular, the middleware platform embeds services for appropriate resource management and for supporting novel Pda-oriented applications.  1 Introduction  The use of wireless Personal Digital Assistant (Pda) devices is foreseen to outrun the one of Pcs in the near future. However, for this to actually happen, there is still the need to devise adequate software and hardware platforms. The use of Pdas should be as convenient as the one of Pcs and in particular must not overly restrict the applications that are supported. Considering the ongoing effort towards providing convenient hardware platforms in industry, this paper focuses on design issu...
HCI
inza99feature
Feature Subset Selection by Bayesian networks based optimization In this paper we perform a comparison among FSS-EBNA, a randomized, populationbased  and evolutionary algorithm, and two genetic and other two sequential search approaches  in the well known Feature Subset Selection (FSS) problem. In FSS-EBNA, the  FSS problem, stated as a search problem, uses the EBNA (Estimation of Bayesian Network  Algorithm) search engine, an algorithm within the EDA (Estimation of Distribution  Algorithm) approach. The EDA paradigm is born from the roots of the GA community  in order to explicitly discover the relationships among the features of the problem and not  disrupt them by genetic recombination operators. The EDA paradigm avoids the use of  recombination operators and it guarantees the evolution of the population of solutions and  the discovery of these relationships by the factorization of the probability distribution of  best individuals in each generation of the search. In EBNA, this factorization is carried out  by a Bayesian network induced by a chea...
ML
matthews00fuzzy
Fuzzy Concepts and Formal Methods: A Fuzzy Logic Toolkit for Z It has been recognised that formal methods are useful as a modelling tool in requirements engineering. Specification languages such as Z permit the precise and unambiguous modelling of system properties and behaviour. However some system problems, particularly those drawn from the IS problem domain, may be difficult to model in crisp or precise terms. It may also be desirable that formal modelling should commence as early as possible, even when our understanding of parts of the problem domain is only approximate. This paper suggests fuzzy set theory as a possible representation scheme for this imprecision or approximation. We provide a summary of a toolkit that defines the operators, measures and modifiers necessary for the manipulation of fuzzy sets and relations. We also provide some examples of the laws which establishes an isomorphism between the extended notation presented here and conventional Z when applied to boolean sets and relations.
ML
75402
Choosing Good Distance Metrics and Local Planners for Probabilistic Roadmap Methods This paper presents a comparative evaluation of different distance metrics and local planners within the context of probabilistic roadmap methods for motion planning. Both C-space and Workspace distance metrics and local planners are considered. The study concentrates on cluttered three-dimensional Workspaces typical, e.g., of mechanical designs. Our results include recommendations for selecting appropriate combinations of distance metrics and local planners for use in motion planning methods, particularly probabilistic roadmap methods. Our study of distance metrics showed that the importance of the translational distance increased relative to the rotational distance as the environment become more crowded. We find that each local planner makes some connections than none of the others do — indicating that better connected roadmaps will be constructed using multiple local planners. We propose a new local planning method we call rotate-at-s that outperforms the common straight-line in C-space method in crowded environments.
AI
42037
A Tutorial on Support Vector Machines for Pattern Recognition Abstract. The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.
ML
lempel01picashow
PicASHOW: Pictorial Authority Search by Hyperlinks on the Web We describe PicASHOW, a fully automated WWW image retrieval system that is based on several link-structure analyzing algorithms. Our basic premise is that a page # displays (or links to) an image when the author of # considers the image to be of value to the viewers of the page. Wethus extend some well known link-based WWW #### ######### schemes to the context of image retrieval.  PicASHOW's analysis of the link structure enables it to retrieve relevant images even when those are stored in les with meaningless names. The same analysis also allows it to identify ##### ########## and ##### ####. We dene these as Web pages that are rich in relevant images, or from which many images are readily accessible.  PicASHOW requires no image analysis whatsoever and no creation of taxonomies for pre-classication of the Web's images. It can be implemented by standard WWW search engines with reasonable overhead, in terms of both computations and storage, and with no change to user query formats. It can thus be used to easily add image retrieving capabilities to standard search engines.  Our results demonstrate that PicASHOW, while relying almost exclusively on link analysis, compares well with dedicated WWW image retrieval systems. We conclude that link analysis, a bona-de eective technique for Web page search, can improve the performance of Web image retrieval, as well as extend its denition to include the retrieval of image hubs and containers.  Keywords  Image Retrieval; Link Structure Analysis; Hubs and Authorities; Image Hubs.  1. 
DB
534720
Boosting Interval-Based Literals: Variable Length and Early Classification In previous works, a system for supervised time series classification has been presented. It is based on boosting very simple classifiers: only one literal. The used predicates are based on temporal intervals. There are two types of predicates: i) relative predicates, such as "increases" and "stays", and ii) region predicates, such as "always" and "sometime", which operate ver regions in the domain of the variable.
DB
212834
Autonomy-Based Multi-Agent Systems: Statistical Issues This paper describes an autonomy-based multiagent system and its application to simulations in the framework of collective robotics. Experimental results measured for a particular task, namely object regrouping, have two outcomes. First, they show that a form of implicit cooperation takes place in the system despite the absence of explicit cooperation protocol, but just in virtue of the design of the system. Second, they indicate that it is possible to effect the way the task is achieved by appropriately introducing obstacles that modify the environment of the agents. A preliminary partial characterization of the concept of emergence is proposed and its illustration to our experimental system is given. Introduction  Our work fits in the framework of Bottom-Up Artificial Intelligence (Brooks 1986), (Brooks 1991) and more particularly, in that of Autonomous Agents (Pfeifer 1995). We are concerned with collective phenomena and their issues and more precisely, the way to carry out solution...
Agents
940
Feature Subset Selection in Text-Learning This paper describes several known and some new methods for feature subset selection on large text data. Experimental comparison given on real-world data collected from Web users shows that characteristics of the problem domain and machine learning algorithm should be considered when feature scoring measure is selected. Our problem domain consists of hyperlinks given in a form of small-documents represented with word vectors. In our learning experiments naive Bayesian classifier was used on text data. The best performance was achieved by the feature selection methods based on the feature scoring measure called Odds ratio that is known from information retrieval.
IR
zhao02criterion
Criterion Functions for Document Clustering: Experiments and Analysis In recent years, we have witnessed a tremendous growth in the volume of text documents available on the Internet,  digital libraries, news sources, and company-wide intranets. This has led to an increased interest in developing  methods that can help users to effectively navigate, summarize, and organize this information with the ultimate  goal of helping them to find what they are looking for. Fast and high-quality document clustering algorithms play an  important role towards this goal as they have been shown to provide both an intuitive navigation/browsing mechanism  by organizing large amounts of information into a small number of meaningful clusters as well as to greatly improve  the retrieval performance either via cluster-driven dimensionality reduction, term-weighting, or query expansion. This  ever-increasing importance of document clustering and the expanded range of its applications led to the development  of a number of new and novel algorithms with different complexity-quality trade-offs. Among them, a class of  clustering algorithms that have relatively low computational requirements are those that treat the clustering problem  as an optimization process which seeks to maximize or minimize a particular clustering criterion function defined  over the entire clustering solution.
IR
bergmark02focused
Focused Crawls, Tunneling, and Digital Libraries Crawling the Web to build collections of documents related  to pre-specified topics became an active area of research during the late  1990's after crawler technology was developed for the benefit of search  engines. Now, Web crawling is being seriously considered as an important  strategy for building large scale digital libraries. This paper considers  some of the crawl technologies that might be exploited for collection  building. For example, to make such collection-building crawls more effective,  focused crawling was developed, in which the goal was to make  a "best-first" crawl of the Web. We are using powerful crawler software  to implement a focused crawl but use tunneling to overcome some of the  limitations of a pure best-first approach. Tunneling has been described  by others as not only prioritizing links from pages according to the page's  relevance score, but also estimating the value of each link and prioritizing  on that as well. We add to this mix by devising a tunneling focused  crawling strategy which evaluates the current crawl direction on the fly to  determine when to terminate a tunneling activity. Results indicate that  a combination of focused crawling and tunneling could be an e#ective  tool for building digital libraries.
IR
477302
Mining Usability Information from Log Files: AMulti-Pronged Approach rooms is configurable by its occupants in how they organize various tools housing their data, documents, and graphics. The TW system provides for synchronous and asynchronous user interactions, but importantly these interactions are in the context of relevant data. The work in this experiment was characterized by several full group meetings (for planning and coordination) interspersed with periods of individual activity (asychronous work) and smaller coordination meetings of two or three team members around the "hand-off" of output from a task used as input for another task.  Collected Data  The native version of TW produces a server-based log-file that contains information about the identity of users entering the distributed application, the identity of the rooms through which users navigate, file uploads, and message passing between users. This set of interactions was deemed too rudimentary for capturing the type of data needed for usability analysis. Since the source co
HCI
236597
Learning and Evaluating Visual Features for Pose Estimation We present a method for learning a set of visual landmarks which are useful for pose estimation. The landmark learning mechanism is designed to be applicable to a wide range of environments, and generalized for different approaches to computing a pose estimate. Initially, each landmark is detected as a local extremum of a measure of distinctiveness and represented by a principal components encoding which is exploited for matching. Attributes of the observed landmarks can be parameterized using a generic parameterization method and then evaluated in terms of their utility for pose estimation. We present experimental evidence that demonstrates the utility of the method. 1 Introduction In this paper, we develop an approach to visionbased robot localization by learning a set of imagedomain landmarks in the robot's environment. The landmarks are learned from a representative set of images obtained during an initial exploration of the environment. No a priori assumptions are made about the...
ML
270678
Real-Time Scheduling for Distributed Agents Providing an environment for a software agent to execute  is very similar to building an operating system for  the execution of general purpose applications. In the  same fashion that an operating system provides a set  of services for the execution of a user request, an agent  framework provides a similar set of services for the execution  of agent actions. Such services include the ability  to communicate with other agents, maintaining the  current state of an executing agent, and selecting an  execution path from a set of possible execution paths.  The particular focus of this paper is the study of Soft  Real-Time agentscheduling in the context of a framework  for the execution of intelligent software agents;  acharacterization of agent performance; and developmentofanenvironment  for testing and comparing the  performance of agent activities. The agent architecture  used for this study, DECAF #Distributed Environment  Centered AgentFramework#, is a software toolkit for  the rapid d...
Agents
obrst97constraints
Constraints and Agents in MADEsmart As part of the DARPA Rapid Design Exploration and Optimization (RaDEO) program, Boeing, Philadelphia, is involved in an on-going concurrent design engineering research project called MADEsmart which seeks to partially automate the Integrated Product Team (IPT) concept used by Boeing for organizing the design engineering process, with the aid of intelligent agent technology. Although currently only in an early stage of development, the project is expected to crucially employ a constraint-centered System Design Management Agent developed by the University of Toronto's IE Department in conjunction with Boeing. The SDMA will use the constraint-based Toronto Ontologies for a Virtual Enterprise (TOVE) ontologies, and its domain theories for design engineering and dependent underlying theories, phrased as KIF/Ontolingua assertions in an axiomatic system running in the constraint logic system ECLiPSe, as its primary knowledge resource to monitor an ongoing design project, offering resource-all...
Agents
moon98scalable
Scalable Algorithms for Large-Scale Temporal Aggregation The ability to model time-varying natures is essential to many database applications such as data warehousing and mining. However, the temporal aspects provide many unique characteristics and challenges for query processing and optimization. Among the challenges is computing temporal aggregates, which is complicated by having to compute temporal grouping. In this paper, we introduce a variety of temporal aggregation algorithms that overcome major drawbacks of previous work. First, for small-scale aggregations, both the worst-case and average-case processing time have been improved significantly. Second, for large-scale aggregations, the proposed algorithms can deal with a database that is substantially larger than the size of available memory. Third, the parallel algorithm designed on a shared-nothing architecture achieves scalable performance by delivering nearly linear scale-up and speed-up. The contributions made in this paper are particularly important because the rate of increase ...
DB
gouveira98feasibility
Feasibility Discussion of a Collaborative Virtual Environment - FINDING ALTERNATIVE WAYS FOR UNIVERSITY MEMBERS INTERACTION This paper discusses the potential impact and roadmap for the creation of a Collaborative Virtual Environment where all university members can interact in novel ways. Some actual NetLab figures are presented to justify this evolution as feasible. A related project that uses the potential created by the "laptops for all" action is a virtual incubator to simulate entrepreneurship bias is presented.
HCI
peng01automatic
Automatic Multi-Lingual Information Extraction Information Extraction (IE) is a burgeoning technique because of the explosion of internet. So far, most of the IE systems are focusing on English text; and most of them are in the supervised learning framework, which requires large amount of human labor; and most of them can only work in narrow domain, which is domain dependent. These systems are difficult to be ported to other languages, other domains because of these inherent shortcomings. Currently, besides western languages like English, there are many other Asian languages which are much di erent from English. In English, words are delimited by white-spaces so computer can easily tokenize the input text string. In many languages like Chinese, Japanese, Thai and Korea, they do not have word boundaries between words. This poses a difficult problem for the information extraction for those languages. In this thesis, we intend to implement a self-contained, language independent automatic IE system. The system is automatic because we are using a unsupervised learning framework in which no labeled data is required for training or a semi-supervised learning framework in which small amount of labeled data and large amount of unlabeled data are used. Specifically, we deal with Chinese and English languages name entity recognition and entity relation extraction, but the system can be easily extended to any other languages and other tasks. We implement an unsupervised Chinese word segmenter, a Chinese POS tagger, and we extend maximum entropy models to incorporate unlabeled data for general information extraction.
IR
nrvag99vagabond
The Vagabond Parallel Temporal Object-Oriented Database System: Versatile Support for Future Applications . In this paper, we discuss features that future database systems should support to deliver  the required functionality and performance to future applications. The most important features are  efficient support for: 1) large objects, 2) isochronous delivery of data, 3) queries on large data sets,  4) full text indexing, 5) multidimensional data, 6) sparse data, and 7) temporal data and versioning.  To efficiently support these features in one integrated system, a new database architecture is needed.  We describe an architecture suitable for this purpose, the Vagabond Parallel Temporal Object-Oriented  Database system. We also describe techniques we have developed to avoid some possible bottlenecks  in a system based on this new architecture.  1 Introduction  The recent years have brought computers into almost every office, and this availability of powerful computers, connected in global networks, has made it possible to utilize powerful data management systems in new application areas....
DB
ambite01simplifying
Simplifying Data Access: The Energy Data Collection (EDC) Project The massive amount of statistical and text data available from government agencies has created a set  of daunting challenges to both research and analysis communities. These problems include  heterogeneity, size, distribution, and control of terminology. At the Digital Government Research  Center we are investigating solutions to these key problems. In this paper we focus on  (1) ontological mappings for terminology standardization, (2) data integration across data bases with  high speed query processing, and (3) interfaces for query input and presentation of results. This  collaboration between researchers from Columbia University and the Information Sciences Institute  of the University of Southern California employs technology developed at both locations, in  particular the SENSUS ontology, the SIMS multi-database access planner, the LKB automated  dictionary and terminology analysis system, and others. The pilot application targets gasoline data  from the Bureau of Labor Statistics, the Energy Information Administration of the Department of  Energy, the Census Bureau, and other government agencies.  1 
DB
prasad96cooperative
Cooperative Learning over Composite Search Spaces: Experiences with a Multi-agent Design System We suggest the use of two learning techniques --- short term and long term --- to enhance search efficiency in a multi-agent design system by letting the agents learn about non-local requirements on the local search process. The first technique allows an agent to accumulate and apply constraining information about global problem solving, gathered as a result of agent communication, to further problem solving within the same problem instance. The second technique is used to classify problem instances and appropriately index and retrieve constraining information to apply to new problem instances. These techniques will be presented within the context of a multi-agent parametricdesign application called STEAM. We show that learning conclusively improves solution quality and processingtime results. Introduction  In this article, we study machine-learning techniques that can be applied within multi-agent systems (MAS) to improve solution quality and processing-time results. A ubiquitous prob...
ML
28031
Beyond Euclidean Eigenspaces: Bayesian Matching for Visual Recognition We propose a novel technique for direct visual matching of images for the purposes of face  recognition and database search. Speci#cally,we argue in favor of a probabilistic measure of  similarity, in contrast to simpler methods which are based on standard Euclidean L2 norms #e.g.,  template matching# or subspace-restricted norms #e.g., eigenspace matching#. The proposed  similarity measure is based on a Bayesian analysis of image di#erences: we model twomutually  exclusive classes of variation between two facial images: intra-personal #variations in appearance  of the same individual, due to di#erent expressions or lighting# and extra-personal #variations in  appearance due to a di#erence in identity#. The high-dimensional probability density functions  for each respective class are then obtained from training data using an eigenspace density  estimation technique and subsequently used to compute a similarity measure based on the a  posteriori probability of membership in the intra-personal class, which is used to rank matches  in the database. The performance advantage of this probabilistic matching technique over  standard Euclidean nearest-neighbor eigenspace matching is demonstrated using results from  ARPA's 1996 #FERET" face recognition competition, in which this algorithm was found to be  the top performer.
ML
matthews99fuzzy
Fuzzy Concepts and Formal Methods: Some Illustrative Examples It has been recognised that formal methods are useful as a modelling tool in requirements engineering. Specification languages such as Z permit the precise and unambiguous modelling of system properties and behaviour. However some system problems, particularly those drawn from the IS problem domain, may be difficult to model in crisp or precise terms. It may also be desirable that formal modelling should commence as early as possible, even when our understanding of parts of the problem domain is only approximate. This paper identifies the problem types of interest and argues that they are characterised by uncertainty and imprecision. It suggests fuzzy set theory as a useful formalism for modelling aspects of this imprecision. The paper illustrates how a fuzzy logic tooklit for Z can be applied to such problem domains. Several examples are presented illustrating the representation of imprecise concepts as fuzzy sets and relations, soft pre- conditions and system requirements as a series of linguistically quantified propositions. 1
AI
sengupta99constructing
Constructing and Transforming CBR Implementations: Techniques for Corporate Memory Management Achieving widespread case-based reasoning support for corporate memories will require the flexibility to integrate implementations with existing organizational resources and infrastructure. Case-based reasoning implementations as currently constructed tend to fall into three categories, characterized by implementation constraints: task-based (task constraints alone), enterprise (integrating databases), and web-based (integrating web representations) . These implementation types represent the possible targets in constructing corporate memory systems, and it is important to understand the strengths of each, how they are built, and how one may be constructed by transforming another. This paper describes a framework that relates the three types of CBR implementation, discusses their typical strengths and weaknesses, and describes practical strategies for building corporate CBR memories to meet new requirements by transforming and synthesizing existing resources.  1 Introduction  Constructi...
DB
faber99using
Using Database Optimization Techniques for Nonmonotonic Reasoning In this paper a program rewriting technique for disjunctive datalog is proposed, which descends from query optimization techniques in relational algebra and reduces the size of the ground instantiation of a program in many cases. As a consequence, the time for generating the ground instantiation and the time for subsequent operations on the ground program is shortened. A part of this technique has already been implemented as a preprocessing step in the Disjunctive Deductive Database System dlv. Using a recently published application, we show that a signicant reduction of the ground program size and a tremendous overall speedup can be achieved. 1 Introduction dlv is a Disjunctive Deductive Database System implementing the Consistent Answer Sets Semantics. Like similar systems in the area of non-monotonic reasoning [6], the kernel modules of dlv operate on a ground instantiation of the input program, i.e., a program that does not contain any variables, but is (semantically) equivalent t...
DB
ramakrishna00similarity
Similarity Query Processing in Image Databases CHITRA is a prototype CBIR system we are building. It uses a four layer data model we have developed, and enables retrieval based on high level concepts, such as "retrieve images of MOUNTAINS", and "retrieve images of MOUNTAINS and SUNSET". This paper deals with some issues about query processing encountered in the implementation of the system. The contributions of this paper can be summarized in terms of processing the following four example queries (I 1 , I 2 , ..,I k are images).  Q 1 : "retrieve images similar to I 1 based on color".  Q 2 : "retrieve images similar to I 1 based on color AND texture".  Q 3 : "retrieve images similar to I 1 ; I 2 ; ::; I k based on color".  Q 4 : "retrieve images similar to I 1 ; I 2 ; ::; I k based on color AND texture".  First a brief review of basic CBIR query processing literature is provided (processing of Q 1 ). Processing of Q 2 involves efficient evaluation of combining functions, a problem that has attracted research attention in recent time...
DB
23381
The CMUnited-97 Robotic Soccer Team: Perception and Multiagent Control Robotic soccer is a challenging research domain which involves multiple  agents that need to collaborate in an adversarial environment to achieve  specificobjectives. In this paper, we describe CMUnited, the team of small  robotic agents that we developed to enter the RoboCup-97 competition. We  designed and built the robotic agents, devised the appropriate vision algorithm,  and developed and implemented algorithms for strategic collaboration  between the robots in an uncertain and dynamic environment. The robots  can organize themselves in formations, hold specificroles, and pursue their  goals. In game situations, they have demonstrated their collaborative behaviors  on multiple occasions. The robots can also switch roles to maximize  the overall performance of the team. We present an overview of the vision  processing algorithm which successfully tracks multiple moving objects and  predicts trajectories. The paper then focusses on the agent behaviors ranging  from low-level individual behaviors to coordinated, strategic team behaviors.
AI
craswell01effective
Effective Site Finding using Link Anchor Information Link-based ranking methods have been described in the literature and applied in commercial Web search engines. However, according to recent TREC experiments, they are no better than traditional content-based methods. We conduct a different type of experiment, in which the task is to find the main entry point of a specific Web site. In our experiments, ranking based on link anchor text is twice as effective as ranking based on document content, even though both methods used the same BM25 formula. We obtained these results using two sets of 100 queries on a 18.5 million docu- ment set and another set of 100 on a 0.4 million document set. This site finding effectiveness begins to explain why many search engines have adopted link methods. It also opens a rich new area for effectiveness improvement, where traditional methods fail.
IR
zelikovitz00improving
Improving Short-Text Classification Using Unlabeled Background Knowledge to Assess Document Similarity We describe a method for improving the classification  of short text strings using a combination  of labeled training data plus a secondary corpus  of unlabeled but related longer documents. We  show that such unlabeled background knowledge  can greatly decrease error rates, particularly if  the number of examples or the size of the strings  in the training set is small. This is particularly  useful when labeling text is a labor-intensive job  and when there is a large amount of information  available about a particular problem on the World  Wide Web. Our approach views the task as one  of information integration using WHIRL, a tool  that combines database functionalities with techniques  from the information-retrieval literature.  1. Introduction  The task of classifying textual data that has been culled from sites on the World Wide Web is both difficult and intensively studied (Cohen & Hirsh, 1998; Joachims, 1998; Nigam et al., 1999). Applications of various machine learning techniqu...
IR
hannebauer99composable
Composable Agents for Patient Flow Control - Preliminary Concepts In this article we describe our research efforts in coping with a trade-off that can be often found in the control and optimization of todays business processes. Though centralized control may achieve better results in controlling the system behavior, there are usually social, technical and security constraints on applying centralized control. Distributed control on the other hand may cope with these constraints but also entails suboptimal results and communicational overhead. Our concept of composable agents tries to allow a dynamic and fluent transition between globalization and localization in business process control by adapting to the current real-world system structure. We are currently evaluating this concept in the framework of a patient flow control project at Charit'e Berlin. Todays applications of information technology face at least two major aspects of business settings. The first aspect is the partially or fully automated execution of complex business processes. This enfo...
Agents
455694
Building Infrastructures for Digital Libraries Digital Libraries today are often monolithic systems. In the future, they will dissolve  into collections of electronic services. The challenge will be to provide an infrastructure  that supports the user in dealing with this multitude of services. Such an infrastructure  should offer integrated access to the combined contents of multiple services, it should provide  active dissemination of new contents, and it needs to support the users in locating and  combining the services most suitable to their needs.  In the Global Info program Infrastructures for Digital Libraries components of such  an infrastructure are being developed. The federated query service DEMETRIOS and the  alerting service HERMES are both integration services that combine underlying services,  i.e., heterogenous information sources. The GIBRALTAR portal provides a meta-service  that supports the user in locating and applying various Digital Library services.  1 
IR
griffiths01query
A Query Calculus for Spatio-Temporal Object Databases The development of any comprehensive proposal for spatio-temporal databases involves significant extensions to many aspects of a non-spatio-temporal architecture. One aspect that has received less attention than most is the development of a query calculus that can be used to provide a semantics for spatio-temporal queries and underpin an effective query optimization and evaluation framework. In this paper, we show how a query calculus for spatiotemporal object databases that builds upon the monoid calculus proposed by Fegaras and Maier for ODMG-compliant database systems can be developed. The paper shows how an extension of the ODMG type system with spatial and temporal types can be accommodated into the monoid approach. It uses several queries over historical (possibly spatial) data to illustrate how, by mapping them into monoid comprehensions, the way is open for the application of a logical optimizer based on the normalization algorithm proposed by Fegaras and Maier.
DB
mataric99making
Making Complex Articulated Agents Dance - An analysis of control methods drawn from robotics, animation, and biology .  We discuss the tradeoffs involved in control of complex articulated agents, and present three implemented controllers for a complex task: a physically-based humanoid torso dancing the Macarena. The three controllers are drawn from animation, biological models, and robotics, and illustrate the issues of joint-space vs. Cartesian space task specification and implementation. We evaluate the controllers along several qualitative and quantitative dimensions, considering naturalness of movement and controller flexibility. Finally, we propose a general combination approach to control, aimed at utilizing the strengths of each alternative within a general framework for addressing complex motor control of articulated agents.  Key words: articulated agent control, motor control, robotics, animation 1. Introduction  Control of humanoid agents, dynamically simulated or physical, is an extremely difficult problem due to the high dimensionality of the control space, i.e., the many degrees of freed...
Agents
park01segmentbased
Segment-Based Approach for Subsequence Searches in Sequence Databases This paper investigates the subsequence searching problem under time warping in sequence databases. Time warping enables to find sequences with similar changing patterns even when they are of different lengths. Our work is motivated by the observation that subsequence searches slow down quadratically as the total length of data sequences increases. To resolve this problem, we propose the SegmentBased Approach for Subsequence Searches (SBASS), which modifies the similarity measure from time warping to piecewise time warping and limits the number of possible subsequences to be compared with a query sequence.  For efficient retrieval of similar subsequences without false dismissal  1  , we extract feature vectors from all data segments exploiting their monotonically changing properties, and build a multi-dimensional index such as R-tree or R  - tree. Using this index, queries are processed with four steps: 1) index filtering, 2) feature filtering, 3) successor filtering, and 4) post-proce...
DB
kakas01abduction
Abduction in Logic Programming This paper is a survey and critical overview of recent work on the extension of Logic Programming to perform Abductive Reasoning (Abductive Logic Programming). We outline the general framework of Abduction and its applications to Knowledge Assimilation and Default Reasoning; and we introduce an argumentation-theoretic approach to the use of abduction as an interpretation for Negation as Failure. We also analyse the links between Abduction and the extension of Logic Programming obtained by adding a form of explicit negation. Finally we discuss the relation between Abduction and Truth Maintenance.  1 Introduction  This paper is a survey and analysis of work on the extension of logic programming to perform abductive reasoning. The purpose of the paper is to provide a critical overview of some of the main research results, in order to develop a common framework for evaluating these results, to identify the main unresolved problems, and to indicate directions for future work. The emphasis i...
DB
sudo01automatic
Automatic Pattern Acquisition for Japanese Information Extraction One of the central issues for information extraction is the cost of customization from one scenario to another. Research on the automated acquisition of patterns is important for portability and scalability. In this paper, we introduce Tree-Based Pattern representation where a pattern is denoted as a path in the dependency tree of a sentence. We outline the procedure to acquire Tree-Based Patterns in Japanese from un-annotated text. The system extracts the relevant sentences from the training data based on TF/IDF scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns. Keywords Information Extraction, Pattern Acquisition 1.
IR
246390
Towards a Model for Spatio-Temporal Schema Selection Schema versioning provides a mechanism for handling change in the structure of database systems and has been investigated widely, both in the context of static and temporal databases. With the growing interest in spatial and spatio-temporal data as well as the mechanisms for holding such data, the spatial context within which data is formatted also becomes an issue. This paper presents a generalised model that accommodates schema versioning within static, temporal, spatial and spatio-temporal relational and object-oriented databases.
DB
yangarber01scenario
Scenario Customization for Information Extraction Information Extraction (IE) is an emerging NLP technology, whose function is to process unstructured, natural language text, to locate specific pieces of information, or facts, in the text, and to use these facts to fill a database. IE systems today are commonly based on pattern matching. The core IE engine uses a cascade of sets of patterns of increasing linguistic complexity. Each pattern consists of a regular expression and an associated mapping from syntactic to logical form. The pattern sets are customized for each new topic, as defined by the set of facts to be extracted. Construction of a pattern base for a new topic is recognized as a time-consuming and expensive process---a principal roadblock to wider use of IE technology in the large. An e#ective pattern base must be precise and have wide coverage. This thesis addresses the portability probl...
IR
godfrey98integrity
Integrity Constraints: Semantics and Applications this paper.) Finally, by having ICs expressed in logic, one can use deduction and logic as one's basic and natural tools for handling, and reasoning over, database specifications (facts, rules, queries, and ICs). This applies for relational databases, with or without rules, and can be applied to object-oriented and object-relational databases as well. There is a broad body of work on logic and relational databases, and a general consensus on what databases (facts and rules) and queries mean. However, there is less work on the meaning of integrity constraints, and certainly no consensus. What is meant by an IC can differ widely. For instance, one may define that ICs must be consistent with the database, or define that they must be provable statements, deducible from the database. (This distinction is discussed in more detail in Section 3.2.) Another view is that ICs really represent meta-knowledge---knowledge about the database itself---and should, perhaps, be written in a nonclassical logic. The general situation becomes more complex when we permit databases to contain indefinite or disjunctive information or to use negation, as with IC 2 above. Subtle but profound differences in meaning can arise due to different interpretations of ICs. In many systems, the semantics for ICs is never made clear; at times, one interpretation seems intended, while at other times, another interpretation is evident. This ambiguity is dangerous, and could allow a database to become corrupt in unanticipated ways. In this chapter, we introduce integrity constraints in a logical framework and overview the various work that has been done on ICs in this context. Section 2 presents the basic definitions for databases and integrity constraints for the chapter. We consider various semantics of ICs,...
DB
486074
Algorithms for Temporal Query Operators in XML Databases The contents of an XML database or XML/Web data warehouse is seldom static. New documents  are created, documents are deleted, and more important: documents are updated. In many  cases, we want to be able to search in historical versions, retrieve documents valid at a certain  time, query changes to documents, etc. This can be supported by extending the system with temporal  database features. In this paper we describe the new query operators needed in order to  support an XML query language which supports temporal operations. We also describe the algorithms  which can make efficient implementation of these query operators possible.  Keywords: XML, temporal databases, query processing  1 
DB
larsen01probabilistic
Probabilistic Hierarchical Clustering with Labeled and Unlabeled Data . This paper presents hierarchical probabilistic clustering methods for unsupervised and supervised learning in datamining applications, where supervised learning is performed using both labeled and unlabeled examples. The probabilistic clustering is based on the previously suggested Generalizable Gaussian Mixture model and is extended using a modified Expectation Maximization procedure for learning with both unlabeled and labeled examples. The proposed hierarchical scheme is agglomerative and based on probabilistic similarity measures. Here, we compare a L 2 dissimilarity measure, error confusion similarity, and accumulated posterior cluster probability measure. The unsupervised and supervised schemes are successfully tested on artificially data and for e-mails segmentation.  1 
IR
454034
Theme-Based Retrieval of Web News Efficient information retrieval of highly dynamic information, such as Web news, is a complex task. As a result, search and retrieval environments for continuously updated news from other sources than the largest media conglomerates are almost absent on the Internet. Global search engines do not index or classify news information from smaller network communities. To address this problem, I developed NewsSearch, a news information management environment designed to improve retrieval efficiency of online news for the smaller networked communities.  NewsSearch search achieves its goal through a combination of techniques:  . Multiple indexing queues, defining multiple gathering schedules, to deal with different publication periodicities.  . Information Retrieval techniques to news, in order to classify them into a pre-defined set of themes.  . Support Vector Machines, which proved to be a fast and reliable classification technique.  NewsSearch proved to be a scalable solution with acceptable storage needs even while managing a fairly large collection of daily publications. A combination of fine tuning of training strategies, noise filtering of Web news documents and multiple classifications, enable NewsSearch to achieve a classification accuracy of 95%.  ACKNOWLEDGEMENTS  This work was supported in part by the PRAXIS project ARIADNE (Pblico Digital --  Praxis XXI, Medida 3.1b) and project SAGRES (Praxis/P/TIT/1676/95).  TABLE OF CONTENTS  CHAPTER I 
IR
533604
Learning reactive robot behaviors with Neural-Q_leaning The purpose of this paper is to propose a Neural-Q_learning approach designed for online learning of simple and reactive robot behaviors. In this approach, the Q_function is generalized by a multi-layer neural network allowing the use of continuous states and actions. The algorithm uses a database of the most recent learning samples to accelerate and guarantee the convergence. Each Neural-Q_learning function represents an independent, reactive and adaptive behavior which maps sensorial states to robot control actions. A group of these behaviors constitutes a reactive control scheme designed to fulfill simple missions. The paper centers on the description of the Neural-Q_learning based behaviors showing their performance with an autonomous underwater vehicle (AUV) in a target following mission. Simulated experiments demonstrate the convergence and stability of the learning system, pointing out its suitability for online robot learning. Advantages and limitations are discussed.
ML
184682
Towards Flexible Multi-Agent Decision-Making Under Time Pressure Abstract — Autonomous agents need considerable computational resources to perform rational decision-making. These demands are even more severe when other agents are present in the environment. In these settings, the quality of an agent’s alternative behaviors depends not only on the state of the environment, but also on the actions of other agents, which in turn depend on the others ’ beliefs about the world, their preferences, and further on the other agents’ beliefs about others, and so on. The complexity becomes prohibitive when large number of agents are present and when decisions have to be made under time pressure. In this paper we investigate strategies intended to tame the computational burden by using off-line computation in conjunction with on-line reasoning. We investigate two approaches. First, we use rules compiled off-line to constrain alternative actions considered during on-line reasoning. This method minimizes overhead, but is not sensitive to changes in realtime demands of the situation at hand. Second, we use performance profiles computed off-line and the notion of urgency (i.e., the value of time) computed on-line to choose the amount of information to be included during on-line deliberation. This method can adjust to various levels of real-time demands, but incurs some overhead associated with iterative deepening. We test our framework with experiments in a simulated anti-air defense domain. The experiments show that both procedures are effective in reducing computation time while offering good performance under time pressure.
Agents
smeaton97relevance
Relevance Feedback and Query Expansion for Searching the Web: A Model for Searching a Digital Library :  A fully operational large scale digital library is likely to be based on a distributed architecture and because of this it is likely that a number of independent search engines may be used to index different overlapping portions of the entire contents of the library. In any case, different media, text, audio, image, etc., will be indexed for retrieval by different search engines so techniques which provide a coherent and unified search over a suite of underlying independent search engines are thus likely to be an important part of navigating in a digital library. In this paper we present an architecture and a system for searching the world's largest DL, the world wide web. What makes our system novel is that we use a suite of underlying web search engines to do the bulk of the work while our system orchestrates them in a parallel fashion to provide a higher level of information retrieval functionality. Thus it is our meta search engine and not the underlying direct search engines th...
IR
ceska01generating
Generating and Using State Spaces of Object-Oriented Petri Nets : The article discusses the notion of state spaces of object-oriented Petri nets associated to the tool called PNtalk and the role of identifiers of dynamically appearing and disappearing instances within these state spaces. Methods of working with identifiers based on sophisticated naming rules and mechanisms for abstracting names are described and compared. Some optimizations of state space generating algorithms for the context of object-oriented Petri nets are briefly mentioned, as well.  Key Words: Petri nets, object-orientation, state spaces, formal analysis and verification 1 Introduction  Methods of formal analysis and verification has been developed as an alternative to simulation approaches of examining properties of complex systems. Although we are not always able to fully verify the behaviour of a system, even partial analysis or verification can reveal some errors which tend to be different from the ones found by simulation due to the different nature of formal analysis and...
Agents
73259
On Case-Based Representability and Learnability of Languages . Within the present paper we investigate case-based representability as well as case-based learnability of indexed families of uniformly recursive languages. Since we are mainly interested in case-based learning with respect to an arbitrary fixed similarity measure, case-based learnability of an indexed family requires its representability, first. We show that every indexed family is case-based representable by positive and negative cases. If only positive cases are allowed the class of representable families is comparatively small. Furthermore, we present results that provide some bounds concerning the necessary size of case bases. We study, in detail, how the choice of a case selection strategy influences the learning capabilities of a case-based learner. We define different case selection strategies and compare their learning power to one another. Furthermore, we elaborate the relations to Gold-style language learning from positive and both positive and negative examples. 1 Introdu...
ML
hawking01which
Which Search Engine is best at finding Online Services? We report results for an independent, blind evaluation of the performance of 11 commercial search engines on 106 online service queries and on 54 topic relevance queries. We found a strong correlation between performance on the two types of query and significant differences between engines.
IR
bailey01analysis
Analysis and Optimisation of Event-Condition-Action Rules on XML XML is a now a dominant standard for storing and exchanging information. With its  increasing use in areas such as data warehousing and e-commerce, there is a rapidly growing  need for rule-based technology to support reactive functionality on XML repositories. Eventcondition  -action (ECA) rules automatically perform actions in response to events and are  a natural facility to support such functionality. In this paper, we study ECA rules in the  context of XML data. We define a simple language for specifying ECA rules on XML  repositories. The language is illustrated by means of some examples, and its syntax and  semantics are then specified more formally. We then investigate methods for analysing and  optimising these ECA rules, a task which has added complexity in this XML setting compared  with conventional active databases.  1 
DB
zhang01maximum
Maximum Likelihood Estimation for Filtering Thresholds Information filtering systems based on statistical retrieval models usually compute a numeric score indicating how well each document matches each profile. Documents with scores above profile-specific dissemination thresholds are delivered.  An optimal dissemination threshold is one that maximizes a given utility function based on the distributions of the scores of relevant and non-relevant documents. The parameters of the distribution can be estimated using relevance information, but relevance information obtained while filtering is biased. This paper presents a new method of adjusting dissemination thresholds that explicitly models and compensates for this bias. The new algorithm, which is based on the Maximum Likelihood principle, jointly estimates the parameters of the density distributions for relevant and nonrelevant documents and the ratio of the relevant document in the corpus. Experiments with TREC-8 and TREC-9 Filtering Track data demonstrate the effectiveness of the algorithm.  Keywords  Information Filtering, Dissemination Threshold, Maximum Likelihood Estimation.  1. 
IR
475405
Using Labeled and Unlabeled Data to Learn Drifting Concepts For many learning tasks, where data is collected  over an extended period of time, one has to cope  two problems. The distribution underlying the data  is likely to change and only little labeled training  data is available at each point in time. A typical  example is information filtering, i. e. the adaptive  classification of documents with respect to a particular  user interest. Both the interest of the user  and the document content change over time. A filtering  system should be able to adapt to such concept  changes. Since users often give little feedback,  a filtering system should also be able to achieve a  good performance, even if only few labeled training  examples are provided. This paper proposes a  method to recognize and handle concept changes  with support vector machines and to use unlabeled  data to reduce the need for labeled data. The  method maintains windows on the training data,  whose size is automatically adjusted so that the estimated  generalization error is minimized. The approach  is both theoretically well-founded as well  as effective and efficient in practice. Since it does  not require complicated parameterization, it is simpler  to use and more robust than comparable heuristics.  Experiments with simulated concept drift scenarios  based on real-world text data compare the  new method with other window management approaches  and show that it can effectively select an  appropriate window size in a robust way. In order  to achieve an acceptable performance with fewer  labeled training examples, the proposed method exploits  unlabeled examples in a transductive way.  1 
IR
schmitt98relating
Relating Chemical Structure to Activity: An Application of the Neural Folding Architecture : This paper is based on the neural folding architecture (FA). The FA is a recurrent neural network architecture which is especially suited for adaptive structure processing, i.e. learning approximations of mappings from symbolic term structures to IR  n  . The main objective of this paper is to demonstrate that the FA can be successfully applied to approximate quantitative structure activity relationships (QSARs), which play an important role during a drug design process. Several approaches for the conversion of a QSAR problem to suitable learning tasks for the FA are presented. Finally the FA is applied to a well-known QSAR benchmark, viz. the inhibition of E. coli dihydrofolate reductase by triazines. The achieved results are compared with results of other machine learning approaches on the same QSAR benchmark, and prove that the FA is significantly better.  Keywords: recurrent neural networks, folding architecture, drug design, quantitative structure activity relationships, inhibit...
ML
szymkowiak01hierarchical
Hierarchical Clustering for Datamining . This paper presents hierarchical probabilistic clustering methods for unsupervised  and supervised learning in datamining applications. The probabilistic clustering  is based on the previously suggested Generalizable Gaussian Mixture model.  A soft version of the Generalizable Gaussian Mixture model is also discussed. The  proposed hierarchical scheme is agglomerative and based on a  L 2 distance metric.  Unsupervised and supervised schemes are successfully tested on artificially data and  for segmention of e-mails.  1 
IR
ross98complex
Complex Aggregation at Multiple Granularities . Datacube queries compute simple aggregates at multiple granularities. In this paper we examine the more general and useful problem of computing a complex subquery involving multiple dependent aggregates at multiple granularities. We call such queries "multi-feature cubes." An example is "Broken down by all combinations of month and customer, find the fraction of the total sales in 1996 of a particular item due to suppliers supplying within 10% of the minimum price (within the group), showing all subtotals across each dimension." We classify multi-feature cubes based on the extent to which fine granularity results can be used to compute coarse granularity results; this classification includes distributive, algebraic and holistic multi-feature cubes. We provide syntactic sufficient conditions to determine when a multi-feature cube is either distributive or algebraic. This distinction is important because, as we show, existing datacube evaluation algorithms can be used to compute multif...
DB
duch98minimal
Minimal Distance Neural Methods A general framework for minimal distance  methods is presented. Radial Basis Functions (RBFs) and Multilayer Perceptrons (MLPs) neural networks are included in this framework as special cases. New versions of minimal distance methods are formulated. A few of them have been tested on a real-world datasets obtaining very encouraging results.  
ML
arleo99neuromimetic
Neuro-Mimetic Navigation Systems: A Computational Model of the Rat Hippocampus : We propose a bio-inspired approach to autonomous navigation  based on some of the components that rats use for navigation. A spatial  model of the environment is constructed by unsupervised Hebbian learning.  The representation consists of a population of localized overlapping place  elds, modeling place cell activity in the rat Hippocampus. Place elds  are established by extracting spatio-temporal properties of the environment  from visual sensory inputs. Visual ambiguities are resolved by means of  path integration. Reinforcement learning is applied to use place cell activity  for goal-oriented navigation. Experimental results obtained with a mobile  Khepera robot are presented.  Keywords: Autonomous robots, hippocampus, place elds, unsupervised  learning, reinforcement learning, population vector coding, path integration.  1. Introduction  The complexity of the autonomous navigation task is inherent in the concept of autonomy: Ideally, an autonomous agent should have a completely ...
ML
rigoll99statistical
Statistical Pattern Recognition Techniques for Multimodal Human Computer Interaction and Multimedia Information Processing This paper presents an extensive overview on statistical pattern recognition methods for a variety of different tasks, related to multimodal human-computer interaction and multimedia information processing. Typical tasks in the area of human-computer interaction include handwriting and gesture recognition, as well as pen-based retrieval of image databases. Multimedia information processing includes algorithms for document processing, video indexing or face recognition. The aim of the paper is to demonstrate to the speech community the usability of classical speech recognition algorithms, such as Hidden Markov Models and related statistical pattern recognition techniques, for a much larger variety of related problems in man-machine-communication and the ecient processing and retrieval of multimedia information.
HCI
orovas98cellular
A Cellular System for Pattern Recognition using Associative Neural Networks : A cellular system for pattern recognition is presented in this paper. The cells are placed in a two dimensional array and they are capable of performing basic symbolic processing and exchanging messages about their state. Following a cellular automata like operation the aim of the system is to transform an initial symbolic description of a pattern to a correspondent object level representation. To this end, a hierarchical approach for the description of the structure of the patterns is followed. The underlying processing engine of the system is the AURA model of associative memory. The system is endowed with a learning mechanism utilizing the distributed nature of the architecture. A dedicated hardware platform is also available. 1 Introduction One of the basic characteristics of cellular automata [1] is their ability for parallel and distributed processing. This is due to the co-operation of relatively simple interconnected processing units called cells. These are connected followin...
ML
154973
A Data Mining Framework for Building Intrusion Detection Models There is often the need to update an installed Intrusion Detection System (IDS) due to new attack methods or upgraded computing environments. Since many current IDSs are constructed by manual encoding of expert knowledge, changes to IDSs are expensive and slow. In this paper, we describe a data mining framework for adaptively building Intrusion Detection (ID) models. The central idea is to utilize auditing programs to extract an extensive set of features that describe each network connection or host session, and apply data mining programs to learn rules that accurately capture the behavior of intrusions and normal activities. These rules can then be used for misuse detection and anomaly detection. New detection models are incorporated into an existing IDS through a meta-learning (or co-operative learning) process, which produces a meta detection model that combines evidence from multiple models. We discuss the strengths of our data mining programs, namely, classification, meta-learning...
ML
bauer99trias
TrIAs: Trainable Information Assistants for Cooperative Problem Solving Software agents are intended to perform certain tasks on behalf of their users. In many cases, however, the agent's competence is not sufficient to produce the desired outcome. This paper presents an approach to cooperative problem solving in which an information agent and its user try to support each other in the achievement of a particular goal. As a side effect the user can extend the agent's capabilities in a programming-by-demonstration dialog, thus enabling it to autonomously perform similar tasks in the future.  1 Introduction  Software agents are intended to autonomously perform certain tasks on behalf of their users. In many cases, however, the agent's competence might not be sufficient to produce the desired outcome. Instead of simply giving up and leaving the whole task to the user, a much better alternative would be to precisely identify what the cause of the current problem is, communicate it to another agent who can be expected to be able (and willing) to help, and use th...
Agents
443654
Markov Techniques for Object Localization With Force-Controlled Robots This paper deals with object localization with forcecontrolled robots in the Bayesian framework [1]. It describes a method based on Markov Localization techniques with a Monte Carlo implementation applied for solving 3D (6 degrees of freedom) global localization problems with force-controlled robots. The approach was successfully applied to problems such as the recursive localization of a box by a robot manipulator.
ML
33385
The Effect of Network Hierarchy Structure on Performance of ATM PNNI Hierarchical Routing Networks deploying hierarchical routing are recursively partitioned into sub-networks that do not reveal the full details of their internal structure outside their domains. Instead, an aggregated view of certain parameters that are associated with traversal within such sub-networks between their border nodes is advertised. The ATM PNNI standard and the Internet Nimrod architecture both adopt this approach for routing.  This paper studies the effectiveness of ATM hierarchical routing protocols on networks with different hierarchical structures by simulation. Our study shows that, in general, the hierarchical source routing performs well compared to the global routing strategy which imposes no hierarchy, while utilizing less storage and communication overhead. For certain networks and topologies, the hierarchical routing performs better than the global routing. Different hierarchies imposed on the same topologies have significantly different performance on the throughput and routing dela...
DB
bederson95pad
PAD++: A Zoomable Graphical Sketchpad for Exploring Alternate Interface Physics We describe Pad++, a zoomable graphical sketchpad that we are exploring as an alternative to traditional window and icon-based interfaces. We discuss the motivation for Pad++, describe the implementation, and present prototype applications. In addition, we introduce an informational physics strategy for interface design and briefly contrast it with current design strategies. We envision a rich world of dynamic persistent informational entities that operate according to multiple physics specifically designed to provide cognitively facile access and serve as the basis for design of new computationally-based work materials.  1  To appear in the Journal of Visual Languages and Computing.  Pad++: A Zoomable Graphical Sketchpad For Exploring Alternate Interface Physics  1 Benjamin B. Bederson James D. Hollan  Computer Science Department University of New Mexico Albuquerque, NM 87131  (bederson@cs.unm.edu, hollan@cs.unm.edu)  Ken Perlin Jonathan Meyer David Bacon  Media Research Laboratory Co...
HCI
zamir98web
Web Document Clustering: A Feasibility Demonstration Abstract Users of Web search engines are often forced to sift through the long ordered list of document “snippets” returned by the engines. The IR community has explored document clustering as an alternative method of organizing retrieval results, but clustering has yet to be deployed on the major search engines. The paper articulates the unique requirements of Web document clustering and reports on the first evaluation of clustering methods in this domain. A key requirement is that the methods create their clusters based on the short snippets returned by Web search engines. Surprisingly, we find that clusters based on snippets are almost as good as clusters created using the full text of Web documents. To satisfy the stringent requirements of the Web domain, we introduce an incremental, linear time (in the document collection size) algorithm called Suffix Tree Clustering (STC). which creates clusters based on phrases shared between documents. We show that STC is faster than standard clustering methods in this domain, and argue that Web document clustering via STC is both feasible and potentially beneficial. 1
IR
folch00typtex
TypTex: Inductive typological text classification by multivariate statistical analysis for NLP systems tuning/evaluation The increasing use of methods in natural language processing (NLP) which are based on huge corpora require that the lexical, morphosyntactic and syntactic homogeneity of texts be mastered. We have developed a methodology and associate tools for text calibration or "profiling" within the ELRA benchmark called "Contribution to the construction of contemporary french corpora" based on multivariate analysis of linguistic features. We have integrated these tools within a modular architecture based on a generic model allowing us on the one hand flexible annotation of the corpus with the output of NLP and statistical tools and on the other hand retracing the results of these tools through the annotation layers back to the primary textual data. This allows us to justify our interpretations.  1. Introduction  Natural Language Processing (NLP) is increasingly dependent on corpus-based methods. The availability of corpora is no longer a problem, as huge and annotated corpora are now readily avail...
IR
91978
Execution Monitoring of High-Level Robot Programs. Imagine a robot that is executing a program  on-line, and, insofar as it is reasonable to do  so, it wishes to continue with this on-line  program execution, no matter what exogenous  events occur in the world. Execution  monitoring is the robot's process of observing  the world for discrepancies between the  actual world and its internal representation  of it, and recovering from such discrepancies.  We provide a situation calculus-based account  of such on-line program executions,  with monitoring. This account relies on a  specification for a single-step interpreter for  the logic programming language Golog . The  theory is supported by an implementation  that is illustrated by a standard blocks world  in which a robot is executing a Golog program  to build a suitable tower. The monitor  makes use of a simple kind of planner for  recovering from malicious exogenous actions  performed by another agent. After performing  the sequence of actions generated by the  recovery procedure, th...
AI
lin02discovering
Discovering Informative Content Blocks from Web Documents In this paper, we propose a new approach to discover informative contents from a set of tabular documents (or Web pages) of a Web site. Our system, InfoDiscoverer, first partitions a page into several content blocks according to HTML tag <TABLE> in a Web page. Based on the occurrence of the features (terms) in the set of pages, it calculates entropy value of each feature. According to the entropy value of each feature in a content block, the entropy value of the block is defined. By analyzing the information measure, we propose a method to dynamically select the entropy-threshold that partitions blocks into either informative or redundant. Informative content blocks are distinguished parts of the page, whereas redundant content blocks are common parts. Based on the answer set generated from 13 manually tagged news Web sites with a total of 26,518 Web pages, experiments show that both recall and precision rates are greater than 0.956. That is, using the approach, informative blocks (news articles) of these sites can be automatically separated from semantically redundant contents such as advertisements, banners, navigation panels, news categories, etc. By adopting InfoDiscoverer as the preprocessor of information retrieval and extraction applications, the retrieval and extracting precision will be increased, and the indexing size and extracting complexity will also be reduced.
IR
sima00computational
The Computational Theory of Neural Networks In the present paper a detailed taxonomy of neural network models with various restrictions is presented with respect to their computational properties. The criteria of classification include e.g. feedforward and recurrent architectures, discrete and continuous time, binary and analog states, symmetric and asymmetric weights, finite size and infinite families of networks, deterministic and probabilistic models, etc. The underlying results concerning the computational power of perceptron, RBF, winner-take-all, and spiking neural networks are briey surveyed and completed by relevant references.
AI
crescenzi01roadrunner
RoadRunner: Towards Automatic Data Extraction from Large Web Sites The paper investigates techniques for extracting data from HTML sites through the use of automatically generated wrappers. To automate the wrapper generation and the data extraction process, the paper develops a novel technique to compare HTML pages and generate a wrapper based on their similarities and differences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach. 1
IR
536962
The Diagnosis Frontend of the dlv System This paper presents the Diagnosis Frontend of dlv, which is a knowledge representation system under development at the Technische Universität Wien. The kernel language of the system is an extension of disjunctive logic programming (DLP) by integrity constraints; it offers frontends to several advanced knowledge representation formalisms. The formal model of diagnosis employed in the frontend includes both abductive diagnosis (over DLP theories) and consistency-based diagnosis. For each of the two diagnosis modalities, generic diagnoses, single error diagnoses, and subset minimal diagnoses are considered. We illustrate the use of the frontend by showing the dlv encodings of several diagnosis problems. Thereafter, we discuss implementation issues. Diagnostic reasoning is implemented on the dlv engine through suitable translations of diagnostic problems into disjunctive logic programs, such that their stable models correspond to diagnoses. For the six kinds of diagnostic reasoning problems emerging from above, such reductions are provided
DB
254041
XML Query Languages: Experiences and Exemplars This paper identifies essential features of an XML query language by examining four existing query languages: XML-QL, YA T L , Lorel, and XQL. The first three languages come from the database community and possess striking similarities. The fourth comes from the document community and lacks some key functionality of the other three.
DB
grolimund95integrating
Integrating Case Based Reasoning and Tabu Search for Solving Optimisation Problems Tabu search is an established heuristic optimisation technique for problems where exact algorithms are not available. It belongs to the same family as simulated annealing or genetic algorithms. It extends the basic iterative improvement scheme by adding control learning. A technique of this kind, intensification, captures experience established on a frequency-based analysis of past search. Experience is reused while the same optimisation process is going on in order to guide search to better solutions.  In this paper, we introduce a case-based reasoning approach for control learning in tabu search. Search experience concerns operator selection and is represented by cases. The aim of case reuse is to improve conflict resolution. While the proposed method is domain independent, we present its application to the NPhard uncapacitated facility location problem. Experimental results show that adding our approach to a basic tabu search optimisation significantly improves solution quality on t...
ML
abiteboul01representing
Representing and Querying XML with Incomplete Information We study the representation and querying of XML with incomplete information. We consider a simple model for XML data and their DTDs, a very simple query language, and a representation system for incomplete information in the spirit of the representations systems developed by Imielinski and Lipski for relational databases. In the scenario we consider, the incomplete information about an XML document is continuously enriched by successive queries to the document. We show that our representation system can represent partial information about the source document acquired by successive queries,  and that it can be used to intelligently answer new queries.  We also consider the impact on complexity of enriching our representation system or query language with additional features. The results suggest that our approach achieves a practically appealing balance between expressiveness and tractability. The research presented here was motivated by the Xyleme project at INRIA, whose objectiveittodevelop a data warehouse for Web XML documents.  1. 
DB
gomoluch01information
Information agents on the move: A survey on load-balancing with mobile agents Information agents process and integrate heterogeneous, distributed information. To achieve this task efficiently, some researchers promote the idea of mobile information agents [13, 53, 44, 20, 10], which migrate between a user's host and other hosts in the network. We outline the concepts behind mobile information agents and give a survey on load balancing, which aims to optimise distributed information processing.
Agents
goller99feature
Feature Extraction and Learning Vector Quantization for Data Structures During the last years, folding architecture networks and the closely related concept of recursive neural networks have been developed for solving supervised learning tasks on data structures. In this paper we address the fundamental problem of finding fixedlength vector representations for structures in an unsupervised way. A solution based on ideas from feature extraction and folding architecture networks is proposed. Furthermore, a new method for supervised learning for data structures which combines ideas from learning vector quantization and folding architecture networks is suggested. 1 Introduction  In almost all fields of scientific and technical reasoning, people and systems assisting them have to deal with structured objects . Examples are chemical structures, algebraic (mathematical) expressions and formulas, software source code, and conceptual and taxonomic graphs. With structured objects we mean objects which are composed of `smaller' objects, which may be structured too. T...
ML
dwyer99patterns
Patterns in Property Specifications for Finite-State Verification Model checkers and other finite-state verification tools allow developers to detect certain kinds of errors automatically. Nevertheless, the transition of this technology from research to practice has been slow. While there are a number of potential causes for reluctance to adopt such formal methods, we believe that a primary cause is that practitioners are unfamiliar with specification processes, notations, and strategies. In a recent paper, we proposed a pattern-based approach to the presentation, codification and reuse of property specifications for finite-state verification. Since then, we have carried out a survey of available specifications, collecting over 500 examples of property specifications. We found that most are instances of our proposed patterns. Furthermore, we have updated our pattern system to accommodate new patterns and variations of existing patterns encountered in this survey. This paper reports the results of the survey and the current status of our pattern syste...
AI
539761
Creatures: Artificial Life Autonomous Software Agents for Home Entertainment This paper gives a technical description of Creatures, a commercial home-entertainment software package. Creatures provides a simulated...
Agents
449211
Formal ReSpecT Logic tuple centres have s own that logic-ba d languages can be e#ectively exploited not only for building individual agents and enabling interagent communication in multi-agent ssG ms butals for  ruli ng inter-agent communications as to builds cial behaviours In this paper, we formally define the notion of logic tuple centre as well as the operationals emantics of the logic-bas d language ReSpecT for the behaviours pecification of logic tuple centres . For this purpos e, we exploit a generals emantic framework for as ynchronous dis tributeds ys tems allowing a coordination medium to be formally denoted in as eparate and independent way with res pect to the whole coordinateds ys tem. This s hows that a logic-bas ed coordination medium does not limit agents and coordination languages to be logic-bas ed, but may ins  tead enable agents of di#erents orts and technologies to be combined and coordinated in an e#ective way by exploiting a logic-bas ed approach. 1 Coordinationm edia form ulti...
Agents
62931
From theory to practice: The UTEP robot in the AAAI 96 and AAAI 97 robot contests In this paper we describe the control aspects of Diablo, the UTEP mobile robot participant in two AAAI robot competitions. In the first competition, event one of the AAAI 96 robot contest, Diablo consistently scored 285  1  out of a total of 295 points. In the second competition, our robot won the first place in the event "Tidy Up" of the home vacuum contest. The main goal in this paper will be to show how the agent theories - based on action theories -- developed at UTEP and by Saffiotti et al. was used in the building of Diablo.  1 Introduction  We participated  2  in the AAAI 96 robot navigation contest [KNH97] and the AAAI 97 home vacuum contest. In the first competition, our team scored 285 points in all runs of the contest out of a total of 295 points and was placed third in the finals. In the second competition, we won the first place in the event "Tidy Up." In this paper we relate theory of agents, particularly the one developed at UTEP, for higher level control, and the one by...
AI
sastry01efficient
Efficient Atomic Cluster Optimization Using A Hybrid Extended Compact Genetic Algorithm With Seeded Population A recent study (Sastry & Xiao, 2001) proposed a highly reliable cluster optimization algorithm
AI
maurer00second
Second Order Sufficient Conditions for Optimal Control Problems with Free Final Time: The Riccati Approach . Second order sufficient conditions (SSC) for control problems with control--state constraints and free final time are presented. Instead of deriving such SSC de initio, the control problem with free final time is tranformed into an augmented control problem with fixed final time for which well-known SSC exist. SSC are then expressed as a condition on the positive definiteness of the second variation. A convenient numerical tool for verifying this condition is based on the Riccati approach where one has to find a bounded solution of an associated Riccati equation satisfying specific boundary conditions. The augmented Riccati equations for the augmented control problem are derived and their modifications on the boundary of the control--state constraint are discussed. Two numerical examples, (1) the classical Earth-Mars orbit transfer in minimal time, (2) the Rayleigh problem in electrical engineering, demonstrate that the Riccati equation approach provides a viable numerical test of SS...
AI
rosario99synthetic
A Synthetic Agent System for Bayesian Modeling Human Interactions When building statistical machine learning models from real data one of the most frequently encountered difficulties is the limited amount of training data compared to what is needed by the specific learning architecture. In order to deal with this problem we have developed a synthetic (simulated) agent training system that let us develop flexible prior models for recognizing human interactions in a pedestrian visual surveillance task. We demonstrate the ability to use these prior models to accurately classify real human behaviors and interactions with no additional tuning or training. 1 Introduction  Agent-based solutions have been developed for many different application domains, and field-tested agent systems are steadily increasing in number. Agents are currently being applied in domains as diverse as computer games and interactive cinema, information retrieval and filtering, user interface design, electronic commerce, and industrial process control. In this paper we propose a nove...
ML
457268
Rule Discovery with a Parallel Genetic Algorithm An important issue in data mining is scalability  with respect to the size of the dataset being  mined. In the paper we address this issue by  presenting a parallel GA for rule discovery. This  algorithm exploits both data parallelism, by  distributing the data being mined across all  available processors, and control parallelism, by  distributing the population of individuals across  all available processors.  1 
ML
mitra01data
Data Mining in Soft Computing Framework: A Survey The present article provides a survey of the available literature on data mining using soft computing. A categorization has been provided based on the di#erent soft computing tools and their hybridizations used, the data mining function implemented, and the preference criterion selected by the model. The utility of the di#erent soft computing methodologies is highlighted. Generally fuzzy sets are suitable for handling the issues related to understandability of patterns, incomplete/noisy data, mixed media information and human interaction, and can provide approximate solutions faster. Neural networks are non-parametric, robust, and exhibit good learning and generalization capabilities in data-rich environments. Genetic algorithms provide e#cient search algorithms to select a model, from mixed media data, based on some preference criterion/objective function. Rough sets are suitable for handling di#erent types of uncertainty in data. Some challenges to data mining and the application of soft computing methodologies are indicated. An extensive bibliography is also included. Keywords--- Knowledge discovery, rule extraction, fuzzy logic, neural networks, genetic algorithms, rough sets, neuro-fuzzy computing I.
ML
34099
Mining Optimized Support Rules for Numeric Attributes Mining association rules on large data sets has received considerable attention in recent years. Association rules are useful for determining correlations between attributes of a relation and have applications in marketing, financial and retail sectors. Furthermore, optimized association rules  are an effective way to focus on the most interesting characteristics involving certain attributes. Optimized association rules are permitted to contain uninstantiated attributes and the problem is to determine instantiations such that either the support, confidence or gain of the rule is maximized. In this paper, we generalize the optimized support association rule problem by permitting rules to contain disjunctions over uninstantiated numeric attributes. Our generalized association rules enable us to extract more useful information about seasonal and local patterns involving the uninstantiated attribute. For rules containing a single numeric attribute, we present a dynamic programming algorith...
ML
widom00wsqdsq
WSQ/DSQ: A Practical Approach for Combined Querying of Databases and the Web www-db.stanford.edu We present WSQ/DSQ (pronounced “wisk-disk”), a new approach for combining the query facilities of traditional databases with existing search engines on the Web. WSQ, for Web-Supported (Database) Queries, leverages results from Web searches to enhance SQL queries over a relational database. DSQ, for Database-Supported (Web) Queries, uses information stored in the database to enhance and explain Web searches. This paper focuses primarily on WSQ, describing a simple, low-overhead way to support WSQ in a relational DBMS, and demonstrating the utility of WSQ with a number of interesting queries and results. The queries supported by WSQ are enabled by two virtual tables, whose tuples represent Web search results generated dynamically during query execution. WSQ query execution may involve many high-latency calls to one or more search engines, during which the query processor is idle. We present a lightweight technique called asynchronous iteration that can be integrated easily into a standard sequential query processor to enable concurrency between query processing and multiple Web search requests. Asynchronous iteration has broader applications than WSQ alone, and it opens up many interesting query optimization issues. We have developed a prototype implementation of WSQ by extending a DBMS with virtual tables and asynchronous iteration; performance results are reported. 1
DB
506507
Adaptive and Intelligent Technologies for Web-based Education The paper provides a review of adaptive and intelligent technologies in a context of Web-based distance education. We analyze what kind of technologies are available right now, how easy they can be implemented on the Web, and what is the place of these technologies in large-scale Web-based education.
HCI
460628
Versus: A Temporal Web Repository Web data warehouses are useful for applications that need to process  large amounts of Web data in a short time. This paper presents  Versus, a Web repository model supporting object versioning and distributed  operation for this kind of applications. Versioning allows applications  to save the time dimension of data, enabling the development  of new Web applications. Distribution allows parallel operation over  massive amounts of data. We also present a prototype implementation  of Versus along with results collected from the analisys of the execution  of a distributed Web crawler simulator implemented as a Versus  application.  1 
DB
ijspeert98evolving
Evolving Swimming Controllers for a Simulated Lamprey With Inspiration From Neurobiology This paper presents how neural swimming controllers for a simulated lamprey can be developed using evolutionary algorithms. A Genetic Algorithm is used for evolving the architecture of a connectionist model which determines the muscular activity of a simulated body. This work is inspired by the biological model developed by Ekeberg which reproduces the Central Pattern Generator observed in the real lamprey [Ekeberg 93]. In evolving artificial controllers, we demonstrate that a Genetic Algorithm can be an interesting design technique for neural controllers and that there exist alternative solutions to the biological connectivity. A variety of neural controllers are evolved which can produce the pattern of oscillations necessary for swimming. These patterns can be modulated through the external excitation applied to the network in order to vary the speed and the direction of swimming. The best evolved controllers cover larger ranges of frequencies, phase lags and speeds of swimming than ...
ML
540018
Using Declarative Constraints to Specify the Data Model of Multi-User Application Complex applications such as multi-user applications may  fruitfully be viewed as being composed of a number of independent agents  which access and modify a shared data structure. We will view this shared  data so as to include the domain data being viewed and edited as well  as the entire user interface state.
Agents
459183
Estimating the Orientation and Recovery of Text Planes in a Single Image A method for the fronto-parallel recovery of paragraphs of text under full  perspective transformation is presented. The horizontal vanishing point of the  text plane is found using an extension of 2D projection profiles. This allows  the accurate segmentation of the lines of text. Analysis of the lines will then  reveal the style of justification of the paragraph, and provide an estimate of  the vertical vanishing point of the plane. The text is finally recovered to a  fronto-parallel view suitable for OCR or other higher-level recognition.
HCI
athnes00human
Human factors in ATC alarms and notifications design: an experimental evaluation With the growing use of computerised working position, alarm design on air traffic control displays is a concern as events to be notified increase in number and diversity. Safety requires visual notifications that can be efficiently detected and understood. It also requires that no information on the radar display is obscured by the visual notifications. We also need to design hierarchies of notifications, from most severe to benign. Taking advantage of the current graphical capabilities of computers, we have identified and explored various dimensions in visual alarm design. We present in this paper an experimental evaluation, in terms of detection time and precision, of several of those dimensions: opacity, size, temporal profile of animation and signal frequency. From our results, we conclude that opacity, size and temporal profile of animation are well suited to introduce some nuances in the way we convey notifications on visual displays. We also show that detection of a given signa...
HCI
301428
Integrating Keyword Search into XML Query Processing Due to the popularity of the XML data format, several query languages for XML have been proposed, specially devised to handle data whose structure is unknown, loose, or absent. While these languages are rich enough to allow for querying the content and structure of an XML document, a varying or unknown structure can make formulating queries a very difficult task. We propose an extension to XML query languages that enables keyword search at the granularity of XML elements, that helps novice users formulate queries, and also yields new optimization opportunities for the query processor. We present an implementation of this extension on top of a commercial RDBMS; we then discuss implementation choices and performance results.  Keywords  XML query processing, full-text index  1 Introduction  There is no doubt that XML is rapidly becoming one of the most important data formats. It is already used for scientific data (e.g., DNA sequences), in linguistics (e.g., the Treebank database at the U...
DB
455785
Methods for Sampling Pages Uniformly from the World Wide Web We present two new algorithms for generating uniformly  random samples of pages from the World Wide Web, building  upon recent work by Henzinger et al. (Henzinger et al.  2000) and Bar-Yossef et al. (Bar-Yossef et al. 2000). Both  algorithms are based on a weighted random-walk methodology.  The first algorithm (DIRECTED-SAMPLE) operates  on arbitrary directed graphs, and so is naturally applicable  to the web. We show that, in the limit, this algorithm  generates samples that are uniformly random. The second  algorithm (UNDIRECTED-SAMPLE) operates on undirected  graphs, thus requiring a mechanism for obtaining inbound  links to web pages (e.g., access to a search engine). With  this additional knowledge of inbound links, the algorithm  can arrive at a uniform distribution faster than DIRECTEDSAMPLE,  and we derive explicit bounds on the time to convergence.  In addition, we evaluate the two algorithms on  simulated web data, showing that both yield reliably uniform  samples of pages. We also compare our results with those of  previous algorithms, and discuss the theoretical relationships  among the various proposed methods.  
IR
140079
Gradient-Based Learning Applied to Document Recognition Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradientbased learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN’s), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.
ML
zambonelli01emergence
On the Emergence of Macro Spatial Structures in Dissipative Cellular Automata, and its Implications for Agent-based Distributed Computing This paper describes the peculiar behavior observed in a class of cellular automata that we have defined as "dissipative", i.e., cellular automata that are "open" and makes it possible for the environment to influence the evolution of the automata. Peculiar in the dynamic evolution of this class of cellular automata is that stable macro-level spatial structures emerge from local interactions among cells, a behavior that does not emerge when the cellular automaton is "closed", i.e., when the state of a cell is not influenced by the external world. On this basis, the paper discusses the relations of the performed experiments with the area of open distributed computing, and in particular of agent-based distributed computing. The basic intuition is that dissipative cellular automata express characteristics that strongly resembles those of wide-area open distributed systems based on autonomous and situated active components -- as agents are. Accordingly, similar sorts of macrolevel behaviors are likely to emerge and need to be studied, controlled, and possibly fruitfully exploited.
Agents
46363
Robust Classification Systems for Imprecise Environments In real-world environments it is usually difficult to specify target operating conditions precisely. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. In some cases, the performance of the hybrid can actually surpass that of the best known classifier. The hybrid is also efficient to build, to store, and to update. Finally, we provide empirical evidence that a robust hybrid classifier is needed for many real-world problems. Introduction  Traditionally, classification systems have been built by experimenting with many different classifiers, comparing their performance and choosing the classifier that performs best....
ML
agouris98intelligent
Intelligent Retrieval Of Digital Images From Large Geospatial Databases In this paper we present the development of a spatial data management system utilizing sketch-based queries for the content-based retrieval of digital images from topographic databases. We discuss our overall strategy and associated algorithmic and implementational aspects, and present the associated database design issues. The query tools devised in this research are employing user-provided sketches of the shape and spatial configuration of the object(s) which should appear in the images to be retrieved. Our strategy is scaleindependent. It is inspired by least-squares matching (lsm), and represents an extension of lsm to function with a variety of raster representations. The results are ranked according to statistical scores and the user can subsequently narrow or broaden his/her search according to the previously obtained results and the purpose of the search. 1 INTRODUCTION  Intelligent image retrieval from large databases is one of the novel applications which are receiving increa...
IR
calvanese00containment
Containment of Conjunctive Regular Path Queries with Inverse Reasoning on queries is a basic problem both in knowledge representation and databases. A fundamental form of reasoning on queries is checking containment, i.e., verifying whether one query yields necessarily a subset of the result of another query. Query containment is crucial in several contexts, such as query optimization, knowledge base verification, information integration, database integrity checking, and cooperative answering. In this paper we address the problem of query containment in the context of semistructured knowledge bases, where the basic querying mechanism, namely regular path queries, asks for all pairs of objects that are connected by a path conforming to a regular expression. We consider conjunctive regular path queries with inverse, which extend regular path queries with the possibility of using both the inverse of binary relations, and conjunctions of atoms, where each atom specifies that one regular path query with inverse holds between two v...
DB
kumova00flexible
Flexible Distributed Database Management with AgentTeam . Ideally, distributed management of relational data should enable the sharing of consistent data and provide system transparency. In contrast to information retrieval, users can access compact data and utilise it for further relational processing. However, distributed database management is still a challenging research area that involves bridging syntactic and semantic heterogeneity of data as well as of functionality. Since, existing distributed database management systems were usually built with a focus on the implementation of some dedicated protocols for distributed database management, they are inflexible for major modifications or exchange of the protocols. In addition, their software architecture usually does not comply with well-known design paradigms, which could facilitate the maintenance of the software system. We present an agent-based approach for flexible distributed database management, where independent distributed database management protocols and methods are...
DB
510071
Modeling Temporal Consistency in Data Warehouses Real-world changes are generally discovered delayed by computer systems. The typical update patterns for traditional data warehouses on an overnight or even weekly basis enlarge this propagation delay until the information is available to knowledge workers. The main contribution of the paper is the identification of two different temporal characterizations of the information appearing in a data warehouse: one is the classical description of the time instant when a given fact occurred, the other represents the instant when the information has been entered into the system. We present an approach for modeling conceptual time consistency problems and introduce a data model that deals with timely delays and supports knowledge workers to determine what the situation was in the past, knowing only the information available at a given instant of time. 1
DB
botchers96layout
Layout Rules for Graphical Web Documents The number of companies, institutions, and individuals competing for attention in the World-Wide Web is growing exponentially. This makes designing informative, easy-to-grasp, and visually appealing documents not only important for userfriendly information presentation, but also the key to success for any information provider. In this paper, we present layout guidelines for textual and graphical, static and dynamic, 2-D and 3-D Web documents which are drawn from fields as diverse as typography, Gestalt psychology, architecture, hypertext authoring, and human-computer interaction. Web documents are classified into five basic types, and our layout rules are applied to each of these. Finally, we show how currently evolving standards (HTML 3.0 for text and still graphics, Java for 2-D animation, and VRML for 3-D worlds) support applying those rules. 1 Introduction  Whenever a new information-conveying technology is invented, it usually takes many years until authors develop new media that ...
HCI
emmerich00implementing
Implementing Incremental Code Migration with XML We demonstrate how XML and related technologies can be used for code mobility at any granularity, thus overcoming the restrictions of existing approaches. By not fixing a particular granularity for mobile code, we enable complete programs as well as individual lines of code to be sent across the network. We define the concept of incremental code mobility as the ability to migrate and add, remove, or replace code fragments (i.e., increments) in a remote program. The combination of fine-grained and incremental migration achieves a previously unavailable degree of flexibility. We examine the application of incremental and fine-grained code migration to a variety of domains, including user interface management, application management on mobile thin clients, for example PDAs, and management of distributed documents.  Keywords  Incremental Code Migration, XML Technologies  1 INTRODUCTION  The increasing popularity of Java and the spread of Webbased technologies are contributing to a growing ...
Agents
williamson00foundations
Foundations for Bayesian networks Bayesian networks are normally given one of two types of foundations: they are either treated purely formally as an abstract way of representing probability functions, or they are interpreted, with some causal interpretation given to the graph in a network and some standard interpretation of probability given to the probabilities specified in the network. In this chapter I argue that current foundations are problematic, and put forward new foundations which involve aspects of both the interpreted and the formal approaches. One standard approach is to interpret a Bayesian network objectively: the graph in a Bayesian network represents causality in the world and the specified probabilities are objective, empirical probabilities. Such an interpretation founders when the Bayesian network independence assumption (often called the causal Markov condition) fails to hold. In §2 I catalogue the occasions when the independence assumption fails, and show that such failures are pervasive. Next, in §3, I show that even where the independence assumption does hold objectively, an agent’s causal knowledge is unlikely to satisfy the assumption with respect to her subjective probabilities, and that slight differences between an agent’s subjective Bayesian network and an objective Bayesian network can lead to large differences between probability distributions determined by these networks. To overcome these difficulties I put forward logical Bayesian foundations in §5. I show that if the graph and probability specification in a Bayesian network are thought of as an agent’s background knowledge, then the agent is most rational if she adopts the probability distribution determined by the
AI
moreno97how
How to Avoid Knowing It All Beliefs have been formally modelled in the last decades using doxastic logics. The possible worlds model and its associated Kripke semantics provide an intuitive semantics for these logics, but they seem to commit us to model agents that are logically omniscient (they believe every classical tautology) and perfect reasoners (their beliefs are closed under classical deductive closure). Thus, this model would not be appropriate to model non-ideal agents, that have resource limitations that prevent them from attaining such levels of doxastic competence. This report contains a statement of these problems and a brief survey of some of the most interesting approaches that have been suggested to overcome them. Contents 1 Formal models of belief 3 1.1 Possible worlds and Kripke semantics . . . . . . . . . . . . . . . . . . . . . 3 1.2 Logical omniscience and perfect reasoning . . . . . . . . . . . . . . . . . . 5 2 Avoiding logical omniscience 7 2.1 Syntactic approaches . . . . . . . ...
Agents
sarawagi00data
Data Mining Models as Services on the Internet The goal of this article is to raise a debate on the usefulness of providing data mining models as services on the internet. These services can be provided by anyone with adequate data and expertise and made available on the internet for anyone to use. For instance, Yahoo or Altavista, given their huge categorized document collection, can train a document classifier and provide the model as a service on the internet. This way data mining can be made accessible to a wider audience instead of being limited to people with the data and the expertise. A host of practical problems need to be solved before this idea can be made to work. We identify them and close with an invitation for further debate and investigation.  1. 
IR
hoppenbrouwers98browsing
Browsing Information Spaces This document contains the generic background and targets of the Advanced Information Space Browser, which is planned to be included in the Decomate-II library information system at Tilburg University. It gives an overview of the current state-of-the-art in information retrieval, focusing especially on topic browsers, thesauri, and semantic networks. Some preliminary ideas for actual implementations are included as well. Keywords: Semantic network, conceptual modeling, topic browsing, document retrieval, Decomate-II. 1 Introduction  The Decomate-II Library System, currently under development at Tilburg University in cooperation with several European partners, aims at a Web-based single point user interface to a multitude of (possibly distributed) databases. A single user query, usually a set of keywords, is mapped to all connected databases, each with its own query language, data schema, and contents. The individual query results are merged together by the system, post-processed to eli...
IR
314651
Management and Query Processing of one dimensional Intervals with the UB-Tree The management and query processing of one dimensional intervals is a special case of extended object handling. One dimensional intervals play an important role in temporal databases and they can also be used for fuzzy matching, fuzzy logic and measuring quality classes, etc. Most existing multidimensional access methods for extended objects do not address this special problem and most of them are main memory access methods that do not support e#cient access to secondary storage.  The research in the application of the UB-Tree to extended objects is part of my doctoral work. The contribution of this article is a specific solution for managing and querying one dimensional intervals with the UB-Tree, a multidimensional extension of the classical B-Tree. The combination of UB-Tree and transformation of extended objects to parameter space is an e#ective solution for this specific problem.  Keywords: one dimensional intervals, extended object handling, point query, range query, spatial data...
DB
clifton01privacy
Privacy Preserving Distributed Data Mining em, there is a simple distributed solution that provides a degree of privacy to the individual sites. An example association rule could be:  Received F lu shot and age > 50 implies hospital admission, where at least 5% of  insured meet all the criteria (support), and at least 30% of those meeting the flu shot  and age criteria actually require hospitalization (confidence).  There are algorithms to e#ciently find all association rules with a minimum level of support. We can easily extend this to the distributed case using the following lemma: If a rule has support > k% valid  The Data  Approach  Figure 1: Data Warehouse approach to Distributed Data Mining  globally, it must have support > k% on at least one of the individual sites.    A distributed algorithm for this would work as follows: Request that each site send all rules with support at least k. For e
DB
oria99defining
Defining Views In An Image Database System : A view mechanism can help handle the complex semantics in emerging  application areas such as image databases. This paper presents the view  mechanism we defined for the DISIMA image database system. Since DISIMA is  being developed on top of an object-oriented database system, we first propose  apowerful object-oriented view mechanism based on the separation between  types (interface functions) and classes that manage objects of the same type.  The image view mechanism uses our object-oriented view mechanism to allow  us to give differentsemantics to the same image. The solution is based on the  distinction between physical salient objects which are interesting objects in an  image and logical salient objects which are the meanings of these objects.  14.1 INTRODUCTION  Views have been widely used in relational database management systems to extend modeling capabilities and to provide data independence. Basically, views in a relational database can be seen as formulae defining virtua...
DB
cal02expressive
On the Expressive Power of Data Integration Systems There are basically two approaches for designing a data integration  system. In the global-as-view (GAV) approach, one maps the concepts in the  global schema to views over the sources, whereas in the local-as-view (LAV) approach,  one maps the sources into views over the global schema. The goal of  this paper is to relate the two approaches with respect to their expressive power.
DB
cohn01missing
The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis.
IR
nottelmann01mind
MIND: An architecture for multimedia information retrieval in federated digital libraries Introduction  Today, people have routine access to a huge number of heterogeneous and distributed digital libraries. To satisfy an information need, relevant libraries have to be selected, the information need has to be reformulated for every library w. r. t. its schema and query syntax, and the results have to be fused. This is an ineffective manual task for which accurate tools are desirable.  MIND (which we are currently developing in an EU project) is an end-to-end solution for federated digital libraries which covers all these issues. We start from information retrieval approaches which focus on retrieval quality, but mostly only consider monomedial and homogeneous sources. We will extend these approaches for dealing with different kinds of media (text, facts, images and transcripts of speech recognition) as well as handling heterogeneous libraries (e.g., with different schemas). Another innovation is that MIND also considers non-co-operating libraries which only provide the 
IR
pirjanian00multirobot
Multi-Robot Target Acquisition using Multiple Objective Behavior Coordination In this paper we propose an approach to multi-robot coordination in the context of cooperative target acquisition. The approach is based on multiple objective behavior coordination extended to multiple robots. It provides mechanisms for distributed command fusion across a group of robots to pursue multiple goals of multiple robots in parallel. The mechanisms enable each robot to select actions that not only benefit itself but also benefit the group as a whole. Experimental results with two mobile robots validate that, by using this method, a group of robots can successfully track and acquire a moving target. 1 Introduction Cooperation of a team of robots in unknown settings poses complex control problems which require solutions that guarantee a suitable trade-off between a multitude of (potentially) conflicting task objectives within and among the robots. For instance, an action that is optimal with respect to a particular robot might be unacceptable with respect to the others. Thus ...
ML
535887
Dynamic Agent Discovery One of the issues that has gained importance in the real world  applications of agent systems is the bootstrapping of agents.
Agents
yang99mixtures
Mixtures of Linear Subspaces for Face Detection We present two methods using mixtures of linear subspaces for face detection in gray level images. One method uses a mixture of factor analyzers to concurrently perform clustering and, within each cluster, perform local dimensionality reduction. The parameters of the mixture model are estimated using an EM algorithm. A face is detected if the probability of an input sample is above a predened threshold. The other mixture of subspaces method uses Kohonen 's self-organizing map for clustering and Fisher Linear Discriminant to nd an optimal projection and a Gaussian distribution to model the class-conditional density function of the projected samples for each class. The parameters of the class-conditional density functions are maximum likelihood estimates and the decision rule is also based on maximum likelihood. A wide range of face images including ones in dierent poses, with dierent expressions and under dierent lighting conditions are used as the training set to capture the varia...
ML
lam00broadcasting
Broadcasting Consistent Data to Mobile Clients with Local Cache Although data broadcast has been shown to be an efficient method for disseminating data items in a mobile computing system with large number of clients, the issue on how to ensure currency and consistency of the data items has not been examined adequately.
HCI
shneiderman99supporting
Supporting Creativity with Advanced Information-Abundant User Interfaces A challenge for human-computer interaction researchers and user interface designers is to construct information technologies that support creativity. This ambitious goal can be attained if designers build on an adequate understanding of creative processes. This paper describes a model of creativity, the four-phase genex framework for generating excellence: - Collect: learn from previous works stored in digital libraries, the web, etc. - Relate: consult with peers and mentors at early, middle and late stages - Create: explore, compose, discover, and evaluate possible solutions - Donate: disseminate the results and contribute to the digital libraries, the web, etc. Within this integrated framework, there are eight activities that require human-computer interaction research and advanced user interface design. This paper concentrates on techniques of information visualization that support creative work by enabling users to find relevant information resources, identify desired items in a se...
HCI
60728
Curio: A Novel Solution for Efficient Storage and Indexing in Data Warehouses Efficient query processing is a critical requirement for data warehousing systems as decision support applications often require interactive response times to answer complex, ad-hoc queries (e.g., aggregations, multi-way joins) over vast repositories of data (e.g., hundreds of gigabytes to terabytes in size). The most common approach used to improve On-Line Analytical Processing (OLAP) query performance is to utilize indexes or access structures to quickly access the base data. A major drawback to this approach is that it often incurs significant overhead as the access structures must be stored in addition to the base data. In this paper, we present Curio, a data repository and OLAP query server, which provides drastically improved performance for ad-hoc queries, while simultaneously reducing the storage costs associated with warehousing. Curio, a data storage and access technology for data warehouses recently developed by Muninn Technologies, LLC, is based on a novel paradigm that all...
DB
257460
More Than Just a Pretty Face: Affordances of Embodiment Prior research into embodied interface agents has found that users like them and find them engaging. In this paper, we argue that embodiment can serve an even stronger function if system designers use actual human conversational protocols in the design of the interface. Communicative behaviors such as salutations and farewells, conversational turn-taking with interruptions, and referring to objects using pointing gestures are examples of protocols that all native speakers of a language already know how to perform and that can thus be leveraged in an intelligent interface. We discuss how these protocols are integrated into Rea, an embodied, multi-modal conversational interface agent who acts as a real-estate salesperson, and we show why embodiment is required for their successful implementation.  INTRODUCTION  There is a qualitative difference between face-to-face conversation and other forms of human-human communication [4]. Businesspeople and academics routinely travel long distances ...
HCI
defalco98optimizing
Optimizing Neural Networks for Time Series Prediction In this paper we investigate the effective design of an appropriate neural network model for time series prediction based on an evolutionary approach. In particular, the Breeder Genetic Algorithms are considered to face contemporaneously the optimization of (i) the design of a neural network architecture and (ii) the choice of the best learning method. The effectiveness of the approach proposed is evaluated on a standard benchmark for prediction models, the Mackey--Glass series. 1. Introduction  The main motivation for time series research is to provide a prediction when a mathematical model of a phenomenon is either unknown or incomplete. A time series consists of measurements or observations of the previous outcomes of a phenomenon that are made sequentially over time. If these consecutive observations are dependent on each other then it is possible to attempt a prediction. Clearly it is supposed that the process is somehow predictable. The time series prediction problems are usually...
ML
nicola99increasing
Increasing the Expressiveness of Analytical Performance Models for Replicated Databases . The vast number of design options in replicated databases requires efficient analytical performance evaluations so that the considerable overhead of simulations or measurements can be focused on a few promising options. A review of existing analytical models in terms of their modeling assumptions, replication schemata considered, and network properties captured, shows that data replication and intersite communication as well as workload patterns should be modeled more accurately. Based on this analysis, we define a new modeling approach named 2RC (2-dimensional replication model with integrated communication). We derive a complete analytical queueing model for 2RC and demonstrate that it is of higher expressiveness than existing models. 2RC also yields a novel bottleneck analysis and permits to evaluate the trade-off between throughput and availability. 1 Introduction  Replication management in distributed databases concerns the decision when and where to allocate physical copies of ...
DB
vaidya02privacy
Privacy Preserving Association Rule Mining in Vertically Partitioned Data Privacy considerations often constrain data mining projects. This paper addresses the problem of association rule mining where transactions are distributed across sources. Each site holds some attributes of each transaction, and the sites wish to collaborate to identify globally valid association rules. However, the sites must not reveal individual transaction data. We present a two-party algorithm for efficiently discovering frequent itemsets with minimum support levels, without either site revealing individual transaction values.
DB
bagnell01autonomous
Autonomous Helicopter Control using Reinforcement Learning Policy Search Methods Many control problems in the robotics field can be cast as Partially Observed Markovian Decision Problems (POMDPs), an optimal control formalism. Finding optimal solutions to such problems in general, however is known to be intractable. It has often been observed that in practice, simple structured controllers suffice for good sub-optimal control, and recent research in the artificial intelligence community has focused on policy search methods as techniques for finding sub-optimal controllers when such structured controllers do exist. Traditional model-based reinforcement learning algorithms make a certainty equivalence assumption on their learned models and calculate optimal policies for a maximumlikelihood Markovian model. In this work, we consider algorithms that evaluate and synthesize controllers under distributions of Markovian models. Previous work has demonstrated that algorithms that maximize mean reward with respect to model uncertainty leads to safer and more robust controll...
ML
11510
The CMUnited-97 Simulator Team . The Soccer Server system provides a rich and challenging multiagent, real-time domain. Agents must accurately perceive and act despite a quickly changing, largely hidden, noisy world. They must also act at several levels, ranging from individual skills to full-team collaborative and adversarial behaviors. This article presents the CMUnited-97 approaches to the above challenges which helped the team to the semifinals of the 29-team RoboCup-97 tournament. 1 Introduction  The Soccer Server system [5] used at RoboCup-97 [2] provides a rich and challenging multiagent, real-time domain. Sensing and acting is noisy, while interagent communication is unreliable and low-bandwidth. In order to be successful, each agent in a team must be able to sense and act in real time: sensations arrive at unpredictable intervals while actions are possible every 100ms. Furthermore, since the agents get local, noisy sensory information, they must have a method of converting their sensory inputs into a good w...
AI
werger00ayllu
Ayllu: Distributed Port-Arbitrated Behavior-Based Control . Distributed control of a team of mobile robots presents a number of unique challenges, including highly unreliable communication, real world task and safety constraints, scalability, dynamic reconfigurability, heterogenous platforms, and a lack of standardized tools or techniques. Similar problems plagued development of single robots applications until the "behavior-based" revolution led to new techniques for robot control based on port-arbitrated behaviors (PAB). Though there are now many implementations of systems for behavior-based control of single robots, the potential for distributing such control across robots for multi-agent control has not until now been fully realized.  This paper presents Ayllu, a system for distributed multi-robot behavioral control. Ayllu allows standard PAB interaction (message passing, inhibition, and suppression) to take place over IP networks, and extends the PAB paradigm to provide for arbitrary scalability. We give a brief overview of the Broadcast...
ML
degaris99evolving
Evolving an Optimal De/Convolution Function for the Neural Net Modules of ATR's Artificial Brain Project This paper reports on efforts to evolve an optimum de/convo-lution function to be used to convert analog to binary signals (spike trains) and vice versa for the binary input/output signals of the neural net circuit modules evolved at electronic speeds by the so-called "CAM-Brain Machine (CBM)" of ATR's Artificial Brain Project [1, 2, 3]. The CBM is an FPGA based piece of hardware which will be used to evolve tens of thousands of cellular automata based neural network circuits or modules at electronic speeds in about a second each, which are then downloaded into humanly architected artificial brains in a large RAM space [2, 3]. Since state-of-the-art programmable FPGAs constrained us to use 1 bit binary signaling in our neural model (the "CoDi-1Bit" model [4]), an efficient de/convolution technique is needed to convert digital signals to analog and vice versa, so that "evolutionary engineers" (EEs) who evolve the many modules, can think it terms of analog signals when they need to, rath...
ML
95871
Query Rewriting for Semistructured Data We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q.  Our algorithm is based on appropriately generalizing containment mappings, the chase, and unification  -- techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries. We show that the algorithm is sound and complete for TSL, i.e., it always finds every TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use available structural constraints (such as DTDs) to find more opportunities for query rewriting. We currently incorporate the algorithm in the TSIMMIS system. 1 Introduction  Recently, many semistructured data models, query and view definition languages have been proposed [2...
DB
jurisica96inductive
Inductive Learning and Case-Based Reasoning This paper describes an application of an inductive learning techniques to case-based reasoning. We introduce two main forms of induction, define case-based reasoning and present a combination of both. The evaluation of the proposed system, called TA3, is carried out on a classification task, namely character recognition. We show how inductive knowledge improves knowledge representation and in turn flexibility of the system, its performance (in terms of classification accuracy) and its scalability.  1. Introduction  Inductive learning is a process of generalizing specific facts or observations [MCM86]. It is a basic strategy by which one can acquire knowledge. There are two main forms associated with inductive learning: 1. Instance-to-class induction, where the learning system is presented with independent instances, representing class and the task is to induce a general description of the class. 2. Clustering problem arises when several objects or situations are presented to a learner...
ML
jantke97logical
Logical Case Memory Systems: Foundations And Learning Issues The focus of this paper is on the introduction of a quite general type of case-based reasoning systems called logical case memory systems.  The development of the underlying concepts has been driven by investigations in certain problems of case-based learning. Therefore, the present development of the target concepts is accompanied by an in-depth discussion of related learning problems. Logical case memory systems provide some formal framework for the investigation and for the application of structural similarity concepts. Those concepts have some crucial advantage over traditional numerical similarity concepts: The result of determining a new case's similarity to some formerly experienced case can be directly taken as a basis for performing case adaptation. Essentially, every logical case memory system consists of two constituents, some partially ordered case base and some partially ordered set of predicates. Cases are terms, in a logical sense. Given some problem case, every predicat...
ML
lester97cosmo
Cosmo: A Life-like Animated Pedagogical Agent with Deictic Believability Life-like animated interface agents for knowledgebased learning environments can provide timely, customized advice to support students' problem solving. Because of their strong visual presence, they hold significant promise for substantially increasing students' enjoyment of their learning experiences. A key problem posed by life-like agents that inhabit artificial worlds is deictic believability. In the same manner that humans refer to objects in their environment through judicious combinations of speech, locomotion, and gesture, animated agents should be able to move through their environment, and point to and refer to objects appropriately as they provide problemsolving advice. In this paper we describe a framework for achieving deictic believability in animated agents. A deictic behavior planner exploits a world model and the evolving explanation plan as it selects and coordinates locomotive, gestural, and speech behaviors. The resulting behaviors and utterances are believable, and...
HCI
neumann99constructing
Constructing A Realistic Head Animation Mesh for a Specific Person This paper addresses the problem of constructing an realistic and complete animation mesh that portrays a specific person's head geometry and texture. Our approach deforms a prototype mesh containing vector-based delineated muscles to fit one or more geometric models obtained from stereo image pairs of a specific person's head. The resulting personalized mesh facilitates animation with the same realism and predictability as the original prototype mesh. The model construction requires some manual interaction, however automatic refinement methods reduce the need for precision. The sensing process is passive and no physical markers are needed on the person's face. Models produced by our method are suited to realistic animations of specific individuals for applications in special effects, games, and 3D teleconferencing.  1. Introduction  Ever since the pioneering work of Frederic I. Parke [1] in 1972, researchers have attempted to generate realistic facial models and animation. Recent inte...
HCI
449513
Learning Hierarchical Task Models by Defining and Refining Examples Task models are used in many areas of computer science including planning, intelligent tutoring, plan recognition, interface design, and decision theory. However, developing task models is a significant practical challenge. We present a task model development environment centered around a machine learning engine that infers task models from examples. A novel aspect of the environment is support for a domain expert to refine past examples as he or she develops a clearer understanding of how to model the domain. Collectively, these examples constitute a "test suite" that the development environment manages in order to verify that manual changes to the evolving task model do not have unintended consequences. 1.
HCI
68836
Vector-Based Natural Language Call Routing This paper describes a domain independent, automatically trained natural language call router for directing incoming calls in a call center. Our call router directs customer calls based on their response to an open-ended "How may I direct your call?" prompt. Routing behavior is trained from a corpus of transcribed and hand-routed calls and then carried out using vectorbased information retrieval techniques. Terms consist of n-gram sequences of morphologically reduced content words, while documents representing routing destinations consists of weighted term frequencies derived from calls to that destination in the training corpus. Based on the statistical discriminating power of the n-gram terms extracted from the caller's request, the caller is 1) routed to the appropriate destination, 2) transferred to a human operator, or 3) asked a disambiguation question. In the last case, the system dynamically generates queries tailored to the caller's request and the destinations with which it is consistent, based on our extension of the vector model. Evaluation of the call router performance over a financial services call center using both accurate transcriptions of calls and fairly noisy speech recognizer output demonstrated robustness in the face of speech recognition errors. Furthermore, our system showed a substantial improvement in performance over existing systems by correctly routing 93.8% of the calls after punting 10.2% of all calls to a human operator on transcription, with approximately 4% degradation in performance when using speech recognizer output with a 23% word error rate.
IR
fels98glovetalkii
Glove-TalkII: A neural network interface which maps gestures to parallel formant speech synthesizer controls Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to 10 control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of GloveTalkII uses several input devices (including a Cyberglove, a ContactGlove, a 3-space tracker, and a foot-pedal), a parallel formant speech synthesizer and 3 neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed, user-defined relationship between hand-position and vowel sound and does not require any training ...
ML
wills00open
An Open Software Infrastructure For Reconfigurable Control Systems Recent advances in software technology have the potential to revolutionize control system design. This paper describes a new software infrastructure for complex control systems, which exploits new and emerging software technologies. It presents an open control platform (OCP) for complex systems, including those that must be reconfigured or customized in real-time for  extreme-performance applications. An application of the OCP to the control system design of an autonomous aerial vehicle is described.  1 Introduction  Complex dynamic systems, such as aircraft, power  systems, and telecommunications networks, present major  challenges to control systems designers. Both the military and civilian sectors of our economy are demanding new and highly sophisticated capabilities from these systems that the current state-of-the-art is not offering. Among them are the following:  . Ability to adapt to a changing environment;  . Ability to reconfigure the control algorithms;  . Plug-and-play exten...
ML
jording97anthropomorphic
An Anthropomorphic Agent for the Use of Spatial Language . In this paper we describe the communication with a responsive  virtual environment with the main emphasis on the processing of  spatial expressions in natural language instructions. This work is part  of the VIENA project in whichwechose interior design as an example  domain. A multiagent system acts as an intelligent mediator between the  user and a graphics system. To make the communication about spatial  relations more intuitive, we developed an anthropomorphic agent which  is graphically visualized in the scene. Considering the human-like #gure  we explain the use of qualitative spatial expressions, like #right of " and  #there".  1 Introduction  Interactive 3-dimensional graphics systems are more useful #e.g. in design#, when users can concentrate on their imaginations and be free from technical considerations. Therefore it is important to improveinteraction with the virtual environmentbyway of natural, intuitive communication forms.  In our work we consider a #virtual interface...
HCI
dinn99active
Active Rule Analysis And Optimisation In The Rock & Roll Deductive Object-Oriented Database Active database systems provide facilities that monitor and respond to changes of  relevance to the database. Active behaviour is normally described using Event Condition Action rules  (ECA-rules), and a number of systems have been developed, based upon different data models, that  support such rules. However, experience using active databases shows that while such systems are  powerful and potentially useful in many applications, they are hard to program and liable to exhibit  poor performance at runtime. This document addresses both of these issues by examining both analysis  and optimisation of active rules in the context of a powerful active database system. It is shown how  rule analysis methods developed for straightforward active rule languages for relational data models  can be extended to take account of rich event description languages and more powerful execution  models. It is also shown how the results of analyses can be exploited by rule optimisers, and that  multiple quer...
DB
375424
Towards a Highly-Scalable and Effective Metasearch Engine A metasearch engine is a system that supports unified access to multiple local search engines. Database selection is one of the main challenges in building a large-scale metasearch engine. The problem is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query. In order to enable accurate selection, metadata that reect the contents of each search engine need to be collected and used. In this paper, we propose a highly scalable and accurate database selection method. This method has several novel features. First, the metadata for representing the contents of all search engines are organized into a single integrated representative. Such a representative yields both computation efficiency and storage efficiency. Second, our selection method is based on a theory for ranking search engines optimally. Experimental results indicate that this new method is very effective. An operational prototype system has been built based on the proposed approach.
IR
mihalcea99word
Word Sense Disambiguation And Its Application To Internet Search ambiguation method presented here is that it provides a ranking of possible associations between words senses, rather than a binary yes/no decision for a possible sense combination. This proves to be particularly useful for Natural Language Processing tasks such as retrieving information related to a particular input question. An important task which can highly benet from a Word Sense Disambiguation method is the Internet search. This thesis presents a possible application of Word Sense Disambiguation techniques for improving the quality of the search on the Internet. Knowing the sense of the words in the input question enables the creation of similarity lists which contain words semantically related to the original keywords, and which can be further used for query extension. The extended query, together with the new lexical operators dened for information extraction, improve both the  precision and the resolution of a search on the Internet. iv  Speci
IR
zunino01representing
Representing Coordination Relationships with Influence Diagrams It is well know the necessity of managing relationships among agents  in a multi-agent system to achieve coordinated behavior. One approach to manage  such relationships consists of using an explicit representation of them, allowing  each agent to choose its actions based on them. Previous work in the area have  considered ideal situations, such as fully known environments, static relationships  and shared mental states. In this paper we propose to represent relationships  among agents and entities in a multi-agent system by using influence diagrams.
Agents
padgett98simple
A Simple Neural Network Models Categorical Perception of Facial Expressions The performance of a neural network that categorizes facial expressions is compared with human subjects over a set of experiments using interpolated imagery. The experiments for both the human subjects and neural networks make use of interpolations of facial expressions from the Pictures of Facial Affect Database [Ekman and Friesen, 1976]. The only difference in materials between those used in the human subjects experiments [Young et al., 1997] and our materials are the manner in which the interpolated images are constructed -- image-quality morphs versus pixel averages. Nevertheless, the neural network accurately captures the categorical nature of the human responses, showing sharp transitions in labeling of images along the interpolated sequence. Crucially for a demonstration of categorical perception [Harnad, 1987], the model shows the highest discrimination between transition images at the crossover point. The model also captures the shape of the reaction time curves of the human s...
ML
kwok98automated
Automated Text Categorization Using Support Vector Machine In this paper, we study the use of support vector machine in text categorization. Unlike other machine learning techniques, it allows easy incorporation of new documents into an existing trained system. Moreover, dimension reduction, which is usually imperative, now becomes optional. Thus, SVM adapts efficiently in dynamic environments that require frequent additions to the document collection. Empirical results on the Reuters-22173 collection are also discussed.  1. Introduction  The increasingly widespread use of information services made possible by the Internet and World Wide Web (WWW) has led to the so-called information overloading problem. Today, millions of online documents on every topic are easily accessible via the Internet. As the available information increases, the inability of people to assimilate and profitably utilize such large amounts of information becomes more and more apparent. Developing user-friendly, automatic tools for efficient as well as effective retrieval ...
ML
oria99visualmoql
VisualMOQL: The DISIMA Visual Query Language Multimedia data are now available to a variety of users ranging from naive to sophisticated. To make querying easy, visual query languages have been proposed. Most of these languages have a low expressive power and have their own query processors. Efforts have been made to design query languages with proper semantics to facilitate query optimization and processing in existing database systems. The majority of multimedia database systems are built on top of object or object-relational database systems with the underlying query facilities inherited. The DISIMA system is being built on top of a commercial OODBMS and we have chosen to extend the standard object-oriented query language OQL with some multimedia functionalities. The resulting language is called MOQL. This paper presents VisualMOQL, a visual query language implementing the image component of MOQL. 1 Introduction  In this paper we present the visual query interface, VisualMOQL, of the DISIMA distributed image database managemen...
DB
wang00concept
Concept Hierarchy Based Text Database Categorization Document categorization as a technique to improve the retrieval of useful documents has been extensively investigated. One important issue in a large-scale metasearch engine is to select text databases that are likely to contain useful documents for a given query. We believe that database categorization can be a potentially effective technique for good database selection, especially in the Internet environment where short queries are usually submitted. In this paper, we propose and evaluate several database categorization algorithms. This study indicates that while some document categorization algorithms could be adopted for database categorization, algorithms that take into consideration the special characteristics of databases may be more effective. Preliminary experimental results are provided to compare the proposed database categorization algorithms. A prototype database categorization system based on one of the proposed algorithms has been developed.
IR
ljungstrand00analysis
An analysis of WebWho: How does awareness of presence affect written messages? We present preliminary results from a study of how awareness of presence affects instant messaging in a computer lab. The easily accessible web based awareness tool, WebWho, visualizes a large university computer lab, allowing students to virtually locate one another and communicate via an instant messaging system. Messages can be sent anonymously, by a conscious act of ticking a box. Cross analyses of sender location (collocated, distributed, and distant), sender status (anonymous vs. identified) and message content were made. Results show that awareness of both physical presence and virtual presence affect the messages, and that these factors affect the text differently. The students use the messaging system to support collaborative work and coordinate social activities, as well as allow for playful behavior.  Keywords  Instant messaging, computer-mediated communication, awareness of presence, web visualization, social coordination  INTRODUCTION  WebWho [15] is a lightweight, web bas...
HCI
panzarasa01organisation
The Organisation of Sociality: A Manifesto for a New Science of MultiAgent Systems . In this paper, we pose and motivate a challenge, namely the need for a new science of multiagent systems. We propose that this new science should be grounded, theoretically on a richer conception of sociality, and methodologically on the extensive use of computational modelling for real-world applications and social simulations. Here, the steps we set forth towards meeting that challenge are mainly theoretical. In this respect, we provide a new model of multi-agent systems that reflects a fully explicated conception of cognition, both at the individual and the collective level. Finally, the mechanisms and principles underpinning the model will be examined with particular emphasis on the contributions provided by contemporary organisation theory.  1. 
Agents
445591
The RoadRunner Project: Towards Automatic Extraction of Web Data Introduction  ROADRUNNER is a research project that aims at developing solutions for automatically extracting data from large HTML data sources. The target of our research are data-intensive Web sites, i.e., HTML-based sites that publish large amounts of data in a fairly complex structure. In our view, we aim at ideally seeing the data extraction process of a data-intensive Web site as a black-box taking as input the URL of an entry point to the site (e.g. the home page), and returning as output data extracted from HTML pages in the site in a structured database-like format.  This paper describes the top-level software architecture of the ROADRUNNER System, which has been specifically designed to automatize the data extraction process. Several components of the system have already been implemented, and preliminary experiments show the feasibility of our ideas. Data-intensive Web sites usually share a number o
IR
lerner99comparative
A Comparative Study of Neural Network Based Feature Extraction Paradigms The projection maps and derived classification accuracies of a neural network (NN) implementation of Sammon's mapping, an auto-associative NN (AANN) and a multilayer perceptron (MLP) feature extractor are compared with those of the conventional principal component analysis (PCA). Tested on five real-world databases, the MLP provides the highest classification accuracy at the cost of deforming the data structure, whereas the linear models preserve the structure but usually with inferior accuracy. 
ML
63224
Shaping a CBR view with XML . Case Based Reasoning has found increasing application on the  Internet as an assistant in Internet commerce stores and as a reasoning agent for  online technical support. The strength of CBR in this area stems from its reuse  of the knowledge base associated with a particular application, thus providing  an ideal way to make personalised configuration or technical information  available to the Internet user. Since case data may be one aspect of a company's  entire corporate knowledge system, it is important to integrate case data easily  within a company's IT infrastructure, using industry specific vocabulary. We  suggest XML as the likely candidate to provide such integration. Some  applications have already begun to use XML as a case representation language.  We review these and present the idea of a standard case view in XML that can  work with the vocabularies or namespaces being developed by specific  industries. Earlier research has produced version 1.0 of a Case Based Mark-up ...
DB
szummer01kernel
Kernel Expansions With Unlabeled Examples Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance. We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy. 1 Introduction In many modern classification problems such as text categorization, very few labeled examples are available but a...
IR
511280
NEXUS - Distributed Data Management Concepts for Location Aware Applications Nowadays, mobile computers like subnotebooks or personal digital  assistants, as well as cellular phones can not only communicate wirelessly, but  they can also determine their position via appropriate sensors like DGPS. Socalled  location aware applications take advantage of this fact and structure information  according to the position of their users. In order to be able to assign  data to a certain location, these information systems have to refer to spatial  computer models. The NEXUS    project, which is supported by the Deutsche  Forschungsgemeinschaft (DFG, German Research Foundation), aims at the development  of a generic infrastructure that serves as a basis for location aware  applications. The central task of this platform deals with the data management.
HCI
441927
Towards Adaptive Fault Tolerance For Distributed Multi-Agent Systems This paper studies how to bring flexibility to fault-tolerant systems. Firstly, multi-agent systems are identified as a very valuable basis for reaching this goal, and reliability is also shown to be a rare and attractive feature for such systems. We then propose a framework for building applications that provide adaptive fault tolerance, and put forward the promising results obtained when testing the implementation of this framework. We conclude with drawing some perspectives of evolution of our work.
Agents
cassell02mack
MACK: Media lab Autonomous Conversational Kiosk In this paper, we describe an embodied conversational kiosk that builds on research in embodied conversational agents (ECAs) and on information displays in mixed reality and kiosk format in order to display spatial intelligence. ECAs leverage people’s abilities to coordinate information displayed in multiple modalities, particularly information conveyed in speech and gesture. Mixed reality depends on users ’ interactions with everyday objects that are enhanced with computational overlays. We describe an implementation, MACK (Media lab Autonomous Conversational Kiosk), an ECA who can answer questions about and give directions to the MIT Media Lab’s various research groups, projects and people. MACK uses a combination of speech, gesture, and indications on a normal paper map that users place on a table between themselves and MACK. Research issues involve users’ differential attention to hand gestures, speech and the map, and flexible architectures for Embodied Conversational Agents that allow these modalities to be fused in input and generation.
HCI
kriegel00managing
Managing Intervals Efficiently in Object-Relational Databases Modern database applications show a growing demand  for efficient and dynamic management of intervals,  particularly for temporal and spatial data  or for constraint handling. Common approaches  require the augmentation of index structures  which, however, is not supported by existing relational  database systems. By design, the new Relational  Interval Tree  1  (RI-tree) employs built-in  indexes on an as-they-are basis and is easy to implement.  Whereas the functionality and efficiency  of the RI-tree is supported by any off-the-shelf relational  DBMS, it is perfectly encapsulated by the  object-relational data model.  The RI-tree requires O(n/b) disk blocks of size b to  store n intervals, O(log b n) I/O operations for insertion  or deletion, and O(h log b n + r/b) I/Os for an  intersection query producing r results. The height  h of the virtual backbone tree corresponds to the  current expansion and granularity of the data space  but does not depend on n. As demonstrated by our  ex...
DB
8956
Error-Correcting Output Coding for Text Classification This paper applies error-correcting output coding (ECOC) to the task of document categorization. ECOC, of recent vintage in the AI literature, is a method for decomposing a multiway classification problem into many binary classification tasks, and then combining the results of the subtasks into a hypothesized solution to the original problem. There has been much recent interest in the machine learning community about algorithms which integrate "advice" from many subordinate predictors into a single classifier, and error-correcting output coding is one such technique. We provide experimental results on several real-world datasets, extracted from the Internet, which demonstrate that ECOC can offer significant improvements in accuracy over conventional classification algorithms. 1 Introduction Error-correcting output coding is a recipe for solving multi-way classification problems. It works in two stages: first, independently construct many subordinate classifiers, each responsible for r...
IR
carr01conceptual
Conceptual Linking: Ontology-based Open Hypermedia This paper describes the attempts of the COHSE project to define and deploy a Conceptual Open Hypermedia Service. Consisting of  . an ontological reasoning service which is used to represent a sophisticated conceptual model of document terms and their relationships;  . a Web-based open hypermedia link service that can offer a range of different linkproviding facilities in a scalable and non-intrusive fashion;  and integrated to form a conceptual hypermedia system to enable documents to be linked via metadata describing their contents and hence to improve the consistency and breadth of linking of WWW documents at retrieval time (as readers browse the documents) and authoring time (as authors create the documents).  Introduction: concepts and metadata  Metadata is data that describes other data to enhance its usefulness. The library catalogue or database schema are canonical examples. For our purposes, metadata falls into three broad categories:  . Catalogue information: e.g. the artist ...
IR
34341
Meta-Learning in Distributed Data Mining Systems: Issues and Approaches Data mining systems aim to discover patterns and extract useful information  from facts recorded in databases. A widely adopted approach to this  objective is to apply various machine learning algorithms to compute descriptive  models of the available data. Here, we explore one of the main  challenges in this research area, the development of techniques that scale up  to large and possibly physically distributed databases.  Meta-learning is a technique that seeks to compute higher-level classifiers  (or classification models), called meta-classifiers, that integrate in some principled  fashion multiple classifiers computed separately over different databases.  This study, describes meta-learning and presents the JAM system (Java Agents  for Meta-learning), an agent-based meta-learning system for large-scale data  mining applications. Specifically, it identifies and addresses several important  desiderata for distributed data mining systems that stem from their additional  complexity co...
ML
63694
Investigating Interactions Between Agent Conversations and Agent Control Components Exploring agent conversation in the context of fine-grained agent coordination research has raised several intellectual questions. The major issues pertain to interactions between different agent conversations, the representations chosen for different classes of conversations, the explicit modeling of interactions between the conversations, and how to address these interactions. This paper is not so ambitious as to attempt to address these questions, only frame them in the context of quantified, scheduling-centric multi-agent coordination. research. 1 Introduction  Based on a long history of work in agents and agent control components for building distributed AI and multi-agent systems, we are attempting to frame and address a set of intellectual questions pertaining to agent conversation. Interaction lies at the heart of the matter; the issue is interaction between different agent conversations, that possibly occur at different levels of abstraction, but also interaction between the m...
Agents
gendelman00fast
Fast File Access for Fast Agents . Mobile agents are a powerful tool for coordinating general purpose  distributed computing, where the main goal is high performance. In this paper  we demonstrate how the inherent mobility of agents may be exploited to achieve  fast file access, which is necessary for most general-purpose applications. We  present a file system for mobile agents based exclusively on local disks of the  participating workstations. The mobility of agents allows us to make all file operations  local, which significantly reduces access time. We also demonstrate  how code files and special system files can be handled efficiently in a localdisk  -based environment.  1 
Agents
langley95applications
Applications of Machine Learning and Rule Induction An important area of application for machine learning is in automating the acquisition of knowledge bases required for expert systems. In this paper, we review the major paradigms for machine learning, including neural networks, instance-based methods, genetic learning, rule induction, and analytic approaches. We consider rule induction in greater detail and review some of its recent applications, in each case stating the problem, how rule induction was used, and the status of the resulting expert system. In closing, we identify the main stages in fielding an applied learning system and draw some lessons from successful applications. Introduction  Machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience. Expert performance requires much domainspecific knowledge, and knowledge engineering has produced hundreds of AI expert systems that are now used regularly in industry. Machine learning aims to provide ...
ML
45542
A Survey of Methods for Scaling Up Inductive Algorithms . One of the defining challenges for the KDD research community is to enable inductive learning algorithms to mine very large databases. This paper summarizes, categorizes, and compares existing work on scaling up inductive algorithms. We concentrate on algorithms that build decision trees and rule sets, in order to provide focus and specific details; the issues and techniques generalize to other types of data mining. We begin with a discussion of important issues related to scaling up. We highlight similarities among scaling techniques by categorizing them into three main approaches. For each approach, we then describe, compare, and contrast the different constituent techniques, drawing on specific examples from published papers. Finally, we use the preceding analysis to suggest how to proceed when dealing with a large problem, and where to focus future research.  Keywords: scaling up, inductive learning, decision trees, rule learning  1. Introduction  The knowledge discovery and data...
ML
wang98structure
The Structure of Object Transportation and Orientation in Human-Computer Interaction An experiment was conducted to investigate the relationship between object transportation and object orientation by the human hand in the context of humancomputer interaction (HCI). This work merges two streams of research: the structure of interactive manipulation in HCI and the natural hand prehension in human motor control. It was found that object transportation and object orientation have a parallel, interdependent structure which is generally persistent over different visual feedback conditions. The notion of concurrency and interdependence of multidimensional visuomotor control structure can provide a new framework for human-computer interface evaluation and design.  Keywords  Direct manipulation, input device, multi-dimensional control, visuomotor control, visual conditions, information processing, interface design, virtual reality.  INTRODUCTION  Object manipulation is a basic operation in humancomputer interaction (HCI). Modern computer technology advances towards affording m...
HCI
373986
From Active Objects to Autonomous Agents This paper studies how to extend the concept of active objects  into a structure of agents. It first discusses the requirements for autonomous  agents that are not covered by simple active objects. We  propose then the extension of the single behavior of an active object  into a set of behaviors with a meta-behavior scheduling their activities.  To make a concrete proposal based on these ideas we describe how we  extended a framework of active objects, named Actalk, into a generic  multi-agent platform, named DIMA. We discuss how this extension  has been implemented. We finally report on one application of DIMA  to simulate economic models.  Keywords: active object, agent, implementation, meta-behavior, modularity, re-usability, simulation.  1 Introduction  Object-oriented concurrent programming (OOCP) is the most appropriate and promising technology to implement agents. The concept of active object may be considered as the basic structure for building agents. Furthermore, the combinat...
Agents
chakrabarti98scalable
Scalable Feature Selection, Classification and Signature Generation for Organizing Large Text Databases Into Hierarchical Topic Taxonomies   We explore how to organize large text databases hierarchically by topic to aid better searching, browsing and filtering. Many corpora, such as internet directories, digital libraries, and patent databases are manually organized into topic hierarchies, also called taxonomies. Similar to indices for relational data, taxonomies make search and access more efficient. However, the exponential growth in the volume of on-line textual information makes it nearly impossible to maintain such taxonomic organization for large, fast-changing corpora by hand.  We describe an automatic system that starts with a small sample of the corpus in which topics have been assigned by hand, and then updates the database with new documents as the corpus grows, assigning topics to these new documents with high speed and accuracy.  To do this, we use techniques from statistical pattern recognition to efficiently separate the feature words, or discriminants, from the noise words at each node of the taxonomy. Usi...
DB
amato98obprm
OBPRM: An Obstacle-Based PRM for 3D Workspaces this paper we consider an obstacle-based prm
AI
117999
From Resource Discovery to Knowledge Discovery on the Internet More than 50 years ago, at a time when modern computers didn't exist yet, Vannevar Bush wrote about a multimedia digital library containing human collective knowledge and filled with "trails" linking materials of the same topic. At the end of World War II, Vannevar urged scientists to build such a knowledge store and make it useful, continuously extendable and more importantly, accessible for consultation. Today, the closest to the materialization of Vannevar's dream is the World-Wide Web hypertext and multimedia document collection. However, the ease of use and accessibility of the knowledge described by Vannevar is yet to be realized. Since the 60s, extensive research has been accomplished in the information retrieval field, and free-text search was finally adopted by many text repository systems in the late 80s. The advent of the World-Wide Web in the 90s helped text search become routine as millions of users use search engines daily to pinpoint resources on the Internet. However, r...
IR
bentley99three
Three Ways to Grow Designs: A Comparison of Evolved Embryogenies for a Design Problem This paper explores the use of growth processes, or embryogenies, to map genotypes to phenotypes within evolutionary systems. Following a summary of the significant features of embryogenies, the three main types of embryogenies in Evolutionary Computation are then identified and explained: external, explicit and implicit. An experimental comparison between these three different embryogenies and an evolutionary algorithm with no embryogeny is performed. The problem set to the four evolutionary systems is to evolve tessellating tiles. In order to assess the scalability of the embryogenies, the problem is increased in difficulty by enlarging the size of tiles to be evolved. The results are surprising, with the implicit embryogeny outperforming all other techniques by showing no significant increase in the size of the genotypes or decrease in accuracy of evolution as the scale of the problem is increased. 1. Introduction The use of computers to evolve solutions to problems has seen a dra...
ML
liu01drawcad
DrawCAD: Using Deductive Object-Relational Databases in CAD Computer-Aided Design (CAD) involves the use of computers in the various stages of engineering design. It has large volumes of data with complex structures that needs to be stored and managed efficiently and properly. In CAD, graphical objects with complex structures can be created by reusing previously created objects. The data of these objects have the references to the other objects they contain. Deductive object-relational databases can be used to compute the complete data of graphical objects that reuse other objects. This is the idea behind the development of the DrawCAD system. DrawCAD is a CAD system built on top of the Relationlog object-relational deductive database system. It facilitates the creation of graphical objects by reusing previously created objects. The DrawCAD system illustrates how CAD systems can be developed, using deductive object-relational databases to store and manage data and also perform the computations that are normally performed by the application program.
DB
godfrey97minimization
Minimization in Cooperative Response to Failing Database Queries When a query fails, it is more cooperative to identify the cause of failure, rather than just to report the empty answer set. If there is not a cause for the query's failure, it is worthwhile to report the part of the query which failed. To identify a minimal failing subquery (MFS) of the query is the best way to do this. (This MFS is not unique; there may be many of them.) Likewise, to identify a maximal succeeding subquery (MSS) can help a user to recast a new query that leads to a non-empty answer set. Database systems do not provide the functionality of these types of cooperative responses. This may be, in part, because algorithmic approaches to finding the MFSs and the MSSs to a failing query are not obvious. The search space of subqueries is large. Despite work on MFSs in the past, the algorithmic complexity of these identification problems had remained uncharted. This paper shows the complexity profile of MFS and MSS identification. It is shown that there exists a simple algorit...
IR
ardissono96uso
Uso Di Piani Di Problem-Solving Nel Riconoscimento Di Piani E Obiettivi In questo articolo si discute il ruolo dei piani di problem-solving nell'interpretazione dei dialoghi in linguaggio naturale. Per "piano di problem-solving" si intende una descrizione dichiarativa dei passi del processo di pianificazione ed esecuzione di azioni linguistiche e di dominio. L'articolo mostra che una rappresentazione appropriata di questi piani e` la base per modellare il comportamento cooperativo degli agenti che partecipano ad un dialogo. I piani di problem-solving sono parte di un'architettura di agente in grado di cooperare con altri agenti. 1 Introduzione L'analisi del Linguaggio Naturale ha un ruolo importante nello sviluppo di interfacce intelligenti: infatti, per rendere un sistema amichevole nei confronti dei suoi utenti, e` importante arricchirlo con una teoria del linguaggio che descrive le strategie comunicative adottate dalle persone per interagire. Inoltre, e` necessario esplicitare nel sistema i concetti basilari di razionalita` e cooperativita`. L'idea e` ...
Agents
lacher01facilitating
Facilitating the Exchange of Explicit Knowledge Through Ontology Mappings In this paper, we give an overview of a system (CAIMAN) that can facilitate the exchange of relevant documents between geographically dispersed people in Communities of Interest. The nature of Communities of Interest prevents the creation and enforcement of a common organizational scheme for documents, to which all community members adhere. Each community member organizes her documents according to her own categorization scheme (ontology). CAIMAN exploits this personal ontology, which is essentially the perspective of a user on a domain, for information retrieval. Related documents are retrieved on a concept granularity level from a central community document repository. To find the related concepts in the queried ontology, CAIMAN performs an ontology mapping. The ontology mapping in CAIMAN is based on a novel approach, which considers the concepts in an ontology implicitly represented by the documents assigned to each concept. Using machine learning techniques for text classification, a concept in a personal ontology is mapped to a concept in a community ontology. The CAIMAN system uses this mapping to provide document publishing and retrieval services both for the community and the user. First results of the prototype system showed that this approach can be a valid alternative to existing techniques for information retrieval.
IR
bellardo00implementing
Implementing a Knowledge Date-a-Base Knowledge-based systems are very useful, but can be  dicult to design because of the complexity of the realworld  knowledge they represent. This paper compares  the experiences of building the same knowledge base by  hand in two dierent systems, Otter and CLIPS. The  knowledge base considered is that of people's preferences  towards others, in the interests of nding a \dating  match." Finally, this paper considers Horn theorems  and their impact on the usefulness of knowledge  systems.  Introduction  Because technology and automation are increasingly becoming a part of everyday life, it is benecial to enable technology to \understand" its application area. An obvious way of doing this is to implement and embed a knowledge base in an application. However, designing a good knowledge base is not trivial. A good knowledge base needs to be general so it can be reused, complete to avoid bad models, and ecient in description and time.  This paper presents the authors' experiences implement...
AI
bederson00jazz
Jazz: An Extensible Zoomable User Interface Graphics Toolkit in Java In this paper we investigate the use of scene graphs as a general approach for implementing two-dimensional (2D) graphical applications, and in particular Zoomable User Interfaces (ZUIs). Scene graphs are typically found in three-dimensional (3D) graphics packages such as Sun's Java3D and SGI's OpenInventor. They have not been widely adopted by 2D graphical user interface toolkits.  To explore the effectiveness of scene graph techniques, we have developed Jazz, a general-purpose 2D scene graph toolkit. Jazz is implemented in Java using Java2D, and runs on all platforms that support Java 2. This paper describes Jazz and the lessons we learned using Jazz for ZUIs. It also discusses how 2D scene graphs can be applied to other application areas.  Keywords  Zoomable User Interfaces (ZUIs), Animation, Graphics, User Interface Management Systems (UIMS), Pad++, Jazz.  INTRODUCTION  Today's Graphical User Interface (GUI) toolkits contain a wide range of built-in user interface objects (also kno...
HCI
miles00designing
Designing Agent-Oriented Systems by Analysing Agent Interactions . We propose a preliminary methodology for agent-oriented software  engineering based on the idea of agent interaction analysis. This approach uses  interactions between undetermined agents as the primary component of analysis  and design. Agents as a basis for software engineering are useful because they  provide a powerful and intuitive abstraction which can increase the comprehensiblity  of a complex design. The paper describes a process by which the designer  can derive the interactions that can occur in a system satisfying the given requirements  and use them to design the structure of an agent-based system, including  the identification of the agents themselves. We suggest that this approach has the  flexibility necessary to provide agent-oriented designs for open and complex applications,  and has value for future maintenance and extension of these systems.  1 
Agents
skarmeas99component
Component Based Agent Construction . In this paper, an agent architecture is proposed that can be used to integrate pre-existing components that provide the domain dependent agent functionality. The key integrating feature of the agent is an active message board that is used for inter-component, hence intra-agent communication. The board is active because it automatically forwards messages to components, they do not have to poll the message board. It does this on the basis of message pattern functions that components place on the board using advertisement messages. These functions can contain component provided semantic tests on the content of the message, they can also communicate with any other component whilst they are being applied. In addition an agent management toolkit, called ALFA, is described which offers a set of agent management services. This toolkit consists of a number of servers for storing the code of the components and symbolic descriptions of agents regarding their component makeup. A third server use...
Agents
vanzwol99searching
Searching Documents on the Intranet Searching for documents on the internet with today’s search engines, which are mainly based on words in a document, is not satisfactory. Results can be improved by also taking the content of a document into account. The Extensible Markup Language (XML) enables us to do semantic tagging and to make the structure of a document explicit. But this describes a document only at the syntactical level. A more ideal situation would be when the XML tagging is also used to define the document at the semantical level. To realize this we allow an author of a document to describe the relevant concepts by means of tags like he would design an object-oriented database schema. In our approach a user searching for a particular document is presented a graphical description of such a schema, that describes the concepts defined for the webspace of an intranet. Via this interface the user can formulate OO-like queries or navigate to relevant web pages. To realize our ideas we are building an architecture based on the concept of an index-database. A prototype is up and running.
DB
ho01economic
Economic Value of EWA Lite: A Functional Theory of Learning in Games This paper describes a theory of learning in decisions and games called EWA Lite, with only one parameter. EWA Lite predicts the time path of individual behavior in any normal-form game (given initial conditions) including new games in which behavior has never been observed.
ML
bollacker98citeseer
CiteSeer: An Autonomous Web Agent for Automatic Retrieval and Identification of Interesting Publications Published research papers available on the World Wide Web (WWW or Web) are often poorly organized, often exist in non-text form (e.g. Postscript) documents, and increase in quantity daily. Significant amounts of time and effort are commonly needed to find interesting and relevant publications on the Web. We have developed a Web based information agent that assists the user in the process of performing a scientific literature search. Given a set of keywords, the agent uses Web search engines and heuristics to locate and download papers. The papers are parsed in order to extract information features such as the abstract and individually identified citations which are placed into an SQL database. The agent's Web interface can be used to find relevant papers in the database using keyword searches, or by navigating the links between papers formed by the citations. Links to both "citing" and "cited" publications can be followed. In addition to simple browsing and keyword searches, the agent ...
Agents
macintyre96future
Future Multimedia User Interfaces this article, we examine some of the work that has been done in these two fields and explore where they are heading. First, we review their often-confusing terminology and provide a brief historical overview. Since both fields rely largely on relatively unusual, and largely immature, hardware technologies, we next provide a high-level introduction to important hardware issues. This is followed by a description of the key approaches to system architecture used by current researchers. We then build on the background provided by these sections to lay out a set of current research issues and directions for future work. Throughout, we attempt to emphasize the many ways in which virtual environments and ubiquitous computing can complement each other, creating an exciting new form of multimedia computing that is far more powerful than either approach would make possible alone.
HCI
zhang99sdcc
The SDCC Framework For Integrating Existing Algorithms for Diverse Data Warehouse Maintenance Tasks Recently proposed view maintenance algorithms tackle the problem of concurrent data updates happening at different autonomous ISs, whereas the EVE system addresses the maintenance of a data warehouse after schema changes of ISs. The concurrency of schema changes and data updates still remains an unexplored problem however. This paper now provides a first solution that guarantees concurrent view definition evolution and view extent maintenance of a DW defined over distributed ISs. For this problem, we introduce a framework called SDCC (Schema change and Data update Concurrency Control) system. SDCC integrates  existing algorithms designed to address view maintenance subproblems, such as view extent maintenance after IS data updates, view definition evolution after IS schema changes, and view extent adaptation after view definition changes, into one system by providing  protocols that enable them to correctly co-exist and collaborate. SDCC tracks any potential faulty updates of the DW ca...
DB
consortium00corporate
Corporate Memory Management through Agents . The CoMMA project (Corporate Memory Management through Agents)  aims at developing an open, agent-based platform for the management of a corporate  memory by using the most advanced results on the technical, the content, and the  user interaction level. We focus here on methodologies for the set-up of multi-agent  systems, requirement engineering and knowledge acquisition approaches.  1. Introduction  How to improve access, share and reuse of both internal and external knowledge in a company? How to improve newcomers' learning and integration in a company? How to enhance technology monitoring in a company? Knowledge Management (KM) aims at solving such problems. Different research communities offer - partial - solutions for supporting KM. The integration of results from these different research fields seems to be a promising approach. This is the motivation of the CoMMA IST project-funded by the European Commission- which started February 2000. The main objective is to implement and ...
Agents
60365
eMediator: A Next Generation Electronic Commerce Server This paper presents eMediator, an electronic commerce server prototype that demonstrates ways in which algorithmic support and game-theoretic incentive engineering can jointly improve the efficiency of ecommerce. eAuctionHouse, the configurable auction server, includes a variety of generalized combinatorial auctions and exchanges, pricing schemes, bidding languages, mobile agents, and user support for choosing an auction type. We introduce two new logical bidding languages for combinatorial markets: the XOR bidding language and the OR-of-XORs bidding language. Unlike the traditional OR bidding language, these are fully expressive. They therefore enable the use of the Clarke-Groves pricing mechanism for motivating the bidders to bid truthfully. eAuctionHouse also supports supply/demand curve bidding. eCommitter, the leveled commitment contract optimizer, determines the optimal contract price and decommitting penalties for a variety of leveled commitment contracting mechanisms, taking into account that rational agents will decommit strategically in Nash equilibrium. It also determines the optimal decommitting strategies for any given leveled commitment contract. eExchangeHouse, the safe exchange planner, enables unenforced anonymous exchanges by dividing the exchange into chunks and sequencing those chunks to be delivered safely in alternation between the buyer and the seller.
DB
aggarwal01intelligent
Intelligent Crawling on the World Wide Web with Arbitrary Predicates The enormous growth of the world wide web in recent years has made it important to perform resource discovery efficiently. Consequently, several new ideas have been proposed in recent years; among them a key technique is focused crawling which is able to crawl particular topical portions of the world wide web quickly without having to explore all web pages. In this paper, we propose the novel concept of intelligent crawling which actually learns characteristics of the linkage structure of the world wide web while performing the crawling. Specifically, the intelligent crawler uses the inlinking web page content, candidate URL structure, or other behaviors of the inlinking web pages or siblings in order to estimate the probability that a candidate is useful for a given crawl. This is a much more general framework than the focused crawling technique which is based on a pre-defined understanding of the topical structure of the web. The techniques discussed in this paper are applicable for crawling web pages which satisfy arbitrary user-defined predicates such as topical queries, keyword queries or any combinations of the above. Unlike focused crawling, it is not necessary to provide representative topical examples, since the crawler can learn its way into the appropriate topic. We refer to this technique as intelligent crawling because of its adaptive nature in adjusting to the web page linkage structure. The learning crawler is capable of reusing the knowledge gained in a given crawl in order to provide more efficient crawling for closely related predicates.
IR
527475
Agent-Oriented Software Engineering ion: The process of defining a simplified model of the system that emphasises some of the details or properties, while suppressing others.  . Organisation  1  : The process of identifying and managing interrelationships between various problem solving components.  Next, the characteristics of complex systems need to be enumerated [8]:  . Complexity frequently takes the form of a hierarchy. That is, a system that is composed of inter-related sub-systems, each of which is in turn hierarchic in structure, until the lowest level of elementary sub-system is reached. The precise nature of these organisational relationships varies between sub-systems, however some generic forms (such as client-server, peer, team, etc.) can be identified. These relationships are not static: they often vary over time.  . The choice of which components in the system are primitive is relatively arbitrary and is defined by the observer's aims and objectives.  . Hierarchic systems evolve more quickly than non-hiera...
Agents
stevens01oiling
OILing the way to Machine Understandable Bioinformatics Resources The complex questions and analyses posed by biologists, as well as the diverse data resources they develop, require the fusion of evidence from different, independently developed and heterogeneous resources. The web as an enabler for interoperability has been an excellent mechanism for data publication and transportation. Successful exchange and integration of information, however, depends on a shared language for communication (a terminology) and a shared understanding of what the data means (an ontology). Without this kind of understanding, semantic heterogeneity remains a problem for both humans and machines. One means of dealing with heterogeneity in bioinformatics resources is through terminology founded upon an ontology. Bioinformatics resources tend to be rich in human readable and understandable annotation, with each resource using its own terminology. These resources are machine readable, but not machine understandable. Ontologies have a role in increasing this machine understanding, reducing the semantic heterogeneity between resources and thus promoting the flexible and reliable interoperation of bioinformatics resources. This paper describes a solution derived from the semantic web (a machine understandable WWW), the Ontology Inference Layer (OIL), as a solution for semantic bioinformatics resources. The nature of the heterogeneity problems are presented along with a description of how metadata from domain ontologies can be used to alleviate this problem. A companion paper in this issue gives an example of the development of a bioontology using OIL. Keywords:  Ontology; OIL; semantics; interoperation; heterogeneity; understanding. 1 
IR
koubarakis99tractable
Tractable Query Answering in Indefinite Constraint Databases: Basic Results and Applications to Querying Spatiotemporal Information . We consider the scheme of indefinite constraint databases  proposed by Koubarakis. This scheme can be used to represent indefinite  information arising in temporal, spatial and truly spatiotemporal  applications. The main technical problem that we address in this paper is  the discovery of tractable classes of databases and queries in this scheme.  We start with the assumption that we have a class of constraints C with  satisfiability and variable elimination problems that can be solved in  PTIME. Under this assumption, we show that there are several general  classes of databases and queries for which query evaluation can be done  with PTIME data complexity. We then search for tractable instances of  C in the area of temporal and spatial constraints. Classes of constraints  with tractable satisfiability problems can be easily found in the literature.  The largest class that we consider is the class of Horn disjunctive  linear constraints over the rationals. Because variable eliminati...
DB
25137
Generating, Executing and Revising Schedules for Autonomous Robot Office Couriers Scheduling the tasks of an autonomous robot office courier and carrying out the scheduled tasks reliably and efficiently pose challenging problems for autonomous robot control. To carry out their jobs reliably and efficiently many autonomous mobile service robots acting in human working environments have to view their jobs as everyday activity: they should accomplish longterm efficiency rather than optimize problem-solving episodes. They should also exploit opportunities and avoid problems flexibly because often robots are forced to generate schedules based on partial information. We propose to implement the controller for scheduled activity by employing concurrent reactive plans that reschedule the course of action whenever necessary and while performing their actions. The plans are represented modularly and transparently to allow for easy transformation. Scheduling and schedule repair methods are implemented as plan transformation rules. Introduction  To carry out their jobs reliably...
AI
215779
Collaborative Maintenance this paper we examine a classical AI problem (knowledge maintenance) and propose an innovative solution (collaborative maintenance) that has been inspired by the recommendation technique of
IR
416413
Sensing Techniques for Mobile Interaction We describe sensing techniques motivated by unique aspects of human-computer interaction with handheld devices in mobile settings. Special features of mobile interaction include changing orientation and position, changing venues, the use of computing as auxiliary to ongoing, real-world activities like talking to a colleague, and the general intimacy of use for such devices. We introduce and integrate a set of sensors into a handheld device, and demonstrate several new functionalities engendered by the sensors, such as recording memos when the device is held like a cell phone, switching between portrait and landscape display modes by holding the device in the desired orientation, automatically powering up the device when the user picks it up the device to start using it, and scrolling the display using tilt. We present an informal experiment, initial usability testing results, and user reactions to these techniques.  Keywords  Input devices, interaction techniques, sensing, contextaware...
HCI
libkin99power
Summary this paper. The main questions addressed in this setting deal with conditions under which it is possible to evaluate queries incrementally.
DB
539207
Self-Adaptive Operator Scheduling using the Religion-Based EA The optimal choice of the variation operators mutation and crossover and their parameters can be decisive for the performance of evolutionary algorithms (EAs). Usually the type of the operators (such as Gaussian mutation) remains the same during the entire run and the probabilistic frequency of their application is determined by a constant parameter, such as a fixed mutation rate. However, recent studies have shown that the optimal usage of a variation operator changes during the EA run. In this study, we combined the idea of self-adaptive mutation operator scheduling with the Religion-Based EA (RBEA), which is an agent model with spatially structured and variable sized subpopulations (religions). In our new model (OSRBEA), we used a selection of different operators, such that each operator type was applied within one specific subpopulation only. Our results indicate that the optimal choice of operators is problem dependent, varies during the run, and can be handled by our self-adaptive OSRBEA approach. Operator scheduling could clearly improve the performance of the already very powerful RBEA and was superior compared to a classic and other advanced EA approaches.
Agents
weiss01predicting
Predicting Telecommunication Equipment Failures from Sequences of Network Alarms The computer and telecommunication industries rely heavily on knowledge-based  expert systems to manage the performance of their networks. These expert systems are  developed by knowledge engineers, who must first interview domain experts to extract  the pertinent knowledge. This knowledge acquisition process is laborious and costly,  and typically is better at capturing qualitative knowledge than quantitative knowledge.  This is a liability, especially for domains like the telecommunication domain, where  enormous amounts of data are readily available for analysis. Data mining holds  tremendous promise for the development of expert systems for monitoring network  performance since it provides a way of automatically identifying subtle, yet important,  patterns in data. This case study describes a project in which a temporal data mining  system called Timeweaver is used to identify faulty telecommunication equipment from  logs of network alarm messages.  Project Overview  Managing the p...
AI
sreerupa98dynamic
Dynamic on-line clustering and state extraction: An approach to symbolic learning Researchers often try to understand the representations that develop in the hidden layers of a neural network during training. Interpretation is difficult because the representations are typically highly distributed and continuous. By "continuous," we mean that if one constructed a scatter plot over the hidden unit activity space of patterns obtained in response to various inputs, examination at any scale would reveal the patterns to be broadly distributed over the space. Such continuous representations are naturally obtained if the input space and activation dynamics are continuous. Continuous representations are not always appropriate. Many task domains might benefit from discrete representations -- representations selected from a finite set of alternatives. Example domains include finite-state machine emulation, data compression, language and higher cognition (involving discrete symbol processing), and categorization. In such domains, standard neural...
ML
kumar01behaviorbased
A Behavior-Based Intelligent Control Architecture with Application to Coordination of Multiple Underwater Vehicles The paper presents a behavior-based intelligent control architecture for designing controllers which, based on their observation of sensor signals, compute the discrete control actions. These control actions then serve as the "set-points" for the lower level controllers. The behavior-based approach yields an intelligent controller which is a cascade of a perceptor and a response controller. The perceptor extracts the relevant symbolic information from the incoming continuous sensor signals, which enables the execution of one of the behaviors. The response controller is a discrete event system that computes the discrete control actions by executing one of the enabled behaviors. The behavioral approach additionally yields a hierarchical two layered response controller, which provides better complexity management. The inputs from the perceptor are used to first compute the higher level activities, called behaviors, and next to compute the corresponding lower level activities, called actio...
ML
santos97introductory
An Introductory Course on Visualization A Visualization course offered twice (1997/98 and 98/99) as an elective in the MSc degree on Electronics and Telecommunications at the University of Aveiro is presented. Its contents, bibliography and teaching methods are described. Some difficulties encountered during the preparation and lecturing of this course are identified.  1. Introduction  Taking into consideration that Visualization is becoming very important and useful in many areas, an introductory course on Vizualization seems a valid contribution to the curriculum of any postgraduation in science or technology and thus it was considered adequate as an elective course of the MSc in Electronics and Telecommunications offered at the University of Aveiro. This post-graduation program includes several courses and a thesis and aims to be a large spectrum degree encompassing mainly one of four areas of Electrical Engineering (Electronics, Telecommunications, Signal Analysis and Processing and Computer Science); this means that it ...
HCI
roddick99towards
Towards a Model for Spatio-Temporal Schema Selection Schema versioning provides a mechanism for handling change in the structure of database systems and has been investigated widely, both in the context of static and temporal databases. With the growing interest in spatial and spatio-temporal data as well as the mechanisms for holding such data, the spatial context within which data is formatted also becomes an issue. This paper presents a generalised model that accommodates schema versioning within static, temporal, spatial and spatio-temporal relational and object-oriented databases.
DB
minar99hive
Hive: Distributed Agents for Networking Things Hive is a distributed agents platform, a decentralized system for building applications by networking local system resources. This paper presents the architecture of Hive, concentrating on the idea of an "ecology of distributed agents" and its implementation in a practical Java based system. Hive provides ad-hoc agent interaction, ontologies of agent capabilities, mobile agents, and a graphical interface to the distributed system. We are applying Hive to the problems of networking "Things That Think," putting computation and communication in everyday places such as your shoes, your kitchen, or your own body. TTT shares the challenges and potentials of ubiquitous computing and embedded network applications. We have found that the flexibility of a distributed agents architecture is well suited for this application domain, enabling us to easily build applications and to reconfigure our systems on the fly. Hive enables us to make our environment and network more alive. This paper is dedic...
Agents
buneman98equality
Equality, Type and Word Constraints As a generalization of inclusion dependencies that are found in relational databases, word constraints have been studied for semistructured data [6] as well as for an objectoriented model [10]. In both contexts, it is assumed that each data entity has a unique identity, and two entities are equal if and only if they have the same identify. In this setting, the decidability of the implication and finite implication problems for word constraints has been established. A question left open is whether these problems are still decidable in the context of an object-oriented model M  ß  which supports complex values with nested structures and complex value equality. This paper provides an answer to that question. We characterize a schema in M  ß  in terms of a type constraint and an equality constraint, and investigate the interaction between these constraints and word constraints. We show that in the presence of equality and type constraint, the implication and finite implication problems for...
DB
chothia01distributed
A Distributed Pi-Calculus with Local Areas of Communication This paper introduces a process calculus designed to capture the phenomenon of names which are known universally but always refer to local information. Our system extends the #-calculus so that a channel name can have within its scope several disjoint local areas. Such a channel name may be used for communication within an area, it may be sent between areas, but it cannot itself be used to transmit information from one area to another. Areas are arranged in a hierarchy of levels, distinguishing for example between a single application, a machine, or a whole network. We give an operational semantics for the calculus, and develop a type system that guarantees the proper use of channels within their local areas. We illustrate with models of an internet service protocol and a pair of distributed agents. 1 
Agents
vogt00how
How Much More is Better? - Characterizing the Effects of Adding More IR Systems to a Combination We present the results of some expansion experiments for solving the routing, data fusion problem using TREC5 systems. The experiments address the question "How much more is better?" when combining the results of multiple information retrieval systems using a linear combination (weighted sum) model. By investigating all 2way, 3-way, 4-way and 10-way combinations of 10 IR systems on 10 queries, we show that: (1) one can expect potentially significant amounts of improvement in performance over the best system used in the combination if enough systems are used, (2) for this number of candidate systems, the point of diminishing returns is reached when around four systems are used in the combination, (3) queries generally have too few relevant documents, causing little correlation in performance between the training set and test set; thus making it difficult to get test set improvement even when multiple systems are used, and (4) if one knows the relative past performance of the candidate s...
IR
raghavan01crawling
Crawling the Hidden Web Current-day crawlers retrieve content only from  the publicly indexable Web, i.e., the set of Web  pages reachable purely by following hypertext  links, ignoring search forms and pages that require  authorization or prior registration. In particular,  they ignore the tremendous amount of high quality  content "hidden" behind search forms, in large  searchable electronic databases. In this paper, we  address the problem of designing a crawler capable  of extracting content from this hidden Web.  We introduce a generic operational model of a  hidden Web crawler and describe how this model  is realized in HiWE (Hidden Web Exposer), a  prototype crawler built at Stanford. We introduce  a new Layout-based Information Extraction  Technique (LITE) and demonstrate its use in automatically  extracting semantic information from  search forms and response pages. We also present  results from experiments conducted to test and  validate our techniques.  1 
IR
dix00multi
Multi Agenten Systeme Coalition Formation 4.4 Payoff Division Overview 98  4 Contract Nets, Coalition Formation 98-1  Chapter 4: Contract Nets, Coalition Formation Multi-Agenten Systeme (VU), SS 00 4.1 General Contract Nets  How to distribute tasks?  .  Global Market Mechanisms. Implementations use a single centralized mediator .  .  Announce, bid, award -cycle. Distributed Negotiation . We need the following: 1. Define a task allocation problem in precise terms. 2. Define a formal model for making bidding and awarding decisions. 4.1 General Contract Nets 99  Chapter 4: Contract Nets, Coalition Formation Multi-Agenten Systeme (VU), SS 00 Definition 4.1 (Task-Allocation Problem)  A task allocation problem is given by 1. a set of tasks T , 2. a set of agents A A A,  3. a cost function cost i i i : 2  T  -#  R#{}  (stating the costs that agent i i i incurs by handling some tasks), and 4. the initial allocation of tasks  #T  init  1 1 1  , . . . , T  init  |A  A A|  #,  where T =  #  i i i#A A A T  init  i i...
DB
ricci01enlightened
Enlightened Agents in TuCSoN In the network-centric computing era, applications often involve sets of autonomous, unpredictable, and possibly mobile entities interacting within open, dynamic, and possibly unreliable environments: Intelligent Environments are a typical case. The complexity of such scenarios requires novel engineering tools, providing effective support from the analysis to the deployment stage. In this paper we illustrate the impact of a general-purpose coordination infrastructure for multiagent systems -- providing a model, a run-time, and suitable deployment tools -- on the engineering of such applications. As a case study, we consider the intelligent management of lights inside a building: despite its simplicity, this problem endorses the typical challenges of this class of applications. The case study is built upon the TuCSoN coordination infrastructure, which provides engineers  with both the abstractions and the run-time support for effectively managing the application complexity.  I. INFRASTR...
Agents
rauterberg98build
BUILD-IT: A Planning Tool for Construction and Design It is time to go beyond the established approaches in humancomputer interaction. With the Augmented Reality (AR) design strategy humans are able to behave as much as possible in a natural way: behavior of humans in the real world with other humans and/or real world objects. Following the fundamental constraints of natural way of interacting we derive a set of recommendations for the next generation of user interfaces: the Natural User Interface (NUI). The concept of NUI is presented in form of a runnable demonstrator: a computer vision-based interaction technique for a planning tool for construction and design tasks.  Keywords  augmented reality, digital desk, natural user interface, computer vision-based interaction  
HCI
antoniou98normal
Normal Forms for Defeasible Logic Defeasible logic is an important logic-programming based nonmonotonic reasoning formalism which has an efficient implementation. It makes use of facts, strict rules, defeasible rules, defeaters, and a superiority relation. Representation results are important because they can help the assimilation of a concept by confining attention to its critical aspects. In this paper we derive some representation results for defeasible logic. In particular we show that the superiority relation does not add to the expressive power of the logic, and can be simulated by other ingredients in a modular way. Also, facts can be simulated by strict rules. Finally we show that we cannot simplify the logic any further in a modular way: Strict rules, defeasible rules, and defeaters form a minimal set of independent ingredients in the logic. 1 Introduction  Normal forms play an important role in computer science. Examples of areas where normal forms have proved fruitful include logic [10], where normal forms o...
DB
evans01itr
ITR: A Framework for Environment-Aware, Massively Distributed Computing physical environment in real-time, and the need to reason about emerging aggregate properties as opposed to individual component behavior. In this research we propose to develop theory, methods and tools for massively distributed, environment-aware computing (more succinctly referred to as swarm computing).  The state of swarm computing today is similar to that of sequential computing in the early 1950s. Developers painstakingly produce swarm programs by designing and programming the actions of individual devices, and converge on an acceptable program through extensive simulation and experimentation. In the pre-compiler era, skeptical programmers believed that a mechanical process could not possibly produce code of comparable quality to that produced by highly skilled machine coders and that the cost of machine time is high enough to outweigh any possible savings in programmer effort. The state of swarm programming today is similar: devices are still expensive enough an
Agents
497508
Human Behavior Models for Game-Theoretic Agents: Case of Crowd Tipping This paper describes an effort to integrate human behavior models from a range of ability, stress, emotion, decision theoretic, and motivation literatures into a game-theoretic framework. Our goal is to create a common mathematical framework (CMF) and a simulation environment that allows one to research and explore alternative behavior models to add realism to software agents -- e.g., human reaction times,  constrained rationality, emotive states, and cultural influences. Our CMF is based on a dynamical, gametheoretic approach to evolution and equilibria in Markov chains representing states of the world that the  agents can act upon. In these worlds the agents' utilities (payoffs) are derived by a deep model of cognitive appraisal of intention achievement including assessment of emotional activation/decay relative to concern ontologies, and subject to (integrated) stress and related constraints. We present the progress to date on the mathematical framework, and on an environment for editing the various elements of the cognitive appraiser, utility generators, concern ontologies, and Markov chains. We summarize a prototype of an example training game for counter-terrorism and crowd management. Future research needs are elaborated including validity issues and the gaps in the behavioral literatures that agent developers must struggle with.
Agents
park01analysis
Analysis and extraction of useful information across networks of Web databases Contents 1 Introduction 2 2 Problem Statement 2 3 Literature Review 3 3.1 Retrieving Text . . . . . . . . . . . . . . . . . . . . . . . . . . 3 3.2 Understanding Music . . . . . . . . . . . . . . . . . . . . . . . 7 3.3 Identifying Images . . . . . . . . . . . . . . . . . . . . . . . . 9 3.4 Extracting Video . . . . . . . . . . . . . . . . . . . . . . . . . 11 4 Work Completed and in Progress 12 5 Research Plan and Time-line 14 A List of Published Work 15 1 1 INTRODUCTION 2 1 Introduction The World Wide Web of documents on the Internet contains a huge amount of information and resources. It has been growing at a rapid rate for nearly a decade and is now one of the main resources of information for many people. The large interest in the Web is due to the fact that it is uncontrolled and easily accessible, no single person owns it and anyone can add to it. The Web has also brought with it a lot of controversy, also due to the
IR
lauzac01view
The View Holder Approach: Utilizing Customized Materialized Views To Create Database Services Suitable For Mobile Database Applications among mobile devices (i.e., a laptop vs. a pager) and the amount of information available from today's database environments and the Internet.  To this end, this dissertation presents the development of customizable view maintenance services, called the View Holder approach, whose middleware mechanism within the fixed network dynamically maintains versions of the views so that to meet the data consistency and currency requirements of a particular mobile client. In a general form, a View Holder can support a community of mobile clients with common interests. The motivation for maintaining versions is to compensate for the data changes that occurred to the materialized views that were used during disconnection as well as to reduce the cost of wireless communication. In order to maintain these views, customized view maintenance is performed at the data sources by translating the mobile machine's request into a materialization program containing a triggering
DB
benitez98using
Using Relevance Feedback In Contentbased Image Metasearch this article with a review of the issues in content-based visual query, then describe the current MetaSeek implementation. We present the results of experiments that evaluated the implementation in comparison to a previous version of the system and a baseline engine that randomly selects the individual search engines to query. We conclude by summarizing open issues for future research.
IR
busetta99jack
JACK Intelligent Agents - Components for Intelligent Agents in Java This paper is organised as follows. Section 2 introduces JACK Intelligent Agents, presenting the approach taken by AOS to its design and outlining its major engineering characteristics. The BDI model is discussed briefly in Section 3. Section 4 gives an outline of how to build an application with JACK Intelligent Agents. Finally, in Section 5 we discuss how the use of this framework can be beneficial to both engineers and researchers. For brevity, we will refer to JACK Intelligent Agents simply as "JACK".
Agents
glance00community
Community Search Assistant This paper describes a new software agent, the community  search assistant, which recommends related searches to  users of search engines. The community search assistant  enables communities of users to search in a collaborative  fashion. All queries submitted by the community are stored  in the form of a graph. Links are made between queries that  are found to be related. Users can peruse the network of  related queries in an ordered way: following a path from a  first cousin, to a second cousin to a third cousin, etc. to a set  of search results. The first key idea behind the use of query  graphs is that the determination of relatedness depends on  the documents returned by the queries, not on the actual  terms in the queries themselves. The second key idea is that  the construction of the query graph transforms single user  usage of information networks (e.g. search) into  collaborative usage: all users can tap into the knowledge  base of queries submitted by others.  Introduction  ...
HCI
382301
Applying Formal Concepts to Learning Systems Validation In the problem area of evaluating complex software systems, there are two distinguished areas of research, development, and application  identified by the two buzzwords validation and verification, respectively. From the perspective adopted by the authors, verification is usually  more formally based and, thus, can be supported by formal reasoning tools like theorem provers, for instance.  The scope of verification approaches is limited by the difficulty of finding a sufficiently complete formalization to built upon. In paramount realistic problem domains, validation seems to be more appropriate, although it is less stringent in character and, therefore, validation results are often less definite. The aim of this paper is to exemplify a validation approach based on a clear and thoroughly formal theory. In this way, validation and  verification should be brought closer to each other.  To allow for precise and sufficiently clear results, the authors have selected the applicatio...
ML
paradiso00design
Design and Implementation of Expressive Footwear As an outgrowth of our interest in dense wireless sensing and expressive applications of wearable computing, we have developed the world's most versatile human-computer interface for the foot. By dense wireless sensing, we mean the remote acquisition of many different parameters with a compact, autonomous sensor cluster. We have developed such a low-power sensor card to measure over 16 continuous quantities and transmit them wirelessly to a remote base station, updating all variables at 50 Hz. We have integrated a pair of these devices onto the feet of dancers and athletes, measuring continuous pressure at 3 points near the toe, dynamic pressure at the heel, bidirectional bend of the sole, height of each foot off conducting strips in the stage, angular rate of each foot about the vertical, angular position of each foot about the Earth's local magnetic field, as well as their tilt and low-G acceleration, 3-axis shock acceleration (from kicks and jumps), and position (via an integrated s...
HCI
425424
Ontobroker: How to make the WWW Intelligent . The World Wide Web can be viewed as the largest knowledge base  that has ever existed. However, its support in query answering and automated  inference is very limited. We propose formalized ontologies as means to enrich  web documents for representing semantic information to overcome this  bottleneck. Ontologies enable informed search as well as the derivation of new  knowledge that is not represented in the WWW. The paper describes a software  tool called Ontobroker that provides the necessary support in realizing this idea.  Basically it provides formalisms and tools for formulating queries, for defining  ontologies, and for annotating HTML documents with ontological information.  1 Introduction  The World Wide Web (WWW) contains huge amounts of knowledge about most subjects one can think of. HTML documents enriched by multi-media applications provide knowledge in different representations (i.e., text, graphics, animated pictures, video, sound, virtual reality, etc.). Hypertext li...
AI
532140
Creatures: Entertainment Software Agents with Artificial Life We present a technical description of Creatures, a commercial home-entertainment software package. Creatures provides a simulatedenvironment in which exist a number of synthetic agents that a user can interact with in real-time. The agents (known as "creatures") are intended as sophisticated "virtual pets". The internal architecture of the creatures is strongly inspired by animal biology. Each creature has a neural network responsible for sensory-motorcoordinationand behavior selection, and an "artificial biochemistry" that models a simple energy metabolism along with a "hormonal" system that interacts with the neural network to model diffuse modulation of neuronal activity and staged ontogenetic development. A biologically inspired learning mechanism allows the neural network to adapt during the lifetime of a creature. Learning includes the ability to acquire a simple verb--object language.
Agents
bruckner01managing
Managing Time Consistency for Active Data Warehouse Environments Abstract. Real-world changes are generally discovered delayed by computer systems. The typical update patterns for traditional data warehouses on an overnight or even weekly basis enlarge this propagation delay until the information is available to knowledge workers. Typically, traditional data warehouses focus on summarized data (at some level) rather than detail data. For active data warehouse environments, also detailed data about individual entities are required for checking the data conditions and triggering actions. Hence, keeping data current and consistent in that context is not an easy task. In this paper we present an approach for modeling conceptual time consistency problems and introduce a data model that deals with timely delays. It supports knowledge workers, to find out, why (or why not) an active system responded to a certain state of the data. Therefore the model enables analytical processing of detail data (enhanced by valid time) based on a knowledge state at a specified instant of time. All states that were not yet knowable to the system at that point in time are consistently ignored. 1.
DB
chen00niagaracq
NiagaraCQ: A Scalable Continuous Query System for Internet Databases Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system’s performance and scalability. 1.
DB
34322
CABINS: A Framework of Knowledge Acquisition and Iterative Revision for Schedule Improvement and Reactive Repair Practical scheduling problems generally require allocation of resources in the presence of a large, diverse and typically conflicting set of constraints and optimization criteria. The ill-structuredness of both the solution space and the desired objectives make scheduling problems difficult to formalize. This paper describes a case-based learning method for acquiring context-dependent user optimization preferences and tradeoffs and using them to incrementally improve schedule quality in predictive scheduling and reactive schedule management in response to unexpected execution events. The approach, implemented in the CABINS system, uses acquired user preferences to dynamically modify search control to guide schedule improvement. During iterative repair, cases are exploited for: (1) repair action selection, (2) evaluation of intermediate repair results and (3) recovery from revision failures. The method allows the system to dynamically switch between repair heuristic actions, each of whi...
ML
540061
Are you ready for Yottabytes? StorHouse - Federated and Object/Relational Solution This paper describes how federated and object/relational database systems can exploit cost-effective active storage hierarchies. By active storage hierarchy we mean a database system that uses all storage media (i.e. optical, tape, and disk) to store and retrieve data and not just disk. A detailed discussion of the Atomic Data Store data warehouse concept can be found in [CB 99]. These also describe a commercial relational database product, StorHouse/Relational Manager (RM), that executes SQL queries directly against data stored in a complete storage hierarchy. This paper focuses on applications that can use, and may even require the use of, emerging federated and object/relational database technologies. Our analysis is based on two products now in development. We will refer to these as StorHouse/Fed (a federated database system that includes StorHouse/RM) and StorHouse/ORM (an Object-Relational database system). We conclude by describing candidate applications (with an emphasis on the federal sector) that can exploit the combination of costeffective active storage hierarchy with federated and/or object/relational database technology.
DB
yu99efficient
Efficient and Effective Metasearch for a Large Number of Text Databases Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). In a metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against the set of representatives of all databases in order to determine the appropriate databases to search. When the number of databases is very large, say in the order of tens of thousands or more, then a traditional metasearch engine may become inefficient as each query needs to be evaluated against too many database representatives. Furthermore, the storage requirement on the site containing the metasearch engine can be very large. In this paper, we propose to use a hierarchy of database representatives to improve the efficiency. We provide an algorithm to search the hierarchy. We show that the retrieval effectiveness of our algorithm is the same as that of evaluating the user query against all database representatives. We als...
IR
wonsever01contextual
Contextual Rules for Text Analysis In this paper we describe a rule-based formalism for the analysis and  labelling of texts segments. The rules are contextual rewriting rules with a  restricted form of negation. They allow to underspecify text segments not  considered relevant to a given task and to base decisions upon context. A parser  for these rules is presented and consistence and completeness issues are  discussed. Some results of an implementation of this parser with a set of rules  oriented to the segmentation of texts in propositions are shown.
IR
grahne01algebraic
Algebraic rewritings for optimizing regular path queries Rewriting queries using views is a powerful technique that has applications in query optimization, data integration, data warehousing etc. Query rewriting in relational databases is by now rather well investigated. However, in the framework of semistructured data the problem of rewriting has received much less attention. In this paper we focus on extracting as much information as possible from algebraic rewritings for the purpose of optimizing regular path queries. The cases when we can find a complete exact rewriting of a query using a set a views are very "ideal." However, there is always information available in the views, even if this information is only partial. We introduce "lower" and "possibility" partial rewritings and provide algorithms for computing them. These rewritings are algebraic in their nature, i.e. we use only the algebraic view definitions for computing the rewritings. This fact makes them a main memory product which can be used for reducing secondary memory and remote access. We give two algorithms for utilizing the partial lower and partial possibility rewritings in the context of query optimization. 
DB
bergamaschi01mobile
Mobile Agents for Information Integration . The large amount of information that is spread over the Internet is an  important resource for all people but also introduces some issues that must be  faced. The dynamism and the uncertainty of the Internet, along with the heterogeneity  of the sources of information are the two main challanges for the  today's technologies. This paper proposes an approach based on mobile agents  integrated in an information integration infrastructure. Mobile agents can significantly  improve the design and the development of Internet applications thanks  to their characteristics of autonomy and adaptability to open and distributed environments,  such as the Internet. MOMIS (Mediator envirOnment for Multiple  Information Sources) is an infrastructure for semi-automatic information integration  that deals with the integration and query of multiple, heterogeneous information  sources (relational, object, XML and semi-structured sources). The aim of  this paper is to show the advantage of the introduction in the MOMIS infrastructure  of intelligent and mobile software agents for the autonomous management  and coordination of the integration and query processes over heterogeneous data  sources.  1 
Agents
101570
Protein Structure Prediction With Evolutionary Algorithms Evolutionary algorithms have been successfully applied to a variety of molecular structure prediction problems. In this paper we reconsider the design of genetic algorithms that have been applied to a simple protein structure prediction problem. Our analysis considers the impact of several algorithmic factors for this problem: the conformational representation, the energy formulation and the way in which infeasible conformations are penalized. Further we empirically evaluate the impact of these factors on a small set of polymer sequences. Our analysis leads to specific recommendations for both GAs as well as other heuristic methods for solving PSP on the HP model. 1 INTRODUCTION  A protein is a chain of amino acid residues that folds into a specific native tertiary structure under certain physiological conditions. A protein's structure determines its biological function. Consequently, methods for solving protein structure prediction (PSP) problems are valuable tools for modern molecula...
ML
382379
Visual Exploration of Temporal Object Databases Two complementary families of users' tasks may be identified during database visualization: data  browsing and data analysis. On the one hand, data browsing involves extensively exploring a subset  of the database using navigational interaction techniques. Classical object database browsers provide  means for navigating within a collection of objects and amongst objects by way of their relationships.  In temporal object databases, these techniques are not sufficient to adequately support time-related  tasks, such as studying a snapshot of a collection of objects at a given instant, or detecting changes  within temporal attributes and relationships. Visual data analysis on the other hand, is dedicated to  the extraction of valuable knowledge by exploiting the human visual perception capabilities. In temporal  databases, examples of data analysis tasks include observing the layout of a history, detecting  regularities and trends, and comparing the evolution of the values taken by two or more histories. In  this paper, we identify several users' tasks related to temporal database exploration, and we propose  three novel visualization techniques addressing them. The first of them is dedicated to temporal object  browsing, while the two others are oriented towards the analysis of quantitative histories. All  three techniques are shown to satisfy several ergonomic properties.  Keywords: temporal database, object database, data browsing and analysis, visualization technique.  1 
DB
445235
Towards a Layered Approach for Agent Infrastructure: The Right Tools for the Right Job It is clear by now that the take-up of agent technologies and the wide use of such technologies in open environments depends on the provision of appropriate infrastructure to support the rapid development of applications. In this paper, we argue that the elements required for the development of infrastructure span three different fields, which, nevertheless, have a great degree of overlap. Middleware technologies, mobile agent and intelligent agent research all have significant contributions to make towards a holistic approach to infrastructure development, but it is necessary to make clear distinctions between the requirements at each level and explain how they can be integrated so as to provide a clearer focus and allow the use of existing technologies. Our view of the requirements for infrastructure to support agent-based systems has been formed through experience with developing an agent implementation environment based on a formal agent framework. We argue that in order to provide support to developers, this infrastructure must address both conceptual concerns relating the different types of entities, and relationships between agent and non-agent entities in the environment, as well as more technical concerns. This paper describes the general requirements for infrastructure, the specific contributions from different areas, and our own efforts in progressing towards them. 1.
Agents
escudero00boosting
Boosting Applied to Word Sense Disambiguation . In this paper Schapire and Singer's AdaBoost.MH boosting  algorithm is applied to the Word Sense Disambiguation (WSD) problem.  Initial experiments on a set of 15 selected polysemous words show that  the boosting approach surpasses Naive Bayes and Exemplar--based approaches,  which represent state--of--the--art accuracy on supervised WSD.  In order to make boosting practical for a real learning domain of thousands  of words, several ways of accelerating the algorithm by reducing the  feature space are studied. The best variant, which we call LazyBoosting,  is tested on the largest sense--tagged corpus available containing 192,800  examples of the 191 most frequent and ambiguous English words. Again,  boosting compares favourably to the other benchmark algorithms.  1 Introduction  Word Sense Disambiguation (WSD) is the problem of assigning the appropriate meaning (sense) to a given word in a text or discourse. This meaning is distinguishable from other senses potentially attributable ...
ML
li01characterizing
Characterizing Operating System Activity In Specjvm98 Benchmarks : Complete system simulation to understand the influence of architecture and  operating systems on application execution has been identified to be crucial for  systems design. This problem is particularly interesting in the context of Java  since it is not only the application that can invoke kernel services, but so does  the underlying Java Virtual Machine (JVM) implementation which runs these  programs. Further, the JVM style (JIT compiler or interpreter) and the manner in  which the different JVM components (such as the garbage collector and class  loader) are exercised, can have a significant impact on the kernel activities. To  investigate these issues, this chapter uses complete system simulation of the  SPECjvm98 benchmarks on the SimOS simulation platform. The execution of  these benchmarks on both JIT compilers and interpreters is profiled in detail.  The kernel activity of SPECjvm98 applications constitutes up to 17% of the  execution time in the large dataset and up to 31% i...
AI
15156
The DEDALE System for Complex Spatial Queries This paper presents dedale, a spatial database system intended to overcome some limitations of current systems by providing an abstract and non-specialized data model and query language for the representation and manipulation of spatial objects. dedale relies on a logical model based on linear constraints, which generalizes the constraint database model of [KKR90]. While in the classical constraint model, spatial data is always decomposed into its convex components, in dedale holes are allowed to fit the need of practical applications. The logical representation of spatial data although slightly more costly in memory, has the advantage of simplifying the algorithms. dedale relies on nested relations, in which all sorts of data (thematic, spatial, etc.) are stored in a uniform fashion. This new data model supports declarative query languages, which allow an intuitive and efficient manipulation of spatial objects. Their formal foundation constitutes a basis for practical query optimizati...
DB
jeh02simrank
SimRank: A Measure of Structural-Context Similarity The problem of measuring "similarity" of objects arises in  many applications, and many domain-specific measures  have been developed, e.g., matching text across documents  or computing overlap among item-sets. We propose  a complementary approach, applicable in any domain  with object-to-object relationships, that measures similarity  of the structural context in which objects occur, based  on their relationships with other objects. Effectively, we  compute a measure that says "two objects are similar if  they are related to similar objects." This general similarity  measure, called SimRank, is based on a simple and intuitive  graph-theoretic model. For a given domain, SimRank  can be combined with other domain-specific similarity  measures. We suggest techniques for efficient computation  of SimRank scores, and provide experimental results  on two application domains showing the computational  feasibility and effectiveness of our approach.
IR
abberley99thisl
The THISL Broadcast News Retrieval System This paper described the THISL spoken document retrieval system for British and North American Broadcast News. The system is based on the ABBOT large vocabulary speech recognizer, using a recurrent network acoustic model, and a probabilistic text retrieval system. We discuss the development of a realtime British English Broadcast News system, and its integration into a spoken document retrieval system. Detailed evaluation is performed using a similar North American Broadcast News system, to take advantage of the TREC SDR evaluation methodology. We report results on this evaluation, with particular reference to the effect of query expansion and of automatic segmentation algorithms. 1. INTRODUCTION  THISL is an ESPRIT Long Term Research project in the area of speech retrieval. It is concerned with the construction of a system which performs good recognition of broadcast speech from television and radio news programmes, from which it can produce multimedia indexing data. The principal obj...
IR
rocha99syntactic
Syntactic Autonomy - Or Why There is no Autonomy Without Symbols and how Self-Organization Systems Might Evolve Them Two different types of agency are discussed based on dynamically coherent and incoherent couplings with an environment respectively. I propose that until a private syntax (syntactic autonomy) is discovered by dynamically coherent agents, there are no significant or interesting types of closure or autonomy. When syntactic autonomy is established, then, because of a process of description-based selected self-organization, open-ended evolution is enabled. At this stage, agents depend, in addition to dynamics, on localized, symbolic memory, thus adding a level of dynamical incoherence to their interaction with the environment. Furthermore, it is the appearance of syntactic autonomy which enables much more interesting types of closures amongst agents which share the same syntax. To investigate how we can study the emergence of syntax from dynamical systems, experiments with cellular automata leading to emergent computation to solve non-trivial tasks are discussed. RNA editing is also mentio...
HCI
wijsen99stringbased
A String-based Model for Infinite Granularities (Extended Abstract) )  Jef Wijsen  Universit'e de Mons-Hainaut  Jef.Wijsen@umh.ac.be  Abstract  In the last few years, the concept of time granularity  has been defined by several researchers, and a glossary  of time granularity concepts has been published. These  definitions often view a time granularity as a (mostly  infinite) sequence of time granules. Although this view  is conceptually clean, it is extremely inefficient or even  practically impossible to represent a time granularity  in this manner. In this paper, we present a practical  formalism for the finite representation of infinite granularities.  The formalism is string-based, allows symbolic  reasoning, and can be extended to multiple dimensions  to accommodate, for example, space.  Introduction  In the last few years, formalisms to represent and to reason about temporal and spatial granularity have been developed in several areas of computer science. Although several researchers have used different definitions of time granularity, they comm...
DB
307463
Maximizing Coverage of Mediated Web Queries Over the Web, mediators are built on large collections of sources to provide integrated access  to Web content (e.g., meta-search engines). In order to minimize the expense of visiting a large  number of sources, mediators need to choose a subset of sources to contact when processing  queries. As fewer sources participate in processing a mediated query, the coverage of the query  goes down. In this paper, we study this trade-off and develop techniques for mediators to  maximize the coverage for their queries while at the same time visiting a subset of their sources.  We formalize the problem; study its complexity; propose algorithms to solve it; and analyze the  theoretical performance guarantees of the algorithms. We also study the performance of our  algorithms through simulation experiments.  1 Introduction  Web sources often provide limited information "coverage." For instance, one type of information source is search engines, such as Lycos [27], Northern Light [29] and Yahoo [30]....
IR
rousskov98cache
Cache Digests This paper presents Cache Digest, a novel protocol and optimization technique for cooperative Web caching. Cache Digest allows proxies to make information about their cache contents available to peers in a compact form. A peer uses digests to identify neighbors that are likely to have a given document. Cache Digest is a promising alternative to traditional per-request query/reply schemes such as ICP. We discuss the design ideas behind Cache Digest and its implementation in the Squid proxy cache. The performance of Cache Digest is compared to ICP using real-world Web caches operated by NLANR. Our analysis shows that Cache Digest outperforms ICP in several categories. Finally, we outline improvements to the techniques we are currently working on. 1 Introduction  One of the most difficult problems in the design of Web cache hierarchies is efficiently locating objects held in neighbor caches. When a cache needs to forward a request, how does it know whether to use a sibling, a parent, or p...
DB
lam99broadcast
Broadcast of Consistent Data to Read-Only Transactions from Mobile Clients In this paper, we study the data inconsistency problem in data broadcast to mobile transactions. While data items in a mobile computing system are being broadcast, update transactions may install new values for the data items. If the executions of update transactions and broadcast of data items are interleaved without any control, the transactions generated by mobile clients, called mobile transactions, may observe inconsistent data values. In this paper, we propose a new protocol, called Update-First with Order (UFO), for concurrency control between read-only mobile transactions and update transactions. We show that although the protocol is simple, all the schedules are serializable when the UFO protocol is applied. Furthermore, the new protocol possesses many desirable properties for mobile computing systems such as the mobile transactions do not need to set any lock before they read a data item from the "air" and the protocol can be applied to different broadcast algorithms. Its performance has been investigated with extensive simulation experiment. The results show that the protocol can maximize the freshness of the data items provided to mobile transactions and the broadcast overhead is not heavy especially when the arrival rate of the update  transactions is not very high.
DB
appiani00multiagent
A Multi-Agent Approach to Vehicle Monitoring in Motorway . This paper describes CaseLP, a prototyping environment for MultiAgent  Systems (MAS), and its adoption for the development of a distributed industrial  application. CaseLP employs architecture definition, communication,  logic and procedural languages to model a MAS from the top-level architecture  down to procedural behavior of each agent's instance. The executable specification  which is obtained can be employed as a rapid prototype which helps in  taking quick decisions on the best possible implementation solutions. Such capabilities  have been applied to a distributed application of Elsag company, in order  to assess the best policies for data communication and database allocation before  the concrete implementation. The application consists in remote traffic control  and surveillance over service areas on an Italian motorway, employing automatic  detection and car plate reading at monitored gates. CaseLP allowed to predict  data communication performance statistics under differe...
Agents
vanschooten00process
Process- and Agent-Based Modelling Techniques for Dialogue Systems and Virtual Environments This text presents results of ongoing research, which is aimed at developing a framework for developing multimodal natural language dialogue systems operating within virtual environments. The aspects of multimodality and presence in a virtual environment are chosen as the main focus of this research. It may be argued that specification techniques would form the basis of such a framework. Therefore, a general overview and evaluation is given of existing specification techniques for interactive systems, based on both literature and previous research results. This includes the object-oriented model, process algebras, interactor models, and agent systems. Agent systems are further subdivided into intentional logics, production rule systems, agent communication languages, agent platforms, and agent architectures. A new agent system is proposed, which is based on update notification mechanisms as found in interactor models, and the `facilitator' function as found in some agent platfo...
Agents
40513
Probabilistic Deduction with Conditional Constraints over Basic Events We study the problem of probabilistic deduction with conditional constraints over basic events. We show that globally complete probabilistic deduction with conditional constraints over basic events is NP-hard. We then concentrate on the special case of probabilistic deduction in conditional constraint trees. We elaborate very efficient techniques for globally complete probabilistic deduction. In detail, for conditional constraint trees with point probabilities, we present a local approach to globally complete probabilistic deduction, which runs in linear time in the size of the conditional constraint trees. For conditional constraint trees with interval probabilities, we show that globally complete probabilistic deduction can be done in a global approach by solving nonlinear programs. We show how these nonlinear programs can be transformed into equivalent linear programs, which are solvable in polynomial time in the size of the conditional constraint trees. 1. Introduction  Dealing wit...
DB
hu00mobile
A Mobile Agent-Based Active Network Architecture Active networks enable customization of network functionality without the lengthy standard-mediated committee processes. Most of the works in the literature utilize the capsules or active packets as the means to transfer code information across active networks. In this paper, we propose an active network infrastructure based on mobile agents technologies. In our prototype implementation, mobile agents are the building blocks of carrying functional customizations, and the active nodes offer software application layers, the Agent Servers, to process mobile agent-specific customizations to facilitate network functionality. Both integrated and discrete operational models of network customizations are supported. In addition, for the application-specific protocol development and deployment, an abstract protocol structure and a protocol loading mechanism are presented. Furthermore, we provide an agent management/control mechanism and devise a protocol management /control mechanism. As a result, improved network functionality can be achieved.  1 
Agents
yang98extraction
Extraction and Classification of Visual Motion Patterns for Hand Gesture Recognition We present a new method for extracting and classifying motion patterns to recognize hand gestures. First, motion segmentation of the image sequence is generated based on a multiscale transform and attributed graph matching of regions across frames. This produces region correspondences and their affine transformations. Second, color information of motion regions is used to determine skin regions. Third, human head and palm regions are identified based on the shape and size of skin areas in motion. Finally, affine transformations defining a region's motion between successive frames are concatenated to construct the region's motion trajectory. Gestural motion trajectories are then classified by a time-delay neural network trained with backpropagation learning algorithm. Our experimental results show that hand gestures can be recognized well using motion patterns.  1 Introduction  This paper is concerned with the problem of detecting two-dimensional motion across image frames and classifyi...
ML
96518
Temporal Objects for Spatio-Temporal Data Models and a Comparison of Their Representations Abstract: Currently, there are strong efforts to integrate spatial and temporal database technology into spatio-temporal database systems. This paper views the topic from a rather fundamental perspective and makes several contributions. First, it reviews existing temporal and spatial data models and presents a completely new approach to temporal data modeling based on the very general notion of temporal object. The definition of temporal objects is centered around the observation that anything that changes over time can be expressed as a function over time. For the modeling of spatial objects the well known concept of spatial data types is employed. As specific subclasses, linear temporal and spatial objects are identified. Second, the paper proposes the database embedding of temporal objects by means of the abstract data type (ADT) approach to the integration of complex objects into databases. Furthermore, we make statements about the expressiveness of different temporal and spatial database embeddings. Third, we consider the combination of temporal and spatial objects into spatio-temporal objects in (relational) databases. We explain various alternatives for spatio-temporal data models and databases and compare their expressiveness. Spatio-temporal objects turn out to be specific instances of temporal objects. 1
DB
240151
Using Digital but Physical Surrogates to Mediate Awareness, Communication and Privacy in Media Spaces Digital but physical surrogates are tangible representations of remote people, typically members of small intimate teams, positioned within an office and under digital control. Surrogates selectively collect and present awareness information about the people they represent. They also react to people's explicit and implicit physical actions: a person's explicit acts include grasping and moving them, while implicit acts include one's proximity to the surrogate. By responding appropriately to these physical actions of people, surrogates can control the communication capabilities of a media space in a natural way. This enables the smooth transition from awareness to casual interaction while mitigating concerns about privacy.  Keywords: Ubiquitous media spaces, awareness, casual interaction, groupware, CSCW.  1. INTRODUCTION  Digital but physical surrogates are tangible representations of remote people, typically members of small intimate teams, positioned within a person's environment. As ...
HCI
460214
Vision-Based User Interface for Interacting with a Virtual Environment Abstract. This paper proposes a new and natural human computer interface for interacting with virtual environments. The 3D pointing direction of a user in a virtual environment is estimated using monocular computer vision. The 2D position of the user’s hand is extracted in the image plane and then mapped to a 3D direction using knowledge about the position of the user’s head and kinematic constraints of a pointing gesture due to the human motor system. Off-line tests of the system show promising results. The implementation of a real time system is currently in progress and is expected to run with 25Hz. 1
HCI
hermsdorf98webadapter
WebAdapter: A prototype of a WWW-browser with new special needs adaptations . This paper presents a prototypical WWW (World Wide Web)-browser called "WebAdapter",  which provides new special needs adaptations for physically handicapped, blind and visually impaired  end-users. These adaptations include near miss tolerances, implementation of sophisticated HTMLguidelines  and advanced speech output. For evaluation purposes a usability test was conducted proving  the suitability of the implemented special needs adaptations. The future goal of this work is a user  interface for all (UI4All) for a standard Webbrowser. With regard to this perspective, the WebAdapter  is still an reactive approach in that it only reacts to shortcomings of common Webbrowsers instead of  proactively integrating a standardized software layer between the user front-end and underlying  applications by which the I/O-Interface can easily and universally be adapted to a variety of different  personal needs of handicapped as well as able-bodied end-users. Thus, the WebAdapter only illustrates...
HCI
nahm02text
Text Mining with Information Extraction The popularity of the Web and the large number of documents available in electronic form has motivated the search for hidden knowledge in text collections. Consequently, there is growing research interest in the general topic of text mining. In this paper, we develop a text-mining system by integrating methods from Information Extraction (IE) and Data Mining (Knowledge Discovery from Databases or KDD). By utilizing existing IE and KDD techniques, text-mining systems can be developed relatively rapidly and evaluated on existing text corpora for testing IE systems. We present a general text-mining framework called DiscoTEX which employs an IE module for transforming natural-language documents into structured data and a KDD module for discovering prediction rules from the extracted data. When discovering patterns in extracted text, strict matching of strings is inadequate because textual database entries generally exhibit variations due to typographical errors, misspellings, abbreviations, and other
IR
zhang01supporting
On Supporting Containment Queries in Relational Database Management Systems Virtually all proposals for querying XML include a class of query we term “containment queries”. It is also clear that in the foreseeable future, a substantial amount of XML data will be stored in relational database systems. This raises the question of how to support these containment queries. The inverted list technology that underlies much of Information Retrieval is well-suited to these queries, but should we implement this technology (a) in a separate loosely-coupled IR engine, or (b) using the native tables and query execution machinery of the RDBMS? With option (b), more than twenty years of work on RDBMS query optimization, query execution, scalability, and concurrency control and recovery immediately extend to the queries and structures that implement these new operations. But all this will be irrelevant if the performance of option (b) lags that of (a) by too much. In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under certain conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementation in an RDBMS can support this class of query much more efficiently.
DB
tsuda00subspace
Subspace Information Criterion for Non-Quadratic Regularizers - Model Selection for Sparse Regressors Non-quadratic regularizers, in particular the # 1 norm regularizer can yield sparse solutions that generalize well. In this work we propose the Generalized Subspace Information Criterion (GSIC) that allows to predict the generalization error for this useful family of regularizers. We show that under some technical assumptions GSIC is an asymptotically unbiased estimator of the generalization error. GSIC is demonstrated to have a good performance in experiments with the # 1 norm regularizer as we compare with the Network Information Criterion and cross-validation in relatively large sample cases. However in the small sample case, GSIC tends to fail to capture the optimal model due to its large variance. Therefore, also a biased version of GSIC is introduced, which achieves reliable model selection in the relevant and challenging scenario of high dimensional data and few samples.  Subspace Information Criterion for Non-Quadratic Regularizers 2 1 
IR
539922
ARQuake: An Outdoor/Indoor Augmented Reality First Person Application This pap er presents an outdoor/indoor augmented re- ality first person applic ationAR(2uake we have developal. ARQuake is an extension of the desktop game Quake, and as such we are investigating how to convert a desktop first person application into an outdoor/indoor mobile augmented reality application. We present an archire  cture for a low cost, moderately accurate six degrees of freedom tracking system based on GP$, digital compass, and fiducial vision-based tracking. Usability issues  such as monster selection, colour, and input devies are  discussed. A second application for AR architectural design visualisation is presented.
HCI
kayan99evaluation
An Evaluation of Real-Time Transaction Management Issues in Mobile Database Systems A critical issue in mobile data management is to respond to real-time data access requirements of the supported application. However, it is difficult to handle real-time constraints in a mobile computing environment due to the physical constraints imposed by the mobile computer hardware and the wireless network technology. In this paper, we present a mobile database system model that takes into account the timing requirements of applications supported by mobile computing systems. We provide a transaction execution model with two alternative execution strategies for mobile transactions and evaluate the performance of the system considering various mobile system characteristics, such as the number of mobile hosts in the system, the handoff process, disconnection, coordinator site relocation, and wireless link failure. Performance results are provided in terms of the fraction of real-time requirements that are satisfied.  1 Introduction  A mobile computing system is a dynamic type of trad...
DB
yang00shopping
A Shopping Agent That Automatically Constructs Wrappers for Semi-Structured Online Vendors . This paper proposes a shopping agent with a robust inductive  learning method that automatically constructs wrappers for semistructured  online stores. Strong biases assumed in many existing systems  are weakened so that the real stores with reasonably complex document  structures can be handled. Our method treats a logical line as a basic  unit, and recognizes the position and the structure of product descriptions  by finding the most frequent pattern from the sequence of logical  line information in output HTML pages. This method is capable of analyzing  product descriptions that comprise multiple logical lines, and  even those with extra or missing attributes. Experimental tests on over  60 sites show that it successfully constructs correct wrappers for most  real stores.  1 Introduction  A shopping agent is a mediator system that extracts the product descriptions from several online stores on a user's behalf. Since the stores are heterogeneous, a procedure for extracting the cont...
IR
buyukkokten99exploiting
Exploiting Geographical Location Information of Web Pages Many information resources on the web are relevant primarily to limited geographical communities.  For instance, web sites containing information on restaurants, theaters, and apartment  rentals are relevant primarily to web users in geographical proximity to these locations.  In contrast, other information resources are relevant to a broader geographical community. For  instance, an on-line newspaper may be relevant to users across the United States. Unfortunately,  the geographical scope of web resources is largely ignored by web search engines. We  make the case for identifying and exploiting the geographical location information of web sites  so that web search engines can rank resources in a geographically sensitive fashion, in addition  to using more traditional information-retrieval strategies. In this paper, we first consider how  to compute the geographical location of web pages. Subsequently, we consider how to exploit  such information in one specific "proof-of-concept" appl...
IR
453469
MRML: Towards an extensible standard for multimedia querying and benchmarking In recent years, the need for databases which query multimedia data by content has  become apparent. Many commercial and non--commercial research groups are trying  to fulfill these needs.  The development of research can be described as moving in two directions  ffl search for new, useful query and interaction paradigms  ffl deeper research to improve the performance of systems that have adopted a given  query paradigm.  The search for new better performance given a query paradigm has led to "clusters" of  systems which are similar in their interaction with the user, and which give a certain  set of interaction capabilities to the user.  It is already visible, that research will move towards systems which enable the user  to formulate multi--paradigm queries in order to further improve results.  As a consequence of the above, there is the need for  ffl A common mechanism for shipping multi--paradigm queries and their results ,  which assures that the right query processor processes th...
HCI
fox98position
Position Estimation for Mobile Robots in Dynamic Environments For mobile robots to be successful, they have to navigate safely in populated and dynamic environments. While recent research has led to a variety of localization methods that can track robots well in static environments, we still lack methods that can robustly localize mobile robots in dynamic environments, in which people block the robot’s sensors for extensive periods of time or the position of furniture may change. This paper proposes extensions to Markov localization algorithms enabling them to localize mobile robots even in densely populated environments. Two different filters for determining the “believability ” of sensor readings are employed. These filters are designed to detect sensor readings that are corrupted by humans or unexpected changes in the environment. The technique was recently implemented and applied as part of an installation, in which a mobile robot gave interactive tours to visitors of the “Deutsches Museum Bonn. ” Extensive empirical tests involving datasets recorded during peak traffic hours in the museum demonstrate that this approach is able to accurately estimate the robot’s position in more than 98 % of the cases even in such highly dynamic environments.
AI
271013
Alerting Services in a Digital Library Environment The classical paradigm of finding information in the WWW by initiating  retrieval and browsing becomes more and more ineffective. Other techniques  have to be considered. Automatic delivery of contents to the user according  to their needs and filtered by her profile of interests is required. Current  implementations of such Alerting Services at content providers side have several  drawbacks. In my research project I evaluate methods and techniques for  Alerting Services with special respect to the area of digital libraries. I intend  to provide a framework that supports design decisions in building alerting services  depending on the infrastructure and desired system parameters.  1 Introduction  Imagine one morning you just arrive at your office and switch on your computer to have a look at the recent news in your special field of research. Little pictures for each topic tell you that some interesting documents arrived. Behind one icon you find for instance the new announcements for c...
IR
zhu00incorporating
Incorporating Quality Metrics in Centralized/Distributed Information Retrieval on the World Wide Web Most information retrieval systems on the Internet rely primarily on similarity ranking algorithms based solely on term frequency statistics. Information quality is usually ignored. This leads to the problem that documents are retrieved without regard to their quality. We present an approach that combines similarity-based similarity ranking with quality ranking in centralized and distributed search environments. Six quality metrics, including the currency, availability, information-to-noise ratio, authority, popularity, and cohesiveness, were investigated. Search effectiveness was significantly improved when the currency, availability, information-to-noise ratio and page cohesiveness metrics were incorporated in centralized search. The improvement seen when the availability, information-to-noise ratio, popularity, and cohesiveness metrics were incorporated in site selection was also significant. Finally, incorporating the popularity metric in information fusion resulted in a significan...
IR
llirbat01filtering
Filtering Algorithms and Implementation for Very Fast Publish/Subscribe Systems Publish/Subscribe is the paradigm in which users express long-term interests (\subscriptions") and some agent \publishes " events (e.g., oers). The job of Publish/Subscribe software is to send events to the owners of subscriptions satis  ed by those events. For example, a user subscription may consist of an interest in an airplane of a certain type, not to exceed a certain price. A published event may consist of an oer of an airplane with certain properties including price. Each subscription consists of a conjunction of (attribute, comparison operator, value) predicates. A subscription closely resembles a trigger in that it is a longlived conditional query associated with an action (usually, informing the subscriber). However, it is less general than a trigger so novel data structures and implementations may enable the creation of more scalable, high performance publish /subscribe systems. This paper describes an attempt at the construction of such algorithms and its implementation. Using a combination of data structures, application-speci  c caching policies, and application-speci  c query processing our system can handle 600 events per second for a typical workload containing 6 million subscriptions.
DB
vanlaerhoven00what
What Shall We Teach Our Pants? If a wearable device can register what the wearer is currently doing, it can anticipate and adjust its behavior to avoid redundant interaction with the user. However, the relevance and properties of the activities that should be recognized depend on both the application and the user. This requires an adaptive recognition of the activities where the user, instead of the designer, can teach the device what he/she is doing. As a case study we connected a pair of pants with accelerometers to a laptop to interpret the raw sensor data. Using a combination of machine learning techniques such as Kohonen maps and probabilistic models, we build a system that is able to learn activities while requiring minimal user attention. This approach to context awareness is more universal since it requires no a priori knowledge about the contexts or the user. 1.
HCI
williamson00approximating
Approximating Discrete Probability Distributions With Bayesian Networks I generalise the arguments of [Chow & Liu 1968] to show that a Bayesian network  satisfying some arbitrary constraint that best approximates a probability distribution  is one for which mutual information weight is maximised. I give a practical procedure  for nding an approximation network and evaluate its application on a range of data  sets.  Articial intelligence requires the ability to reach conclusions that may be far from certain. For example an expert system for medical diagnosis may be given the symptoms of some patient and asked to provide a diagnosis | even though the background knowledge and symptom information may not be enough to determine for sure which problem actually besets the patient. Probability theory provides a plausible model for reasoning under uncertainty, since one would expect a diagnosis to be relatively probable, given the symptoms. This paper addresses practical issues to do with the implementation of probabilistic reasoning.  The plan is rst to discuss...
AI
bernstein00data
Data Warehouse Scenarios for Model Management Model management is a framework for supporting meta-data related  applications where models and mappings are manipulated as first class objects  using operations such as Match, Merge, ApplyFunction, and Compose. To demonstrate  the approach, we show how to use model management in two scenarios  related to loading data warehouses. The case study illustrates the value of model  management as a methodology for approaching meta-data related problems. It  also helps clarify the required semantics of key operations. These detailed  scenarios provide evidence that generic model management is useful and, very  likely, implementable.
DB
decleir99database
A Database Approach for Modeling and Querying Video Data Indexing video data is essential for providing content based access. In this paper, we consider how database technology can offer an integrated framework for modeling and querying video data. As many concerns in video (e.g., modeling and querying) are also found in databases, databases provide an interesting angle to attack many of the problems. From a video applications perspective, database systems provide a nice basis for future video systems. More generally, database research will provide solutions to many video issues even if these are partial or fragmented. From a database perspective, video applications provide beautiful challenges. Next generation database systems will need to provide support for multimedia data (e.g., image, video, audio). These data types require new techniques for their management (i.e., storing, modeling, querying, etc.). Hence new solutions are significant.  This paper develops a data model and a rule-based query language for video content based indexing a...
DB
14656
Optimizing Queries with Object Updates Object-oriented databases (OODBs) provide powerful data abstractions and modeling facilities but they usually lack a suitable framework for query processing and optimization. Even though there is an increasing number of recent proposals on OODB query optimization, only few of them are actually focused on query optimization in the presence of object identity and destructive updates, features often supported by most realistic OODB languages. This paper presents a formal framework for optimizing object-oriented queries in the presence of side effects. These queries may contain object updates at any place and in any form. We present a language extension to the monoid comprehension calculus to express these object-oriented features and we give a formal meaning to these extensions. Our method is based on denotational semantics, which is often used to give a formal meaning to imperative programming languages. The semantics of our language extensions is expressed in terms of our monoid calculu...
DB
navarro00adding
Adding Compression to Block Addressing Inverted Indexes . Inverted index compression, block addressing and sequential search on compressed text are three techniques that have been separately developed for efficient, low-overhead text retrieval. Modern text compression techniques can reduce the text to less than 30% of its size and allow searching it directly and faster than the uncompressed text. Inverted index compression obtains significant reduction of their original size at the same processing speed. Block addressing makes the inverted lists point to text blocks instead of exact positions and pay the reduction in space with some sequential text scanning.  In this work we combine the three ideas in a single scheme. We present a compressed inverted file that indexes compressed text and uses block addressing. We consider different techniques to compress the index and study their performance with respect to the block size. We compare the index against three separate techniques for varying block sizes, showing that our index is superior to each isolated approach. For instance, with just 4% of extra space overhead the index has to scan less than 12% of the text for exact searches and about 20% allowing one error in the matches. Keywords: Text compression, inverted files, block addressing, text databases.  1. 
DB
84358
Using WG-Log to represent semistructured data: the example of OEM . In this paper we discuss the possibility to represent synthetically semistructured information via a loose notion of schema: we say that data are semistructured when, although some structure is present, it is not as strict, regular, or complete as the one required by the traditional database management systems. Our proposal is based on WG-Log, a graph based language for the representation of WWW site information. We show how information encoded in a typical semistructured information model, as OEM, can be represented and queried by means of the WG-Log language, and how the TSIMMIS and WG-Log Web Query System can be integrated to allow site content exploration and exploitation by means of WG-Log. 1 Introduction  We say that data are semistructured when, although some structure is present, it is not as strict, regular, or complete as the one required by the traditional database management systems (see [Abi97] for a survey on semistructured data). Information is semistructured also when...
DB
giampapa00agent
Agent Interoperation Across Multagent System Boundaries Recently the number of autonomous agents and multiagent systems (MAS) that have been developed by different developers has increased. Despite efforts for the creation of standards (eg. in communication languages, registration protocols etc.), it is clear that at least in the near term heterogeneous agents and MASs will be prevalent. Therefore, mechanisms that allow agents and/or MASs to interoperate and transact are needed. In this paper we report on a case study and lessons learned of an interoperator agent we developed. We discuss requirements for interoperation mechanisms, resulting challenges and our design decisions and implementation of the RETSINA-OAA InterOperator
Agents
drori00using
Using Text Elements by Context to Display Search Results in Information Retrieval Systems - Model and Research Results Information retrieval systems display search results by various methods. This paper focuses on a model for displaying a list of search results by means of textual elements that utilize a new information unit that replaces the currently used information unit. The paper includes a short description of several studies that support the model.  1. Introduction  Because of the growth in the number and scope of global databases, a special approach is required to locating information, from the perspective of the user interface. The Internet, as it exists today, is an outstanding example of a broad-base, unfocused database. Most Internet search engines display their information as a serially ordered list of results (with a partial attempt at ranking the results). In most cases, this list includes the document title, URL and, at times, the first few lines of the document. The information, as currently displayed to the user, is incomplete and insufficiently focused on the search query. This requi...
IR
frey99estimating
Estimating Mixture Models of Images and Inferring Spatial Transformations Using the EM Algorithm Presented at the IEEE Conference on Computer Vision and Pattern Recognition, Ft. Collins, CO, June, 1999. Mixture modeling and clustering algorithms are effective, simple ways to represent images using a set of data centers. However, in situations where the images include background clutter and transformations such as translation, rotation, shearing and warping, these methods extract data centers that include clutter and represent dierent transformations of essentially the same data. Taking face images as an example, it would be more useful for the dierent clusters to represent dierent poses and expressions, instead of cluttered versions of dierent translations, scales and rotations. By including clutter and transformation as unobserved, latent variables in a mixture model, we obtain a new \transformed mixture of Gaussians", which is invariant to a specied set of transformations. We show how a linear-time EM algorithm can be used to t this model by jointly estimating a mixture mo...
ML
decker95environment
Environment Centered Analysis and Design of Coordination Mechanisms Environment Centered Analysis and Design of Coordination Mechanisms  May 1995 KEITH S. DECKER B.S., Carnegie Mellon University M.S., Rensselaer Polytechnic Institute Ph.D., University of Massachusetts Amherst Directed by: Professor Victor R. Lesser Committee: Professor Paul R. Cohen Professor John A. Stankovic Professor Douglas L. Anderton Coordination, as the act of managing interdependencies between activities, is one of the central research issues in Distributed Artificial Intelligence. Many researchers have shown that there is no single best organization or coordination mechanism for all environments. Problems in coordinating the activities of distributed intelligent agents appear in many domains: the control of distributed sensor networks; multi-agent scheduling of people and/or machines; distributed diagnosis of errors in local-area or telephone networks; concurrent engineering; `software agents' for information gathering. The design of coordination mechanisms for groups of compu...
Agents
lawrence00context
Context in Web Search Web search engines generally treat search requests  in isolation. The results for a given query  are identical, independent of the user, or the context  in which the user made the request. Nextgeneration  search engines will make increasing  use of context information, either by using explicit  or implicit context information from users, or by  implementing additional functionality within restricted  contexts. Greater use of context in web  search may help increase competition and diversity  on the web.
IR
sattler01data
A Data Preparation Framework based on a Multidatabase Language Integration and analysis of data from different sources have to deal with several problems resulting from potential heterogeneities. The activities addressing these problems are called data preparation and are supported by various available tools. However, these tools process mostly in a batch-like manner not supporting the iterative and explorative nature of the integration and analysis process. In this work we present a framework for important data preparation tasks based on a multidatabase language. This language offers features for solving common integration and cleaning problems as part of query processing. Combining data preparation mechanisms and multidatabase query facilities permits applying and evaluating different integration and cleaning strategies without explicit loading and materialization of data. The paper introduces the language concepts and discusses their application for individual tasks of data preparation.
DB
singletary01symbiotic
Symbiotic Interfaces For Wearable Face Recognition We introduce a wearable face detection method that exploits constraints in face scale and orientation imposed by the proximity of participants in near social interactions. Using this method we describe a wearable system that perceives “social engagement,” i.e., when the wearer begins to interact with other individuals. One possible application is improving the interfaces of portable consumer electronics, such as cellular phones, to avoid interrupting the user during face-to-face interactions. Our experimental system proved> 90 % accurate when tested on wearable video data captured at a professional conference. Over three hundred individuals were captured, and the data was separated into independent training and test sets. A goal is to incorporate user interface in mobile machine recognition systems to improve performance. The user may provide real-time feedback to the system or may subtly cue the system through typical daily activities, such as turning to face a speaker, as to when conditions for recognition are favorable. 1
HCI
340342
Learning Intonation Rules for Concept-to-Speech Generation We aim to design and develop a Concept-to-Speech (CTS) generation system, a speech synthesis system producing speech from semantic representations, by integrating language generation with speech synthesis. We focus on five issues (1) how to employ newly available accurate discourse, semantic, and syntactic information produced by a natural language generation system to improve the quality of synthesized speech. (2) how to extend an existing general purpose natural language generation system to a Concept-to-Speech generation tool. (3) how to integrate rule induction into a CTS architecture to separate the domain-dependent prosodic rules from the prosody generation component. (4) how to design a speech interface language to facilitate a more flexible and open CTS architecture. (5) how to build prosody models and explore their use in multimedia synchronization. Unlike previous CTS research, we utilize linguistic constraints provided by a general purpose natural language generation system;...
ML
190915
First-Order Queries On Finite Structures Over The Reals We investigate properties of finite relational structures over the  reals expressed by first-order sentences whose predicates are the relations  of the structure plus arbitrary polynomial inequalities, and  whose quantifiers can range over the whole set of reals. In constraint  programming terminology, this corresponds to Boolean real polynomial  constraint queries on finite structures. The fact that quantifiers  range over all reals seems crucial; however, we observe that each sentence  in the first-order theory of the reals can be evaluated by letting  each quantifier range over only a finite set of real numbers without  changing its truth value. Inspired by this observation, we then show  that when all polynomials used are linear, each query can be expressed  uniformly on all finite structures by a sentence of which the quantifiers  range only over the finite domain of the structure. In other words,  linear constraint programming on finite structures can be reduced to  ordinary query evaluation as usual in finite model theory and databases.  Moreover, if only "generic" queries are taken into consideration,  we show that this can be reduced even further by proving that such  Dept. Math. & Computer Sci., University of Antwerp (UIA), Universiteitsplein 1, B-2610 Antwerp, Belgium. E-mail: pareda@uia.ac.be.  y  Dept. WNI, University of Limburg (LUC), B-3590 Diepenbeek, Belgium. E-mail: vdbuss@luc.ac.be.  z  Computer Science Department, Indiana University, Bloomington, IN 47405-4101, USA. E-mail: vgucht@cs.indiana.edu.  1  queries can be expressed by sentences using as polynomial inequalities  only those of the simple form x ! y.  1 
DB
8263
An Architecture for Mobile BDI Agents BDI (Belief, Desire, Intention) is a mature and commonly adopted architecture for Intelligent Agents. BDI Agents are autonomous entities able to work in teams and react to changing environmental conditions. However, the current computational model adopted by BDI has problems which, amongst other limitations, prevent the development of mobile agents. In this paper, we discuss an architecture,  TOMAS (Transaction Oriented Multi Agent System), that addresses these issues by combining BDI and the distributed nested transaction paradigms. An algorithm is presented which enable agents in TOMAS to become mobile.  1 Introduction  Intelligent Agents are a very active area of AI research [WJ95] [Sho93]. Of the various agent architectures which have been proposed, BDI (Belief, Desire, Intention) [RG92] is probably the most mature and has been adopted by a few industrial applications. BDI Agents are autonomous entities able to work in teams and react to changing environmental conditions. Mobile m...
Agents
517801
PlanetP: Infrastructure Support for P2P Information Sharing Storage technology trends' are providing massive storage in extremely small packages while declining computing costs' are resulting in a rising number of devices' per person. The confluence of these trends' are presenting a new, critical challenge to storage and file system designers': how to enable users' to effectively manage, use, and share huge amounts' of data stored across a multitude of devices. In this paper, we present a novel middleware storage system, PlanetP, which is designed from first principles as' a peerto -peer (P2P), semantically indexed storage layer. PlanetP makes two novel design choices to meet the above challenge. First, PlanetP concentrates on content-based querying for information retrieval and assumes that the unit of storage is a shipper of XML, allowing it to index arbitrary data for search and retrieval regardless of the applications' used to create and manipulate the data. Second, PlanetP adopts' a P2P approach, avoiding centralization of storage and indexing. This' makes PlanetP particularly suitable for information sharing among ad hoc groups of users, each of which may have to manage data distributed across multiple devices'. PlanetP is' targeted for groups of up to 1000 users'; results' from studying communities of lO0-200 peers' running on a cluster of PCs indicates that PlanetP should scale well to the 1000-member threshold. Finally, we describe BreezeFS, a semantic file system that we have implemented to validate PlanetP's utility.
IR
jensen99learning
Learning Quantitative Knowledge for Multiagent Coordination A central challenge of multiagent coordination is reasoning  about how the actions of one agent affect the  actions of another. Knowledge of these interrelationships  can help coordinate agents --- preventing conflicts  and exploiting beneficial relationships among actions.  We explore three interlocking methods that  learn quantitative knowledge of such non-local effects  in TAEMS, a well-developed framework for multiagent  coordination. The surprising simplicity and effectiveness  of these methods demonstrates how agents can  learn domain-specific knowledge quickly, extending the  utility of coordination frameworks that explicitly represent  coordination knowledge.  Introduction  A major challenge of designing effective multiagent systems is managing non-local effects --- situations where the actions of one agent impact the performance of other agents' actions. For example, one agent's action can enable, disable, facilitate, or hinder the actions of other agents. Poor accounting for ...
AI
liu00design
Design and Implementation of the ROL System ROL is a deductive object-oriented database system developed at the University of Regina. It eectively integrates important features of deductive databases and object-oriented databases in a uniform framework and provides a uniform rule-based declarative language for dening, manipulating and querying a database. This paper describes the latest implementation of ROL.  1 Introduction  In the past decade a lot of interests arose in integrating deductive and object-oriented databases to gain the best of the two approaches such as recursion, declarative querying, and rm logical foundations from deductive approaches, and object identity, complex objects, classes, class hierarchy, property inheritance with overriding and schema from object-oriented approach. A number of deductive object-oriented database languages have been proposed, such as O-logic [17], revised O-logic [11], C-logic [8], IQL [2], IQL2[1], Flogic [10], LOGRES [7], LLO [16], LOL [6], CORAL++[19], Datalog  method  [3], DLT ...
DB
504568
Gleaning Answers From the Web Introduction  This position paper summarizes my recent and ongoing research on Web information extraction and retrieval. I describe    wrapper induction and verification techniques for extracting data from structured sources;    boosted wrapper induction, an extension of these techniques to handle natural text;    ELIXIR, our e#cient and expressive language for XML information retrieval ;    techniques and applications for text genre classification; and    stochastic models for XML schema alignment.  The unifying theme of these various research projects is to develop enabling technologies that facilitate the rapid development of large Web services for data access and integration.  2 Wrapper induction and verification   A wide variety of valuable textual information resides on the Web, but very little is in a machineunderstandable form such as XML. Instead, the content
IR
winikoff01simplifying
Simplifying the Development of Intelligent Agents Intelligent agents is a powerful Artificial Intelligence technology which shows considerable  promise as a new paradigm for mainstream software development. However, despite  their promise, intelligent agents are still scarce in the market place. A key reason for this  is that developing intelligent agent software requires significant training and skill: a typical  developer or undergraduate struggles to develop good agent systems using the Belief Desire  Intention (BDI) model (or similar models). This paper identifies the concept set which we  have found to be important in developing intelligent agent systems and the relationships between  these concepts. This concept set was developed with the intention of being clearer,  simpler, and easier to use than current approaches. We also describe briefly a (very simplified)  example from one of the projects we have worked on (RoboRescue), illustrating the  way in which these concepts are important in designing and developing intelligent software  agents.  Keywords: AI Architectures, distributed AI, multiagent systems, reactive control, software  agents.  1 
Agents
449669
Distance-From-Boundary As A Metric For Texture Image Retrieval A new metric is proposed for texture image retrieval, which is based on the signed distance of the images in the database to a boundary chosen by the query. This novel metric has three advantages: 1) the boundary distance measures are relatively insensitive to the sample distributions; 2) same retrieval results can be obtained with respect to different (but visually similar) queries; 3) retrieval performance can be improved. The boundaries are obtained by using a statistical learning algorithm called support vector machine (SVM), and hence the boundaries can be simply represented by some vectors and their combination coefficients. Experimental results on the Brodatz texture database indicate that a significantly better retrieval performance can be achieved as compared to the traditional Euclidean distance based approach. This technique can be further developed to learn pattern similarities among different texture classes and used in relevance feedback.  
IR
48055
STARTS: Stanford Proposal for Internet Meta-Searching Document sources are available everywhere, both within the internal networks of organizations and on the Internet. Even individual organizations use search engines from different vendors to index their internal document collections. These search engines are typically incompatible in that they support different query models and interfaces, they do not return enough information with the query results for adequate merging of the results, and finally, in that they do not export metadata about the collections that they index (e.g., to assist in resource discovery). This paper describes STARTS,  an emerging protocol for Internet retrieval and search that facilitates the task of querying multiple document sources.  STARTS has been developed in a unique way. It is not a standard, but a group effort coordinated by Stanford's Digital Library project, and involving over 11 companies and organizations. The objective of this paper is not only to give an overview of the STARTS protocol proposal, but...
IR
20419
An Efficient Index Structure for OID Indexing in Parallel Temporal Object-Oriented Database Systems . In an object-oriented database system based on logical OIDs, an OID index (OIDX) is necessary to convert from logical OID to physical location. In a temporal objectoriented database system (TOODB), this OIDX also contains the timestamps of the object versions. We have in a previous paper studied OIDX performance with a relatively simple index. The studies have shown that OIDX maintenance can be quite costly, especially objects updates, because in a temporal OODB, the OIDX needs to be updated every time an object is updated. This has convinced us that a new index structure, particularly suitable to TOODB requirements, is necessary. In this report, we describe an efficient OID index structure for TOODBs, which we call The Vagabond Temporal OID Index (VTOIDX). The main goals of the VTOIDX is 1) support for temporal data, while still having index performance close to a non-temporal/one version database system, 2) efficient object-relational operation, and 3) easy tertiary storage migrati...
DB
smola98general
General Cost Functions for Support Vector Regression The concept of Support Vector Regression is extended to a more general class of convex cost functions. Moreover it is shown how the resulting convex constrained optimization problems can be efficiently solved by a Primal--Dual Interior Point path following method. Both computational feasibility and improvement of estimation is demonstrated in the experiments. 1. Introduction  1.1. Risk Minimization  In the following we will consider the problem of regression estimation: For some probability density function p(x; y) on R  n\Omega  R and some cost function  C(¸) find a function f that minimizes the following risk functional.  R[f ] =  Z  C(y \Gamma f(x))p(x; y)dxdy (1) However we do not know p(x; y). Instead we only have observations f(x 1 ; y 1 ); : : : (x ` ; y ` )g; x i 2  R  n  ; y i 2 R at hand that have been drawn iid (independent identially distributed) from p(x; y). Hence the first guess would be to replace p by the empirical density derived from our observations and minimize the...
ML
baron99voxelbased
A Voxel Based Representation for Evolutionary Shape Optimisation AND KEYWORDS PAGE  Title: A Voxel Based Representation for Evolutionary Shape  Optimisation  Abstract  A voxel-based shape representation when integrated with an evolutionary algorithm offers a number of potential advantage for shape optimisation. Topology need not be predefined, geometric constraints are easily imposed and, with adequate resolution, any shape can be approximated to arbitrary accuracy. However, lack of boundary smoothness, length of chromosome and inclusion of small holes in the final shape have been stated as problems with this representation. This paper describes two experiments performed in an attempt to address some of these problems. Firstly, a design problem with only a small computational cost of evaluating candidate shapes was used as a testbed for designing genetic operators for this shape representation. Secondly, these operators were refined for a design problem using a more costly finite element evaluation. It was concluded that the voxel representation can, with careful design of genetic operators, be useful in shape optimisation.  Keywords: shape optimisation, evolutionary algorithms, voxel representation.  1. 
ML
70288
Simultaneous Learning of Negatively Correlated Neural Networks A new approach to designing neural network ensembles has been proposed recently [1]. Experimental studies on some regression tasks have shown that the new approach performs significantly better than previous ones [1]. This paper presents a new algorithm for designing neural network ensembles for classification problems with noise. This new algorithm is different from that used for regression tasks although the idea is similar. The idea behind this new algorithm is to encourage different individual networks in an ensemble to learn different parts or aspects of the training data so that the whole ensemble can learn the whole training data better. Negatively correlated networks are trained with a novel correlation penalty term in the error function to encourage such specialisation. In our algorithm, individual networks are trained simultaneously rather than sequentially. This provides an opportunity for different networks to interact with other and to specialise. Experiments on two real-w...
ML
chang98global
Global Integration of Visual Databases Different visual databases have been designed in various locations. The global integration of such databases can enable users to access data across the world in a transparent manner. In this paper, we investigate an approach to the design and creation of an integrated information system which supports global visual query access to various visual databases over the Internet. Specifically, a metaserver including a hierarchical metadatabase, a metasearch agent, and a query manager is designed to support such an integration. The metadatabase houses abstracted data about individual remote visual databases. To support visual contentbased queries, the abstracted data in the metadatabase reflect the semantics of each visual database. The query manager extracts the feature contents from the queries. The metasearch agent processes the queries by matching their feature contents with the metadata. A list of relevant database sites is derived for efficient retrieval of the query in the selected dat...
IR
114563
Unsupervised Learning from Dyadic Data Dyadic data refers to a domain with two finite sets of objects in which observations are made for dyads, i.e., pairs with one element from either set. This includes event co-occurrences, histogram data, and single stimulus preference data as special cases. Dyadic data arises naturally in many applications ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent framework for unsupervised learning from dyadic data by statistical mixture models. Our approach covers different models with flat and hierarchical latent class structures and unifies probabilistic modeling and structure discovery. Mixture models provide both, a parsimonious yet flexible parameterization of probability distributions with good generalization performance on sparse data, as well as structural information about data-inherent grouping structure. We propose an annealed version of the standard Expectation Maximization algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains.
ML
ambite01fast
Fast Approximate Evaluation of OLAP Queries for Integrated Statistical Data We have developed a mediator architecture that integrates statistical information about energy products  from several government agencies, such as the Bureau of Labor Statistics, the Energy Information  Administration, and the California Energy Commission. Our architecture has a dual mode of operation.  First, our system can retrieve live data from databases and web sources from these agencies. This allows  the users to obtain completely up-to-date data. However, for complex analytical queries that typically  require large amounts of data and processing, live access does not offer the level of interactivity that  some users require. Second, our system can warehouse the information from the data sources to allow  for complex analytical queries to be executed much more efficiently. However, the data would be only  as recent as the last update to the data warehouse. In this paper we describe the architecture and focus on  how to perform analytical queries against the data warehouse very efficiently. We present results using  a fast wavelet-based technique for progressive evaluation of range-sum queries. This technique allows  for returning an approximate result to the query very efficiently and for fast convergence to the exact  result. We envision users exploring many complex queries using the very fast approximate results as  guidance and only obtaining the exact results for those queries that are deemed of interest. We present  experimental results showing the efficiency of both approximate and exact queries.  1 
DB
414073
Towards Efficient Multi-Feature Queries in Heterogeneous Environments Applications like multimedia databases or enterprisewide information management systems have to meet the challenge of efficiently retrieving best matching objects from vast collections of data. We present a new algorithm Stream-Combine for processing multi-feature queries on heterogeneous data sources. Stream-Combine is selfadapting to different data distributions and to the specific kind of the combining function. Furthermore we present a new retrieval strategy that will essentially speed up the output of relevant objects.
IR
203721
Spatial Agents Implemented in a Logical Expressible Language In this paper, we present a multi-layered architecture  for spatial and temporal agents. The focus is  laid on the declarativity of the approach, which  makes agent scripts expressive and well understandable.  They can be realized as (constraint) logic  programs. The logical description language is able  to express actions or plans for one and more autonomous  and cooperating agents for the RoboCup  (Simulator League). The system architecture hosts  constraint technology for qualitative spatial reasoning,  but quantitative data is taken into account, too.  The basic (hardware) layer processes the agent's  sensor information. An interface transfers this lowlevel  data into a logical representation. It provides  facilities to access the preprocessed data and supplies  several basic skills. The second layer performs  (qualitative) spatial reasoning. On top of this, the  third layer enables more complex skills such as  passing, offside-detection etc. At last, the fourth  layer establishes acting as a team both by emergent  and explicit cooperation. Logic and deduction provide  a clean means to specify and also to implement  teamwork behavior.  1 
Agents
makar01hierarchical
Hierarchical Multi-Agent Reinforcement Learning In this paper we investigate the use of hierarchical reinforcement learning to speed up the acquisition of cooperative multi-agent tasks. We extend the MAXQ framework to the multi-agent case. Each agent uses the same MAXQ hierarchy to decompose a task into sub-tasks. Learning is decentralized, with each agent learning three interrelated skills: how to perform subtasks, which order to do them in, and how to coordinate with other agents. Coordination skills among agents are learned by using joint actions at the highest level(s) of the hierarchy. The Q nodes at the highest level(s) of the hierarchy are configured to represent the joint task-action space among multiple agents. In this approach, each agent only knows what other agents are doing at the level of sub-tasks, and is unaware of lower level (primitive) actions. This hierarchical approach allows agents to learn coordination faster by sharing information at the level of sub-tasks, rather than attempting to learn coordination taking into account primitive joint state-action values. We apply this hierarchical multi-agent reinforcement learning algorithm to a complex AGV scheduling task and compare its performance and speed with other learning approaches, including at multi-agent, single agent using MAXQ, selfish multiple agents using MAXQ (where each agent acts independently without communicating with the other agents), as well as several well-known AGV heuristics like "first come first serve", "highest queue first" and "nearest station first". We also compare the tradeoffs in learning speed vs. performance of modeling joint action values at multiple levels in the MAXQ hierarchy.
ML
adya00generalized
Generalized Isolation Level Definitions Commercial databases support different isolation levels to allow programmers to trade off consistency for a potential gain in performance. The isolation levels are defined in the current ANSI standard, but the definitions are ambiguous and revised definitions proposed to correct the problem are too constrained since they allow only pessimistic (locking) implementations. This paper presents new specifications for the ANSI levels. Our specifications are portable; they apply not only to locking implementations, but also to optimistic and multi-version concurrency control schemes. Furthermore, unlike earlier definitions, our new specifications handle predicates in a correct and flexible manner at all levels.  1. Introduction  This paper gives new, precise definitions of the ANSISQL isolation levels [6]. Unlike previous proposals [13, 6, 8], the new definitions are both correct (they rule out all bad histories) and implementation-independent. Our specifications allow a wide range of concurr...
DB
303350
Learning human arm movements by imitation: Evaluation of a biologically-inspired connectionist architecture . This paper is concerned with the evaluation of a model of human imitation of arm movements. The  model consists of a hierarchy of artificial neural networks, which are abstractions of brain regions involved in  visuo-motor control. These are the spinal cord, the primary and pre-motor cortexes (M1 & PM), the cerebellum,  and the temporal cortex. A biomechanical simulation is developed which models the muscles and the complete  dynamics of a 37 degree of freedom humanoid. Input to the model are data from human arm movements  recorded using video and marker-based tracking systems.  The model's performance is evaluated for reproducing reaching movements and oscillatory movements of the  two arms. Results show a high qualitative and quantitative agreement with human data. In particular, the model  reproduces the well known features of reaching movements in humans, namely the bell-shaped curves for the  velocity and quasi-linear hand trajectories. Finally, the model's performance is compar...
ML
510434
Differential Join Prices for Parallel Queues: Social Optimality, Dynamic Pricing Algorithms and Application to Internet Pricing We consider a system of identical parallel queues served by a single server and distinguished only by the price charged at entry. A Poisson stream of customers joins the queue by a greedy policy that minimizes a `disutility' that combines price and congestion. A special case of linear disutility is analyzed for which it is shown that the individually optimal greedy queue join policy is nearly socially optimal. For this queueing system, a Markov decision theoretic framework is formulated for dynamic pricing in the general case. This queueing system has application in the pricing of Internet services.
ML
206655
Detection of Heterogeneities in a Multiple Text Database Environment As the number of text retrieval systems (search engines) grows rapidly on the World Wide Web, there is an increasing need to build search brokers (metasearch engines) on top of them. Often, the task of building an effective and efficient metasearch engine is hindered by the heterogeneities among the underlying local search engines. In this paper, we first analyze the impact of various heterogeneities on building a metasearch engine. We then present some techniques that can be used to detect the most prominent heterogeneities among multiple search engines. Applications of utilizing the detected heterogeneities in building better metasearch engines will be provided.   
IR
couvreur99speaker
Speaker Tracking in Broadcast Audio Material in the Framework of the THISL Project In this paper, we present a first approach to build an automatic system for broadcast news speaker-based segmentation. Based on a Chop-and-Recluster method, this system is developed in the framework of the THISL project. A metric-based segmentation is used for the Chop procedure and different distances have been investigated. The Recluster procedure relies on a  bottom-up clustering of segments obtained beforehand and represented by non-parametricmodels. Various hierarchical clustering schemes have been tested. Some experiments on BBC broadcast news recordings show that the system can detect real speaker changes with high accuracy (mean error ' 0.7s) and fair false alarm rate (mean false alarm rate ' 5.5% ). The Recluster  procedure can produce homogeneous clusters but it is not already robust enough to tackle too complex classification tasks. 1. INTRODUCTION  THISL (THematic Indexing of Spoken Language)  1  is an ESPRIT Long Term Research project that is investigating the development ...
IR
radev02probabilistic
Probabilistic Question Answering on the Web Web-based search engines such as Google and NorthernLight return documents that are relevant to a user query, not answers to user questions. We have developed an architecture that augments existing search engines so that they support natural language question answering. The process entails five steps: query modulation, document retrieval, passage extraction, phrase extraction, and answer ranking. In this paper we describe some probabilistic approaches to the last three of these stages. We show how our techniques apply to a number of existing search en-1 Radev et al. 2 gines and we also present results contrasting three different methods for question answering. Our algorithm, probabilistic phrase reranking (PPR), uses proximity and question type features and achieves a total reciprocal document rank of.20 on the TREC8 corpus. Our techniques have been implemented as a Web-accessible system, called NSIR.
IR
64225
Recognition of Human Action Using Moment-Based Features The performance of different classification approaches is evaluated using a view-based approach for motion representation. The view-based approach uses computer vision and image processing techniques to register and process the video sequence [6, 23]. Two motion representations called Motion Energy Images and Motion History Images [6] are then constructed. These representations collapse the temporal component in a way that no explicit temporal analysis or sequence matching is needed. Statistical descriptions are then computed using momentbased features and dimensionality reduction techniques. For these tests, we used 7 Hu moments, which are invariant to scale and translation. Principal Components Analysis is used to reduce the dimensionality of this representation. The system is trained using different subjects performing a set of examples of every action to be recognized. Given these samples, K-nearest neighbor, Gaussian, and Gaussian mixture classifiers are used to recognize new acti...
AI
althoff99using
Using Case-Based Reasoning for Supporting Continuous Improvement Processes The goal of the IPQM project -- a collaboration of the Fraunhofer Institute for Manufacturing Engineering and Automation (IPA) in Stuttgart and the Fraunhofer Institute for Experimental Software Engineering (IESE) in  Kaiserslautern -- is to develop a technical infrastructure to support continuous improvement processes. We  describe the approach we took in some detail and focus on the implementation of the IPQM system and its currently ongoing evaluation in the healthcare sector. We also give an outlook on intended extensions of the  system and its application in other domains.
ML
weiss99achieving
Achieving Coordination through Combining Joint Planning and Joint Learning . There are two major approaches to activity coordination in multiagent systems. First, by endowing the agents with the capability to jointly plan, that is, to jointly generate hypothetical activity sequences. Second, by endowing the agents with the capability to jointly learn, that is, to jointly choose the actions to be executed on the basis of what they know from experience about the interdependencies of their actions. This paper describes a new algorithm called JPJL ("Joint Planning and Joint Learning") that combines both approaches. The primary motivation behind this algorithm is to bring together the advantages of joint planning and joint learning while avoiding their disadvantages. Experimental results are provided that illustrate the potential benefits and shortcomings of the JPJL algorithm. 1 Motivation  Multiagent Systems (MAS)---systems in which several interacting, intelligent and autonomous entities called agents pursue some set of goals or perform some set of tasks---have...
Agents
resnik99mining
Mining the Web for Bilingual Text STRAND (Resnik, 1998) is a language- independent system for automatic discovery of text in parallel translation on the World Wide Web. This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally ewluating performance. The most recent end-product is an automaticaJly acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language.
IR
mundell99towards
Towards a Virtual Operating Environment - Exploring Immersive Virtual Interface Design Using a Simple VR Image Viewer This paper explores the virtual interface from a design perspective. A simple virtual reality image  viewer application is tested with a variety of users, producing a virtual interface design reference model,  and a set of virtual interface design guidelines.  2  Acknowledgements  A massive thank you to Shaun Bangay, my supervisor, for priceless guidance.  Thank you to Mike Rorke for help within the CoRgi system and as an expert user, Fred Fourie for help with 3D Studio Max and the CoRgi ase readers, and Colin Dembovski for collision detection that always did work. Thanks also to the Rhodes VRSIG members, all the testers, the CoRgi documentation team and Phatwax.  3  Contents  1 Introduction 7 1.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.2 Introduction to Virtual Reality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.3 Research Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...
HCI
rasheed97guided
Guided Crossover: A New Operator for Genetic Algorithm Based Optimization Genetic algorithms (GAs) have been extensively used in different domains as a means of doing global optimization in a simple yet reliable manner. They have a much better chance of getting to global optima than gradient based methods which usually converge to local sub optima. However, GAs have a tendency of getting only moderately close to the optima in a small number of iterations. To get very close to the optima, the GA needs a very large number of iterations. Whereas gradient based optimizers usually get very close to local optima in a relatively small number of iterations. In this paper we describe a new crossover operator which is designed to endow the GA with gradient-like abilities without actually computing any gradients and without sacrificing global optimality. The operator works by using guidance from all members of the GA population to select a direction for exploration. Empirical results in two engineering design domains and across both binary and floating point representa...
ML
tarau99jinni
Jinni: Intelligent Mobile Agent Programming at the Intersection of Java and Prolog . Jinni (Java INference engine and Networked Interactor), is a lightweight, multi-threaded, logic programming language, intended to be used as a flexible scripting tool for gluing together knowledge processing components and Java objects in distributed applications. Jinni threads are coordinated through blackboards, local to each process. Associative search based on term unification (a variant of Linda) is used as the basic synchronization mechanism. Threads are controlled with tiny interpreters following a scripting language based on a subset of Prolog. Mobile threads, implemented by capturing first order continuations in a compact data structure sent over the network, allow Jinni to interoperate with remote high performance BinProlog servers for CPU-intensive knowledge processing and with other Jinni components over the Internet. The synergy of these features makes Jinni a convenient development platform for distributed AI, and in particular, for building intelligent autonomous agent...
Agents
tawfik01temporal
Temporal Matching under Uncertainty Temporal matching is the problem of matching  observations to predefined temporal patterns  or templates. This problem arises  in many applications including medical and  model-based diagnosis, plan-recognition, and  temporal databases. This work examines the  sources of uncertainty in temporal matching  and presents a probabilistic technique to perform  temporal matching under uncertainty.  This technique is then applied to the problem  of finding the onset of infection with Toxoplasma   Gondii.  1 Introduction  Temporal matching is the process of matching a limited set of observations to known temporal evolution patterns in order to identify the stage of evolution or determine the onset of the temporally evolving pattern. Given a sequence of observations, and some temporal evolution patterns, a temporal match consistent with the sequence of observations maps observation times to particular points in a pattern.  Formally, S is a set of states, TR is the set of time points measured rel...
DB
ehlert01intelligent
Intelligent Driving Agents: The Agent Approach to Tactical Driving in Autonomous Vehicles and Traffic Simulation Computer traffic simulation is important for making new traffic-control strategies. Microscopic traffic simulators can model traffic flow in a realistic manner and are ideal for agent-based vehicle control. In this paper we describe a model of a reactive agent that is used to control a simulated vehicle. The agent is capable of tactical-level driving and has different driving styles. To ensure fast reaction times, the agent's driving task is divided in several competing and reactive behaviour rules. The agent is implemented and tested in a prototype traffic simulator program. The simulator consists of an urban environment with multi-lane roads, intersections, traffic lights, light controllers and vehicles. Every vehicle is controlled by a driving agent and all agents have individual behaviour settings. Preliminary experiments have shown that the agents exhibit human-like behaviour ranging from slow and careful to fast and aggressive driving behaviour.
Agents
martin00knowledge
Knowledge Retrieval and the World Wide Web }) )$ Figure 5. Images, knowledge indexations, and a customized query interface contained within one document. The sample query shows how the command spec, which looks for specializations of a conceptual graph, can be used to retrieve images CGs indexed. (Figure 7 gives the results.) [Cat]->(On)->[Table]. In WordNet, cat has five meanings (feline, gossiper, x-ray, beat, and vomit), and table has five meanings (array, furniture, tableland, food, and postpone) . In the WebKB ontology, the relation type On connects a concept of type Spatial_entity to another concept of the same type. Thus, WebKB can infer that beat and vomit are not the intended meanings for cat, and array and postpone are not the intended meanings for table. To further identify the intended meanings, WebKB could prompt the following questions to the user: "Does cat refer to feline, gossiper, x-ray, or something else?" and "Does table refer to furniture, tableland, food, or something else?" Finally, knowledge state...
IR
wang99critical
A Critical Note on Stable Model Semantics In this paper, we argue that both the stable model semantics and  its three-valued version are conceptually flawed.  1 Introduction  The semantics of logic programming and deductive databases has been extensively studied in the past two decades. A whole spectrum of semantic theories for logic programs have been proposed, ranging from those that infer very little information from a logic program ("skeptical") to those that infer a great deal ("credulous"). The most skeptical semantics is the well-founded semantics [4] while the most credulous is the the stable model semantics [6] and its different but equivalent three-valued versions, including 3-stable models [8], partial stable models [9] and preferred extensions [2].  The chief drawback of the stable semantics is that a two-valued stable model is not defined for all logic programs. In the three-valued variant a model is always defined but that model may leave all atoms undetermined [8]. The original problem of the non-existence of a ...
DB
baxter01dab
DAB: Interactive Haptic Painting with 3D Virtual Brushes We present a novel painting system with an intuitive haptic interface, which serves as an expressive vehicle for interactively creating painterly works. We introduce a deformable, 3D brush model, which gives the user natural control of complex brush strokes. The force feedback enhances the sense of realism and provides tactile cues that enable the user to better manipulate the paint brush. We have also developed a bidirectional, two-layer paint model that, combined with a palette interface, enables easy loading of complex blends onto our 3D virtual brushes to generate interesting paint effects on the canvas. The resulting system, DAB, provides the user with an artistic setting, which is conceptually equivalent to a real-world painting environment. Several users have tested DAB and were able to start creating original art work within minutes.
HCI
491166
Engineering AgentSpeak(L): A Formal Computational Model Perhaps the most successful agent architectures, and certainly the best known, are those based on the Belief-Desire-Intention (BDI) framework. Despite the wealth of research that has accumulated on both formal and practical aspects of this framework, however, there remains a gap between the formal models and the implemented systems. In this paper, we build on earlier work by Rao aimed at narrowing this gap, by developing a strongly-typed, formal, yet computational model of the BDI-based AgentSpeak(L) language. AgentSpeak(L) is a programming language, based on the Procedural Reasoning System (PRS) and the Distributed Multi-Agent Reasoning System (dMARS), which determines the behaviour of the agents it implements. In developing the model, we add to Rao's work, identify some omissions, and progress beyond the description of a particular language by giving a formal specification of a general BDI architecture that can be used as the basis for providing further formal specifications of more sophisticated systems.
Agents
gottlob02monadic
Monadic Queries over Tree-Structured Data Monadic query languages over trees currently receive considerable interest in the database community, as the problem of selecting nodes from a tree is the most basic and widespread database query problem in the context of XML. Partly a survey of recent work done by the authors and their group on logical query languages for this problem and their expressiveness, this paper provides a number of new results related to the complexity of such languages over so-called axis relations (such as "child" or "descendant") which are motivated by their presence in the XPath standard or by their utility for data extraction (wrapping).
IR
kinny97modelling
Modelling and Design of Multi-Agent Systems Abstract. Agent technologies are now being applied to the development of large-scale commercial and industrial software systems. Such systems are complex, involving hundreds, perhaps thousands of agents, and there is a pressing need for system modelling techniques that permit their complexity to be e ectively managed, and principled methodologies to guide the process of system design. Without adequate techniques to support the design process, such systems will not be su ciently reliable, maintainable or extensible, will be di cult to comprehend, and their elements will not be re-usable. In this paper, we present techniques for modelling agents and multi-agent systems which adapt and extend existing Object-Oriented representation techniques, and a methodology which provides a clear conceptual framework to guide system design and speci cation. We have developed these techniques for systems of agents based upon a particular Belief-Desire-Intention architecture, but have soughttoprovide a framework for the description of agent systems that is su ciently general to be applicable to other agent architectures, and which may be extended in various ways. 1
Agents
goldstein99automatic
Automatic Text Summarization of Multiple Documents Scientists have retrieved what appear to be normal human eggs from human ovarian tissue that was grafted onto research mice. This is the first research group to obtain mature, potentially fertilizable eggs. Results of the research are being presented today at the conference of the European Society of Human Reproduction and Embryology. A report published last year demonstrated that ovarian tissue which was frozen and then replaced into a woman's body resulted in ovulation and menstruation. Such methods are being considered for women being treated for cancer with methods that would severely diminish or destroy their reproductive chances. However, there is concern that the retransplanted tissue might contain cancer cells. The current study proposes to reduce that risk. This is yet another step toward enabling women to freeze ovarian tissue in their early 20's, when it is generally most productive, to delay reproduction until their later years.
IR
257648
Accurate Estimation of the Cost of Spatial Selections Optimizing queries that involve operations on spatial data requires estimating the selectivity and cost of these operations. In this paper, we focus on estimating the cost of spatial selections, or window queries, where the query windows and data objects are general polygons. Cost estimation techniques previously proposed in the literature only handle rectangular query windows over rectangular data objects, thus ignoring the very significant cost of exact geometry comparison (the refinement step in a “filter and refine” query processing strategy). The cost of the exact geometry comparison depends on the selectivity of the filtering step and the average number of vertices in the candidate objects identified by this step. In this paper, we introduce a new type of histogram for spatial data that captures the complexity and size of the spatial objects as well as their location. Capturing these attributes makes this type of histogram useful for accurate estimation, as we experimentally demonstrate. We also investigate sampling-based estimation approaches. Sampling can yield better selectivity estimates than histograms for polygon data, but at the high cost of performing exact geometry comparisons for all the sampled objects. 1.
DB
bauer00programming
Programming by Demonstration for Information Agents this article we will refer to the user in the female form, while the agent will be referred to using male forms.
IR
sachdev98spatial
3D Spatial Layouts Using A-Teams Spatial layout is the problem of arranging a set of components in an enclosure such that a set of objectives and constraints is satisfied. The constraints may include non-interference of objects, accessibility requirements and connection cost limits. Spatial layout problems are found primarily in the domains of electrical engineering and mechanical engineering in the design of integrated circuits and mechanical or electromechanical artifacts. Traditional approaches include ad-hoc (or specialized) heuristics, Genetic Algorithms and Simulated Annealing. The A-Teams approach provides a way of synergistically combining these approaches in a modular agent based fashion. A-Teams are also open to the addition of new agents. Modifications in the task requirements translate to modifications in the agent mix. In this paper we describe how modular A-Team based optimization can be used to solve 3 dimensional spatial layout problems.  
Agents
450284
Smart-Its Friends: A Technique for Users to Easily Establish Connections between Smart Artefacts . Ubiquitous computing is associated with a vision of everything being  connected to everything. However, for successful applications to emerge, it will  not be the quantity but the quality and usefulness of connections that will  matter. Our concern is how qualitative relations and more selective connections  can be established between smart artefacts, and how users can retain control  over artefact interconnection. We propose context proximity for selective  artefact communication, using the context of artefacts for matchmaking. We  further suggest to empower users with simple but effective means to impose the  same context on a number of artefacts. To prove our point we have  implemented Smart-Its Friends, small embedded devices that become  connected when a user holds them together and shakes them.  1 
HCI
zini01caselp
CaseLP, A Rapid Prototyping Environment For Agent Based Software Intelligent agents and multi-agent systems are increasingly recognized as an innovative approach for analyzing, designing and implementing complex, heterogeneous and distributed software applications. The agent-based view offers a powerful and high level conceptualization that software engineers can exploit to considerably improve the way in which software is realized. Agent-based software engineering is a recent and very interesting research area. Due to its novelty, there is still no evidence of well-established practices for the development of agent-based applications and thus experimentation in this direction is very important. This dissertation
Agents
242172
Agent-Oriented Software Engineering Agent-oriented techniques represent an exciting new means of analysing, designing and building complex  software systems. They have the potential to significantly improve current practice in software  engineering and to extend the range of applications that can feasibly be tackled. Yet, to date, there have  been few serious attempts to cast agent systems as a software engineering paradigm. This paper seeks to  rectify this omission. Specifically, it will be argued that: (i) the conceptual apparatus of agent-oriented  systems is well-suited to building software solutions for complex systems and (ii) agent-oriented  approaches represent a genuine advance over the current state of the art for engineering complex systems.  Following on from this view, the major issues raised by adopting an agent-oriented approach to  software engineering are highlighted and discussed.
Agents
wu00discriminantem
Discriminant-EM Algorithm with Application to Image Retrieval In many vision applications, the practice of supervised learning faces several difficulties, one of which is that insufficient labeled training data result in poor generalization. In image retrieval, we have very few labeled images from query and relevance feedback so that it is hard to automatically weight image features and select similarity metrics for image classification. This paper investigates the possibility of including an unlabeled data set to make up the insufficiency of labeled data. Different from most current research in image retrieval, the proposed approach tries to cast image retrieval as a transductive learning problem, in which the generalization of an image classifier is only defined on a set of images such as the given image database. Formulating this transductive problem in a probabilistic framework, the proposed algorithm, Discriminant-EM (D-EM), not only estimates the parameters of a generative model, but also finds a linear transformation to relax the assumption of pro...
IR
itti99learning
Learning to Detect Salient Objects in Natural Scenes Using Visual Attention In the primate's visual system, selective attention rapidly selects conspicuous image locations to be analyzed in more details. Such selection is guided by several low-level feature extraction mechanisms, which detect candidate salient locations based on their local properties for a given feature type (e.g., intensity, color, orientation or motion). One difficulty which arises is how the information from different modalities should be combined into a single "saliency map" controlling where visual attention should be focused. Here, we quantitatively compare three feature combination strategies (simple summation, learned linear summation, and contents-based non-linear normalization) using three databases of natural color images. 1 Introduction  Bottom-up or saliency-based visual attention allows primates to detect, in real time, nonspecific conspicuous targets from cluttered visual environments. Reproducing such nonspecific target detection capability in artificial systems has important ...
ML
good99combining
Combining Collaborative Filtering with Personal Agents for Better Recommendations Information filtering agents and collaborative filtering both  attempt to alleviate information overload by identifying  which items a user will find worthwhile. Information  filtering (IF) focuses on the analysis of item content and  the development of a personal user interest profile.  Collaborative filtering (CF) focuses on identification of  other users with similar tastes and the use of their opinions  to recommend items. Each technique has advantages and  limitations that suggest that the two could be beneficially  combined.  This paper shows that a CF framework can be used to  combine personal IF agents and the opinions of a  community of users to produce better recommendations  than either agents or users can produce alone. It also  shows that using CF to create a personal combination of a  set of agents produces better results than either individual  agents or other combination mechanisms. One key  implication of these results is that users can avoid having  to select among ag...
IR
decarolis00verbal
Verbal and Nonverbal Discourse Planning this paper we first describe our enriched discourse generator explaining the 2 sets of rules (trigger and regulation) we have added. We also review the di#erent types of gaze communicative acts. Finally we present the variables defining the context and how they modify the computation of the display of the communicative acts.
Agents
kostiadis99multithreaded
A Multi-threaded Approach to Simulated Soccer Agents for the RoboCup Competition To meet the timing requirements set by the RoboCup soccer server simulator, this paper proposes a multi-threaded approach to simulated soccer agents for the RoboCup competition. At its higher level each agent works at three distinct phases: sensing, thinking and acting. Instead of the traditional single threaded approaches, POSIX threads have been used here to break down these phases and implement them concurrently. The details of how this parallel implementation can significantly improve the agent's responsiveness and its overall performance are described. Implementation results show that a multi-threaded approach clearly outperforms a singlethreaded one in terms of efficiency, responsiveness and scalability. The proposed approach will be very efficient in multi-processor systems.  1. Introduction  The creation of the robotic soccer, the robot world cup initiative (RoboCup), is an attempt to foster AI and intelligent robotics research by providing a standard problem where wide range o...
Agents
wang00using
Using Software Agents to Support Evolution of Distributed Workflow Models This paper outlines a high-level design of how software agents can be used combined with an existing CAGIS Process Centred Environment to deal with evolution of distributed, fragmented workflow models. Our process centred environment allows process fragments of the same workflow model to be located in workspaces that are geographically distributed. These process fragments can be changed independently in local workspaces causing consistency problems. We propose to use software mobile agents, offering awareness services solving conflicting updates of process fragment. Our solution is illustrated using some scenarios.  Keywords: Process centred environments, software agents, workflow model consistency, workflow model evolution, distribution, fragmentation.  1 Introduction  Dealing with evolution of workflow processes is not a trivial matter. One simple solution to this problem is to have one centralised workflow model, that cannot be changed after it is instanciated. In practice, it is ho...
IR
conati00toward
Toward Computer-Based Support of Meta-Cognitive Skills: a Computational Framework to Coach Self-Explanation this paper, we describe how these solutions have been implemented in a computer tutor that coaches self-explanation within Andes, a tutoring system for Newtonian physics. We also present the results of a formal study to evaluate the usability and effectiveness of the system. Finally, we discuss some hypotheses to explain the obtained results, based on the analysis of the data collected during the study.  INTRODUCTION  Research on Intelligent Tutoring Systems (ITS) has been increasingly affecting education. While for many years ITS remained confined to research labs, today they have started moving into the classroom, showing their effectiveness for learning and influencing the structure of traditional curricula (Koedinger, Anderson, H., & Mark, 1995). However, existing ITS still target only a limited part of the learning process. They generally focus on teaching problem solving and domain specific cognitive skills.  The long-term goal of our research is to explore innovat
HCI
445175
On Securely Scheduling A Meeting When people want to schedule a meeting, their agendas must be compared to find a time suitable for all participants. At the same time, people want to keep their agendas private. This paper presents several approaches which intend to solve this contradiction. A custom-made protocol for secure meeting scheduling and a protocol based on secure distributed computing are discussed. The security properties and complexity of these protocols are compared. A trade-off between trust and bandwidth requirements is shown to be possible by implementing the protocols using mobile agents.  Keywords: mobile agents, secure distributed computation, meeting scheduling  1. 
Agents
376145
Building Efficient and Effective Metasearch Engines Frequently a user's information needs are stored in the databases of multiple search engines. It is inconvenient and inefficient for an ordinary user to invoke multiple search engines and identify useful documents from the returned results. To support unified access to multiple search engines, a metasearch engine can be constructed. When a metasearch engine receives a query from a user, it invokes the underlying search engines to retrieve useful information for the user. Metasearch engines have other benefits as a search tool such as increasing the search coverage of the Web and improving the scalability of the search. In this article, we survey techniques that have been proposed to tackle several underlying challenges for building a good metasearch engine. Among the main challenges, the database selection problem is to identify search engines that are likely to return useful documents to a given query. The document selection problem is to determine what documents to retrieve from each identified search engine. The result merging problem is to combine the documents returned from multiple search engines. We will also point out some problems that need to be further researched.
IR
abdennadher00experimental
An Experimental CLP Platform for Integrity Constraints and Abduction Integrity constraint and abduction are important in query-answering systems for enhanced query processing and for expressing knowledge in databases. A straightforward characterization of the two is given in a subset of the language CHR  _  , originally intended for writing constraint solvers to be applied for CLP languages. This subset has a strikingly simple computational model that can be executed using existing, Prolog-based technology. Together with earlier results, this confirms CHR  _  as a multiparadigm platform for experimenting with combinations of top-down and bottom-up evaluation, disjunctive databases and, as shown here, integrity constraint and abduction  1 Introduction  Constraint logic programming (CLP) [10] is established as an extension to logic programming that adds higher expressibility and in some cases more efficient query evaluation. CLP has also given rise to a field of constraint databases [14].  In the present paper, we suggest CLP techniques applied for defini...
DB
dewhurst98knowledge
Knowledge Discovery from Client-Server Databases . The subject of this paper is the implementation of knowledge discovery in databases. Specifically, we assess the requirements for interfacing tools to client-server database systems in view of the architecture of those systems and of "knowledge discovery processes". We introduce the concept of a query frontier of an exploratory process, and propose a strategy based on optimizing the current query frontier rather than individual knowledge discovery algorithms. This approach has the advantage of enhanced genericity and interoperability. We demonstrate a small set of query primitives, and show how one example tool, the well-known decision tree induction algorithm C4.5, can be rewritten to function in this environment. 1 Introduction  Relational databases are the current dominant database technology in industry, and many organizations have collected large amounts of data in so-called  data warehouses expressly for the purpose of decision support and data mining. In general the data must ...
DB
hirai99webbase
WebBase : A repository of web pages In this paper, we study the problem of constructing and maintaining a large shared repository of web pages. We discuss the unique characteristics of such a repository, propose an architecture, and identify its functional modules. We focus on the storage manager module, and illustrate how traditional techniques for storage and indexing can be tailored to meet the requirements of a web repository. To evaluate design alternatives, we also present experimental results from a prototype repository called WebBase, that is currently being developed at Stanford University. Keywords : Repository, WebBase, Architecture, Storage management 1 Introduction A number of important applications require local access to substantial portions of the web. Examples include traditional text search engines [Google] [Avista], related page services [Google] [Alexa], and topic-based search and categorization services [Yahoo]. Such applications typically access, mine or index a local cache or repository of web...
IR
beuster00mia
MIA - An Ubiquitous Multi-Agent Web Information System This paper gives a brief overview about AI methods and techniques we have developed for building ubiquitous web information systems. These methods from areas of machine learning, logic programming, knowledge representation and multi-agent systems are discussed in the context of our prototypical information system MIA. MIA is a web information system for mobile users, who are equipped with a PDA (Palm Pilot), a cellular phone and a GPS device or cellular WAP phone. It captures the main issues of ubiquitous computing: location awareness,  anytime information access and PDA technology.  1 Introduction  Nowadays, the biggest but also the most chaotic and unstructured source of information is the World-WideWeb. Making this immense amount of information available for ubiquitous computing in daily life is a great challenge. Besides hardware issues for wireless ubiquitous computing, that still are to be solved (wireless communication, blue-tooth technologies, wearable computing units, integrat...
HCI
deloach01specifying
Specifying Agent Behavior as Concurrent Tasks Approved for public release; distribution unlimited Software agents are currently the subject of much research in many interrelated fields. While much of the agent community has concentrated on building exemplar agent systems, defining theories of agent behavior and inter-agent communications, there has been less emphasis on defining the techniques required to build practical agent systems. While many agent researchers refer to tasks performed by roles within a multiagent system, few really define the what they mean by tasks. We believe that the definition of tasks is critical in order to completely define what an agent within a multiagent system. Tasks not only define the types of internal processing an agent must do, but also how interactions with other agents relate to those internal processes. In this report, we define concurrent tasks, which specify a single thread of control that defines a task that the agent can perform and integrates inter-agent as well as intra-agent interactions. We typically think of concurrent tasks as defining how a role decides what actions to take, not necessarily what the agent does. This is an important distinction when talking about agents since hard-coding specific behavior may not be the ideal case. Often agents incorporate the concept of plans and planning to
Agents
amin01agentbased
Agent-Based Distance Vector Routing Mobile Agents are being proposed for an increasing variety of applications. Distance Vector Routing (DVR) is an example of one application that can benefit from an agent-based approach. DVR algorithms, such as RIP, have been shown to cause considerable network resource overhead due to the large number of messages generated at each host/router throughout the route update process. Many of these messages are wasteful since they do not contribute to the route discovery process. However, in an agent-based solution, the number of messages is bounded by the number of agents in the system. In this paper, we present an agent-based solution to DVR. In addition, we will describe agent migration strategies that improve the performance of the route discovery process, namely Random Walk and Structured Walk.
Agents
11423
Sensor Fault Detection and Identification in a Mobile Robot Multiple model adaptive estimation (MMAE) is used to detect and identify sensor failures in a mobile robot. Each estimator is a Kalman filter with a specific embedded failure model. The filter bank also contains one filter which has the nominal model embedded within it. The filter residuals are postprocessed to produce a probabilistic interpretation of the operation of the system. The output of the system at any given time is the confidence in the correctness of the various embedded models. As an additional feature the standard assumption that the measurements are available at a constant, common frequency, is relaxed. Measurements are assumed to be asynchronous and of varying frequency. The particularly difficult case of 'soft' sensor failure is also handled successfully. A system architecture is presented for the general problem of failure detection and identification in mobile robots. As an example, the MMAE algorithm is demonstrated on a Pioneer I robot in the case of three different sensor failures.
AI
41721
Extracting Collocations from Text Corpora A collocation is a habitual word combination. Collocational knowledge is essential for many tasks in natural language processing. We present a method for extracting collocations from text corpora. By comparison with the SUSANNE corpus, we show that both high precision and broad coverage can be achieved with our method. Finally, we describe an application of the automatically extracted collocations for computing word similarities.
DB
iksal01revisiting
Revisiting and Versioning in Virtual Special Reports Adaptation/personalization is one of the main issues for web  applications and require large repositories. Creating adaptive web applications  from these repositories requires to have methods to facilitate web application  creation and management and to ensure reuse, sharing and exchange of data  through the internet/intranet. Virtual documents deal with these issues. In our  framework, we are interested in adaptive virtual documents for author-oriented  web applications providing several reading strategies to readers. These  applications have the following characteristics: authors have know-how which  enables them to choose document contents and to organize them in one or more  consistent ways. A reading strategy and the corresponding content are  semantically coherent and convey a particular meaning to the readers. Such  author's know-how can be represented at knowledge level and then be used for  generating web documents dynamically, for ensuring reader comprehension and  for sharing and reuse. Then an adaptive virtual document can be computed on  the fly by means of a semantic composition engine using: i) an overall  document structure -- for instance a narrative structure - representing a reading  strategy for which node contents are linked at run time, according to user's  needs for adaptation, ii) an intelligent search engine and semantic metadata  relying on semantic web initiative, and iv) a user model. In this paper, we focus  on a semantic composition engine enabling us to compute on the fly  adaptive/personalized web documents in the ICCARS project. Its main goal is  to assist the journalist in building adaptive special reports. In such a framework,  adaptation, personalization and reusability are central issues for delivering  adaptive special reports.
IR
cho00estimating
Estimating Frequency of Change Many online data sources are updated autonomously and independently. In this paper, we make the case for estimating the change frequency of the data, to improve web crawlers, web caches and to help data mining. We first identify various scenarios, where different applications have different requirements on the accuracy of the estimated frequency. Then we develop several "frequency estimators" for the identified scenarios. In developing the estimators, we analytically show how precise/effective the estimators are, and we show that the estimators that we propose can improve precision significantly. 1 Introduction With the explosive growth of the internet, many data sources are available online. Most of the data sources are autonomous and are updated independently of the clients that access the sources. For instance, popular news web sites, such as CNN and NY Times, update their contents periodically, whenever there are new developments. Also, many online stores update the price/availab...
DB
kreuziger92application
Application Of Machine Learning To Robotics - An Analysis Robotics is one of the most challenging applications of Machine Learning (ML) techniques. It is characterized by direct interaction with a real world, sensory feedback and an enormous complexity of the control system. In recent years several approaches to apply ML to specific robotics tasks have been published. Nevertheless we are still far from a complete autonomous robot control system with learning components. This paper aims at pointing out the problems and possible applications for integrating learning capabilities into a robot control system and then describes a new integrated system architecture which shows up a set of components necessary for (partially) autonomous systems with learning facilities. 1. Introduction  In recent years there has been an increasing interest in applying machine learning techniques to robotics. The applications are manipulator as well as mobile system tasks. The learning techniques used range from rote learning [25, 22, 1, 8, 27] and inductive learning...
ML
aron99efficient
Efficient Support for P-HTTP in Cluster-Based Web Servers This paper studies mechanisms and policies for supporting HTTP/1.1 persistent connections in cluster-based Web servers that employ content-based request distribution. We present two mechanisms for the efficient, content-based distribution of HTTP/1.1 requests among the back-end nodes of a cluster server. A trace-driven simulation shows that these mechanisms, combined with an extension of the locality-aware request distribution (LARD) policy, are effective in yielding scalable performance for HTTP/1.1 requests. We implemented the simpler of these two mechanisms, back-end forwarding. Measurements of this mechanism in connection with extended LARD on a prototype cluster, driven with traces from actual Web servers, confirm the simulation results. The throughput of the prototype is up to four times better than that achieved by conventional weighted round-robin request distribution. In addition, throughput with persistent connections is up to 26% better than without.   
DB
4251
Human Performance on Clustering Web Pages: A Preliminary Study With the increase in information on the World Wide Web it has become difficult to quickly find desired information without using multiple queries or using a topic-specific search engine. One way to help in the search is by grouping HTML pages together that appear in some way to be related. In order to better understand this task, we performed an initial study of human clustering of web pages, in the hope that it would provide some insight into the difficulty of automating this task. Our results show that subjects did not cluster identically; in fact, on average, any two subjects had little similarity in their web-page clusters. We also found that subjects generally created rather small clusters, and those with access only to URLs created fewer clusters than those with access to the full text of each web page. Generally the overlap of documents between clusters for any given subject increased when given the full text, as did the percentage of documents clustered. When analyzing individual subjects, we found that each had different behavior across queries, both in terms of overlap, size of clusters, and number of clusters. These results provide a sobering note on any quest for a single clearly correct clustering method for web pages.
ML
muth99integrating
Integrating Light-Weight Workflow Management Systems within Existing Business Environments Workflow management systems support the efficient, largely au-  tomated execution of business processes. However, using a workflow management system typically requires implementing the  application's control flow exclusively by the workflow management  system. This approach is powerful if the control flow is specified and implemented from scratch, but it has severe drawbacks if a workflow management system is to be integrated within environments  with existing solutions for implementing control flow. Usual-  ly, the existing solutions are too complex to be substituted by the workflow management system at once. Hence, the workflow management system must support an incremental integration, i.e. the reuse of existing implementations of control flow as well as their in-  cremental substitution. Extending the workflow management system's functionality ac-  cording to future application needs, e.g. by worklist and history management, must also be possible. In particular, at the beginning  of...
DB
253088
Efficient and Extensible Algorithms for Multi Query Optimization Complex queries are becoming commonplace, with the growing use of decision support systems.  These complex queries often have a lot of common sub-expressions, either within a single query, or  across multiple such queries run as a batch. Multi-query optimization aims at exploiting common subexpressions  to reduce evaluation cost. Multi-query optimization has hither-to been viewed as impractical,  since earlier algorithms were exhaustive, and explore a doubly exponential search space.  In this paper we demonstrate that multi-query optimization using heuristics is practical, and provides  significant benefits. We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU,  which are based on simple modifications to the Volcano search strategy, and a greedy heuristic. Our  greedy heuristic incorporates novel optimizations that improve efficiency greatly. Our algorithms are  designed to be easily added to existing optimizers. We present a performance study comparing the  algo...
DB
226856
Query Optimization in Kess - An Ontology-Based KBMS This paper presents an approach for the implementation of query optimization techniques in Kess  (the Knowledge Enhanced SQL Server). Kess is a Knowledge Database Management System (KBMS)  that uses a semantic ontology-based data model. We have classified our query optimization techniques  in two different categories: 1. semantic-based and 2. data access path related. These techniques use  a compiler optimization approach to simplify query predicates and use caching to optimize memory  hierarchy performance for the evaluation of ontology-related predicates. This work also presents the  results of the implementation of such optimizations in Kess in the form of a performance analysis.  1 Introduction  This paper presents an approach for the implementation of query optimization techniques in Kess (the Knowledge Enhanced SQL Server). Its main novel feature, when compared to conventional SQL servers, is the capability to store knowledge and to use it in a manner that is analogous to deducti...
DB
doan99efficiently
Efficiently Ordering Query Plans for Data Integration interface to a multitude of data sources. Given a user query formulated in this interface, the system translates it into a set of query plans. Each plan is a query formulated over the data sources, and specifies a way to access sources and combine data to answer the user query.
DB
inoue99computing
Computing Extended Abduction Through Transaction Programs this paper, we propose a computational mechanism for extended abduction. When a background theory is written in a normal logic program, we introduce its transaction program for computing extended abduction. A transaction program is a set of non-deterministic production rules that declaratively specify addition and deletion of abductive hypotheses. Abductive explanations are then computed by the fixpoint of a transaction program using a bottom-up model generation procedure. The correctness of the proposed procedure is shown for the class of acyclic covered abductive logic programs. In the context of deductive databases, a transaction program provides a declarative specification of database update.  Keywords: abduction, nonmonotonic reasoning, database update 1. Introduction  1.1. Motivation and background Abduction is inference to best explanations, and has recently been recognized as a very important form of reasoning in both AI [2
DB
aoki99realtime
Realtime Personal Positioning System for Wearable Computers Context awareness is an important functionality for wearable computers. In particular, the computer should know where the person is in the environment. This paper proposes an image sequence matching technique for the recognition of locations and previously visited places. As in single word recognition in speech recognition, a dynamic programming algorithm is proposed for the calculation of the similarity of different locations. The system runs on a stand alone wearable computer such as a Libretto PC. Using a training sequence a dictionary of locations is created automatically. These locations are then be recognized by the system in realtime using a hatmounted camera.  1. Introduction  Obtaining user location is one of the important functions for wearable computers in two applications. One is automatic self-summary, and the other is contextaware user interface. In self-summary, the user is wearing a small camera and a small computer, capturing and recording every event of his/her daily ...
HCI
95715
Efficient Use of Signatures in Object-Oriented Database Systems . Signatures are bit strings, which are generated by applying some hash function on some or all of the attributes of an object. The signatures of the objects can be stored separately from the objects themselves, and can later be used to filter out candidate objects during perfect match queries. In an object-oriented database system (OODB) using logical OIDs, an object identifier index (OIDX) is needed to map from logical OID to the physical location of the object. In this report we show how the signatures can be stored in the OIDX, and used to reduce the average object access cost in a system. We also extend this approach to transaction time temporal OODBs (TOODB), where this approach is even more beneficial, because maintaining signatures comes virtually for free. We develop a cost model that we use to analyze the performance of the proposed approaches, and this analysis shows that substantial gain can be achieved. Keywords: Signatures, object-oriented database systems, temporal objec...
DB
448928
Form-Based Proxy Caching for Database-Backed Web Sites  Web caching proxy servers are essential for improving web performance and scalability, and recent research has focused on making proxy caching work for database-backed web sites. In this paper, we explore a new proxy caching framework that exploits the query semantics of HTML forms. We identify two common classes of form-based queries from real-world database-backed web sites, namely, keyword-based queries and function-embedded queries. Using typical examples of these queries, we study two representative caching schemes within our framework: (i) traditional passive query caching, and (ii) active query caching, in which the proxy cache can service a request by evaluating a query over the contents of the cache. Results from our experimental implementation show that our form-based proxy is a general and flexible approach that efficiently enables active caching schemes for database-backed web sites. Furthermore, handling query containment at the proxy yields significant performance advantages over passive query caching, but extending the power of the active cache to do full semantic caching appears to be less generally effective.
DB
345377
Evaluation of Item-Based Top-N Recommendation Algorithms The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of  recommender systems---a personalized information filtering technology used to identify a set of N items that will  be of interest to a certain user. User-based Collaborative filtering is the most successful technology for building  recommender systems to date, and is extensively used in many commercial recommender systems. Unfortunately, the  computational complexity of these methods grows linearly with the number of customers that in typical commercial  applications can grow to be several millions. To address these scalability concerns item-based recommendation  techniques have been developed that analyze the user-item matrix to identify relations between the different items,  and use these relations to compute the list of recommendations.  In this paper we present one such class of item-based recommendation algorithms that first determine the similarities  between the various ite...
IR
sun98some
Some Experiments with a Hybrid Model for Learning Sequential Decision Making To deal with sequential decision tasks, we present a learning model Clarion, which is a hybrid connectionist model consisting of both localist and distributed representations, based on the two-level approach proposed in Sun (1995). The model learns and utilizes procedural and declarative knowledge, tapping into the synergy of the two types of processes. It unifies neural, reinforcement, and symbolic methods to perform on-line, bottom-up learning. Experiments in various situations are reported that shed light on the working of the model. 1 Introduction  This paper presents a hybrid model that unifies neural, symbolic, and reinforcement learning into an integrated architecture. It addresses the following three issues: (1) It deals with concurrent on-line learning: It allows a situated agent to learn continuously from on-going experience in the world, without the use of preconstructed data sets or preconceived concepts. (2) The model learns not only low-level procedural skills but also hi...
AI
goldberg99genetic
Genetic and Evolutionary Algorithms in the Real World Introduction  Since 1992, I have made regular trips to Japan to give talks about genetic algorithms (GAs)---search procedures based on the mechanics of natural selection and genetics. Back during my first visit, the use of genetic and evolutionary algorithms (GEAs) was restricted to a relatively small cadre of devoted specialists. Today, Japanese researchers and practitioners are ably advancing the state of GEA art and application across a broad front. Around the globe, from traditional and cutting-edge optimization in engineering and operations research to such non-traditional areas as drug design, financial prediction, data mining, and the composition of poetry and music, GEAs are grabbing attention and solving problems across a broad spectrum of human endeavor. Of course, science and technology go through fads and fashions much like those of apparel, food, and toys, and many practitioners---in Japan and elsewhere---are wondering whether GEAs, like so many methods that have c
ML
sadri99computational
Computational Logic and Multi-Agent Systems: a Roadmap Agent-based computing is an emerging computing paradigm that has  proved extremely successful in dealing with a number of problems arising  from new technological developments and applications. In this paper we  report the role of computational logic in modeling intelligent agents, by  analysing existing agent theories, agent-oriented programming languages  and applications, as well as identifying challenges and promising directions  for future research.  1 Introduction  In the past ten years the eld of agent-based computing has emerged and greatly expanded, due to new technological developments such as ever faster and cheaper computers, fast and reliable interconnections between them as well as the emergence of the world wide web. These developments have at the same time opened new application areas, such as electronic commerce, and posed new problems, such as that of integrating great quantities of information and building complex software, embedding legacy code. The establishment o...
Agents
abecker01decor
The Decor Toolbox For Workflow-Embedded Organizational Memory Access : We shortly motivate the idea of business-process oriented knowledge management (BPOKM) and sketch  the basic approaches to achieve this goal. Then we describe the DECOR (Delivery of context-sensitive  organisational knowledge) project which develops, tests, and consolidates new methods and tools for  BPOKM. DECOR builds upon the KnowMore framework (Abecker et al 1998; Abecker et al 2000) for  organizational memories (OM), but tries to overcome some limitations of this approach. In the DECOR  project, three end-user environments serve as test-beds for validation and iterative improvement of  innovative approaches to build:  (1) knowledge archives organised around formal representations of business processes to facilitate  navigation and access,  (2) active information delivery services which - in collaboration with a workflow tool to support weaklystructured  knowledge-intensive work - offer the user in a context-sensitive manner helpful information from the knowledge archive, and  (3...
IR
seo00reinforcement
A Reinforcement Learning Agent for Personalized Information Filtering This paper describes a method for learning user's interests in the Web-based personalized information filtering system called WAIR. The proposed method analyzes user's reactions to the presented documents and learns from them the profiles for the individual users. Reinforcement learning is used to adapt the term weights in the user profile so that user's preferences are best represented. In contrast to conventional relevance feedback methods which require explicit user feedbacks, our approach learns user preferences implicitly from direct observations of user behaviors during interaction. Field tests have been made which involved 7 users reading a total of 7,700 HTML documents during 4 weeks. The proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods.
IR
litman99empirically
Empirically Evaluating an Adaptable Spoken Dialogue System Recent technological advances have made it possible to build real-time, interactive  spoken dialogue systems for a wide variety of applications. However, when users  do not respect the limitations of such systems, performance typically degrades. Although  users differ with respect to their knowledge of system limitations, and although different  dialogue strategies make system limitations more apparent to users, most current systems  do not try to improve performance by adapting dialogue behavior to individual users. This  paper presents an empirical evaluation of TOOT, an adaptable spoken dialogue system for  retrieving train schedules on the web. We conduct an experiment in which 20 users carry  out 4 tasks with both adaptable and non-adaptable versions of TOOT, resulting in a corpus  of 80 dialogues. The values for a wide range of evaluation measures are then extracted from  this corpus. Our results show that adaptable TOOT generally outperforms non-adaptable  TOOT, and that the utility of adaptation depends on TOOT's initial dialogue strategies.
DB
son02qos
QoS Management in Web-based Real-Time Data Services The demand for real-time data services has been increasing recently. Many e-commerce applications and webbased information services are becoming very sophisticated in their data needs. They span the spectrum from low level status data, e.g., stock prices, to high level aggregated data, e.g., recommended selling/buying point. In these applications, it is desired to process user requests within their deadlines using fresh data, which reflect the current market status. Current web-based data services are poor at processing user requests in a timely manner using fresh data. To address this problem, we present a framework for guaranteed real-time data services in unpredictable environments. We also introduce a possible application of our approach in the distributed environment. 1
DB
degaris00simulating
Simulating the Evolution of 2D Pattern Recognition on the CAM-Brain Machine, an Evolvable Hardware Tool for Building a 75 Million Neuron Artificial Brain This paper presents some simulation results of the evolution of 2D visual pattern recognizers to be implemented  very shortly on real hardware, namely the "CAM-Brain Machine" (CBM), an FPGA based piece of evolvable hardware  which implements a genetic algorithm (GA) to evolve a 3D cellular automata (CA) based neural network circuit  module, of approximately 1,000 neurons, in about a second, i.e. a complete run of a GA, with 10,000s of circuit  growths and performance evaluations. Up to 65,000 of these modules, each of which is evolved with a humanly  specified function, can be downloaded into a large RAM space, and interconnected according to humanly specified  artificial brain architectures. This RAM, containing an artificial brain with up to 75 million neurons, is then updated  by the CBM at a rate of 130 billion CA cells per second. Such speeds will enable real time control of robots and  hopefully the birth of a new research field that we call "brain building". The first such artif...
ML
layzell98new
A New Research Tool for Intrinsic Hardware Evolution . The study of intrinsic hardware evolution relies heavily on commercial FPGA devices which can be configured in real time to produce physical electronic circuits. Use of these devices presents certain drawbacks to the researcher desirous of studying fundamental principles underlying hardware evolution, since he has no control over the architecture or type of basic configurable element. Furthermore, analysis of evolved circuits is difficult as only external pins of FPGAs are accessible to test equipment. After discussing current issues arising in intrinsic hardware evolution, this paper presents a new test platform designed specifically to tackle them, together with experimental results exemplifying its use. The results include the first circuits to be evolved intrinsically at the transistor level. 1 Introduction  In recent years, evolutionary algorithms (EAs) have been applied to the design of electronic circuitry with significant results being attained using both computer simulations...
AI
cvetkovic99genetic
Genetic Algorithms Based Systems For Conceptual Engineering Design this paper we try to integrate methods of preferences and scenarios with Genetic Algorithms used to perform multi--objective optimisation. The goal is to make a system that will be able to work together with the designer during the conceptual design phase, where interaction and designer knowledge are sometimes more important than accuracy. MODULE OPTIMISATION CONSTRAINT HANDLING MODULE FUZZY RULES HANDLING MODULE
ML
21283
TEMPOS: A Temporal Database Model Seamlessly Extending ODMG This paper presents Tempos, a set of models and languages intended to seamlessly extend the ODMG object database standard with temporal functionalities. The proposed models exploit object-oriented technology to meet some important, yet traditionally neglected design criteria, related to legacy code migration and representation independence.  Tempos has been fully formalized both at the syntactical and the semantical level and implemented on top of the O 2 DBMS. Its suitability in regard to applications' requirements has been validated through concrete case studies from various contexts.  Keywords: temporal databases, temporal data models, temporal query languages, time representation, upward compatibility, object-oriented databases, ODMG R'esum'e  Ce document pr'esente Tempos : un ensemble de mod`eles et de langages con¸cus pour 'etendre le standard pour Bases de Donn'ees `a objets ODMG, par des fonctionnalit'es temporelles. Les mod`eles d'ecrits exploitent les possibilit'es de la tech...
DB
dean99finding
Finding Related Pages in the World Wide Web When using traditional search engines, users have to formulate queries to describe their information need. This paper discusses a different approach toweb searching where the input to the search process is not a set of query terms, but instead is the URL of a page, and the output is a set of related web pages. A related web page is one that addresses the same topic as the original page. For example, www.washingtonpost.com is a page related to www.nytimes.com, since both are online newspapers. We describe two algorithms to identify related web pages. These algorithms use only the connectivity information in the web (i.e., the links between pages) and not the content of pages or usage information. We haveimplemented both algorithms and measured their runtime performance. To evaluate the e ectiveness of our algorithms, we performed a user study comparing our algorithms with Netscape's \What's Related " service [12]. Our study showed that the precision at 10 for our two algorithms are 73 % better and 51 % better than that of Netscape, despite the fact that Netscape uses both content and usage pattern information in addition to connectivity information.
IR
funkhouser02search
A Search Engine for 3D Models As the number of 3D models available on the Web grows, there is an increasing need for a  search engine to help people find them. Unfortunately, traditional text-based search techniques  are not always effective for 3D data. In this paper, we investigate new shape-based search  methods. The key challenges are to develop query methods simple enough for novice users  and matching algorithms robust enough to work for arbitrary polygonal models. We present a  web-based search engine system that supports queries based on 3D sketches, 2D sketches, 3D  models, and/or text keywords. For the shape-based queries, we have developed a new matching  algorithm that uses spherical harmonics to compute discriminating similarity measures without  requiring repair of model degeneracies or alignment of orientations. It provides 46--245%  better performance than related shape matching methods during precision-recall experiments,  and it is fast enough to return query results from a repository of 20,000 models in under a  second. The net result is a growing interactive index of 3D models available on the Web (i.e.,  a Google for 3D models).
IR
77029
Redundancy and Inconsistency Detection in Large and Semi-structured Case Bases With the dramatic proliferation of case based reasoning systems in commercial applications, many case bases are now becoming legacy systems. They represent a significant portion of an organization's assets, but they are large and difficult to maintain. One of the contributing factors is that these case bases are often large and yet unstructured or semi-structured; they are represented in natural language text. Adding to the complexity is the fact that the case bases are often authored and updated by different people from a variety of knowledge sources, making it highly likely for a case base to contain redundant and inconsistent knowledge. In this paper, we present methods and a system for maintaining large and semi-structured case bases. We focus on two difficult problems in case-base maintenance: redundancy and inconsistency detection. These two problems are particularly pervasive when one deals with an semi-structured case base. We will discuss both algorithms and a system for solvi...
ML
482081
Verifying Sequential Consistency on Shared-Memory Multiprocessors by Model Checking The memory model of a shared-memory multiprocessor is a contract between the designer and programmer of the multiprocessor. The sequential consistency memory model specifies a total order among the memory (read and write) events performed at each processor. A trace of a memory system satisfies sequential consistency if there exists a total order of all memory events in the trace that is both consistent with the total order at each processor and has the property that every read event to a location returns the value of the last write to that location. Descriptions of shared-memory systems are typically parameterized by the number of processors, the number of memory locations, and the number of data values. It has been shown that even for finite parameter values, verifying sequential consistency on general shared-memory systems is undecidable. We observe that, in practice, shared-memory systems satisfy the properties of causality and data independence. Causality is the property that values of read events flow from values of write events. Data independence is the property that all traces can be generated by renaming data values from traces where the written values are distinct from each other. If a causal and data independent system also has the property that the logical order of write events to each location is identical to their temporal order, then sequential consistency can be verified algorithmically. Specifically, we present a model checking algorithm to verify sequential consistency on such systems for a finite number of processors and memory locations and an arbitrary number of data values.  1 
HCI
496883
Secure Communication for Secure Agent-Based Electronic Commerce Applications Although electronic commerce is a relatively new concept, it  has already become a normal aspect of our daily life. The software agent  technology is also relatively new. In the area of electronic commerce, software  agents could be used for example to search for the lowest prices and  the best services, to buy goods on behalf of a user, etc. These applications  involve a number of security issues, such as communications security, system  security, and application security, that have to be solved. This paper  describes how communications security is added to a lightweight agent  framework. Secure agent-based electronic commerce applications require  communications security services. Adding these services is a rst basis  and an important enabler for the framework in order to be used for secure  electronic commerce applications.
Agents
setzkorn02evolving
Evolving Rule-Based Trading Systems In this study, a market trading rulebase is optimised using genetic programming  (GP). The rulebase is comprised of simple relationships between technical  indicators, and generates signals to buy, sell short, and remain inactive. The  methodology is applied to prediction of the Standard & Poor's composite index  (02-Jan-1990 to 18-Oct-2001). Two potential market systems are inferred: a simple  system using few rules and nodes, and a more complex system. Results are  compared with a benchmark buy-and-hold strategy. Neither trading system was  found capable of consistently outperforming this benchmark. More complicated  rulebases, in addition to being difficult to understand, are susceptible to overfitting.
ML
525023
Towards a Network File System for Roaming Users Pervasive computing aims at offering access to user's data, anytime, anywhere, in a transparent manner. However, realizing such a vision necessitates several improvements in the way servers and user's terminals interact. Most notably, client terminals should not tightly rely on an information server which can be temporarily unavailable in a mobile situation. They should rather exploit all information servers available in a given context through loose coupling with both centralized servers and groups of terminals, in a serverless manner. In this position paper, we present the design rationale of a network file system that implements transparent adaptive file access according to the users' specific situations (e.g. device in use, network connectivity, etc).
HCI
kasif98probabilistic
A Probabilistic Framework for Memory-Based Reasoning In this paper, we propose a probabilistic framework for Memory-Based Reasoning (MBR). The framework allows us to clarify the technical merits and limitations of several recently published MBR methods and to design new variants. The proposed computational framework consists of three components: a specification language to define an adaptive notion of relevant context for a query; mechanisms for retrieving this context; and local learning procedures that are used to induce the desired action from this context. Based on the framework we derive several analytical and empirical results that shed light on MBR algorithms. We introduce the notion of an MBR transform, and discuss its utility for learning algorithms. We also provide several perspectives on memory-based reasoning from a multi-disciplinary point of view. 1 Introduction  Reasoning can be broadly defined as the task of deciding what action to perform in a particular state or in response to a given query. Actions can range from admit...
AI
487698
Gongeroos'99 Team This article presents Gongeroos'99 approach to the Robocup simulator league challenge.
Agents
wagner99toward
Toward Generalized Organizationally Contexted Agent Control Generalized domain-independent approaches to agent control enable control components to be used for a wide variety of applications. This abstraction from the domain context implies that contextual behavior is not possible or that it requires violation of the domain-independent objective. We discuss how context is used in the generalized framework and our current focus on the addition of organizational context in agent control. 1 Introduction  From the vantage point of a long history of research in agents and agent control components for building distributed AI and multi-agent systems, we have focused our recent efforts on approaching agent control from a generalized domainindependent perspective. In implementation terms, the objective is to develop a set of agent control components that can be bundled with domain problem solvers or legacy applications to create agents that can meet real-time deadlines (and real-resource constraints) and coordinate activities with other agents. This pap...
Agents
445220
Managing Change on the Web Increasingly, digital libraries are being defined that collect pointers to World-Wide Web based resources rather than hold the resources themselves. Maintaining these collections is challenging due to distributed document ownership and high fluidity. Typically a collection's maintainer has to assess the relevance of changes with little system aid. In this paper, we describe the Walden's Paths Path Manager, which assists a maintainer in discovering when relevant changes occur to linked resources. The approach and system design was informed by a study of how humans perceive changes of Web pages. The study indicated that structural changes are key in determining the overall change and that presentation changes are considered irrelevant.  Categories and Subject Descriptors  I.3.7 [Digital Libraries]: User issues;  H.5.4 [Hypertext/Hypermedia]: Other (maintenance)  General Terms  Algorithms, Management, Design, Reliability, Experimentation, Human Factors, Verification.  Keywords  Walden's Paths, Path Maintenance.  1. 
IR
459526
Knowledge Base Support For Design And Synthesis Of Multiagent Systems  
Agents
plagianakos01tumor
Tumor Detection in Colonoscopic Images Using Hybrid Methods for on-Line Neural Network Training In this paper the effectiveness of a new Hybrid Evolutionary Algorithm in on-line Neural Network training for tumor detection is investigated. To this end, a Lamarck-inspired combination of Evolutionary Algorithms and Stochastic Gradient Descent is proposed. The Evolutionary Algorithm works on the termination point of the Stochastic Gradient Descent. Thus, the method consists in a Stochastic Gradient Descent-based on-line training stage and an Evolutionary Algorithm-based retraining stage. On-line training is considered eminently suitable for large (or even redundant) training sets and/or networks; it also helps escaping local minima and provides a more natural approach for learning nonstationary tasks. Furthermore, the notion of retraining aids the hybrid method to exhibit reliable and stable performance, and increases the generalization capability of the trained neural network. Experimental results suggest that the proposed hybrid strategy is capable to train on-line, efficiently and effectively. Here, an artificial neural network architecture has been successfully used for detecting abnormalities in colonoscopic video images.
AI
vitter99external
External Memory Algorithms and Data Structures Data sets in large applications are often too massive to fit completely inside the computer's internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this paper, we survey the state of the art in the design and analysis of external memory algorithms and data structures (which are sometimes referred to as "EM" or "I/O" or "out-of-core" algorithms and data structures). EM algorithms and data structures are often designed and analyzed using the parallel disk model (PDM). The three machine-independent measures of performance in PDM are the number of I/O operations, the CPU time, and the amount of disk space. PDM allows for multiple disks (or disk arrays) and parallel CPUs, and it can be generalized to handle tertiary storage and hierarchical memory. We discuss several important paradigms for how to solve batched and online problems efficiently in external memory. Programming tools and environments are available for simplifying the programming task. The TPIE system (Transparent Parallel I/O programming Environment) is both easy to use and efficient in terms of execution speed. We report on some experiments using TPIE in the domain of spatial databases. The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.  
DB
454124
Using Text Classifiers for Numerical Classification Consider a supervised learning problem in which examples contain both numerical- and text-valued features. To use traditional featurevector -based learning methods, one could treat the presence or absence of a word as a Boolean feature and use these binary-valued features together with the numerical features. However, the use of a text-classification system on this is a bit more problematic --- in the most straight-forward approach each number would be considered a distinct token and treated as a word. This paper presents an alternative approach for the use of text classification methods for supervised learning problems with numerical-valued features in which the numerical features are converted into bag-of-words features, thereby making them directly usable by text classification methods. We show that even on purely numerical-valued data the results of textclassification on the derived text-like representation outperforms the more naive numbers-as-tokens representation and, more importantly, is competitive with mature numerical classification methods such as C4.5 and Ripper.  1 
IR
bruderlin97hierarchical
Hierarchical Agent Interface for Animation Asynchronous, Hierarchical Agents (AHAs) provide a vertically structured multilevel abstraction hierarchy. In this paper, we argue that this multilevel hierarchy is a convenient way to create a human-agent interface at multiple levels of abstraction. In this way, the agent has several layers of specification (input) and visualization  (output) which facilitates users with problem solving, because such an interface parallels the hierarchical and iterative nature of human creative thought processes. The AHA interface presents an intuitive, intimate interface which supports interactions on a scale from direct manipulation to delegation, depending on the user's choice. Another feature of this interface is its two modes of interaction: direct device interaction (mouse clicking) and interpretive, command line or scripting mode. This way, agents can be "forced" to perform certain activities via mouse clicks (direct control), or they can be programmed via scripts on the fly. We present example...
Agents
afrati02answering
Answering queries using views with arithmetic comparisons We consider the problem of answering queries using views, where queries and views are conjunctive queries with arithmetic comparisons (CQACs) over dense orders. Previous work only considered limited variants of this problem, without giving a complete solution. We have developed a novel algorithm to obtain maximally-contained rewritings (MCRs) for queries having left (or right) semi-interval-comparison predicates. For semi-interval queries, we show that the language of finite unions of CQAC rewritings is not sufficient to find a maximally-contained solution, and identify cases where datalog is sufficient. Finally, we show that it is decidable to obtain equivalent rewritings for CQAC queries.
DB
mann01partial
Partial Replication in the Vesta Software Repository The Vesta repository is a special-purpose replicated file system, developed as part of the Vesta software configuration management system. One of the major goals of Vesta is to make all software builds reproducible. To this end, the repository provides an append-only name space; new names can be inserted, but once a name exists, its meaning cannot change. More concretely, all files and some designated directories are immutable, while the remaining directories are appendable, allowing new names to be defined but not allowing existing names to be redefined. The data stored
HCI
441239
Visual Specification of Queries for Finding Patterns in Time-Series Data Widespread interest in discovering features and trends in time- series has generated a need for tools that support interactive exploration.This paper introduces timeboxes: a powerful graphical, directmanipulation metaphor for the specification of queries over time-series datasets. Our TimeFinder implementation of timeboxes supports interactive formulation and modification of queries, thus speeding the process of exploring time-series data sets and guiding data mining. TimeFinder includes windows for timebox queries, individual time-series, and details-on-demand. Other features include drag-and-drop support for query-by-example and graphical envelopes for displaying the extent of the entire data set and result set from a given query. Extensions involving increased expressive power and general temporal data sets are discussed.
DB
paradiso00footnotes
FootNotes: Personal Reflections on the Development of Instrumented Dance Shoes and their Musical Applications This paper describes experiences in designing and developing an extremely  versatile, multimodal sensor interface built entirely into a pair of shoes. I discuss the  system design, trace its motivations and goals, then describe its applications in  interactive music for dance performance, summarizing lessons learned and future  possibilities.  ____________________________________  1) The Inspiration  Although the idea of instrumenting shoes for interactive music performance had crossed my mind before, the moment at which I decided to pursue this project can be traced to a demonstration that I attended with my Media Lab colleague Tod Machover in November of 1996. We were visiting some of our research sponsors and colleagues at a Yamaha development laboratory in the Shinjuku section of Tokyo, where they showed us the latest version of their Miburi musical controller [1]. The Miburi is an electronic vest, with bend sensors at various joints to monitor dynamic articulation and a pair of ha...
HCI
wang99hierarchical
Hierarchical Optimization of Policy-Coupled Semi-Markov Decision Processes One general strategy for approximately solving large Markov decision processes is "divide-and-conquer": the original problem is decomposed into sub-problems which interact with each other, but yet can be solved independently by taking into account the nature of the interaction. In this paper we focus on a class of "policy-coupled" semi-Markov decision processes (SMDPs), which arise in many nonstationary real-world multi-agent tasks, such as manufacturing and robotics. The nature of the interaction among sub-problems (agents) is more subtle than that studied previously: the components of a sub-SMDP, namely the available states and actions, transition probabilities and rewards, depend on the policies used in solving the "neighboring" sub-SMDPs. This "strongly-coupled" interaction among subproblems causes the approach of solving each sub-SMDP in parallel to fail. We present a novel approach whereby many variants of each sub-SMDP are solved, explicitly taking into account the different mod...
Agents
artale01reasoning
Reasoning over Conceptual Schemas and Queries in Temporal Databases This paper introduces a new logical formalism,  intended for temporal conceptual modelling,  as a natural combination of the wellknown  description logic DLR and pointbased  linear temporal logic with Since and  Until. The expressive power of the resulting   DLRUS logic is illustrated by providing  a systematic formalisation of the most important  temporal entity-relationship data models  appeared in the literature. We define  a query language (where queries are nonrecursive  Datalog programs and atoms are  complex DLRUS expressions) and investigate  the problem of checking query containment  under the constraints defined by  DLRUS conceptual schemas, as well as the  problems of schema satisfiability and logical  implication. Although it is shown that reasoning  in full DLRUS is undecidable, we  identify the decidable (in a sense, maximal)  fragment DLR  US  by allowing applications  of temporal operators to formulas and entities  only (but not to relation expressions). We obtain  the following hierarchy of complexity results:  (a) reasoning in DLR US with atomic  formulas is EXPTIME-complete, (b) satisfiability  and logical implication of arbitrary  DLR US formulas is EXPSPACE-complete,  and (c) the problem of checking query containment  of non-recursive Datalog queries under   DLR US constraints is decidable in 2EXPTIME.  
DB
brazier01automated
Automated Servicing of Agents Agents need to be able to adapt to changes in their environment. One way to achieve this, is to service agents when needed. A separate servicing facility, an agent factory, is capable of automatically modifying agents. This paper discusses the feasibility of automated servicing.
Agents
454519
Fine-Granularity Signature Caching in Object Database Systems In many of the emerging application areas for database systems, data is viewed as a collection  of objects, the access pattern is navigational, and a large fraction of the accesses are perfect match  accesses/queries on one or more words in text strings in the objects in these databases. A typical  example of such an application area is XML/Web storage. In order to reduce the object access  cost, signature files can be used. However, traditional signature file maintenance is costly, and to  be beneficial, a low update rate and high query selectivity is needed to make the maintenance and  use of signatures beneficial. In this report, we present the SigCache approach. Instead of storing  the signatures in separate signature files, the signatures are stored together with their objects. In  addition, the most frequently accessed signatures are stored in a main memory signature cache  (SigCache). Because the signatures are much smaller than the objects, the increase in update cost  is not s...
DB
cabri00xml
XML Dataspaces for Mobile Agent Coordination This paper presents XMARS, a programmable coordination architecture for  Internet applications based on mobile agents. In XMARS, agents coordinate --  both with each other and with their current execution environment -- through  programmable XML dataspaces, accessed by agents in a Linda-like fashion. This  suits very well the characteristics of the Internet environment: on the one hand, it  offers all the advantages of XML in terms of interoperability and standard  representation of information; on the other hand, it enforces open and uncoupled  interactions, as required by the dynamicity of the environment and by the mobility  of the application components. In addition, coordination in XMARS is made more  flexible and secure by the capability of programming the behaviour of the  coordination media in reaction to the agents' accesses. An application example  related to the management of on-line academic courses shows the suitability and  the effectiveness of the XMARS architecture.  Ke...
Agents
191426
Knowledge Management through Ontologies Most enterprises agree that knowledge is an essential asset for success and survival on a increasingly competitive and global market. This awareness is one of the main reasons for the exponential growth of knowledge management in the past decade. Our approach to knowledge management is based on ontologies, and makes knowledge assets intelligently accessible to people in organizations. Most company-vital knowledge resides in the heads of people, and thus successful knowledge management does not only consider technical aspects, but also social ones. In this paper, we describe an approach to intelligent knowledge management that explicitly takes into account the social issues involved. The proof of concept is given by a large-scale initiative involving knowledge management of a virtual organization. 1 Introduction  According to Information Week (Angus et al., 1998) "the business problem that knowledge management is designed to solve is that knowledge acquired through experience doesn't ge...
DB
vanlaerhoven00teaching
Teaching Context to Applications Enhancing applications by adding one or more  sensors is not new. Incorporating machinelearning  techniques to fuse the data from the  sensors into a high-level context description is  less obvious. This paper describes an architecture  that combines a hierarchy of self-organizing  networks and a Markov chain to enable on-line  context recognition. Depending on both user and  application, the user can teach a context  description to the system whenever he or she  likes to, as long as the behavior of the sensors is  different enough. Finally, consequences and  complications of this new approach are  discussed.  1 
HCI
180347
Path Constraints on Semistructured and Structured Data We present a class of path constraints of interest in connection with both structured and semi-structured databases, and investigate their associated implication problems. These path constraints are capable of expressing natural integrity constraints that are not only a fundamental part of the semantics of the data, but are also important in query optimization. We show that in semistructured databases, despite the simple syntax of the constraints, their associated implication problem is r.e. complete and finite implication problem is co-r.e. complete. However, we establish the decidability of the implication problems for several fragments of the path constraint language, and demonstrate that these fragments suffice to express important semantic information such as inverse relationships and local database constraints commonly found in object-oriented databases. We also show that in the presence of types, the analysis of path constraint implication becomes more delicate. We demonstrate so...
DB
wang99discovering
Discovering Structural Association of Semistructured Data Many semistructured objects are similarly, though not identically, structured. We study the  problem of discovering "typical" substructures of a collection of semistructured objects. The  discovered structures can serve the following purposes: (a) the "table-of-contents" for gaining  general information of a source, (b) a road map for browsing and querying information sources,  (c) a basis for clustering documents, (d) partial schemas for providing standard database access  methods, (e) user/customer's interests and browsing patterns. The discovery task is impacted  by structural features of semistructured data in a non-trivial way and traditional data mining  frameworks are inapplicable. We define this discovery problem and propose a solution.  1 Introduction  1.1 Motivation  Many on-line documents, such as HTML, Latex, BibTex, SGML files and those found in digital libraries, are semistructured. Semistructured data arises when the source does not impose a rigid structure (such as the ...
DB
395537
Indexing Techniques for Continuously Evolving Phenomena The management of spatial, temporal, and  spatiotemporal data is becoming increasingly  important in a wide range of applications.  This ongoing Ph.D. project focuses on applications  where spatial or temporal aspects of  objects are continuously changing and there  is a need for indexing techniques that "track"  the changing data, even in-between explicit  updates. In spatiotemporal applications, there  is a need to record and efficiently query the  history, the current state, and the predicted  future behavior of continuously moving objects,  such as vehicles, mobile telephones, and  people. Likewise, in temporal applications  and spatiotemporal applications with discrete  change, time intervals may be naturally related  to the current time, which continuously  progresses. The paper outlines the research  agenda of the Ph.D. project and describes  briefly two access methods developed so far  in this project.  1 Introduction  Recent years have shown both an increase in the amounts of ...
DB
hayzelden99agent
Agent Technology in Communications Systems: An Overview Telecommunications infrastructures are a natural application domain for the distributed Software Agent  paradigm. The authors clarify the potential application of software agent technology in legacy and future  communications systems, and provide an overview of publicly available research on software agents as  used for network management. The authors focus on the so called "stationary intelligent agent" type of  software agent, although the paper also reviews the reasons why mobile agents have made an impact in  this domain. The authors' objective is to describe some of the intricacies of using the software agent  approach in the management of communications systems. The paper is in four main sections. The first  section provides a brief introduction to software agent technology. The second section considers general  problems of network management and the reasons why software agents may provide a suitable solution.  The third section reviews some selected research on agents in a telec...
Agents
453948
Determining When to Use an Agent-Oriented Software Engineering Paradigm . With the emergence of agent-oriented software engineering  techniques, software engineers have a new way of conceptualizing complex  distributed software requirements. To help determine the most appropriate  software engineering methodology, a set of defining criteria is required. In this  paper, we describe out approach to determining these criteria, as well as a  technique to assist software engineers with the selection of a software  engineering methodology based on those criteria.  1 
Agents
74893
Identifying Distinctive Subsequences in Multivariate Time Series by Clustering Most time series comparison algorithms attempt to discover what the members of a set of time series have in common. We investigate a different problem, determining what distinguishes time series in that set from other time series obtained from the same source. In both cases the goal is to identify shared patterns, though in the latter case those patterns must be distinctive as well. An efficient incremental algorithm for identifying distinctive subsequences in multivariate, real-valued time series is described and evaluated with data from two very different sources: the response of a set of bandpass filters to human speech and the sensors of a mobile robot.  1 Introduction  Given two or more sequences of discrete tokens, a dynamic programming algorithm exists for finding the longest common subsequence they share (Cormen, Leiserson, & Rivest 1990). This basic algorithm has been adapted in various ways to find patterns shared by real-valued time series as well (Kruskall & Sankoff 1983). ...
AI
boley99document
Document Categorization and Query Generation on the World Wide Web Using WebACE We present WebACE, an agent for exploring and categorizing documents on the World Wide Web based on a user profile. The heart of the agent is an unsupervised categorization of a set of documents, combined with a process for generating new queries that is used to search for new related documents and for filtering the resulting documents to extract the ones most closely related to the starting set. The document categories are not given a priori. We present the overall architecture and describe two novel algorithms which provide significant improvement over Hierarchical Agglomeration Clustering and AutoClass algorithms and form the basis for the query generation and search component of the agent. We report on the results of our experiments comparing these new algorithms with more traditional clustering algorithms and we show that our algorithms are fast and scalable. y  Authors are listed alphabetically.  1 Introduction  The World Wide Web is a vast resource of information and services t...
IR
papazoglou99contextualizing
Contextualizing the Information Space in Federated Digital Libraries Rapid growth in the volume of documents, their diversity, and terminological variations render federated digital libraries increasingly difficult to manage. Suitable abstraction mechanisms are required to construct meaningful and scalable document clusters, forming a cross-digital library information space for browsing and semantic searching. This paper addresses the above issues, proposes a distributed semantic framework that achieves a logical partitioning of the information space according to topic areas, and provides facilities to contextualize and landscape the available document sets in subject-specific categories.
IR
prendinger00hyper
The Hyper System: Knowledge Reformation for Efficient First-order Hypothetical Reasoning . We present the Hyper system that implements a new approach  to knowledge compilation, where function-free first-order acyclic  Horn theories are transformed to propositional logic. The compilation  method integrates techniques from deductive databases (relevance reasoning)  and theory transformation via unfold/fold transformations, to  obtain a compact propositional representation. The transformed theory  is more compact than the ground version of the original theory in terms  of significantly less and mostly shorter clauses. This form of compilation,  called knowledge (base) reformation, is important since the most efficient  reasoning methods are defined for propositional theories, while knowledge  is most naturally expressed in a first-order language. In particular,  we will show that knowledge reformation allows low-order polynomial  time inference to find a near-optimal solution in cost-based first-order  hypothetical reasoning (or `abduction') problems. We will also present  ex...
DB
cui99practical
Practical Lineage Tracing in Data Warehouses We consider the view data lineage problem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formalize the problem and present a lineage tracing algorithm for relational views with aggregation. Based on our tracing algorithm, we propose a number of schemes for storing auxiliary views that enable consistent and efficient lineage tracing in a multisource data warehouse. We report on a performance study of the various schemes, identifying which schemes perform best in which settings. Based on our results, we have implemented a lineage tracing package in the WHIPS data warehousing system prototype at Stanford. With this package, users can select view tuples of interest, then efficiently "drill down" to examine the source data that produced them. 1 Introduction Data warehousing systems collect data from multiple distributed sources, integrate the information as materialized v...
DB
459286
Enabling Technologies for Interoperability We present a new approach, which proposes to minimize  the numerous problems existing in order to have  fully interoperable GIS. We discuss the existence of  these heterogeneity problems and the fact that they  must be solved to achieve interoperability. These problems  are addressed on three levels: the syntactic, structural  and semantic level. In addition, we identify the  needs for an approach performing semantic translation  for interoperability and introduce a uniform description  of contexts. Furthermore, we discuss a conceptual  architecture Buster (Bremen University Semantic  Translation for Enhanced Retrieval) which can provide  intelligent information integration based on a reclassification  of information entities in a new context.  Lastly, we demonstrate our theories by sketching a real  life scenario.  
DB
161260
Adapting Hidden Markov Models for ASL Recognition by Using Three-dimensional Computer Vision Methods We present an approach to continuous American Sign Language (ASL) recognition, which uses as input three-dimensional data of arm motions. We use computer vision methods for three-dimensional object shape and motion parameter extraction and an Ascension Technologies Flock of Birds interchangeably to obtain accurate three-dimensional movement parameters of ASL sentences, selected from a 53-sign vocabulary and a widely varied sentence structure. These parameters are used as features for Hidden Markov Models (HMMs). To address coarticulation e#ects and improve our recognition results, we experimented with two di#erent approaches. The first consists of training context-dependent HMMs and is inspired by speech recognition systems. The second consists of modeling transient movements between signs and is inspired by the characteristics of ASL phonology. Our experiments verified that the second approach yields better recognition results.  1. INTRODUCTION  Sign language and gesture recognition h...
HCI
neumann99experience
Experience with EMERALD to Date After summarizing the EMERALD architecture and the evolutionary process from which EMERALD has evolved, this paper focuses on our experience to date in designing, implementing, and applying EMERALD to various types of anomalies and misuse. The discussion addresses the fundamental importance of good software engineering practice and the importance of the system architecture -- in attaining detectability, interoperability, general applicability, and future evolvability. It also considers the importance of correlation among distributed and hierarchical instances of EMERALD, and needs for additional detection and analysis components. 1. Introduction EMERALD (Event Monitoring Enabling Responses to Anomalous Live Disturbances) [6, 8, 9] is an environment for anomaly and misuse detection and subsequent analysis of the behavior of systems and networks. EMERALD is being developed under DARPA/ITO Contract number F30602-96-C-0294 and applied under DARPA/ISO Contract number F30602-98-C-0059. EMER...
AI
mittal99selecting
Selecting Text Spans for Document Summaries: Heuristics and Metrics Human-quality text summarization systems are difficult to design, and even more difficult to evaluate, in part because documents can differ along several dimensions, such as length, writing style and lexical usage. Nevertheless, certain cues can often help suggest the selection of sentences for inclusion in a summary. This paper presents an analysis of news-article summaries generated by sentence extraction. Sentences are ranked for potential inclusion in the summary using a weighted combination of linguistic features -- derived from an analysis of news-wire summaries. This paper evaluates the relative effectiveness of these features. In order to do so, we discuss the construction of a large corpus of extraction-based summaries, and characterize the underlying degree of difficulty of summarization at different compression levels on articles in this corpus. Results on our feature set are presented after normalization by this degree of difficulty.
IR
naumann00assessment
Assessment Methods for Information Quality Criteria Information quality (IQ) is one of the most important aspects of information integration on the Internet. Many projects realize and address this fact by gathering and classifying IQ criteria. Hardly ever do the projects address the immense difficulty of assessing scores for the criteria. This task must precede any usage of criteria for qualifying and integrating information.  After reviewing previous attempts to classify IQ criteria, in this paper we also classify criteria, but in a new, assessment-oriented way. We identify three sources for IQ scores and thus, three IQ criterion classes, each with different general assessment possibilities. Additionally, for each criterion we give detailed assessment methods. Finally, we consider confidence measures for these methods. Confidence expresses the accuracy, lastingness, and credibility of the individual assessment methods.  1 Introduction  Low information quality is one of the most pressing problems for consume rs of information that is di...
IR
schuster98neural
Neural Networks for Speech Processing this article. Currently (1998), successful use of NNs for speech processing is mainly limited to
ML
sastry01cluster
Cluster Optimization Using Extended Compact Genetic Algorithm This study presents an ecient atomic  cluster optimization algorithm that utilizes  a hybrid extended compact genetic algorithm  along with an eciency enhancement  technique called seeding. Empirical results  indicate that the population size and total  number of function evaluations scale  up with the cluster size as O n  0:83    and  O n  2:45    respectively. The results also indicate  that the proposed algorithm is not  only very reliable in predicting lowest energy  structures, but also has a better scale  up of number of function evaluations with  the cluster size.
AI
coellocoello99treating
Treating Constraints As Objectives For Single-Objective Evolutionary Optimization This paper presents a new approach to handle constraints using evolutionary algorithms. The new technique treats constraints as objectives, and uses a multiobjective optimization approach to solve the re-stated single-objective optimization problem. The new approach is compared against other numerical and evolutionary optimization techniques in several engineering optimization problems with different kinds of constraints. The results obtained show that the new approach can consistently outperform the other techniques using relatively small sub-populations, and without a significant sacrifice in terms of performance.
ML
heintz00robosoc
RoboSoc a System for Developing RoboCup Agents for Educational Use This report describes RoboSoc, a system for developing RoboCup agents designed especially, but not only, for educational use. RoboSoc is designed to be as general, open, and easy to use as possible and to encourage and simplify the modification, extension and sharing of RoboCup agents, and parts of them. To do this I assumed four requirements from the user: she wants the best possible data, use as much time as possible for the decision making, rather act on incomplete information than not act at all, and she wants to manipulate the objects found in the soccer environment. RoboSoc consists of three parts: a library of basic objects and utilities used by the rest of the system, a basic system handling the interactions with the soccer server and the timing of the agent, and a framework for world modeling and decision support. The framework defines three concepts, views, predicates and skills. The views are specialized information processing units responsible for a specific part of the wo...
Agents
li00automatic
Automatic Text Detection and Tracking in Digital Video Text which appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this paper we present algorithms for detecting and tracking text in digital video. Our system implements a scalespace feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: an SSD (Sum of Squared Difference)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly. Keywords Text Detection, Text Tracking, Video Indexing, Digital Libraries, Neural Network I. Introduction The continued proliferation of large amounts of digital video has increased demand for true content based indexing and retrieval systems. Traditionally, content has been indexed primaril...
IR
viegas00collections
Collections - Adapting The Display of Personal Objects for Different Audiences Although current networked systems and online applications provide new opportunities for displaying and sharing personal information, they do not account for the underlying social  contexts that frame such interactions. Existing categorization and management mechanisms for digital content have been designed to focus on the data they handle without much  regard for the social circumstances within which their content is shared. As we share large collections of personal information over mediated environments, our tools need to account  for the social scenarios that surround our interactions.  This thesis presents Collections: an application for the management  of digital pictures according to their intended audiences.  The goal is to create a graphical interface that  supports the creation of fairly complex privacy decisions concerning  the display of digital photographs. Simple graphics  are used to enable the collector to create a wide range of  audience arrangements for her digital pho...
HCI
441654
Optimal Aggregation Algorithms for Middleware Assume that each object in a database has m grades, or scores, one for each of m attributes. ForexamzUan  object can ave a color grade, t at tells ow red it is, and a s ape grade, t at tells ow round it is. For eac attribute, t ere is a sorted list, w ic lists eac object and its grade under t at attribute, sorted by grade ( ig est grade first). Eac object is assigned an overall grade, t at is obtained  bycom`-T  ng t e attribute grades using a  fixedm  notone aggregation function,orcombining rule, suc as mh or average. To determ`h  t e top k objects, t at is, k objects wit t e ig est overall grades, t e naive  algoritm mor  access every object in t e database, to find its grade under eac attribute. Fagin as given an  algoritm  ("Fagin's Algorit mit or FA) t at is mh mh` efficient. For som mm'T` e aggregation functions, FA isoptim  al wit ig probability in t e worst case. We analyze an elegant andrem'-`-  lysim  ple  algoritm  ("t e t res old  algoritm  ", or TA) t at  isoptim  al in am` stronger sense t an FA. We s ow t at TA is essentiallyoptim  al, not just for som mmT-k' aggregation functions, but for all of tem and not just in a ig-probability worst-case sense, but over every database. Unlike FA, w ic requires large buffers (w ose sizemz  grow unboundedly as t e database size grows), TA requires only asmkzz constant-size buffer. TA allows early stopping, w ic yields, in a precise sense,  anapproxim  ate version of t e top k answers. We distinguis two types of access: sorted access (w ere t em`U'  ewaresystem  obtains t e grade of an object insom sorted list by proceeding t roug t e list  sequentiallyfrom  t e top), and  random  access (w ere t e  mzUETh. resystem  requests t e grade of object in a list, and obtains it in one step). We consider t e scenarios w ere  ra...
ML
32197
Active Disks: Programming Model, Algorithms and Evaluation Several application and technology trends indicate that it might be both pro#table and feasible to move computation closer to the data that it processes. In this paper, we evaluate Active Disk architectures which integrate signi #cant processing power and memory into a disk drive and allow application-speci#c code to be downloaded and executed on the data that is being read from #written to# disk. The key idea is to o#oad bulk of the processing to the diskresident processors and to use the host processor primarily for coordination, scheduling and combination of results from individual disks. To program Active Disks, we propose a stream-based programming model which allows disklets to be executed e#ciently and safely. Simulation results for a suite of six algorithms from three application domains #commercial data warehouses, image processing and satellite data processing# indicate that for these algorithms, Active Disks outperform conventional-disk architectures.  1 Introduction  Severa...
DB
fjeld98graspable
Graspable interfaces: Establishing design principles PhD Research Plan for Morten Fjeld. Topic: Design of Tangible User Interfaces
HCI
galhardas00extensible
An Extensible Framework for Data Cleaning Data integration solutions dealing with large amounts of data have been strongly required in the last few years.  Besides the traditional data integration problems (e.g. schema integration, local to global schema mappings),  three additional data problems have to be dealt with: (1) the absence of universal keys across dierent databases  that is known as the object identity problem, (2) the existence of keyboard errors in the data, and (3) the presence  of inconsistencies in data coming from multiple sources. Dealing with these problems is globally called the data  cleaning process. In this work, we propose a framework which oers the fundamental services required by this  process: data transformation, duplicate elimination and multi-table matching. These services are implemented  using a set of purposely designed macro-operators. Moreover, we propose an SQL extension for specifying  each of the macro-operators. One important feature of the framework is the ability of explicitly includ...
DB
roumeliotis99circumventing
Circumventing Dynamic Modeling: Evaluation of the Error-State Kalman Filter applied to Mobile Robot Localization The mobile robot localization problem is treated as a two-stage iterative estimation process. The attitude is estimated first and is then available for position estimation. The indirect (error state) form of the Kalman filter is developed for attitude estimation when applying gyro modeling. The main benefit of this choice is that complex dynamic modeling of the mobile robot and its interaction with the environment is avoided. The filter optimally combines the attitude rate information from the gyro and the absolute orientation measurements. The proposed implementation is independent of the structure of the vehicle or the morphology of the ground. The method can easily be transfered to another mobile platform provided it carries an equivalent set of sensors. The 2D case is studied in detail first. Results of extending the approach to the 3D case are presented. In both cases the results demonstrate the efficacy of the proposed method.  1 Introduction  On July 4th 1997, the Mars Pathfinde...
AI
slivinskas01adaptable
Adaptable Query Optimization and Evaluation in Temporal Middleware Time-referenced data are pervasive in most real-world databases. Recent advances in temporal query languages  show that such database applications may benefit substantially from built-in temporal support in the  DBMS. To achieve this, temporal query optimization and evaluation mechanisms must be provided, either within  the DBMS proper or as a source level translation from temporal queries to conventional SQL. This paper proposes  a new approach: using a middleware component on top of a conventional DBMS. This component accepts  temporal SQL statements and produces a corresponding query plan consisting of algebraic as well as regular SQL  parts. The algebraic parts are processed by the middleware, while the SQL parts are processed by the DBMS.  The middleware uses performance feedback from the DBMS to adapt its partitioning of subsequent queries into  middleware and DBMS parts. The paper describes the architecture and implementation of the temporal middleware  component, termed TANGO, which is based on the Volcano extensible query optimizer and the XXL query  processing library. Experiments with the system demonstrate the utility of the middleware`s internal processing  capability and its cost-based mechanism for apportioning the processing between the middleware and the  underlying DBMS.  Index terms: temporal databases, query processing and optimization, cost-based optimization, middleware  1 
DB
214340
A representation-independent temporal extension of ODMG's Object Query Language TEMPOS is a set of models providing a framework for extending database systems with temporal functionalities. Based on this framework, an extension of the ODMG's object database standard has been defined. This extension includes a hierarchy of abstract datatypes for managing temporal values and histories, as well as temporal extensions of ODMG's object model, schema definition language and query language. This paper focuses on the latter, namely TEMPOQL. With respect to related proposals, the main originality of TEMPOQL is that it allows to manipulate histories regardless of their representations, by composition of functional constructs. Thereby, the abstraction principle of object-orientation is fulfilled, and the functional nature of OQL is enforced. In fact, TEMPOQL goes further in preserving OQL's structure, by generalizing most standard OQL constructs to deal with histories. The overall proposal has been fully formalized both at the syntactical and the semantical level and impleme...
DB
pfoser00novel
Novel Approaches to the Indexing of Moving Object Trajectories The domain of spatiotemporal applications is a treasure trove  of new types of data and queries. However, work in this area  is guided by related research from the spatial and temporal  domains, so far, with little attention towards the true nature  of spatiotemporal phenomena. In this work, the focus is on a  spatiotemporal sub-domain, namely the trajectories of  moving point objects. We present new types of  spatiotemporal queries, as well as algorithms to process  those. Further, we introduce two access methods this kind of  data, namely the Spatio-Temporal R-tree (STR-tree) and the  Trajectory-Bundle tree (TB-tree). The former is an R-tree  based access method that considers the trajectory identity in  the index as well, while the latter is a hybrid structure, which  preserves trajectories as well as allows for R-tree typical  range search in the data. We present performance studies that  compare the two indices with the R-tree (appropriately  modified, for a fair comparison) under a varying set of  spatiotemporal queries, and we provide guidelines for a  successful choice among them.   
DB
wagner01xmlbased
An XML-based Multimedia Middleware for Mobile Online Auctions Pervasive Internet services today promise to provide users with a quick and convenient access  to a variety of commercial applications. However, due to unsuitable architectures and  poor performance user acceptance is still low. To be a major success mobile services have  to provide device-adapted content and advanced value-added Web services. Innovative enabling  technologies like XML and wireless communication may for the first time provide a  facility to interact with online applications anytime anywhere. We present a prototype implementing  an efficient multimedia middleware approach towards ubiquitous value-added  services using an auction house as a sample application. Advanced multi-feature retrieval  technologies are combined with enhanced content delivery to show the impact of modern  enterprise information systems on today's e-commerce applications.  Keywords: mobile commerce, online auctions, middleware architectures, pervasive  Internet technology, multimedia database appli...
IR
du97categorization
Categorization of Software Errors that led to Security Breaches A set of errors known to have led to security breaches in computer systems was analyzed. The analysis led to a categorization of these errors. After examining several proposed schemes for the categorization of software errors a new scheme was developed and used. This scheme classifies errors by their cause, the nature of their impact, and the type of change, or fix, made to remove the error. The errors considered in this work are found in a database maintained by the COAST laboratory. The categorization is the first step in the investigation of the effectiveness of various measures of code coverage in revealing software errors that might lead to security breaches. 1 Introduction  We report the outcome of an effort to categorize errors in software that are known to have led to security breaches. The set of errors used in this study came from a database of errors developed in the COAST laboratory [10]. Several existing schemes for the categorization of software errors were evaluated for ...
Agents
tsap01feedback
Feedback From Video For Virtual Reality Navigation Important preconditions for wide acceptance of virtual reality systems include their comfort, ease and naturalness to use. Most existing trackers suer from discomfortrelated issues. For example, body-based trackers (such as hand controllers, joysticks or helmet attachments) restrict spontaneity and naturalness of motion, whereas groundbased devices (e.g., hand controllers) limit the workspace by literally binding an operator to the ground. Controls have similar problems. This paper describes using real-time video with registered depth information (from a commercially available camera) for virtual reality navigation. A camera-based setup can replace cumbersome trackers. The method includes selective depth processing for increased speed, and a robust skin-color segmentation for handling illumination variations.
HCI
parsons98agents
Agents That Reason and Negotiate By Arguing The need for negotiation in multi-agent systems stems from the requirement  for agents to solve the problems posed by their interdependence upon one another.  Negotiation provides a solution to these problems by giving the agents  the means to resolve their conflicting objectives, correct inconsistencies in their  knowledge of other agents' world views, and coordinate a joint approach to domain  tasks which benefits all the agents concerned. We propose a framework,  based upon a system of argumentation, which permits agents to negotiate in  order to establish acceptable ways of solving problems. The framework provides  a formal model of argumentation-based reasoning and negotiation, details  a design philosophy which ensures a clear link between the formal model and  its practical instantiation, and describes a case study of this relationship for a  particular class of architectures (namely those for belief-desire-intention agents).  1 Introduction  An increasing number of software app...
Agents
reitmayr01mobile
Mobile Collaborative Augmented Reality The combination of mobile computing and collaborative Augmented Reality into a single system makes the power of computer enhanced interaction and communication in the real world accessible anytime and everywhere. This paper describes our work to build a mobile collaborative Augmented Reality system that supports true stereoscopic 3D graphics, a pen and pad interface and direct interaction with virtual objects. The system is assembled from offthe -shelf hardware components and serves as a basic test bed for user interface experiments related to computer supported collaborative work in Augmented Reality. A mobile platform implementing the described features and collaboration between mobile and stationary users are demonstrated. 
HCI
327415
How Developmental Psychology and Robotics Complement Each Other This paper presents two complementary ideas relating  the study of human development and the  construction of intelligent artifacts. First, the use  of developmental models will be a critical requirement  in the construction of robotic systems that  can acquire a large repertoire of motor, perceptual,  and cognitive capabilities. Second, robotic  systems can be used as a test-bed for evaluating  models of human development much in the same  way that simulation studies are currently used  to evaluate cognitive models. To further explore  these ideas, two examples from the author's own  work will be presented: the use of developmental  models of hand-eye coordination to simplify the  task of learning to reach for a visual target and  the use of a humanoid robot to evaluate models  of normal and abnormal social skill development.  Introduction  Research on human development and research on the construction of intelligent artifacts can and should be complementary. Studies of human developm...
ML
336904
Learning to Recognize 3D Objects A learning account for the problem of object recognition is developed within the  PAC (Probably Approximately Correct) model of learnability. The key assumption  underlying this work is that objects can be recognized (or, discriminated) using simple  representations in terms of \syntactically" simple relations over the raw image. Although  the potential number of these simple relations could be huge, only a few of  them are actually present in each observed image and a fairly small number of those  observed is relevant to discriminating an object.  We show that these properties can be exploited to yield an ecient learning approach  in terms of sample and computational complexity, within the PAC model. No  assumptions are needed on the distribution of the observed objects and the learning  performance is quantied relative to its past experience. Most importantly, the success  of learning an object representation is naturally tied to the ability to represent it as a  function of some in...
ML
sato01parameter
Parameter Learning of Logic Programs for Symbolic-statistical Modeling We propose a logical/mathematical framework for statistical parameter learning of parameterized logic programs, i.e. de nite clause programs containing probabilistic facts with a parameterized distribution. It extends the traditional least Herbrand model semantics in logic programming to distribution semantics, possible world semantics with a probability distribution which is unconditionally applicable to arbitrary logic programs including ones for HMMs, PCFGs and Bayesian networks. We also propose a new EM algorithm, the graphical EM algorithm, thatrunsfora class of parameterized logic programs representing sequential decision processes where each decision is exclusive and independent. It runs on a new data structure called support graphs describing the logical relationship between observations and their explanations, and learns parameters by computing inside and outside probability generalized for logic programs. The complexity analysis shows that when combined with OLDT search for all explanations for observations, the graphical EM algorithm, despite its generality, has the same time complexity as existing EM algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside algorithm for PCFGs, and the one for singly connected Bayesian networks that have beendeveloped independently in each research eld. Learning experiments with PCFGs using two corpora of moderate size indicate that the graphical EM algorithm can signi cantly outperform the Inside-Outside algorithm. 1.
ML
powell00impact
The Impact of Database Selection on Distributed Searching Abstract The proliferation of online information resources increases the importance of effective and efficient distributed searching. Distributed searching is cast in three parts – database selection, query processing, and results merging. In this paper we examine the effect of database selection on retrieval performance. We look at retrieval performance in three different distributed retrieval testbeds and distill some general results. First we find that good database selection can result in better retrieval effectiveness than can be achieved in a centralized database. Second we find that good performance can be achieved when only a few sites are selected and that the performance generally increases as more sites are selected. Finally we find that when database selection is employed, it is not necessary to maintain collection wide information (CWI), e.g. global idf. Local information can be used to achieve superior performance. This means that distributed systems can be engineered with more autonomy and less cooperation. This work suggests that improvements in database selection can lead to broader improvements in retrieval performance, even in centralized (i.e. single database) systems. Given a centralized database and a good selection mechanism, retrieval performance can be improved by decomposing that database conceptually and employing a selection step. 1
DB
behringer98improving
Improving the Registration Precision by Visual Horizon Silhouette Matching A system for enhancing the situational awareness in an outdoor  scenario by Augmented Reality (AR) techniques can utilize visual  clues for improving registration precision. If visible, terrain silhouettes  provide unique features to be matched with digital elevation  map (DEM) data. The best match of a visually extracted silhouette  with the DEM silhouette provides camera/observer orientation (elevation  and azimuth). We have developed such a registration system  which runs on a PC (200 MHz) and is being ported to a wearable  AR system.  1 Introduction and Context  1.1 Augmented Reality  In the past years, augmented reality (AR) has gained significant attention due to rapid progress in several key areas (wearable computing, virtual reality rendering)[2]. AR technology provides means of intuitive information presentation for enhancing the situational awareness and perception by exploiting the natural and familiar human interaction modalities with the environment. Completely immersive A...
HCI
abonyi00structure
Structure Identification of Fuzzy Classifiers For complex and high-dimensional problems, data-driven  identification of classifiers has to deal with structural issues like the selection of the relevant features and effective initial partition of the input domain. Therefore, the identification of fuzzy classifiers is a challenging topic. Decision-tree (DT) generation algorithms are effective in feature selection and extraction of crisp classification rules, hence they can be used for the initialization of fuzzy systems. Because fuzzy classifiers have much flexible decision boundaries than DTs, fuzzy models can be more parsimonious than DTs. Hence, to get compact, easily interpretable and transparent classification system, a new structure identification algorithm is proposed, where genetic algorithm (GA) based parameter optimization of the DT initialized fuzzy sets is combined with similarity based rule base simplification algorithms. The performance of the approach is studied on a specially designed artificial data. An application to the Cancer classification problem is also shown.
ML
pantic00expert
Expert System for Automatic Analysis of Facial Expressions This paper discusses our expert system called Integrated System for Facial Expression Recognition (ISFER), which performs recognition and emotional classification of human facial expression from a still full-face image. The system consists of two major parts. The first one is the ISFER Workbench, which forms a framework for hybrid facial feature detection. Multiple feature detection techniques are applied in parallel. The redundant information is used to define unambiguous face geometry containing no missing or highly inaccurate data. The second part of the system is its inference engine called HERCULES, which converts low level face geometry into high level facial actions, and then this into highest level weighted emotion labels.
AI
bollacker99system
A System For Automatic Personalized Tracking of Scientific Literature on the Web We introduce a system as part of the CiteSeer digital library project for automatic tracking of scientific literature that is relevant to a user’s research interests. Unlike previous systems that use simple keyword matching, CiteSeer is able to track and recommend topically relevant papers even when keyword based query profiles fail. This is made possible through the use of a heterogenous profile to represent user interests. These profiles include several representations, including content based relatedness measures. The CiteSeer tracking system is well integrated into the search and browsing facilities of CiteSeer, and provides the user with great flexibility in tuning a profile to better match his or her interests. The software for this system is available, and a sample database is online as a public service.
IR
paradis98virtual
A Virtual Document Interpreter for Reuse of Information . The importance of reuse of information is well recognised for electronic publishing. However, it is rarely achieved satisfactorily because of the complexity of the task: integrating different formats, handling updates of information, addressing document author's need for intuitiveness and simplicity, etc. An approach which addresses these problems is to dynamically generate and update documents through a descriptive definition of virtual documents. In this paper we present a document interpreter that allows gathering information from multiple sources, and combining it dynamically to produce a virtual document. Two strengths of our approach are: the generic information objects that we use, which enables access to distributed, heterogeneous data sources; and the interpreter's evaluation strategy, which permits a minimum of re-evaluation of the information objects from the data sources. Keywords: : Virtual Documents, Information Reuse, Active Documents, Document synthesis. 1. Introducti...
DB
308770
Reclustering of HEP Data in Object-Oriented Databases The Large Hadron Collider (LHC), build at CERN, will enter operation in 2005. The experiments at the LHC will generate some 5 PB of data per year, which are stored in an ODBMS.  A good object clustering on the disk drives will be critical to achieve a high data throughput required by future analysis scenarios. This paper presents a new reclustering algorithm for HEP data that maximizes the read transfer rate for objects contained in multiple overlapping collections. It works by decomposing the stored objects into a number of chunks and rearranging them by means of heuristics solving the traveling salesman problem with Hamming distance. Furthermore experimental results of a prototype are presented.  Keywords: object-oriented databases, scientific databases, object clustering, query optimisation  1 Introduction  The ATLAS experiment [1] at CERN, due to take data in the year 2005 will store approximately 1 PB (10  15  bytes) of data per year. Data taking is expected to last 15 or more yea...
DB
flake01towards
Towards UML-based Analysis and Design of Multi-Agent Systems The visual modeling facilities of the UML do not provide sufficient means to support the design of multi-agent systems. In this paper, we are investigating the development phases of requirements analysis, design, and code generation for multi agent systems. In the requirements analysis phase, we are using extended use case diagrams to identify agents and their relationship to the environment. In the design phase, we are using stereotyped class and object diagrams to model different agent types and their related goals and strategies. While these diagrams define the static agent system architecture, dynamic agent behavior is modeled in statecharts with respect to the BDI  1 agent approach. Concerning code generation, we show how the used diagrams can be taken to generate code for CASA, our executable agent specification language that is integrated into an existing multi-agent framework.  1 
Agents
444527
The Evaluation of Microplanning and Surface Realization in the Generation of Multimodal Acts of Communication In this paper, we describe an application domain which requires the computational simulation of human-human communication in which one of the interlocutors has an expressive communication disorder. The importance and evaluation of a process, called here microplanning and surface realization, for such communicative agents is discussed and a related exploratory study is described.  1 
Agents
moran99improvement
Improvement in a Lazy Context: An Operational Theory for Call-By-Need The standard implementation technique for lazy functional languages is call-by-need, which ensures that an argument to a function in any given call is evaluated at most once. A significant problem with call-by-need is that it is difficult --- even for compiler writers --- to predict the effects of program transformations. The traditional theories for lazy functional languages are based on call-by-name models, and offer no help in determining which transformations do indeed optimize a program. In this article we present an operational theory for callby -need, based upon an improvement ordering on programs:  M is improved by N if in all program-contexts C,  when C[M ] terminates then C[N ] terminates at least as cheaply. We show that this improvement relation satisfies a "context lemma", and supports a rich inequational theory, subsuming the call-by-need lambda calculi of Ariola et al.  [AFM  +  95]. The reduction-based call-by-need calculi are inadequate as a theory of lazy-program tran...
ML
56093
What Sort Of Control System Is Able To Have A Personality? This paper outlines a design-based methodology for the study of mind as a part of the broad discipline of Artificial Intelligence. Within that framework some architectural requirements for human-like minds are discussed, and some preliminary suggestions made regarding mechanisms underlying motivation, emotions, and personality. A brief description is given of the `Nursemaid' or `Minder' scenario being used at the University of Birmingham as a framework for research on these problems. It may be possible later to combine some of these ideas with work on synthetic agents inhabiting virtual reality environments. 1 Introduction: Personality belongs to a whole agent  Most work in AI addresses only cognitive aspects of the design of intelligent agents, e.g. vision and other forms of perception, planning, problem solving, the learning of concepts and generalisations, natural language processing, motor control etc. Only a tiny subset of AI research has been concerned with motivation and emotion...
Agents
marsh00maintaining
Maintaining the Illusion of Interacting Within a 3D Virtual Space It is widely thought to more or less a degree, that a sense of presence may be induced in users of new and emerging media technologies, such as, the Internet, digital television and cinema (supporting interaction), teleconferencing and 3D virtual reality systems. In this paper, it is argued that presence presupposes that participants are absorbed in the illusion of interacting within the visual spaces created by these media. That is, prior to the possibility of any inducement of presence, participants need to be absorbed in the illusion conveyed by the media. Without this, participants' attention is broken and the illusion is lost. Hence, the potential to induce presence in participants ceases. To encourage participants to lose sight of the means of representation and be drawn into the illusion conveyed by these media, this paper proposes the development of design principles to increase participants' experience. In an attempt to inform design principles, this paper focuses on another artificial although highly successful visual medium - film. By way of example, this paper concentrates on one medium, virtual reality, and proposes design principles that attempt to maintain the illusion of interacting within 3D virtual space. This attempts to provide a platform through the resourceful blend of hardware and software Virtual Reality (VR) enabling technologies on which to support a well designed virtual environment and hence, from which the inducement of presence in participants may develop.
HCI
cabrera01proactive
Proactive Detection of Distributed Denial of Service Attacks using MIB Traffic Variables - A Feasibility Study In this paper we propose a methodology for utilizing Network Management Systems for the early detection of Distributed Denial of Service (DDoS) Attacks. Although there are quite a large number of events that are prior to an attack (e.g. suspicious logons, start of processes, addition of new files, sudden shifts in traffic, etc.), in this work we depend solely on information from MIB (Management Information Base) Traffic Variables collected from the systems participating in the Attack. Three types of DDoS attacks were effected on a Research Test Bed, and MIB variables were recorded. Using these datasets, we show how there are indeed MIB-based precursors of DDoS attacks This work was supported by the Air Force Research Laboratory (Rome, NY - USA) under contract F30602-00-C-0126 to Scientific Systems Company, and by Aprisma's University Fellowship Program 1999/2000. 1 that render it possible to detect them before the Target is shut down. Most importantly, we describe how the relevant MI...
DB
fent00logical
Logical Update Queries as Open Nested Transactions . The rule-based update language ULTRA has been designed for the  specification of complex database updates in a modular fashion. The logical semantics  of update goals is based on update request sets, which correspond to  deferred basic updates in the database. The declarative character of the logical  semantics leaves much freedom for various evaluation strategies, among them  a top-down resolution, which can be naturally mapped onto a system of nested  transactions. In this paper, we extend this operational model as follows: Not only  the basic operations are performed and committed independently from the toplevel  transaction, but also complex operations defined by update rules. This leads  to an open nested transaction hierarchy, which allows to exploit the semantical  properties of complex operations to gain more concurrency. On the other hand,  high-level compensation is necessary and meta information must be provided by  the programmer. We present the key elements of this combi...
DB
310158
Naive Bayes and Exemplar-Based approaches to Word Sense Disambiguation Revisited . This paper describes an experimental comparison between two standard supervised learning methods, namely Naive Bayes and Exemplar--based classification, on the Word Sense Disambiguation (WSD) problem. The aim of the work is twofold. Firstly, it attempts to contribute to clarify some confusing information about the comparison between both methods appearing in the related literature. In doing so, several directions have been explored, including: testing several modifications of the basic learning algorithms and varying the feature space. Secondly, an improvement of both algorithms is proposed, in order to deal with large attribute sets. This modification, which basically consists in using only the positive information appearing in the examples, allows to improve greatly the efficiency of the methods, with no loss in accuracy. The experiments have been performed on the largest sense--tagged corpus available containing the most frequent and ambiguous English words. Results show that the Exemplar-based approach to WSD is generally superior to the Bayesian approach, especially when a specific metric for dealing with symbolic attributes is used.
ML
blum01learning
Learning from Labeled and Unlabeled Data using Graph Mincuts Many application domains suffer from not  having enough labeled training data for  learning. However, large amounts of unlabeled  examples can often be gathered  cheaply. As a result, there has been a great  deal of work in recent years on how unlabeled  data can be used to aid classification. We  consider an algorithm based on finding minimum  cuts in graphs, that uses pairwise relationships  among the examples in order to  learn from both labeled and unlabeled data.
IR
codish98widening
Widening Pos for Efficient and Scalable Groundness Analysis of Logic Programs The domain of positive Boolean functions, Pos, is by now well established for the analysis of the variable dependencies that arise within logic programs. Analyses based on Pos that use binary decision diagrams have been shown to be efficient for a wide range of practical programs. However, independent of the representation, assuming that P != NP , an (unwidened) Pos analysis can never come with any efficiency guarantees because of its potentially explosive behaviour. This paper proposes a simple widening for (goal-dependent) groundness analysis of logic programs that guarantees scalability and efficiency. Experimental results indicate that the widening induces only a very small loss of precision.
DB
71076
Formalisms for Multi-Agent Systems This report is the result of a panel discussion at the First UK Workshop on Foundations of Multi-Agent Systems (FoMAS '96). All members of the panel are authors, listed alphabetically. as knowledge representation language, for direct manipulation within an agent system, is exemplified in the work of Konolige on formalisms for modelling belief, and logic as a programming language is evidenced in the work of Fisher on Concurrent METATEM. All of these strands of work can claim some measure of success. However, a common failing of formal work (both in AI and multi-agent systems) is that its role is not clear. Formal agent theories are agent specifications, not only in the sense of providing descriptions and constraints on agent behaviour, but also in the sense that one understands the term `specification' from mainstream software engineering, namely that they provide a base from which to design, implement and verify agent systems. Agents are a natural next step for software engineering; they represent a fundamentally new way of considering complex distributed systems, containing societies of cooperating autonomous components. If we aim to build such systems, then principled techniques will be required for their design and implementation. We aim to assist the development of such systems by providing formalisms and notations that can be used to specify the desirable behaviour of agents and multi-agent systems; a requirement is that we should be able to move in a principled way from specifications of such systems to implementations. The properties identified by using a formalism serve to measure and evaluate implementations of agent systems. Some properties currently seem to be unimplementable, because they deal with an idealised aspect of agency, such as knowledge. Still, t...
Agents
bonchi01data
Data Mining for Intelligent Web Caching The paper presents a vertical application of data warehousing and data mining technology: intelligent web caching. We introduce several ways to construct intelligent web caching algorithms that employ predictive models of web requests; the general idea is to extend the LRU policy of web and proxy servers by making it sensible to web access models extracted from web log data using data mining techniques. Two approaches have been studied in particular, one based on association rules and another based on decision trees. The experimental results of the new algorithms show substantial improvement over existing LRUbased caching techniques, in terms of hit rate, i.e., the fraction of web documents directly retrieved in the cache. We designed and developed a prototypical system, which supports data warehousing of web log data, extraction of data mining models and simulation of the web caching algorithms, around an architecture that integrates the various phases in the knowledge discovery process. The system supports a systematic evaluation and benchmarking of the proposed algorithms with respect to existing caching strategies. 1
DB
griffiths95concept
On Concept Space and Hypothesis Space in Case-Based Learning Algorithms . In order to learn more about the behaviour of case-based reasoners as learning systems, we formalise a simple case-based learner as a PAC learning algorithm. We show that the case-based representation  hCB; oei is rich enough to express any boolean function. We define a family of simple case-based learning algorithms which use a single, fixed similarity measure and we give necessary and sufficient conditions for the consistency of these learning algorithms in terms of the chosen similarity measure. Finally, we consider the way in which these simple algorithms, when trained on target concepts from a restricted concept space, often output hypotheses which are outside the chosen concept space. A case study investigates this relationship between concept space and hypothesis space and concludes that the case-based algorithm studied is a less than optimal learning algorithm for the chosen, small, concept space. 1 Introduction  The performance of a case-based reasoning system [13] will chan...
ML
484682
Intrusion Detection: A Bibliography This document contains more than 600 references, dated from 1980 to 2001. We undoubtedly have forgotten some important citations, either through oversight or ignorance. Moreover, errors may remain in the citations. Thus, we ask for your indulgence and, more importantly, for your help. Send us a note if you nd any errors and let us know of any omissions
Agents
75800
Learning To Locate An Object in 3D Space From A Sequence Of Camera Images This paper addresses the problem of determining an object's 3D location from a sequence of camera images recorded by a mobile robot. The approach presented here allows people to "train" robots to recognize specific objects, by presenting it examples of the object to be recognized. A decision tree method is used to learn significant features of the target object from individual camera images. Individual estimates are integrated over time using Bayes rule, into a probabilistic 3D model of the robot's environment. Experimental results illustrate that the method enables a mobile robot to robustly estimate the 3D location of objects from multiple camera images. 1 INTRODUCTION  In recent years, there has been significant progress in the field of mobile robotics. Applications such as robots that guide blind or mentally handicapped people, robots that clean large office buildings and department stores, robots that assist people in recreational activities, etc., are slowly getting in reach. Man...
ML
388587
Ubiquitous Web Information Agents . This paper gives a brief overview about the AI methods and techniques  we have developed for building ubiquitous web information systems. These methods  from areas of machine learning, logic programming, knowledge representation   and multi-agent systems are discussed in the context of our prototypical information  system MIA. MIA is a web information system for mobile users, who  are equipped with a PDA (Palm Pilot), a cellular phone and a GPS device or cellular  WAP phone. It captures the main issues of ubiquitous computing: location  awareness, anytime information access and PDA technology.  1 Introduction  Nowadays, the biggest but also the most chaotic and unstructured source of information is the World-Wide-Web. Making this immense amount of information available for ubiquitous computing in daily life is a great challenge. Besides hardware issues for wireless ubiquitous computing, that still are to be solved (wireless communication, blue-tooth technologies, wearable computing u...
HCI
yong01cooperative
Cooperative Coevolution of Multi-Agent Systems In certain tasks such as pursuit and evasion, multiple agents need to coordinate their behavior to achieve a common goal. An interesting question is, how can such behavior best be evolved? When the agents are controlled with neural networks, a powerful method is to coevolve them in separate subpopulations, and test together in the common task. In this paper, such a method, called Multi-Agent ESP (Enforced Subpopulations) is presented, and demonstrated in a prey-capture task. The approach is shown more efficient and robust than evolving a single central controller for all agents. The role of communication in such domains is also studied, and shown to be unnecessary and even detrimental if effective behavior in the task can be expressed as role-based cooperation rather than synchronization. 1
ML
deloach01analysis
Analysis and Design using MaSE and agentTool This paper provides an overview of the work being done at the Air Force Institute of Technology on the Multiagent Systems Engineering methodology and the associated agentTool environment. Our research is focused on discovering methods and techniques for engineering practical multiagent systems. It uses the abstraction provided by multiagent systems for developing intelligent, distributed software systems.  
Agents
47625
Combining Labeled and Unlabeled Data with Co-Training We consider the problem of using a large unlabeled sample to boost performance of a learning algorithm when only a small set of labeled examples is available. In particular, we consider a setting in which the description of each example can be partitioned into two distinct views, motivated by the task of learning to classify web pages. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks that point to that page. We assume that either view of the example would be su cient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment amuch smaller set of labeled examples. Speci cally, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm's predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to signi cant improvement of hypotheses in practice. As part of our analysis, we provide new re-
ML
302575
Leveled Commitment Contracts and Strategic Breach In automated negotiation systems for self-interested agents, contracts have traditionallybeen binding. Theydo not accommodate future events. Contingency contracts address this, but are often impractical. As an alternative, we propose le�eled commitment contracts. The level of commitment is set bybreach penalties. To be freed from the contract, an agent simply pays the penalty to the other party. A self-interested agent will be reluctant to breach because the other partymight breach, in which case the former agent is freed from the contract, does not incur a penalty, and collects a penalty from the breacher. We show that, despite such strategic breach, leveled commitment increases the expected payoff to both contract parties and can enable deals that are impossible under full commitment. Asymmetric beliefs are also discussed. Different decommitting mechanisms are introduced and compared. Practical prescriptions for market designers are provided. A contract optimizer is provided on the web. Journal ofEconomic Literature
Agents
shapiro98embodied
Embodied Cassie We have enhanced a computational cognitive agent by embodying it with real and simulated bodies operating in real and simulated worlds. This has allowed us to experiment with various ways that embodiment influences the creation and meaning of the agent's beliefs and other terms in its knowledge base, including: symbol-grounding by perception and action; firstperson privileged knowledge; the representation and use of indexicals; having a personal sense of time; and low-level bodily awareness.  Introduction  We have been engaged in a series of projects in which Cassie, the SNePS cognitive agent (Shapiro & Rapaport 1987; Shapiro 1989; Shapiro & Rapaport 1991; 1992; Shapiro & The SNePS Implementation Group 1998), has been incorporated into a hardware or softwaresimulated cognitive robot. The capabilities of the embodied Cassie have included: input and output in fragments of English; reasoning; performance of primitive and composite acts; and vision. In this paper, I give an overview of the...
AI
landau01atnosferes
ATNoSFERES: a Model for Evolutive Agent Behaviors This paper introduces ATNoSFEERS, a model aimed at designing evolutive and adaptive behaviors for agents or multi-agent systems. We first discuss briefly the main problems raised by classical evolutionary models, which are not intended to produce agents or behaviors but to solve problems. Then we provide detailed explanations about the model we propose and its components. We also show through a simple example how the system works, and give some experimental results. Finally, we discuss the features of our model and propose extensions.
Agents
kushmerick02finitestate
Finite-state approaches to Web information extraction Introduction  An information agent is a distributed system that receives a goal through its user interface, gathers information relevant to this goal from a variety of sources, processes this content as appropriate, and delivers the results to the users. We focus on the second stage in this generic architecture. We survey a variety of information extraction techniques that enable information agents to automatically gather information from heterogeneous sources.  For example, consider an agent that mediates package-delivery requests. To satisfy such requests, the agent might need to retrieve address information from geographic services, ask an advertising service for freight forwarders that serve the destination, request quotes from the relevant freight forwarders, retrieve duties and legal constraints from government sites, get weather information to estimate transportation delays, etc.  Information extraction (IE) is a form of shallow document processing that involves populating 
IR
cunningham01developing
Developing Language Processing Components with GATE (a User Guide) Contents  1 Introduction 3 1.1 How to Use This Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Structure of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.5 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2 How To. . . 14 2.1 Download GATE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2 Install and Run GATE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3 [D,F] Configure GATE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.4 Build GATE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.5 [D,F] Create a New CREOLE Resource . . . . . . . . . . . . . . . . . . . . 18 2.6<F11
IR
brandes01visual
Visual Ranking of Link Structures (Extended Abstract) Methods for ranking World Wide Web resources according to their position in the link structure of the Web are receiving considerable attention, because they provide the first effective means for search engines to cope with the explosive growth and diversification of the Web.
IR
iglesias99survey
A Survey of Agent-Oriented Methodologies . This article introduces the current agent-oriented methodologies. It discusseswhat approacheshave been followed (mainly extending existing objectoriented and knowledge engineering methodologies), the suitability of these approaches for agent modelling, and some conclusions drawn from the survey. 1 Introduction Agent technology has received a great deal of attention in the last few years and, as a result, the industry is beginning to get interested in using this technology to develop its own products. In spite of the different developed agent theories, languages, architectures and the successful agent-based applications, very little work for specifying (and applying) techniques to develop applications using agent technology has been done. The role of agent-oriented methodologies is to assist in all the phases of the life cycle of an agent-based application, including its management. This article reviews the current approaches to the development of an agent-oriented (AO) methodology. ...
Agents
422169
Keys for XML this paper. The proposal extends the key speci  cation of XML Data by allowing one to specify keys in terms of XPath [24] expressions. There are a number of technical problems in connection with XPath. XPath is a relatively complex language in which one can not only move down the document tree, but also sideways or upwards, not to mention that predicates and functions can be embedded as well. The problem with XPath is that questions about equivalence or inclusion of XPath expressions are, as far as the authors are aware, unresolved; and these issues are importantifwewant to reason about keys as wedo  in relational databases. Yet until we know how to determine the equivalence of XPath expressions, there is no general method of saying whether two such speci  cations are equivalent. Another technical issue is value equality. XML Schema restricts equality to text, but the authors have encountered cases in whichkeys are not so restricted. See Section 7.1 for a more detailed discussion. However, the main reason for writing this paper is that none of the existing key proposals address the issue of hierarchical keys, which appear to be ubiquitous in hierarchically structured databases, especially in scienti  c data formats. A top-level key may be used to identify components of a document, and within each component a secondary key is used to identify sub-components, and so on. Moreover, the authors believe that the use of keys for citing parts of a document is suciently important that it is appropriate to consider key speci  cation independently of other proposals for constraining the structure of XML documents
IR
onoda98asymptotic
An asymptotic analysis of AdaBoost in the binary classification case Recent work has shown that combining multiple versions of weak classifiers such as decision trees or neural networks results in reduced test set error. To study this in greater detail, we analyze the asymptotic behavior of AdaBoost type algorithms. The theoretical analysis establishes the relation between the distribution of margins of the training examples and the generated voting classification rule. The paper shows asymptotic experimental results for the binary classification case underlining the theoretical findings. Finally, the relation between the model complexity and noise in the training data, and how to improve AdaBoost type algorithms in practice are discussed. 1 Introduction  An ensemble is a collection of neural networks or other types of classifiers (predictors) that are trained for the same task. Boosting and other ensemble learning methods have been used recently with great success for several applications, e. g. OCR [6, 4]. In this work we investigate the functioning o...
ML
26655
Hierarchical Models for Screening of Iron Deficiency Anemia We investigate the problem of classifying individuals based on estimated density functions for each individual. The problem is similar to conventional classification in that there is labelled training data, but different in that the underlying measurements are not feature vectors but histograms or density estimates. We describe a general framework based on probabilistic hierarchical models for modelling such data and illustrate how the model lends itself to classification. We contrast this approach with two other alternatives: (1) directly defining distance between densities using a cross-entropy distance measure, and (2) using parameters of the estimated densities as feature vectors for a standard discriminative classification framework. We evaluate all three methods on a realworld medical diagnosis problem. The hierarchical modeling and density-distance approaches are most accurate, yielding crossvalidated error rates in the range of 1 to 2%. We conclude by discussing the relative me...
ML
fernyhough99constructing
Constructing Qualitative Event Models Automatically from Video Input We describe an implemented technique for generating event models automatically based on qualitative reasoning and a statistical analysis of video input. Using an existing tracking program which generates labelled contours for objects in every frame, the view from a fixed camera is partitioned into semantically relevant regions based on the paths followed by moving objects. The paths are indexed with temporal information so objects moving along the same path at different speeds can be distinguished. Using a notion of proximity based on the speed of the moving objects and qualitative spatial reasoning techniques, event models describing the behaviour of pairs of objects can be built, again using statistical methods. The system has been tested on a traffic domain and learns various event models expressed in the qualitative calculus which represent human observable events. The system can then be used to recognise subsequent selected event occurrences or unusual behaviours. 1 Introduction D...
ML
mandl01topographic
Topographic Maps Based on Kohonen Self Organizing Maps An Empirical Approach Two-dimensional maps are a valuable interface element for the visualization of information retrieval results or other large sets of objects. Various methods exist for the creation of these maps. This article describes a comparative evaluation of topographic maps based on "Kohonen Self Organizing Maps" (SOM). These results show that the mapping method has to be chosen very carefully and different methods should be tested for an application. KEYWORDS: self organizing maps, Kohonen maps, topic maps, 2D maps, evaluation, information retrieval, information visualization
HCI
315280
Planned Disconnections for Mobile Databases As mobility permeates todays computing environment, we envision application infrastructures that will increasingly use mobile technologies. Traditional database applications will need to integrate mobile entities: people and computers. In this paper, we develop a distributed database framework for mobile environments. A key requirement in such an environment is to support frequent connection and disconnection of database sites.  1 Introduction  As mobility permeates into todays computing and communication arena, we envision application infrastructures that will increasingly rely on mobile technologies. Current mobility applications tend to have a large central server and use mobile platforms only as caching devices. We want to elevate the role of mobile computers to first class entities in the sense that they allow the mobile user work/update capabilities independent of a central server. In such an environment, several mobile computers may collectively form the entire distributed syste...
DB
streitz01roomware
Roomware: Towards the next generation of human-computer interaction based on an integrated design of real and virtual worlds In the past, a central mainframe computer provided terminals for many users. In the current age of the personal desktop computer, there is one computer for one person. Observation of early adopters and predictions about the future point to an era where each person will have multiple devices and computational power will be ubiquitous. Against this background, we present a vision for the workspaces of the future and a user-centered approach for an integrated design of virtual information spaces and real architectural spaces. The resulting environments are called cooperative buildings. The design approach is based on the roomware concept. By roomware, we mean computer-augmented objects resulting from the integration of room elements, e.g., walls, doors, furniture (tables, chairs, etc.) with computer-based information devices. They are part of the vision that the world around us will be the interface to information -- where the computer as a device will disappear and people's interaction w...
HCI
kon00secure
Secure Dynamic Reconfiguration of Scalable CORBA Systems with Mobile Agents . As various Internet services, electronic commerce, and information and communication systems permeate our lives, their continual availability becomes a dominant issue. But continuing software evolution requires system recon guration. Running systems must upgrade their components or change their con guration parameters. In addition, Internet services often need to serve thousands or millions of users. This scenario raises three conicting issues that need to be resolved: availability, con gurability, and scalability. We propose the use of mobile recon guration agents for the ecient, secure, and scalable dynamic recon guration of Internet systems. In our system, the agents are deployed within the object-oriented architecture of a CORBA-compliant ORB that supports safe recon guration both of its middleware engine and of user applications. Using a graphical front-end, administrators build recon guration agents and specify the topology of the recon guration network the agents traverse to recon gure the distributed system on-the-y. The agents collect, group, and return the results of inspection and recon guration operations in the distributed system to the administrator. 1
Agents
hatzilygeroudis01hymes
HYMES: A HYbrid Modular Expert System with Efficient Inference and Explanation A HYbrid Modular Expert System, called HYMES, is presented. HYMES provides a dual representation scheme: a symbolic one, based on conventional symbolic rules, and a hybrid one, based on neumles, a kind of rules that combine a symbolic and a connectionist representation. Symbolic rules are internally converted into neumles, for efficiency reasons. In this way, hybrid modular knowledge bases can be constructed.
AI
532128
An Approach to Relate the Web Communities Through Bipartite Graphs The Web harbors a large number of community structures. Early detection of community structures has many purposes such as reliable searching and selective advertising. In this paper we investigate the problem of extracting and relating the web community structures from a large collection of Web-pages by performing hyper-link analysis. The proposed algorithm extracts the potential community signatures by extracting the corresponding dense bipartite graph (DBG) structures from the given data set of web pages. Further, the proposed algorithm can also be used to relate the extracted community signatures. We report the experimental results conducted on 10 GB TREC (Text REtrieval Conference) data collection that contains 1.7 million pages and 21.5 million links. The results demonstrate that the proposed approach extracts meaningful community signatures and relates them.
IR
267965
Discovery of Similarity Computations of Search Engines Two typical situations in which it is of practical interest to determine the similarities of text documents to a query due to a search engine are: (1) a global search engine, constructed on top of a group of local search engines, wishes to retrieve the set of local documents globally most similar to a given query; and (2) an organization wants to compare the retrieval performance of search engines. The dot-product function is a widely used similarity function. For a search engine using such a function, we can determine its similarity computations if how the search engine sets the weights of terms is known, which is usually not the case. In this paper, techniques are presented to discover certain mathematical expressions of these formulas and the values of embedded constants when the dot-product similarity function is used. Preliminary results from experiments on the WebCrawler search engine are given to illustrate our techniques.  1  Categories and Subject Descriptors  H.3 [Information...
IR
scerri99ease
The EASE Actor Development Environment In interactive simulations it is often desirable to have intelligent actors  playing the roles of humans. Drawing on a wide range of previous  work this paper presents a system that is intended to reduce some of the  diculties involved in the development of actors. We present a system  called EASE (End-user Actor Specication Environment) that provides  tools and methods to support end user development of intelligent actors.  The tools support the whole development process from design to testing.  The EASE actor architecture is a multi-agent system where a process of  contract making and negotiation between agents determines the actions  of the actor.  1 Introduction  In modern, complex, interactive simulations it is often highly desirable to have intelligent actors playing the roles of humans. The actors' task is dicult { sensing the (simulated) environment, choosing a course of action that exibly and intelligently follows designer intentions and sending appropriate commands back to ...
Agents
140169
Modeling Emotion-Based Decision-Making This paper presents a computational approach to EmotionBased Decision-Making that models important aspects of emotional processing and integrates these with other models of perception, motivation, behavior, and motor control. A particular emphasis is placed on using some of the mechanisms of emotions as building blocks for the acquisition of emotional memories that serve as biasing signals during the process of making decisions and selecting actions. We have successfully followed this approach to develop and control several different autonomous agents, including both synthetic agents and physical robots.  Introduction  Most theories of human reasoning and decision-making fall between two different positions. The first one argues that we make decisions in a way similar to that of solving problems in formal logic. According to this view, when faced with a problem, we form a list of all different options and their possible outcomes, and then we use logic in its best sense to perform a cos...
Agents
bertino98definition
Definition And Analysis Of Index Organizations For Object-Oriented Database Systems The efficient execution of queries in object-oriented databases requires the design of specific indexing techniques, to efficiently deal with predicates against nested attributes or against class inheritance hierarchies. Indexing techniques so far proposed can be classified into three groups: inheritance indexing techniques, whose goal is to support queries along inheritance hierarchies; aggregation indexing techniques, dealing with the efficient evaluation of nested predicates; integrated techniques. The aim of this paper is to analyze two techniques providing an integrated support, the path index and the nested-inherited index, with respect to traditional techniques, such as the multi-index and the inherited multi-index. The analysis is performed assuming that multi-valued attributes, as well as instances with null attribute values, are present in the database. For this purpose, the paper first presents the considered techniques. An extension of the path index, firstly defined in [6]...
DB
328087
Incremental Document Clustering for Web Page Classification Motivated by the benefits in organizing the documents in Web search engines, we  consider the problem of automatic Web page classification. We employ the clustering  techniques. Each document is represented by a feature vector. By analyzing the  clusters formed by these vectors, we can assign the documents within the same  cluster to the same class automatically. Our contributions are the following: (1)  We propose a feature extraction mechanism which is more suitable to Web page  classification. (2) We introduce a tree structure called the DC-tree to make the  clustering process incremental and less sensitive to the document insertion order.  (3) We show with experiments on a set of Internet documents from Yahoo! that the  proposed clustering algorithm can classify Web pages effectively.  Keywords: Incremental update, Tree, Document, Clustering, Web, Classification  0  1 Introduction  The popularity of the Internet has caused a continuous massive increase in the amount of Web pages (o...
IR
anastasi99reliable
A Reliable Multicast Protocol for Distributed Mobile Systems: Design and Evaluation Abstract Reliable multicast is a powerful communication primitive for structuring distributed programs in which multiple processes must closely cooperate together. In this paper we propose a protocol for supporting reliable multicast in a distributed system that includes mobile hosts and evaluate the performance of our proposal through simulation. We consider a scenario in which mobile hosts communicate with a wired infrastructure by means of wireless technology. Our proposal provides several novel features. The sender of each multicast may select among three increasingly strong delivery ordering guarantees: FIFO, Causal, Total. Movements do not trigger the transmission of any message in the wired network as no notion of hand-off is used. The set of senders and receivers (group) may be dynamic. Size of data structures at mobile hosts, size of message headers, number of messages in the wired network for each multicast, are all independent on the number of group members. The wireless network is assumed to provide only incomplete spatial coverage and message losses could occur even within cells. Movements are not negotiated and a mobile host that leaves a cell may enter any other cell, perhaps after a potentially long disconnection. The simulation results show that the proposed protocol has good performance and good scalability properties. 1
HCI
cussens99loglinear
Loglinear Models for First-Order Probabilistic Reasoning Recent work on loglinear models in probabilistic constraint logic programming is applied to first-order probabilistic reasoning. Probabilities are defined directly on the  proofs of atomic formulae, and by marginalisation on the atomic formulae themselves. We use Stochastic Logic Programs (SLPs) composed of labelled and unlabelled definite clauses to define the proof probabilities. We have a conservative extension of first-order reasoning, so that, for example, there is a one-one mapping between logical and random variables. We show how, in this framework, Inductive Logic Programming (ILP) can be used to induce the features of a loglinear model from data. We also compare the presented framework with other approaches to first-order probabilistic reasoning. Keywords: loglinear models, constraint logic programming, inductive logic programming  1 Introduction  A framework which merges first-order logical and probabilistic inference in a theoretically sound and applicable manner promises ma...
AI
509713
Discovering Seeds of New Interest Spread from Premature Pages Cited by Multiple Communities The World Wide Web is a great source of new topics significant  for trend birth and creation. In this paper, we propose a method for  discovering topics, which stimulate communities of people into earnest  communications on the topics' meaning, and grow into a trend of popular  interest. Here, the obtained are web pages which absorb attentions of  people from multiple interest-communities. It is shown by a experiments  to a small group of people, that topics in such pages can trigger the  growth of peoples' interests, beyond the bounds of existing communities.
IR
park99costbased
A Cost-Based Object Buffer Replacement Algorithm for Object-Oriented Database Systems Many object-oriented database systems manage object buffers to provide fast access to objects. Traditional buffer replacement algorithms based on fixed-length pages simply assume that the cost incurred by operating a buffer is proportional to the number of buffer faults. However, this assumption no longer holds in an object buffer where objects are of variable-lengths and the cost of replacing an object varies for each object. In this paper, we propose a cost-based replacement algorithm for object buffers. The proposed algorithm replaces the objects that have minimum costs per unit time and unit space. The cost model extends the previous page-based one to include the replacement costs and the sizes of objects. The performance tests show that the proposed algorithm is almost always superior to the LRU-2 algorithm and in some cases is more than twice as fast. The idea of cost-based replacement can be applied to any buffer management architectures that adopt earlier algorithms. It is espe...
DB
38848
An Agent-Based Approach for Manufacturing Integration - The CIIMPLEX Experience The production management system used by most manufacturers today is comprised of disconnected planning and execution processes, and lacks the support for interoperability and collaboration needed for enterprise-wide integration. This situation often prevents the manufacturer from fully exploring market opportunities in a timely fashion. To address this problem, we are exploring an agent-based approach to intelligent enterprise integration. In this approach, a set of agents with specialized expertise can be quickly assembled to help with the gathering of relevant information and knowledge, to cooperate with each other and with other parts of the production management system and humans to arrive at timely decisions in dealing with various enterprise scenarios. The proposed multi-agent system, including its architecture and implementation, are presented and demonstrated through an example integration scenario involving real planning and execution software systems.  1. Introduction  The p...
Agents
448486
A Web Odyssey: from Codd to XML INTRODUCTION  The Web presents the database area with vast opportunities and commensurate challenges. Databases and the Web are organically connected at many levels. Web sites are increasingly powered by databases. Collections of linked Web pages distributed across the Internet are themselves tempting targets for a database. The emergence of XML as the lingua franca of the Web brings some much needed order and will greatly facilitate the use of database techniques to manage Web information.  This paper will discuss some of the developments related to the Web from the viewpoint of database theory. As we shall see, the Web scenario requires revisiting some of the basic assumptions of the area. To be sure, database theory remains as valid as ever in the classical setting, and the database industry will continue to representamulti-billion dollar target of applicability for the foreseeable future. But the Web represents an opportunityofanentirely di#erent scale. We are th
DB
ferhatosmanoglu01constrained
Constrained Nearest Neighbor Queries In this paper we introduce the notion of constrained nearest neighbor queries (CNN) and propose a  series of methods to answer them. This class of queries can be thought of as nearest neighbor queries with  range constraints. Although both nearest neighbor and range queries have been analyzed extensively in  previous literature, the implications of constrained nearest neighbor queries have not been discussed. Due  to their versatility, CNN queries are suitable to a wide range of applications from GIS systems to reverse  nearest neighbor queries and multimedia applications. We develop methods for answering CNN queries  with different properties and advantages. We prove the optimality (with respect to I/O cost) of one of the  techniques proposed in this paper. The superiority of the proposed technique is shown by a performance  analysis.  1 Introduction  Two dimensional range queries are used frequently in various applications such as spatial databases [Sam89,  GG98] and Geographic Infor...
IR
wermter01emergent
Emergent Neural Computational Architectures based on Neuroscience Present approaches for computing do not have the performance, flexibility and reliability of neural information processing systems. In order to overcome this, conventional computing systems could benefit from various characteristics of the brain such as modular organisation, robustness, timing and synchronisation, and learning and memory storage in the central nervous system. This overview incorporates some of the key research issues in the field of biologically inspired computing systems.
Agents
lawrence98context
Context and Page Analysis for Improved Web Search NEC Research Institute has developed a metasearch engine that improves the efficiency of Web searches by downloading and analyzing each document and then displaying results that show the query terms in context. Several popular and useful search engines such as AltaVista, Excite, HotBot, Infoseek, Lycos, and Northern Light attempt to maintain full-text indexes of the World Wide Web. However, relying on a single standard search engine has limitations. The standard search engines have limited coverage, 1,2 outdated databases, and are sometimes unavailable due to problems with the network or the engine itself. The precision of standard engine results can also vary because they generally focus on handling queries quickly and use relatively simple ranking schemes. 3 Rankings can be further muddled by keyword spamming to increase a page's rank order. Often, the relevance of a particular page is obvious only after loading it and finding the query terms. Metasearch engines, such as MetaCrawler and SavvySearch, attempt to contend with the problem of limited coverage by submitting queries to several standard search engines at once. 4,5 The primary advantages of metasearch engines are that they combine the results of several search engines and present a consistent user interface. 5 However, most metasearch engines rely on the documents and summaries returned by standard search engines and so inherit their limited precision and vulnerability to keyword spamming. We developed the NEC Research Institute (NECI) metasearch engine [now called Inquirus] to improve the efficiency and precision of Web search by downloading and analyzing each document and then displaying results that show the query terms in
IR
ardissono01tailoring
Tailoring the Interaction with Users in Web Stores . We describe the user modeling and personalization techniques adopted in SETA, a prototype toolkit for the construction of adaptive Web stores which customize the interaction with users. The Web stores created using SETA suggest the items best fitting the customers' needs and adapt the layout and the description of the store catalog to their preferences and expertise.  SETA uses stereotypical information to handle the user models and applies personalization rules to dynamically generate the hypertextual pages presenting products. The system adapts the graphical aspect, length and terminology used in the descriptions to parameters like the user's receptivity, expertise and interests. Moreover, it maintains a model associated with each person the goods are selected for; in this way, multiple criteria can be applied for tailoring the selection of items to the preferences of their beneficiaries.  Keywords: user modeling, personalized information presentation, customization of Web stores, ...
IR
debenham99multiagent
A Multi-Agent System for Emergent Process Management A multi-agent system manages emergent business processes.  The agents in this system all have the same generic  architecture. The generic agent architecture is a three-layer  BDI, hybrid, multi-agent architecture. The architecture copes  with plans whose goals develop and mutate. The agents in the  system choose their course of action on the basis of estimates  of the likelihood of a choice leading to success, and on  estimates of the time, cost and value of making a choice.  1 Introduction  Emergent processes are business processes; they are distinct from production workflows [1]. Emergent processes are opportunistic in nature whereas production workflows are routine. Emergent processes are inherently distributed and involve asynchronous parallel work. What amounts to a "satisfactory conclusion" of an emergent process is not generally known until the process is well advanced. Further, the tasks involved in an emergent process are typically not predefined and emerge as the process deve...
Agents
442160
Goal Directed Adaptive Behavior in Second-Order Neural Networks: Leaning and Evolving in the MAXSON architecture The paper presents a neural network architecture (MAXSON) based on second-order connections that can learn a multiple goal approach/avoid task using reinforcement from the environment. It also enables an agent to learn vicariously, from the successes and failures of other agents. The paper shows that MAXSON can learn certain spatial navigation tasks much faster than traditional Q-learning, as well as learn goal directed behavior, increasing the agent's chances of long-term survival. The paper shows that an extension of MAXSON (V-MAXSON) enables agents to learn vicariously, and this improves the overall survivability of the agent population.
ML
kowalski98executing
Executing Suspended Logic Programs . We present an extension of Logic Programming (LP) which, in addition to ordinary LP clauses, also includes integrity constraints, explicit representation of disjunction in the bodies of clauses and in goals, and suspension of atoms as in concurrent logic languages. The resulting framework aims to unify Constraint Logic Programming (CLP), Abductive Logic Programming (ALP) and Semantic Query Optimisation (SQO) in deductive databases. We present a proof procedure for the new framework, simplifying and generalising previously proposed proof procedures for ALP. We discuss applications of the framework, formulating traditional problems from LP, ALP, CLP and SQO. Keywords: Logic Programming (LP), Constraint Logic Programming (CLP), Abductive Logic Programming (ALP), Semantic Query Optimisation (SQO) in Deductive Databases.   The second author is supported by the EPSRC project "Logic-based multi-agent systems". The third author is supported by ONR grant N00014-96-1-1057. The authors are grat...
DB
tong00restricted
Restricted Bayes Optimal Classifiers We introduce the notion of restricted Bayes optimal classifiers. These classifiers attempt to combine the flexibility of the generative approach to classification with the high accuracy associated with discriminative learning. They first create a model of the joint distribution over class labels and features. Instead of choosing the decision boundary induced directly from the model, they restrict the allowable types of decision boundaries and learn the one that minimizes the probability of misclassification relative to the estimated joint distribution. In this paper, we investigate two particular instantiations of this approach. The first uses a non-parametric density estimator — Parzen Windows with Gaussian kernels — and hyperplane decision boundaries. We show that the resulting classifier is asymptotically equivalent to a maximal margin hyperplane classifier, a highly successful discriminative classifier. We therefore provide an alternative justification for maximal margin hyperplane classifiers. The second instantiation uses a mixture of Gaussians as the estimated density; in experiments on real-world data, we show that this approach allows data with missing values to be handled in a principled manner, leading to improved performance over regular discriminative approaches.
IR
tan01infocockpit
The Infocockpit: Providing Location and Place to Aid Human Memory Our work focuses on building and evaluating computer system interfaces that make information memorable. Psychology research tells us people remember spatially distributed information based on its location relative to their body, as well as the environment in which the information was learned. We apply these principles in the implementation of a multimodal prototype system, the Infocockpit (for "Information Cockpit"). The Infocockpit not only uses multiple monitors surrounding the user to engage human memory for location, but also provides ambient visual and auditory displays to engage human memory for place. We report a user study demonstrating a 56% increase in memory for information presented with our Infocockpit system as compared to a standard desktop system.
HCI
521000
Discovering Fuzzy Classification Rules with Genetic Programming and Co-Evolution In essence, data mining consists of extracting knowledge from  data. This paper proposes a co-evolutionary system for discovering fuzzy  classification rules. The system uses two evolutionary algorithms: a genetic  programming (GP) algorithm evolving a population of fuzzy rule sets and a  simple evolutionary algorithm evolving a population of membership function  definitions. The two populations co-evolve, so that the final result of the coevolutionary  process is a fuzzy rule set and a set of membership function  definitions which are well adapted to each other. In addition, our system also  has some innovative ideas with respect to the encoding of GP individuals representing  rule sets. The basic idea is that our individual encoding scheme incorporates  several syntactical restrictions that facilitate the handling of rule  sets in disjunctive normal form. We have also adapted GP operators to better  work with the proposed individual encoding scheme.
ML
hindriks00architecture
Architecture for Agent Programming Languages .  As the field of agent-based systems continues to expand rapidly, one of the most significant problems lies in being able to compare and evaluate the relative benefits and disadvantages of different systems. In part, this is due to the various different ways in which these systems are presented. One solution is to develop a set of architectural building blocks that can be used as a basis for further construction (to avoid re-inventing wheels), and to ensure a strong and effective, yet simple and accessible, means of presentation that allows for comparison and analysis of agent systems. In this paper, we address this issue in providing just such an architectural framework by using the 3APL agent programming language as a starting point for identification and specification of more general individual agent components. This provides three additional benefits: it moves the work further down the road of implementation, contributes to a growing library of agent techniques and features, and allows a detailed comparison of different agent-based systems specified in similar ways.  1 
Agents
olson99subpixel
Subpixel Localization and Uncertainty Estimation Using Occupancy Grids We describe techniques for performing mobile robot localization using occupancy grids that allow subpixel localization and uncertainty estimation in the pixelized pose space. The techniques are based on a localization method where matching is performed between the visible landmarks at the current robot position and a previously generated map of the environment. A likelihood function over the space of possible robot positions is formulated as a function of the probability distribution for the map matching error. Subpixel localization and uncertainty estimation are performed by fitting the likelihood function with a parameterized surface. The performance of the method is analyzed using synthetic experiments and an example is given using the Rocky 7 Mars rover prototype.  1 Introduction  Localization is a critical issue in mobile robotics. If the robot does not know where it is, it cannot effectively plan movements, locate objects, or reach goals. It is important to not only perform accur...
AI
458630
Slow Technology - Designing for Reflection Ascomputex are increcrxUkD wove intothe fabric ofexC[[OP life inteOPxC( dete mayhave tochange -- from creCOUC only fast and e ficiej tools tobe use during alimite time inspej(O situations, to crexPk1 tek1[OxC that surrounds us and the eee is a part of our activitie for longpegxUP oftime We preCUD  slowtechnWOJ,  : adeUD( ageD( for teP[[[Px aime at reU[[jUx andmomekj ofmekDD rek rathe thane ficieP1 inpejOP(xU[1 The aim of this pape is to dekjCC ade[j( philosophy for slowtexCPC(Cx , to discuss gecuss decus principle and to reUjjP some basic issue inintekPxUj dete from a more philosophical point ofvie . We discuss escussx of  son-zz e  andin;`110,HO  art  asinstance of slowtexCOC11x and as exkjCU[ on howthe deUP principle canbe applie inpractice    Keywords    : slow te xUUD[O , deU((OP human-computex inte-comput ubiquitous computing, sonitureiquitous computin  2000Springe-VexUPC Tobe publishe as: Hallns, L. &Re(OPDx J. Slow TekOOPxPP DeeOPx for ReUjD1[x Journal ofPeCkkCj and Ubiquitous Computing.Springe -VexUCj    162  1 
HCI
kendall97application
The Application Of Object- Oriented Analysis To Agent Based Systems Agents are important software abstractions for distributed problem solving and autonomous, pro- active behavior. They have been used in many applications, including manufacturing, enterprise integration, network management, and advanced user interfaces for worldwide web applications. As agent systems become more prevalent, the need arises for software engineering methodologies. The relationships between agents and objects, and the role of object oriented analysis in multiagent system development are discussed here. The approach is illustrated with a case study from discrete parts manufacturing.  1.0 INTRODUCTION  Numerous examples can be found of applications of agent based systems to enterprise integration  31  ; concurrent engineering  5  ; and manufacturing  26  . Many agent based systems have also been developed for network management  31  , scheduling  9  , and advanced user interfaces  28  . These systems exhibit significant advances in distributed problem solving and pro-active ...
HCI
fertig99fuzzy
A Fuzzy Beam-Search Rule Induction Algorithm . This paper proposes a fuzzy beam search rule induction algorithm for the classification task. The use of fuzzy logic and fuzzy sets not only provides us with a powerful, flexible approach to cope with uncertainty, but also allows us to express the discovered rules in a representation more intuitive and comprehensible for the user, by using linguistic terms (such as low, medium, high) rather than continuous, numeric values in rule conditions. The proposed algorithm is evaluated in two public domain data sets. 1 Introduction  This paper addresses the classification task. In this task the goal is to discover a relationship between a goal attribute, whose value is to be predicted, and a set of predicting attributes. The system discovers this relationship by using known-class examples, and the discovered relationship is then used to predict the goal-attribute value (or the class) of unknown-class examples. There are numerous rule induction algorithms for the classification task. However, ...
ML
deweerdt00plan
A Plan Fusion Algorithm for Multi-Agent Systems We introduce an algorithm for cooperative planning in multi-agent systems. The algorithm enables the agents to combine (fuse) their plans in order to increase their joint profits. A computational resources and skills framework is developed for representing the planned activities of an agent under time constraints. Using this resource-skill framework, we present an ecient (polynomial time) algorithm that fuses the plans of a group of agents in such a way that their joint profits improve. The framework and the algorithm are illustrated using a simplified example from the freight transport domain. 1 Introduction Recently, much attention has been given to the topic of cooperation and cooperative planning in multiagent systems. Usually, the starting point for research on this problem is the observation that there exist classes of problems that cannot be solved by a single agent in isolation, but require several agents to work together in an interactive way, coordinating their plans and sharing ...
Agents
carver00methodology
A Methodology for Using Intelligent Agents to provide Automated Intrusion Response This paper proposes a new methodology for adaptive, automated intrusion response (IR) using software agents. The majority of intrusion response systems (IRSs) react to attacks by generating reports or alarms. This introduces a window of vulnerability between when an intrusion is detected and when action is taken to defend against the attack. Research by Cohen indicates that the success of an attack is dependent on the time gap between detection and response. If skilled attackers are given ten hours after they are detected and before a response, they will be successful 80% of the time. At thirty hours, the attacker almost never fails [1]. The proposed methodology addresses this window of vulnerability by providing an automated response to incidents using a heterogeneous collection of software agents. These agents collaborate to protect the computer system against attack and adapt their response tactics until the system administrator can take an active role in the defense of the system.
AI
fjeld98build
BUILD-IT: an intuitive design tool based on direct object manipulation this paper, means human action in a world
HCI
sandholm01algorithm
Algorithm for Optimal Winner Determination in Combinatorial Auctions Combinatorial auctions, i.e. auctions where bidders can bid on com-binations of items, tend to lead to more e cient allocations than tra-ditional auctions in multi-item auctions where the agents ' valuations of the items are not additive. However, determining the winners so as to maximize revenue is NP-complete. First, existing approaches for tackling this problem are reviewed: exhaustive enumeration, dynamic programming, approximation algorithms, and restricting the allow-able combinations. Then we present our search algorithm for optimal winner determination. Experiments are shown on several bid distri-butions. The algorithm allows combinatorial auctions to scale up to signi cantly larger numbers of items and bids than prior approaches to optimal winner determination by capitalizing on the fact that the space of bids is necessarily sparsely populated in practice. The algo-rithm does this by provably su cient selective generation of children in the search tree, by using a secondary search for fast child genera-tion, by heuristics that are accurate and optimized for speed, and by four methods for preprocessing the search space. Patent pending. A highly optimized implementation of the algorithm is available for licensing both for research and commercial purposes. Please contact the author. 1 1
Agents
sheng99graph
A Graph Query Language and Its Query Processing Many new database applications involve querying of graph data. In this paper, we present an objectoriented graph data model, and an OQL like graph query language, GOQL. The data model and the language are illustrated in the application domain of multimedia presentation graphs. We then discuss the query processing techniques for GOQL, more specifically, the translation of GOQL into an operatorbased language, called O-Algebra, extended with operators to deal with paths and sequences. We also discuss different approaches for efficient implementations of algebra operators for paths and sequences. 1 Introduction  Many database applications such as hypertext applications, geographic information systems, world wide web searching, and heterogeneous information integration, etc., require modeling and querying of graph data ([Guti94, GBPV 94, MeMM 96, BDHS 96, AQMWW 96, AM 98, FFKLS 98, LSBBOO98]). In this paper, we present a data model, and an OQL-like query language GOQL, for querying graphs. ...
DB
235466
Supporting Trust in Virtual Communities At any given time, the stability of a community depends on the right balance of trust and distrust. Trust is also the basis of economic activities, making such things as credit agreements, business contracts and customer confidence possible. In real-life, we are faced with increasingly complex decisions due to the increasingly daunting range of options provided by the Internet and uncertainty in the credibility of virtual entities. In short, information overload, increased uncertainty and risk taking a prominent feature of modern living. We as members of society cope with these complexities and uncertainties by relying on a vital social phenomenon, trust, which forms the basis of all social interaction. However, the ability to reason about trust is absent from the virtual medium, making virtual communities of real and artificial agents fragile. This is unsatisfactory as all `virtual' interactions are ultimately human-bound. Therefore we need a trust model to allow artificial agents to ...
Agents
412974
A Taxonomy of Web Agents In this paper we propose a taxonomy of web agents, which encompasses agents that provide a text-based interface to for example information retrieval services as well as avatarembodied guides that help visitors to navigate in virtual environments. Our taxonomy must be regarded as an instrument to delineate targets for research and the realization of prototype applications that demonstrate the usefulness of agent-based intelligence on the Web. In addition, we deploy our agent-taxonomy to establish the implications particular target applications have with respect to software architecture and computational resources.  1. Introduction  There is a lot of interest and work in the research and development of agent technology with applications on the Web. Many types of web agents have been proposed in recent years, which range from domain-dependent agents, like e-commerce agents, information gathering agents, to function-dependent agents, like negotiation agents, cooperating agents. In addition...
Agents
meng99database
Database Query Formation from Natural Language using Semantic Modeling and Statistical Keyword Meaning Disambiguation This paper describes a natural language interface to database systems which is based on the query formation capabilities of a High-level Query Formulator. The formulator relies on the Semantic Graph of the database, which is a model of the data stored in the database. The natural language interface accepts a user input in natural language and extracts the necessary information needed by the formulator. This extraction process is performed using keywords obtained from the Semantic Graph and the database. Because keywords may have several meanings within a given domain, keyword meaning disambiguation is done using a statistical approach which involves comparing vectors of n-grams. N-grams are n contiguous words within a given text of natural language and they are capable of capturing lexical context. Traditionally, natural language interfaces have been heavy with grammars and other knowledge, but have been wide-ranging in functionality. The interface presented in this paper is more porta...
DB
stader98intelligent
Intelligent Support for Enterprise Modelling Enterprise modelling- integrating models of all pertinent aspects of an enterprise- is essential to the management of change in organisations. An integrated view of an organisation provides insight into what aspects may bechanged, how they may be changed, and what the overall e ect of speci c changes will be. AIAI at the University ofEdinburgh has an ongoing research programme which focuses on the use AI techniques to cover the requirements of enterprise modelling and the tools to support it. The AI techniques used in AIAI's programme range from knowledge representation, ontologies and process modelling techniques to visualisation techniques, intelligent work ow and coordination technology. The techniques are combined in an integrated toolset delivered on an agent-based architecture. Part of AIAI's programme is the Enterprise project which has been instrumental in determining the requirements for enterprise modelling and in the development ofan integrated toolset to support it. The results of the Enterprise project show thatwhen combined with task management support, enterprise models may directly control the operation of an organisation. Based on the results of the Enterprise project, AIAI's TBPM project currently addresses coordination issues of enterprise modelling support. In this paper, we rst describe the requirements for enterprise modelling and enactment in general. We then discuss the Enterprise Toolset which was designed and was implemented to address these requirements. Finally, weevaluate the toolset and describe extensions that are currently being undertaken. AIAI-TR-220/0.1 Page 1 1
AI
schuldt99transactional
Transactional Coordination Agents for Composite Systems Composite systems are collections of autonomous, heterogeneous, and distributed software applications. In these systems, data dependencies are continuously violated by local operations and therefore, coordination processes are necessary to guarantee overall correctness and consistency. Such coordination processes must be endowed with some form of execution guarantees, which require the participating subsystems to have certain database functionality (such as atomicity of local operations, order-preservation and either compensation of operations or the deferment of their commit). However, this functionality is not present in many applications and must be implemented by a transactional coordination agent coupled with the application. In this paper, we discuss the requirements to be met by the applications and their associated transactional coordination agents. We identify a minimal set of functionality the applications must provide in order to participate in transactional coordination pro...
DB
rousseau97improvisational
Improvisational Synthetic Actors with Flexible Personalities We provide synthetic agents as intelligent actors that can improvise their behaviors in interactive environments without detailed planning, just as human improvisers do. Their behavior is based on the directions they receive: high-level scenarios, user commands, and personalities of the characters portrayed. We propose a social-psychological model that enables an author to define a  character's personality influenced by moods and interpersonal relationships. Using examples of characters and experiments with users, we show how such a model can be exploited by synthetic actors to produce performances that are theatrically interesting, believable, and diverse. Keywords: synthetic actors, personality, improvisation, believability, entertainment.  1. Introduction  Personality is the set of psychological traits that distinguish an individual from all others and characterize his or her behavior (Hayes-Roth et al. 1996). Such traits can easily be recognized by others, and people are commonly ...
Agents
jordan98introduction
An Introduction to Variational Methods for Graphical Methods . This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.
ML
19017
eMediator: a Next Generation Electronic Commerce Server This paper presents eMediator, an electronic commerce server prototype that demonstrates ways in which algorithmic support and game-theoretic incentive engineering can jointly improve the efficiency of ecommerce. eAuctionHouse, the configurable auction server, includes a variety of generalized combinatorial auctions and exchanges, pricing schemes, bidding languages, mobile agents, and user support for choosing an auction type. We introduce two new logical bidding languages for combinatorial markets: the XOR bidding language and the OR-of-XORs bidding language. Unlike the traditional OR bidding language, these are fully expressive. They therefore enable the use of the Clarke-Groves pricing mechanism for motivating the bidders to bid truthfully. eAuctionHouse also supports supply/demand curve bidding. eCommitter, the leveled commitment contract optimizer, determines the optimal contract price and decommitting penalties for a variety of leveled commitment contracting mechanisms, taking into account that rational agents will decommit strategically in Nash equilibrium. It also determines the optimal decommitting strategies for any given leveled commitment contract. eExchangeHouse, the safe exchange planner, enables unenforced anonymous exchanges by dividing the exchange into chunks and sequencing those chunks to be delivered safely in alternation between the buyer and the seller.
Agents
505452
Three New Algorithms for Projective Bundle Adjustment with minimal parameters Bundle adjustment is a technique used to compute the maximum likelihood estimate of structure and motion from image feature correspondences. It practice, large non-linear systems have to be solved, most of the time using an iterative optimization process starting from a sub-optimal solution obtained by using linear methods. The behaviour, in terms of convergence, and the computational cost of this process depend on the parameterization used to represent the problem, i.e. of structure and motion.
AI
roth01programming
Programming Satan's Agents Mobile agent security is still a young discipline and most naturally, the focus up to the time of writing was on inventing new cryptographic protocols for securing various aspects of mobile agents. However, past experience shows that protocols can be flawed, and flaws in protocols can remain unnoticed for a long period of time. The game of breaking and fixing protocols is a necessary evolutionary process that leads to a better understanding of the underlying problems and ultimately to more robust and secure systems. Although, to the best of our knowledge, little work has been published on breaking protocols for mobile agents, it is inconceivable that the multitude of protocols proposed so far are all flawless. As it turns out, the opposite is true. We identify flaws in protocols proposed by Corradi et al., Karjoth et al., and Karnik et al., including protocols based on secure coprocessors.
Agents
mirmehdi01extracting
Extracting Low Resolution Text with an Active Camera for OCR Reading text in any scene is useful in the context of wearable computing, robotic vision or as an aid for visually handicapped people. Here, we present a novel automatic text reading system using an active camera focused on text regions already located in the scene (using our recent work). A region of text found is analysed to determine the optimal zoom that would foveate onto it. Then a number of images are captured over the text region to reconstruct a high-resolution mosaic of the whole region. This magnified image of the text is good enough for reading byhumans or for recognition by OCR. Even with a low resolution camera we obtained very good results.
HCI
53191
Layered Learning This paper presents layered learning, a hierarchical  machine learning paradigm. Layered  learning applies to tasks for which learning  a direct mapping from inputs to outputs  is intractable with existing learning algorithms.  Given a hierarchical task decomposition  into subtasks, layered learning seamlessly  integrates separate learning at each  subtask layer. The learning of each subtask  directly facilitates the learning of the next  higher subtask layer by determining at least  one of three of its components: (i) the set of  training examples; (ii) the input representation;  and/or (iii) the output representation.  We introduce layered learning in its domainindependent  general form. We then present  a full implementation in a complex domain,  namely simulated robotic soccer.  1. Introduction  Machine learning (ML) algorithms select a hypothesis from a hypothesis space based on a set of training examples such that the chosen hypothesis is predicted to characterize unseen examples...
ML
aslam98static
Static and Dynamic Information Organization with Star Clusters In this paper we present a system for static and dynamic information organization and show our evaluations of this system on TREC data. We introduce the off-line and on-line star clustering algorithms for information organization. Our evaluation experiments show that the off-line star algorithm outperforms the single link and average link clustering algorithms. Since the star algorithm is also highly efficient and simple to implement, we advocate its use for tasks that require clustering, such as information organization, browsing, filtering, routing, topic tracking, and new topic detection.
DB
glover01improving
Improving Category Specific Web Search by Learning Query Modifications A user searching for documents within a specific category using a general purpose search engine might have a difficult time finding valuable documents. To improve category specific search, we show that a trained classifier can recognize pages of a specified category with high precision by using textual content, text location, and HTML structure. We show that query modifications to web search engines increase the probability that the documents returned are of the specific category.  We evaluate the effectiveness of several query modifications on real search engines, showing that the approach is highly effective for locating personal homepages and calls for papers.  1: Introduction  Typical web search engines index millions of pages across a variety of categories, and return results ranked by expected topical relevance. Only a small percentage of these pages may be of a specific category, for example, personal homepages, or calls for papers. A user may examine large numbers of pages abou...
IR
cugola01peerware
PeerWare: Core Middleware Support for Peer-to-Peer and Mobile Systems The pervasiveness of computer networks, together with the availability of wireless links, are steering distributed systems towards scenarios where computing is increasingly decentralized, decoupled, and dynamically reconfigurable. The popularity of and demand for applications that exploit mobile and peer-to-peer interactions is a symptom of such change. Nevertheless, by and large these applications are being built in an ad hoc manner, and often with architectures that, by sticking to the traditional client-server paradigm, do not fully capture and support the peculiar requirements of the new scenario.
IR
merz99genetic
Genetic Algorithms for Binary Quadratic Programming In this paper, genetic algorithms for the unconstrained binary quadratic programming problem (BQP) are presented. It is shown that for small problems a simple genetic algorithm with uniform crossover is sufficient to find optimum or best-known solutions in short time, while for problems with a high number of variables (n  200) it is essential to incorporate local search to arrive at high-quality solutions. A hybrid genetic algorithm incorporating local search is tested on 40 problem instances of sizes containing between n = 200 and n = 2500. The results of the computer experiments show that the approach is comparable to alternative heuristics such as tabu search for small instances and superior to tabu search and simulated annealing for large instances. New best solutions could be found for 14 large problem instances. 1 INTRODUCTION  In the unconstrained binary quadratic programming problem (BQP), a symmetric rational n \Theta n matrix  Q = (q ij ) is given, and a binary vector of leng...
ML
donini98allog
AL-log: Integrating Datalog and Description Logics . We presentanintegrated system for knowledge representation, called AL-log, based on description logics and the deductive database language Datalog. AL-log embodies two subsystems, called structural and relational. The former allows for the de#nition of structural knowledge about classes of interest #concepts# and membership relation between objects and classes. The latter allows for the de#nition of relational knowledge about objects described in the structural component. The interaction between the two components is obtained by allowing constraints within Datalog clauses, thus requiring the variables in the clauses to range over the set of instances of a speci#ed concept. We propose a method for query answering in AL-log based on constrained resolution, where the usual deduction procedure de#ned for Datalog is integrated with a method for reasoning on the structural knowledge.  Keywords: Description Logics, Deductive Databases, Datalog, Object-based Knowledge Representation, Query A...
DB
ferragina98string
The String B-Tree: A New Data Structure for String Search in External Memory and its Applications. We introduce a new text-indexing data structure, the String B-Tree, that can be seen as a link between some traditional external-memory and string-matching data structures. In a short phrase, it is a combination of B-trees and Patricia tries for internal-node indices that is made more effective by adding extra pointers to speed up search and update operations. Consequently, the String B-Tree overcomes the theoretical limitations of inverted files, B-trees, prefix B-trees, suffix arrays, compacted tries and suffix trees. String B-trees have the same worst-case performance as B-trees but they manage unbounded-length strings and perform much more powerful search operations such as the ones supported by suffix trees. String B-trees are also effective in main memory (RAM model) because they improve the online suffix tree search on a dynamic set of strings. They also can be successfully applied to database indexing and software duplication.
DB
chen00features
FEATURES: Real-time Adaptive Feature Learning and Document Learning for Web Search In this paper we report our research on building Features - an intelligent web search engine that is able to perform real-time adaptive feature (i.e., keyword) and document learning. Not only does Features learn from the user's document relevance feedback, but also automatically extracts and suggests indexing keywords relevant to a search query and learns from the user's keyword relevance feedback so that it is able to speed up its search process and to enhance its search performance. We design two efficient and mutual-benefiting learning algorithms that work concurrently, one for feature learning and the other for document learning. Features employs these algorithms together with an internal index database and a real-time meta-searcher so to perform adaptive real-time learning to find desired documents with as little relevance feedback from the user as possible. The architecture and performance of Features are also discussed. 1 Introduction As the world wide web rapidly evo...
IR
445127
Social Mental Shaping: Modelling the Impact of Sociality on the Mental States of Autonomous Agents This paper presents a framework that captures how the social nature of agents that are situated in a multi-agent environment impacts upon their individual mental states. Roles and social relationships provide an abstraction upon which we develop the notion of social mental shaping. This allows us to extend the standard Belief-DesireIntention model to account for how common social phenomena (e.g. cooperation, collaborative problem-solving and negotiation) can be integrated into a unified theoretical perspective that reflects a fully explicated model of the autonomous agent's mental state.  Keywords: Multi-agent systems, agent interactions, BDI models, social influence.  3  1. 
Agents
knowles99pareto
The Pareto Archived Evolution Strategy : A New Baseline Algorithm for Pareto Multiobjective Optimisation Most popular evolutionary algorithms for multiobjective optimisation maintain a population of solutions from which individuals are selected for reproduction. In this paper, we introduce a simpler evolution scheme for multiobjective problems, called the Pareto Archived Evolution Strategy (PAES). We argue that PAES may represent the simplest possible non-trivial algorithm capable of generating diverse solutions in the Pareto optimal set. The algorithm is identified as being a (1 + 1) evolution strategy, using local search from a population of one but using a reference archive of previously found solutions in order to identify the approximate dominance ranking of the current and candidate solution vectors. PAES is intended as a good baseline approach, against which more involved methods may be compared, and may also serve well in some real-world applications when local search seems superior to or competitive with population-based methods. The performance of the new algorithm is compared w...
ML
196686
Text Categorization Using Weight Adjusted k-Nearest Neighbor Classification . Automatic text categorization is an important task that can  help people finding information on huge online resources. Text categorization  presents unique challenges due to the large number of attributes  present in the data set, large number of training samples, attribute dependency,  and multi-modality of categories. Existing classification techniques  have limited applicability in the data sets of these natures. In this  paper, we present a Weight Adjusted k-Nearest Neighbor (WAKNN)  classification that learns feature weights based on a greedy hill climbing  technique. We also present two performance optimizations of WAKNN  that improve the computational performance by a few orders of magnitude,  but do not compromise on the classification quality. We experimentally  evaluated WAKNN on 52 document data sets from a variety  of domains and compared its performance against several classification  algorithms, such as C4.5, RIPPER, Naive-Bayesian, PEBLS and VSM.  Experimental results ...
ML
492009
Mining the Web to Create Minority Language Corpora The Web is a valuable source of language specific resources but the process of collecting, organizing and utilizing these resources is difficult. We describe CorpusBuilder, an approach for automatically generating Web-search queries for collecting documents in a minority language. It differs from pseudo-relevance feedback in that retrieved documents are labeled by an automatic language classifier as relevant or irrelevant, and this feedback is used to generate new queries. We experiment with various query-generation methods and query-lengths to find inclusion/exclusion terms that are helpful  for retrieving documents in the target language and find that using odds-ratio scores calculated over the documents acquired so far was one of the most consistently accurate query-generation methods. We also describe experiments using a handful of words elicited from a user instead of initial documents and show that the methods perform similarly. Experiments applying the same approach to multiple languages are also presented showing that our approach generalizes to a variety of languages.  1. 
IR
morse00collablogger
CollabLogger: A Tool for Visualizing Groups At Work The CollabLogger is a visual tool that supports usability analyses of human-computer interaction in a team environment. Participants in our computer-mediated activity were engaged in a small-scale manufacturing testbed project. Interactions of the group were mediated by Teamwave Workplace  1  and the members performed both synchronous and asynchronous activities depending on their availability, project requirements, and due to chance meetings in the collaborative space. The software  was instrumented to log users' interactions with the system and each other. The CollabLogger addresses the problem of helping investigators analyze the volumes of log data that groupware tools can generate. Visual tools are powerful when large amounts of diverse data present  themselves. The place-based collaboration environment offered by Teamwave Workplace provided a level of organization that allowed us to create a visual interface with which to perform exploratory sequential data analysis. Preliminary ...
HCI
cunningham99experience
Experience with a Language Engineering Architecture: Three Years of GATE GATE, the General Architecture for Text Engineering, aims to provide a software infrastructure for researchers and developers working in the area of natural language processing. A version of GATE has now been widely available for three years. In this paper we review the objectives which motivated the creation of GATE and the functionality and design of the current system. We discuss the strengths and weaknesses of the current system, identify areas for improvement and present plans for implementing these improvements. Introduction  We think that if you're researching human language processing you should probably not be writing code to: ffl store data on disk;  ffl display data;  ffl load processor modules and data stores into processes;   ffl initiate and administer processes;  ffl divide computation between client and server;  ffl pass data between processes and machines. A Domain-Specific Software Architecture (DSSA) for language processing should do all this for you. You will have t...
IR
low01simulated
Simulated 3D Painting This technical report looks at the motivation for simulating painting directly on 3D objects, and investigates the main issues faced by such systems. These issues include the provision of natural user interfaces, the reproduction of realistic brush effects and the surface parameterization for texture mapping so that the results of the painting can be stored on texture maps. The technical report further investigates the issues involved in using a haptic interface for simulating 3D painting, and the issues in surface parameterization for texture mapping with application to 3D painting. A survey of some work related to 3D painting, haptic rendering and surface parameterization for texture mapping is presented.  1 
HCI
dutech01multiagent
Multi-Agent Systems by Incremental Gradient Reinforcement Learning situated with local and scalable  perceptions,  .  have identical capabilities,  .  are possibly heterogeneous,  .  cooperate,  .  do not directly communicate.  Each agent learns its behavior on its own.  11/22  IJCAI'01  # # # # # # Bloc merging (the problem)  .  reward:  +3 if blocs are merged  .  actions:  N  W E  S  .  perceptions:  -- dir(agent)  #4  -- dir(yellow bloc)    -- dir(blue bloc)    -- near(yellow bloc)    -- near(blue bloc)    total    1024/4  (MDP with 2 agents and 2 cubes for an 8    8 world: 15.248.024 states !!!)  12/22  IJCAI'01  # # # # # # An agent learns a policy:  # :  But the <  O, A  > is not markovian:  .  convergence is not assured,  .  stochastic policies should perform better  [SJJ94].  1b  1a  A (-R)  B(+R)  B(-R)  A(+R)  13/22  IJCAI'01  # # # # # # Multi-agent framework:  .  each agent considers other  agents as part of the environment,    .  all agents learn, therefore  evolve,    unpredictable transitions.  Q-learning    Baxter's gradient descent  A
Agents
goldman00enhancing
Enhancing Supervised Learning with Unlabeled Data In many practical learning scenarios, there is  a small amount of labeled data along with  a large pool of unlabeled data. Many supervised  learning algorithms have been developed  and extensively studied. We present  a new "co-training" strategy for using unlabeled  data to improve the performance  of standard supervised learning algorithms.  Unlike much of the prior work, such as the  co-training procedure of Blum and Mitchell  (1998), we do not assume there are two redundant  views both of which are sufficient for  perfect classification. The only requirement  our co-training strategy places on each supervised  learning algorithm is that its hypothesis  partitions the example space into a set of  equivalence classes (e.g. for a decision tree  each leaf defines an equivalence class). We  evaluate our co-training strategy via experiments  using data from the UCI repository.  1. Introduction  In many practical learning scenarios, there is a small amount of labeled data along with a lar...
IR
stolzenburg00spatial
Spatial Agents Implemented in a Logical Expressible Language In this paper, we present a multi-layered architecture  for spatial and temporal agents. The focus is  laid on the declarativity of the approach, which  makes agent scripts expressive and well understandable.  They can be realized as (constraint) logic  programs. The logical description language is able  to express actions or plans for one and more autonomous  and cooperating agents for the RoboCup  (Simulator League). The system architecture hosts  constraint technology for qualitative spatial reasoning,  but quantitative data is taken into account, too.  The basic (hardware) layer processes the agent's  sensor information. An interface transfers this lowlevel  data into a logical representation. It provides  facilities to access the preprocessed data and supplies  several basic skills. The second layer performs  (qualitative) spatial reasoning. On top of this, the  third layer enables more complex skills such as  passing, offside-detection etc. At last, the fourth  layer establishes acting as a team both by emergent  and explicit cooperation. Logic and deduction provide  a clean means to specify and also to implement  teamwork behavior.  1 
Agents
arimura01efficient
Efficient Learning of Semi-structured Data from Queries This paper studies the polynomial-time learnability of the  classes of ordered gapped tree patterns (OGT) and ordered gapped forests  (OGF) under the into-matching semantics in the query learning model  of Angluin. The class OGT is a model of semi-structured database query  languages, and a generalization of both the class of ordered/unordered  tree pattern languages and the class of non-erasing regular pattern languages.
IR
537176
Mansion: A Structured Middleware Environment for Agents Developing processes intended to roam in largescale heterogeneous distributed systems is difficult: their environment is unstructured, and interoperability issues often emerge. Mansion is a new paradigm to provide a large-scale structured environment for mobile agents (autonomous migrating processes). Security, transparency, and interoperability are important design guidelines for the development of Mansion.
Agents
502491
Agents Acting and Moving in Healthcare Scenario: A Paradigm for Telemedical Collaboration The present paper describes a novel approach to the analysis and development of telemedicine systems, based on the multi-agent paradigm. An agent is an autonomous, social, reactive and proactive entity, sometimes also mobile. Since telemedicine is grounded on communication and sharing of resources, agents are suitable for its analysis and implementation, and we adopted them for developing a prototype telemedical agent.
Agents
76283
A Parametric Alternative to Grids for Occupancy-Based World Modeling In the paper, we consider an occupancy-based approach for range data fusion, as it is used in mobile robotics. We tackle the major problem of this approach, which is the redundancy of stored and processed data caused by using the grid representation of the occupancy function, by proposing a parametric piece-wise linear representation. When applied to the vision-based world exploration, the new representation is shown to have advantages over the former one, which include its suitability for radial range data, its efficiency in representing and fusing range data, and its convenience for navigation map extraction. The proposed technique is implemented on a mobile robot, Boticelli. The results obtained from running the robot are presented. 1 Introduction  In mobile robot world exploration, the occupancybased  approach is one of the most commonly used [7, 13, 3, 2, 8, 4, 11]. In this approach, the exploration policy is determined by the occupancy model of the world which is built from the r...
AI
johnson99cambridge
The Cambridge University Spoken Document Retrieval System This paper describes the spoken document retrieval system that we have been developing and assesses its performance using automatic transcriptions of about 50 hours of broadcast news data. The recognition engine is based on the HTK broadcast news transcription system and the retrieval engine is based on the techniques developed at City University. The retrieval performance over a wide range of speech transcription error rates is presented and a number of recognition error metrics that more accurately reflect the impact of transcription errors on retrieval accuracy are defined and computed. The results demonstrate the importance of high accuracy automatic transcription. The final system is currently being evaluated on the 1998 TREC-7 spoken document retrieval task. 1.
IR
323163
Implementation and Performance Analysis of Incremental Equations of Nested Relations View materialization is an important way of improving the  performance of query processing. When an update occurs to  the source data from which a materialized view is derived, the  materialized view has to be updated so that it is consistent  with the source data. This update process is called view maintenance.  The incremental method of view maintenance, which  computes the new view using the old view and the update to  the source data, is widely preferred to full view recomputation  when the update is small in size. The small update size  becomes an important concept for measuring the cheap performance  of the incremental methods. In this paper, we investigate  what is the limit of the small update size, which we call  size limit for the incremental maintenance. When the size of  an update exceeds the limit, the incremental maintenance is no  longer cheaper than the view recomputation. The investigation  is based on incremental equations for operators in the nested  relational model...
DB
lin02acird
ACIRD: Intelligent Internet Documents Organization and Retrieval This paper presents an intelligent Internet information system, Automatic Classifier for the Internet Resource Discovery (ACIRD), which uses machine learning techniques to organize and retrieve Internet documents. ACIRD consists of a knowledge acquisition process, document classifier and two-phase search engine. The knowledge acquisition process of ACIRD automatically learns classification knowledge from classified Internet documents. The document classifier applies learned classification knowledge to classify newly collected Internet documents into one or more classes. Experimental results indicate that ACIRD performs as well or better than human experts in both knowledge acquisition and document classification. By using the learned classification knowledge and the given class lattice, the ACIRD two-phase search engine responds to user queries with hierarchically structured navigable results (instead of a conventional flat ranked document list), which greatly aids users in locating information from numerous, diversified Internet documents.
IR
42875
Automatic Text Detection and Tracking in Digital Video Text which either appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this paper we present algorithms for detecting and tracking text components that appear within digital video frames. Our system implements a scale-space feature extractor that feeds an artificial neural processor to extract textual regions and track their movement over time. The extracted regions can then be used as input to an appropriate Optical Character Recognition system which produces indexible keywords. Keywords: Text Detection, Text Tracking, Video Indexing, Digital Libraries, Neural Network, Wavelet The support of this effort by the Department of Defense under contract MDA 9049-6C-1250 is gratefully acknowledged.  1 Introduction  The increasing availability of online digital imagery and video has rekindled interest in the problems of how to index multimedia informa...
IR
xie02locality
Locality in Search Engine Queries and Its Implications for Caching Caching is a popular technique for reducing both server load and user response time in distributed systems. In this paper, we are interested in the question of whether caching might be effective for search engines as well. We study two real search engine traces by examining query locality and its implications for caching. Our trace analysis results show that: (1) Queries have significant locality, with query frequency following a Zipf distribution. Very popular queries are shared among different users and can be cached at servers or proxies, while 16 % to 22% of the queries are from the same users and should be cached at the user side. Multiple-word queries are shared less and should be cached mainly at the user side. (2) If caching is to be done at the user side, short-term caching for hours will be enough to cover query temporal locality, while server/proxy caching should be based on longer periods such as days. (3) Most users have small lexicons when submitting queries. Frequent users who submit many search requests tend to reuse a small subset of words to form queries. Thus, with proxy or user side caching, prefetching based on user lexicon looks promising.
IR
pagonis99evolving
Evolving Personal Agent Environments To Reduce Internet Information Overload: Initial Considerations this paper we will address the other major information overload problem, arising from habitual surfing. We will present our proposed research programme and also outline and support a potential solution. So far this issue has been far less visited and researched, though with the ever increasing use of computers at home and in the workplace, as well as the imminent arrival of information appliances, this may well change [6].
IR
17752
Learning Situation-Specific Control In Multi-Agent Systems   The work presented in this thesis deals with techniques to improve problem solving control skills of cooperative agents through machine learning. In a multi-agent system, the local problem solving control of an agent can interact in complex and intricate ways with the problem solving control of other agents. In such systems, an agent cannot make effective control decisions based purely on its local problem solving state. Effective cooperation requires that the global problem-solving state influence the local control decisions made by an agent. We call such an influence cooperative control. An agent with a purely local view of the problem solving situation cannot learn ...
ML
liu99partial
Partial and Complete Tuples and Sets in Deductive Databases . In a nested relational or complex object database, nested tuples and sets are used to represent real world objects. For various reasons, such tuples and sets can be partial or complete. In this paper, we discuss how to support them in deductive databases. In particular, we present a deductive database language RLOG  II  that supports partial and complete tuples and sets based on Relationlog. This work provides a firm logical foundation for nested relational and complex object databases that have both partial and complete tuples and sets. 1 Introduction  In a nested relational database or complex object database, nested tuples and sets are used to represent real world objects. A relation is just a set of tuples and a database is a tuple of relations. For various reasons, the information about real world objects in a database may be incomplete. Hence, both tuples and sets can be partial or complete. In the past several years, a sub-problem, that is, a database which contains partial an...
DB
noh99uncertain
Uncertain Knowledge Representation and Communicative Behavior in Coordinated Defense This paper reports on results we obtained on communication among artificial and human agents interacting in a simulated air defense domain. In our research, we postulate that the artificial agents use a decision-theoretic method to select optimal communicative acts, given the characteristics of the particular situation. Thus, the agents we implemented compute the expected utilities of various alternative communicative acts, and execute the best one. The agents use a probabilistic frame-based knowledge formalism to represent the uncertain information they have about the domain and about the other agents present. We build on our earlier work that uses the Recursive Modeling Method (RMM) for coordination, and apply RMM to rational communication in an anti-air defense domain. In this domain, distributed units coordinate and communicate to defend a specified territory from a number of attacking missiles. We measure the benefits of rational communication by showing the improvement in the qua...
Agents
25225
An Automatic Closed-Loop Methodology for Generating Character Groundtruth for Scanned Documents Abstract—Character groundtruth for real, scanned document images is crucial for evaluating the performance of OCR systems, training OCR algorithms, and validating document degradation models. Unfortunately, manual collection of accurate groundtruth for characters in a real (scanned) document image is not practical because (i) accuracy in delineating groundtruth character bounding boxes is not high enough, (ii) it is extremely laborious and time consuming, and (iii) the manual labor required for this task is prohibitively expensive. In this paper we describe a closed-loop methodology for collecting very accurate groundtruth for scanned documents. We first create ideal documents using a typesetting language. Next we create the groundtruth for the ideal document. The ideal document is then printed, photocopied and then scanned. A registration algorithm estimates the global geometric transformation and then performs a robust local bitmap match to register the ideal document image to the scanned document image. Finally, groundtruth associated with the ideal document image is transformed using the estimated geometric transformation to create the groundtruth for the scanned document image. This methodology is very general and can be used for creating groundtruth for documents in typeset in any language, layout, font, and style. We have demonstrated the method by generating groundtruth for English, Hindi, and FAX document images. The cost of creating groundtruth using our methodology is minimal. If character, word or zone groundtruth is available for any real document, the registration algorithm can be used to generate the corresponding groundtruth for a rescanned version of the document. Index Terms—Automatic real groundtruth, document image analysis, OCR, performance evaluation, image registration, geometric transformations, image warping. ——————— — F ———————— 1
DB
brown99active
Active User Interfaces For Building Decision-Theoretic Systems Knowledge elicitation/acquisition continues to be a bottleneck to constructing decisiontheoretic  systems. Methodologies and techniques for incremental elicitation/acquisition of  knowledge especially under uncertainty in support of users' current goals is desirable. This  paper presents PESKI, a probabilistic expert system development environment. PESKI provides  users with a highly interactive and integrated suite of intelligent knowledge engineering  tools for decision-theoretic systems. From knowledge acquisition, data mining, and verification  and validation to a distributed inference engine for querying knowledge, PESKI is based  on the concept of active user interfaces -- actuators to the human-machine interface. PESKI  uses a number of techniques to reduce the inherent complexity of developing a cohesive, realworld  knowledge-based system. This is accomplished by providing multiple communication  modes for human-computer interaction and the use of a knowledge representation endowed  with the ability to detect problems with the knowledge acquired and alert the user to these  possible problems. We discuss PESKI's use of these intelligent assistants to help users with  the acquisition of knowledge especially in the presence of uncertainty.
HCI
vogler99toward
Toward Scalability in ASL Recognition: Breaking Down Signs into Phonemes In this paper we present a novel approach to continuous, whole-sentence ASL recognition that uses phonemes instead of whole signs as the basic units. Our approach is based on a sequential phonological model of ASL. According to this model the ASL signs can be broken into movements and holds, which are both considered phonemes.  This model does away with the distinction between whole signs and epenthesis movements that we made in previous work [13]. Instead, epenthesis movements are just like the other movements that constitute the signs.  We subsequently train Hidden Markov Models (HMMs) to recognize the phonemes, instead of whole signs and epenthesis movements that we recognized previously [13]. Because the number of phonemes is limited, HMM-based training and recognition of the ASL signal becomes computationally more tractable and has the potential to lead to the recognition of large-scale vocabularies.  We experimented with a 22 word vocabulary, and we achieved similar recognition r...
HCI
miagkikh99generalized
A Generalized Approach to Handling Parameter Interdependencies in Probabilistic Modeling and Reinforcement Learning Optimization Algorithms This paper generalizes our research on parameter interdependencies in reinforcement learning algorithms for optimization problem solving. This generalization expands the work to a larger class of search algorithms that use explicit search statistics to form feasible solutions. Our results suggest that genetic algorithms can both enrich and benefit from probabilistic modeling, reinforcement learning, ant colony optimization or other similar algorithms using values to encode preferences for parameter assignments. The approach is shown to be effective on both the Asymmetric Traveling Salesman and the Quadratic Assignment Problems. Introduction  There has been a recent upsurge of interest in a family of search algorithms that store past experience not only as the best solutions generated, but also abstract representations of the decision processes employed. This interest is provoked by a number of factors. First, since solution memory is always limited, search algorithms which store only b...
ML
kusiak01galgorithm
G-Algorithm for Extraction of Robust Decision Rules-Children's Postoperative Intra-Atrial Arrhythmia Case Study Clinical medicine is facing a challenge of knowledge discovery from the growing volume of data. In this paper, a data mining algorithm (G-algorithm) is proposed for extraction of robust rules that can be used in clinical practice for better understanding and prevention of unwanted medical events. The G-algorithm is applied to the data set obtained for children born with a malformation of the heart (univentricular heart). As the result of the Fontan surgical procedure, designed to palliate the children, 10%--35% of patients postoperatively develop an arrhythmia known as the intra-atrial reentrant tachycardia. There is an obvious need to identify the children that may develop the tachycardia before the surgery is performed. Prior attempts to identify such children with statistical techniques have been unrewarding. The G-algorithm discussed in this paper shows that there exists an unambiguous relationship between measurable features and the tachycardia. The data set used in this study shows that, for 78.08% of infants, the occurrence of tachycardia can be accurately predicted. The authors' prior computational experience with diverse medical data sets indicates that the percentage of accurate predictions may become even higher if data on additional features is collected for a larger data set.
ML
nigam99text
Text Classification from Labeled and Unlabeled Documents using EM  This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.  We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.  
DB
milicevic00socratenon
Socratenon and its Application to the Learning of Italian Language tudents (the product was developed through a cooperation between universities in Salerno and Belgrade). This paper presents the basic elements of the Socratenon application and implementation philosophy, and discusses its possibilities in the general languagelearning environment. It describes three different experiments and explains the lessons learned. The stress is on the statistical analysis of success of those who used our Web-based product and those who relied on the classical approaches.  1 Introduction  Rapid growth of Internet as a medium and Internet technologies has led to the point when education can be detached from humans and books as the only possessors of knowledge. From the early days, Internet has been exploited in educational institutions for dissemination of research results, and knowledge in general. First shapes were unpolished and required a lot of attention from the users. Such sources of knowledge also included wrestling with several resources of
DB
goodman01representation
On Representation and Approximation of Operations in Boolean Algebras Several universal approximation and universal representation results  are known for non-Boolean multi-valued logics such as fuzzy logics. In  this paper, we show that similar results can be proven for multi-valued  Boolean logics as well.  Introduction. The study of Boolean algebras started when G. Boole [1] considered the simplest Boolean algebra -- the two-valued algebra B (2) = f0; 1g (=ffalse,trueg). In this algebra, the operation  ([),  ("), and negation a  0 (:a) have the direct logical meaning of "or", "and", and "not". It is known that in this Boolean algebra, an arbitrary operation, i.e., an arbitrary function B \Theta : : : \Theta B ! B, can be represented as a superposition of these three basic logical operations: e.g., the implication a ! b can be represented as b :a, etc. Logic is still one of the area of application of Boolean algebras, but, starting from the classical Kolmogorov's monograph [4], Boolean algebras -- namely, algebras of events -- became an important tool i...
ML
aha98omnipresence
The Omnipresence of Case-Based Reasoning in Science and Application A surprisingly large number of research disciplines have contributed towards the development of knowledge on lazy problem solving, which ischaracterized by its storage of ground cases and its demand driven response to queries. Case-based reasoning (CBR) is an alternative, increasingly popular approach for designing expert systems that implements this approach. This paper lists pointers to some contributions in some related disciplines that o er insights for CBR research. We then outline a small number of Navy applications based on this approach that demonstrate its breadth of applicability. Finally, we list a few successful and failed attempts to apply CBR, and list some predictions on the future roles of CBR in applications. 1 Case-Based Reasoning Case-based reasoning (CBR) is a multi-disciplinary subject that focuses on the reuse of experiences (i.e., cases). It is di cult to nd consensus on more detailed de nitions of CBR because it means di erent things to di erent groups of people. For example, consider its interpretation by the following three groups: Cognitive Scientists: CBR is a plausible high-level model for cognitive processing (Kolodner,
ML
498960
Mobile Agent Organizations Mobile agents are a useful paradigm -- other than a useful technology -- for the development of complex Internet applications. However, the effective development of mobile agent applications requires suitable models and infrastructures. This paper proposes an organizational approach to the high-level design of mobile agent applications. The idea is to models the Internet as a multiplicity of local and active organizational contexts, intended as the places where coordination activities of application agents occur and are ruled. The paper discusses the advantages and the generality of such an approach, also with the help of a case study in the area of tourist assistance.
Agents
horn01ai
AI in Medicine on its way from knowledge-intensive to data-intensive systems The last 20 years of research and development in the field of artificial intelligence in medicine show a path from knowledge-intensive systems, which try to capture the essential knowledge of experts in a knowledge-based system, to data-intensive systems available today. Nowadays enormous amounts of information is accessible electronically. Large data sets are collected continuously monitoring physiological parameters of patients. Knowledgebased  systems are needed to make use of all these data available and to help us to cope with the information explosion. In addition, temporal data analysis and intelligent information visualization can help us to get a summarized view of the change over time of clinical parameters. Integrating AIM modules into the daily-routine software environment of our care providers gives us a great chance for maintaining and improving quality of care.  1 AIM: a partial view of its scope and potential  This paper gives a personalized view of research and develop...
AI
ramampiaro00supporting
Supporting Distributed Cooperative Work in CAGIS This paper describes how the CAGIS environment can be used to manage work-processes, cooperative processes, and how to share and control information in a distributed, heterogeneous environment. We have used a conference organising process as a scenario and applied our CAGIS environment on this process. The CAGIS environment consists of three main parts: a document management system, a process management system, and a transaction management system. Keywords: Web-based software engineering, Internet computing: JAVA, XML, Intelligent agent software, Database systems, Document modelling, Process modelling, Transaction modelling. 1 Introduction After the introduction of the Internet, more and more projects are taking place in heterogeneous environments where both people, information and working processes are distributed. Work is often dynamic and cooperative and involves multiple actors with different kinds of needs. In these settings there is a need to help people coordinate their ...
IR
ianni01intelligent
Intelligent Anticipated Exploration of Web Sites In this paper we describe a web search agent, called Global Search Agent (hereafter  GSA for short). GSA integrates and enhances several search techniques in order to  achieve significant improvements in the user-perceived quality of delivered information as  compared to usual web search engines. GSA features intelligent merging of relevant documents  from different search engines, anticipated selective exploration and evaluation of  links from the current resuk set, automated derivation of refined queries based on user relevance  feedback. System architecture as well as experimental accounts are also illustrated.
IR
538247
Web Page Classification Using Spatial Information Extracting and processing information from web pages is an important  task in many areas like constructing search engines, information retrieval, and data  mining from the Web. Common approach in the extraction process is to represent a  page as a "bag of words" and then to perform additional processing on such a flat  representation. In this paper we propose a new, hierarchical representation that  includes browser screen coordinates for every HTML object in a page. Such spatial  information allows the definition of heuristics for recognition of common page  areas such as header, left and right menu, footer and center of a page. We show a  preliminary experiment where our heuristics are able to correctly recognize objects  in 73% of cases. Finally, we show that a Naive Bayes classifier, taking into account  the proposed representation, clearly outperforms the same classifier using only  information about the content of documents.
IR
zhang01pvm
PVM: Parallel View Maintenance Under Concurrent Data Updates of Distributed Sources Data warehouses (DW) are built by gathering information from distributed information  sources (ISs) and integrating it into one customized repository. In recent years, work has begun  to address the problem of view maintenance of DWs under concurrent data updates of  different ISs. Popular solutions such as ECA and Strobe achieve such concurrent maintenance  however with the requirement of quiescence of the ISs. More recently, the SWEEP solution  releases this quiescence requirement using a local compensation strategy that now processes all  update messages in a sequential manner. To optimize upon this sequential processing, we have  developed a parallel view maintenance algorithm, called PVM, that incorporates all benefits of  previous maintenance approaches while offering improved performance due to parallelism. In  order to perform parallel view maintenance, we have identified two critical issues: (1) detecting  maintenance-concurrent data updates in a parallel mode, and (2) correcting the problem that  the DW commit order may not correspond to the DW update processing order due to parallel  maintenance handling. In this work, we provide solutions to both issues. Given a modular  component-based system architecture, we insert a middle-layer timestamp assignment module  for detecting maintenance-concurrent data updates without requiring any global clock synchronization.  In addition, we introduce the negative counter concept as a simple yet elegant solution  to solve the problem of variant orders of committing effects of data updates to the DW. We  have proven the correctness of PVM to guarantee that our strategy indeed generates the correct  final DW state. We have implemented both SWEEP and PVM in our EVE data warehousing  system. Our performance study demonstrates ...
DB
bradford98performance
Performance and Memory-Access Characterization of Data Mining Applications This paper characterizes the performance and memoryaccess behavior of a decision tree induction program, a previously unstudied application used in data mining and knowledge discovery in databases. Performance is studied via RSIM, an execution driven simulator, for three uniprocessor models that exploit Instruction Level Parallelism (ILP) to varying degrees. Several properties of the program are noted. Outof -order dispatch and multiple-issue provide a significant performance advantage: 50%--250% improvement in IPC for out-of-order versus in-order, and 5%--120% improvement in IPC for four-way issue versus singleissue. Multiple-issue provides a greater performance improvement for larger L2 cache sizes, when the program is limited by CPU performance; out-of-order dispatch provides a greater performance improvement for smaller L2 cache sizes. The program has a very small instruction footprint: an 8-kB L1 instruction cache is sufficient to bring the instruction miss rate below 0.1%. A smal...
DB
billsus00learning
A Learning Agent for Wireless News Access We describe a user interface for wireless information devices, specifically designed to facilitate learning about users' individual interests in daily news stories. User feedback is collected unobtrusively to form the basis for a content-based machine learning algorithm. As a result, the described system can adapt to users' individual interests, reduce the amount of information that needs to be transmitted, and help users access relevant information with minimal effort.  Keywords  Wireless, intelligent information access, news, user modeling, machine learning.  1. INTRODUCTION  Driven by the explosive growth of information available on the Internet, intelligent information access has become a central research area in computer science. The 20  th  century is commonly characterized as "The Information Age", and the sheer amount of information readily available today has created novel challenges. Numerous intelligent information agents -- software tools that provide personalized assistanc...
HCI
mann02testing
Testing Access To External Information Sources in a Mediator Environment This paper discusses the testing of communication in the increasingly important class of distributed information systems that are based on a mediator architecture. A mediator integrates existing information sources into a new application. In order to answer complex queries, the mediator splits them up into subqueries which it sends to the information sources, and it combines the replies to answer the original query. Since the information sources are usually remote, autonomous systems, the access to them can be erroneous, most notably when the information source is subject to modifications. Such errors may result in incorrect behaviour of the whole system. This paper addresses the problem of deciding whether an information source as part of a mediatory system was successfully queried or not. The primary contribution is a formal framework for the general information access testing problem. Besides proposing several solutions, it is investigated what the most important quality measures of such testing methods are. Moreover, the practical usability of the presented approaches is demonstrated on a real-world application using Web-based information sources. Several empirical experiments are conductedto compare the presented methods with previous work in the field.
IR
210930
The Complexity of Revising Logic Programs A rule-based program will return a set of answers to each query. An impure program, which includes the Prolog cut "!" and "not(\Delta)" operators, can return different answers if its rules are re-ordered. There are also many reasoning systems that return only the first answer found for each query; these first answers, too, depend on the rule order, even in pure rule-based systems. A theory revision algorithm, seeking a revised rule-base whose expected accuracy, over the distribution of queries, is optimal, should therefore consider modifying the order of the rules. This paper first shows that a polynomial number of training "labeled queries" (each a query paired with its correct answer) provides the distribution information necessary to identify the optimal ordering. It then proves, however, that the task of determining which ordering is optimal, once given this distributional information, is intractable even in trivial situations; e.g.,  even if each query is an atomic literal, we are...
AI
smola98from
From Regularization Operators to Support Vector Kernels We derive the correspondence between regularization operators used in Regularization Networks and Hilbert Schmidt Kernels appearing in Support Vector Machines. More specifically, we prove that the Green's Functions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties. As a by--product we show that a large number of Radial Basis Functions namely conditionally positive definite functions may be used as Support Vector kernels. 1 INTRODUCTION  Support Vector (SV) Machines for pattern recognition, regression estimation and operator inversion exploit the idea of transforming into a high dimensional feature space where they perform a linear algorithm. Instead of evaluating this map explicitly, one uses Hilbert Schmidt Kernels k(x; y) which correspond to dot products of the mapped data in high dimensional space, i.e.  k(x; y) = (\Phi(x) \Delta \Phi(y)) (1) with \Phi : R  n  ! F denoting the map into feature space. Mostly, this m...
ML
domingos00mining
Mining High-Speed Data Streams Many organizations today have more than very large databases; they have databases that grow without limit at a rate of several million records per day. Mining these continuous data streams brings unique opportunities, but also new challenges. This paper describes and evaluates VFDT, an anytime system that builds decision trees using constant memory and constant time per example. VFDT can incorporate tens of thousands of examples per second using o#-the-shelf hardware. It uses Hoe#ding bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner. We study VFDT's properties and demonstrate its utility through an extensive set of experiments on synthetic data. We apply VFDT to mining the continuous stream of Web access data from the whole University of Washington main campus.
HCI
sun99hybrid
A Hybrid Architecture for Situated Learning of Reactive Sequential Decision Making In developing autonomous agents, one usually emphasizes only (situated) procedural knowledge, ignoring more explicit declarative knowledge. On the other hand, in developing symbolic reasoning models, one usually emphasizes only declarative knowledge, ignoring procedural knowledge. In contrast, we have developed a learning model Clarion, which is a hybrid connectionist model consisting of both localist and distributed representations, based on the two-level approach proposed in Sun (1995).  Clarion learns and utilizes both procedural and declarative knowledge, tapping into the synergy of the two types of processes, and enables an agent to learn in situated contexts and generalize resulting knowledge to different scenarios. It unifies connectionist, reinforcement, and symbolic learning in a synergistic way, to perform on-line, bottom-up learning. This summary paper presents one version of the architecture and some results of the experiments. Key Words: hybrid models, sequential decision ...
ML
bishop98gtm
GTM: The Generative Topographic Mapping Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis which is based on a linear transformations between the latent space and the data space. In this paper we introduce a form of non-linear latent variable model called the Generative Topographic Mapping for which the parameters of the model can be determined using the EM algorithm. GTM provides a principled alternative to the widely used Self-Organizing Map (SOM) of Kohonen (1982), and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multi-phase oil pipeline.  GTM: The Generative Topographic Mapping 2 1 Introduction  Many data sets exhibit significant correlations between the variables. One way to capture such structure is to model the distribution of the data in term...
ML
sim99learning
Learning Visual Landmarks for Pose Estimation We present an approach to vision-based mobile robot localization, even without an a-priori pose estimate. This is accomplished by learning a set of visual features called image-domain landmarks. The landmark learning mechanism is designed to be applicable to a wide range of environments. Each landmark is detected as a local extremum of a measure of uniqueness and represented by an appearance-based encoding. Localization is performed using a method that matches observed landmarks to learned prototypes and generates independent position estimates for each match. The independent estimates are then combined to obtain a final position estimate, with an associated uncertainty. Quantitative experimental evidence is presented that demonstrates that accurate pose estimates can be obtained, despite changes to the environment.
ML
karypis00concept
Concept Indexing - A Fast Dimensionality Reduction Algorithm with Applications to Document Retrieval & Categorization In recent years, we have seen a tremendous growth in the volume of text documents available on the Internet, digital libraries, news sources, and company-wide intranets. This has led to an increased interest in developing meth-ods that can efficiently categorize and retrieve relevant information. Retrieval techniques based on dimensionality reduction, such as Latent Semantic Indexing (LSI), have been shown to improve the quality of the information being retrieved by capturing the latent meaning of the words present in the documents. Unfortunately, the high computa-tional requirements of LSI and its inability to compute an effective dimensionality reduction in a supervised setting limits its applicability. In this paper we present a fast dimensionality reduction algorithm, called concept indexing (CI) that is equally effective for unsupervised and supervised dimensionality reduction. CI computes a k-dimensional representation of a collection of documents by first clustering the documents into k groups, and then using the cen-troid vectors of the clusters to derive the axes of the reduced k-dimensional space. Experimental results show that the dimensionality reduction computed by CI achieves comparable retrieval performance to that obtained using LSI, while requiring an order of magnitude less time. Moreover, when CI is used to compute the dimensionality reduction in a supervised setting, it greatly improves the performance of traditional classification algorithms such as C4.5 and kNN. 1
IR
meng98data
Data Visualization, Indexing and Mining Engine - A Parallel Computing Architecture for Information Processing Over the Internet ion : : : : : : : : : : : 9 3.2 Spatial Representation Of the System's Networks : : : : : : : : : : : : : : : : : : : : : : 10 3.3 Display And Interaction Mechanisms : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11 3.3.1 Overviews, orientation, and network abstraction : : : : : : : : : : : : : : : : : : 11 3.3.2 Other Navigation And Orientation Tools : : : : : : : : : : : : : : : : : : : : : : : 12 3.3.3 Head-tracked Stereoscopic Display : : : : : : : : : : : : : : : : : : : : : : : : : : 12 4 Web Access of Geographic Information System Data 13  4.1 Functions Provided by GIS2WEB : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 13 4.2 Structure : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 14 5 ParaCrawler --- Parallel Web Searching With Machine Learning 15  5.1 A Machine Learning Approach Towards Web Searching : : : : : : : : : : : : : : : : : : 15 6 DUSIE --- Interactive Content-based Web Structuring 16  6.1 Creating t...
IR
fox99gateway
The Gateway System: Uniform Web Based Access to Remote Resources Exploiting our experience developing the WebFlow system, we designed the Gateway system to provide seamless and secure access to computational resources at ASC MSRC. The Gateway follows our commodity components strategy, and it is implemented as a modern three-tier system. Tier 1 is a highlevel front end for visual programming, steering, run-time data analysis and visualization that is built on top of the Web and OO commodity standards. Distributed object-based, scalable, and reusable Web server and Object broker middleware forms Tier 2. Back-end services comprise Tier 3. In particular, access to high-performance computational resources is provided by implementing the emerging standard for metacomputing API.  1. Introduction  The last few years have seen the growing power and capability of commodity computing and communication technologies largely driven by commercial distributed information systems. All of them can be abstracted to a three-tier model with largely independent clients c...
DB
chan99possible
A Possible World Semantics for Disjunctive Databases We investigate the fundamental problem of when a ground atom in a disjunctive database is assumed false. There are basically two different approaches for inferring negative information for disjunctive databases; they are Minker's Generalized Closed World Assumption (GCWA) and Ross and Topor's Disjunctive Database Rule (DDR). A problem with the GCWA is that disjunctive clauses are sometimes interpreted exclusively, even when they are intended for inclusive interpretation. On the other hand, the DDR always interprets disjunctive clauses inclusively. We argue that neither approach is satisfactory. Whether a disjunctive clause is interpreted exclusively or inclusively should be specified explicitly. Negative information should then be inferred according to the stated intent of the disjunctive clauses. A database semantics called PWS is proposed to solve the aforementioned problem. We also show that for propositional databases with no negative clauses, the problem of determining...
DB
huget02extending
Extending Agent UML Protocol Diagrams this paper is to present some new features that we propose. Several features deal with reliability such as triggering actions and exception handling. These new features are in fact proposed due to our work in electronic commerce and in supply chain management [16]. As much as possible, we apply these new features to needs in the supply chain management example
Agents
462285
Query Processing for Moving Objects with Space-Time Grid Storage Model With growing popularity of mobile computing devices and wireless communications, managing dynamically changing information about moving objects is becoming feasible. In this paper, we implement a system that manages such information and propose two new algorithms. One is an efficient range query algorithm with a ltering step which efficiently determines if a polyline corresponding to the trajectory of a moving object intersects with a given range. The other is a nearest neighbor query algorithm. We study the performance of the system, which shows that despite the filtering step, for moderately large ranges, the range query algorithm we propose outperforms the algorithm without filtering.
DB
helmer01automated
Automated Discovery of Concise Predictive Rules for Intrusion Detection This paper details an essential component of a multi-agent distributed knowledge network system  for intrusion detection. We describe a distributed intrusion detection architecture, complete with a  data warehouse and mobile and stationary agents for distributed problem-solving to facilitate building,  monitoring, and analyzing global, spatio-temporal views of intrusions on large distributed systems. An  agent for the intrusion detection system, which uses a machine learning approach to automated discovery  of concise rules from system call traces, is described.  We use a feature vector representation to describe the system calls executed by privileged processes.  The feature vectors are labeled as good or bad depending on whether or not they were executed during  an observed attack. A rule learning algorithm is then used to induce rules that can be used to monitor  the system and detect potential intrusions. We study the performance of the rule learning algorithm on  this task with an...
Agents
327456
Reinforcement Learning for a Vision Based Mobile Robot Reinforcement learning systems improve behaviour based on scalar rewards from a critic. In this work vision based behaviours, servoing and wandering, are learned through a Q-learning method which handles continuous states and actions. There is no requirement for camera calibration, an actuator model, or a knowledgeable teacher. Learning through observing the actions of other behaviours improves learning speed. Experiments were performed on a mobile robot using a real-time vision system. 1 Introduction Collision free wandering and visual servoing are building blocks for purposeful robot behaviours such as foraging, target pursuit and landmark based navigation. Visual servoing consists of moving some part of a robot to a desired position using visual feedback [15]. Wandering is an environment exploration behaviour [6]. In this work we demonstrate real-time learning of wandering and servoing on a real robot. Learning eliminates the calibration process and leads to flexible behaviour....
ML
8922
Using Explicit Requirements and Metrics for Interface Agent User Model Correction The complexity of current computer systems and software warrants  research into methods to decrease the cognitive load on users.  Determining how to get the right information into the right form with  the right tool at the right time has become a monumental task --- one  necessitating intelligent interfaces agents with the ability to predict  the users' needs or intent.  An accurate user model is considered necessary for effective prediction  of user intent. Methods for maintaining accurate user models  is the main thrust of this paper. We describe an approach for dynamically  correcting an interface agent's user model based on utility-theory.  We explicitly take into account an agent's requirements and metrics  for measuring the agent's effectiveness of meeting those requirements.  Using these requirements and metrics, we develop a requirements utility  function that determines when a user model should be corrected  and how. We present a correction model based on a multi-agent bidding...
HCI
deloach99multiagent
Multiagent Systems Engineering: A Methodology And Language for Designing Agent Systems This paper overviews MaSE and provides a high-level introduction to one critical component used within MaSE, the Agent Modeling Language. Details on the Agent Definition Language and detailed agent design are left for a future paper.
Agents
french98evaluating
Evaluating Database Selection Techniques: A Testbed and Experiment We describe a testbed for database selection techniques and an experiment conducted using this testbed. The testbed is a decomposition of the TREC/TIPSTER data that allows analysis of the data along multiple dimensions, including collection-based and temporal-based analysis. We characterize the subcollections in this testbed in terms of number of documents, queries against which the documents have been evaluated for relevance, and distribution of relevant documents. We then present initial results from a study conducted using this testbed that examines the effectiveness of the gGlOSS approach to database selection. The databases from our testbed were ranked using the gGlOSS techniques and compared to the gGlOSS  Ideal(l) baseline and a baseline derived from TREC relevance judgements. We have examined the degree to which several gGlOSS estimate functions approximate these baselines. Our initial results confirm that the gGlOSS estimators are excellent predictors of the Ideal(l) ranks but...
IR
61706
Integrated Document Caching and Prefetching in Storage Hierarchies Based on Markov-Chain Predictions .<F3.733e+05> Large multimedia document archives may hold a major fraction of their data in tertiary storage libraries for cost reasons. This paper develops an integrated approach to the vertical data migration between the tertiary, secondary, and primary storage in that it reconciles speculative prefetching, to mask the high latency of the tertiary storage, with the replacement policy of the document caches at the secondary and primary storage level, and also considers the interaction of these policies with the tertiary and secondary storage request scheduling. The integrated migration policy is based on a continuoustime Markov chain model for predicting the expected number of accesses to a document within a specified time horizon. Prefetching is initiated only if that expectation is higher than those of the documents that need to be dropped from secondary storage to free up the necessary space. In addition, the possible resource contention at the tertiary and secondary storage is tak...
AI
bartoli02nonlinear
On the Non-Linear Optimization of Projective Motion Using Minimal Parameters I address the problem of optimizing projective motion over a minimal  set of parameters. Most of the existing works overparameterize the problem.
AI
brandt00antisocial
Antisocial Bidding in Repeated Vickrey Auctions In recent years auctions have become more and more important in the eld  of multiagent systems as useful mechanisms for resource allocation and task  assignment. In many cases the Vickrey (second-price sealed-bid) auction is  used as a protocol that prescribes how the individual agents have to interact  in order to come to an agreement. The main reasons for choosing the Vickrey  auction are the low bandwidth and time consumption due to just one round  of bidding and the existence of a dominant bidding strategy under certain  conditions. We show that the Vickrey auction, despite its theoretical benets,  is inappropriate if \antisocial" agents participate in the auction process. More  specically, an antisocial attitude for economic agents that makes reducing  the prot of competitors their main goal besides maximizing their own prot  is introduced. Under this novel condition, agents need to deviate from the  dominant truth-telling strategy. This report presents a strategy for bidders...
Agents
roy00computational
A Computational Model of Word Learning from Multimodal Sensory Input How do infants segment continuous streams of speech to discover words of their language? Current theories emphasize the role of acoustic evidence in discovering word boundaries (Cutler 1991; Brent 1999; de Marcken 1996; Friederici & Wessels 1993; see also Bolinger & Gertsman 1957). To test an alternate hypothesis, we recorded natural infant-directed speech from caregivers engaged in play with their pre-linguistic infants centered around common objects. We also recorded the visual context in which the speech occurred by capturing images of these objects. We analyzed the data using two computational models, one of which processed only acoustic recordings, and a second model which integrated acoustic and visual input. The models were implemented using standard speech and vision processing techniques enabling the models to process sensory data. We show that using visual context in conjunction with spoken input dramatically improves learning when compared with using acoustic evidence alone....
ML
456508
Focused Web Crawling: A Generic Framework for Specifying the User Interest and for Adaptive Crawling Strategies Compared to the standard web search engines, focused  crawlers yield good recall as well as good  precision by restricting themselves to a limited domain.  In this paper, we do not introduce another focused  crawler, but we introduce a generic  framework for focused crawling consisting of two  major components: (1) Specification of the user interest  and measuring the resulting relevance of a  given web page. The proposed method of specifying  the user interest by a formula combining atomic  topics significantly improves the expressive  power of the user. (2) Crawling strategy. Ordering  the links at the crawl frontier is a challenging task  since pages of a low relevance may be on a path to  highly relevant pages. Thus, tunneling may be necessary.  The explicit specification of the user interest  allows us to define topic-specific strategies for  tunneling. Our system Ariadne is a prototype implementation  of the proposed framework. An experimental  evaluation of different crawling  strategies demonstrates the performance gain obtained  by focusing a crawl and by dynamically  adapting the focus.  1
ML
45929
Mixed Initiative in Interactions between Software Agents We have been working during the past several years on techniques for modeling the way that software agents can take and release the initiative while interacting together. We are interested in building multiagent systems composed of software agents that can interact with human users in sophisticated ways which are analogous to human conversations. In this paper, we describe two projects we have worked on: a multiagent approach for simulating conversations between software agents, and the Virtual Theater. 1. Introduction  The need for software agents that assist users in achieving various tasks, collaborate with them, entertain them, or even act on their behalf is getting greater. Software agents are computer systems that exploit their own knowledge bases, have their own goals and their own capabilities, perform actions, and interact with other agents as well as with people. Autonomy is an essential characteristic of such agents, which they express when they take the initiative. Agents t...
Agents
dyreson99capturing
Capturing and Querying Multiple Aspects of Semistructured Data Motivated to a large extent by the substantial and  growing prominence of the World-Wide Web and  the potential benefits that may be obtained by applying  database concepts and techniques to web  data management, new data models and query languages  have emerged that contend with web data.  These models organize data in graphs where nodes  denote objects or values and edges are labeled  with single words or phrases. Nodes are described  by the labels of the paths that lead to them, and  these descriptions serve as the basis for querying.  This paper proposes an extensible framework for  capturing and querying meta-data properties in a  semistructured data model. Properties such as  temporal aspects of data, prices associated with  data access, quality ratings associated with the  data, and access restrictions on the data are considered.  Specifically, the paper defines an extensible  data model and an accompanying query  language that provides new facilities for matching,  slicing, col...
DB
jonker99multiagent
A Multi-Agent Architecture for an Intelligent Website in Insurance . In this paper a multi-agent architecture for intelligent Websites is presented and applied in insurance. The architecture has been designed and implemented using the compositional development method for multi-agent systems DESIRE. The agents within this architecture are based on a generic broker agent model. It is shown how it can be exploited to design an intelligent Website for insurance, developed in co-operation with the software company Ordina Utopics and an insurance company. 1 Introduction  An analysis of most current business Websites from the perspectives of marketing and customer relations suggests that Websites should become more active and personalised, just as in the nonvirtual case where contacts are based on human servants. Intelligent agents provide the possibility to reflect at least a number of aspects of the nonvirtual situation in a simulated form, and, in addition, enables to use new opportunities for one-to-one marketing, integrated in the Website. The generic a...
Agents
wess94casebased
Case-Based and Symbolic Classification Algorithms . Contrary to symbolic learning approaches, that represent a learned concept explicitly, case-based approaches describe concepts implicitly  by a pair (CB;sim), i.e. by a measure of similarity sim and a set CB of cases. This poses the question if there are any differences concerning the learning power of the two approaches. In this article we will study the relationship between the case base, the measure of similarity, and the target concept of the learning process. To do so, we transform a simple symbolic learning algorithm (the version space algorithm) into an equivalent case-based variant. The achieved results strengthen the hypothesis of the equivalence of the learning power of symbolic and casebased methods and show the interdependency between the measure used by a case-based algorithm and the target concept. 1 Introduction  In this article we want to compare the learning power of two important learning paradigms -- the symbolic and the case-based approach [1, 4]. As a first step ...
ML
463357
Co-X: Defining what Agents Do Together Discussions of agent interactions frequently characterize behavior  as Coherent, collaborative, cooperative, competitive, or   coordinated. We propose a series of formal distinctions among   these terms and several others. We argue that all of these are specializations  of the more foundational category of correlation,   which can be measured by the joint information of a system. We   also propose congruence as a category orthogonal to the others,  reflecting the degree to which correlation and its specializations   satisfy user requirements. Then we explore the degree to which   lack of correlation can arise purposefully, and show the need to   use formal stochasticity in cases where such lack of correlation is   truly necessary (such as in stochastic search).   Keywords   Coordination, correlation, competition, contention, cooperation,   congruence, communication, command, constraint, construction,   conversation, stigmergy, agent interaction   1. 
Agents
zhu98line
On-Line Analytical Mining of Association Rules With wide applications of computers and automated data collection tools, massive amounts of data have been continuously collected and stored in databases, which creates an imminent need and great opportunities for mining interesting knowledge from data. Association rule mining is one kind of data mining techniques which discovers strong association or correlation relationships among data. The discovered rules may help market basket or cross-sales analysis, decision making, and business management. In this thesis, we propose and develop an interesting association rule mining approach, called on-line analytical mining of association rules, which integrates the recently developed OLAP (on-line analytical processing) technology with some efficient association mining methods. It leads to flexible, multi-dimensional, multi-level association rule mining with high performance. Several algorithms are developed based on this approach for mining various kinds of associations in multi-dimensional ...
ML
503243
Rivalry and Interference with a Head Mounted Display Perceptual factors that effect monocular, transparent (a.k.a "see-thru") head  mounted displays include binocular rivalry, visual interference, and depth of focus.
HCI
200188
Automatic Landmark Selection for Navigation with Panoramic Vision The use of visual landmarks for robot navigation is a promising field. It is apparent that the success of navigating by visual landmarks depends on the landmarks chosen. This paper reviews a monocular camera system proposed by [ Bianco and Zelinsky, 1999 ] which automatically selects landmarks and uses them for localisation and navigation tasks. The monocular system's landmark selection policy results in a limited area in which the robot can successfully localise itself. A new landmark navigation system which uses a panoramic vision system to overcome this restriction is proposed and the first steps taken in the development of the new system are reported. 1 Introduction  Visual navigation is one of the key problems in making successful autonomous robots. Vision as a sensor is the richest source of information about a mobile agents enviroment and as such contains information vital to solving navigation problems. One limit to visual navigation is the narrow field of view offered by norma...
Agents
518472
Incremental Learning of Control Knowledge For Nonlinear Problem Solving In this paper we advocate a learning method where a deductive  and an inductive strategies are combined to efficiently learn control  knowledge. The approach consists of initially bounding the explanation  to a predetermined set of problem solving features. Since there is no  proof that the set is sufficient to capture the correct and complete explanation  for the decisions, the control rules acquired are then refined,  if and when applied incorrectly to new examples. The method is especially  significant as it applies directly to nonlinear problem solving, where  the search space is complete. We present hamlet, a system where we  implemented this learning method, within the context of the prodigy  architecture. hamlet learns control rules for individual decisions corresponding  to new learning opportunities offered by the nonlinear problem  solver that go beyond the linear one. These opportunities involve, among  other issues, completeness, quality of plans, and opportunistic decision  making. Finally, we show empirical results illustrating hamlet's learning  performance.
ML
konrad99model
Model Generation for Natural-Language Semantic Analysis . Semantic analysis refers to the analysis of semantic representations by inference on the basis of semantic information and world knowledge. I present some potential applications of model generators and model generation theorem provers in the construction and analysis of natural-language semantics where both the process of model generation and the computed models are valuable sources of information. I discuss Bry and Torge's hyper-resolution tableaux calculus EP as an approach to model generation for natural-language semantic analysis. 1 Introduction  One goal of modern natural-language semantics has been to capture the conditions under which a sentence can be uttered truthfully. In logic-based semantic formalisms such as Discourse Representation Theory [17] or Montague Grammar [22], the representations constructed for natural-language sentences are logical formulas whose truth-conditions are described by their models. The construction of semantic representations requires, amongst oth...
DB
524920
AutoDoc: A Search and Navigation Tool for Web-Based Program Documentation We present a search and navigation tool for use with automatically generated program documentation, which builds trails in the information space. Three user interfaces are suggested, which show the web pages in context, and hence better explain the structure of the code.
IR
craswell00server
Server Selection on the World Wide Web We evaluate server selection methods in a Web environment, modeling a digital library which makes use of existing Web search servers rather than building its own index. The evaluation framework portrays the Web realistically in several ways. Its search servers index real Web documents, are of various sizes, cover different topic areas and employ different retrieval methods. Selection is based on statistics extracted from the results of probe queries submitted to each server. We evaluate published selection methods and a new method for enhancing selection based on expected search server effectiveness. Results show CORI to be the most effective of three published selection methods. CORI selection steadily degrades with fewer probe queries, causing a drop in early precision of as much as 0#05 (one relevant document out of 20). Modifying CORI selection based on an estimation of expected effectiveness disappointingly yields no significant improvement in effectiveness. However, modifying COR...
IR
stewart99single
Single Display Groupware: A Model for Co-present Collaboration We introduce a model for supporting collaborative work between people that are physically close to each other. We call this model Single Display Groupware (SDG). In this paper, we describe this model, comparing it to more traditional remote collaboration. We describe the requirements that SDG places on computer technology, and our understanding of the benefits and costs of SDG systems. Finally, we describe a prototype SDG system that we built and the results of a usability test we ran with 60 elementary school children.  Keywords  CSCW, Single Display Groupware, children, educational applications, input devices, Pad++, KidPad.  INTRODUCTION  In the early 1970's, researchers at Xerox PARC created an atmosphere in which they lived and worked with technology of the future. When the world's first personal computer, the Alto, was invented, it had only a single keyboard and mouse. This fundamental design legacy has carried through to nearly all modern computer systems. Although networks have...
HCI
azuma99tracking
Tracking in Unprepared Environments for Augmented Reality Systems Many Augmented Reality applications require accurate tracking. Existing tracking techniques require prepared environments to ensure accurate results. This paper motivates the need to pursue Augmented Reality tracking techniques that work in unprepared environments, where users are not allowed to modify the real environment, such as in outdoor applications. Accurate tracking in such situations is difficult, requiring hybrid approaches. This paper summarizes two 3DOF results: a real-time system with a compass -- inertial hybrid, and a non-real-time system fusing optical and inertial inputs. We then describe the preliminary results of 5- and 6-DOF tracking methods run in simulation. Future work and limitations are described.
HCI
jacob99software
A Software Model and Specification Language for Non-WIMP User Interfaces We present a software model and language for describing and programming the fine-grained aspects of interaction in a non-WIMP user interface, such as a virtual environment. Our approach is based on our view that the essence of a non-WIMP dialogue is a set of continuous relationships---most of which are temporary. The model combines a data-flow or constraint-like component for the continuous relationships with an event-based component for discrete interactions, which can enable or disable individual continuous relationships. To demonstrate our approach, we present the PMIW user interface management system for non-WIMP interactions, a set of examples running under it, a visual editor for our user interface description language, and a discussion of our implementation and our restricted use of constraints for a performance-driven interactive situation. Our goal is to provide a model and language that captures the formal structure of non-WIMP interactions in the way that various previous te...
HCI
keim99proverb
Proverb: The Probabilistic Cruciverbalist We attacked the problem of solving crossword puzzles by computer: given a set of clues and a crossword grid, try to maximize the number of words correctly filled in. After an analysis of a large collection of puzzles, we decided to use an open architecture in which independent programs specialize in solving specific types of clues, drawing on ideas from information retrieval, database search, and machine learning. Each expert module generates a (possibly empty) candidate list for each clue, and the lists are merged together and placed into the grid by a centralized solver. We used a probabilistic representation throughout the system as a common interchange language between subsystems and to drive the search for an optimal solution.  Proverb, the complete system, averages 95.3% words correct and 98.1% letters correct in under 15 minutes per puzzle on a sample of 370 puzzles taken from the New York Times and several other puzzle sources. This corresponds to missing roughly 3 words or 4 l...
IR
455336
Alternative Representations and Abstractions for Moving Sensors Databases Moving sensors refers to an emerging class of data intensive applications that impacts disciplines such as communication, health-care, scientific applications, etc. These applications consist of a fixed number of sensors that move and produce streams of data as a function of time. They may require the system to match these streams against stored streams to retrieve relevant data (patterns). With communication, for example, a speaking impaired individual might utilize a haptic glove that translates hand signs into written (spoken) words. The glove consists of sensors for dierent nger joints. These sensors report their location and values as a function of time, producing streams of data. These streams are matched against a repository of spatio-temporal streams to retrieve the corresponding English character or word.  The contributions of this study are two folds. First, it introduces a framework to store and retrieve "moving sensors" data. The framework advocates physical data independence and software-reuse. Second, we investigate alternative representations for storage and retrieval of data in support of query processing. We quantify the tradeoff associated with these alternatives using empirical data from RoboCup soccer matches.  
DB
184462
A Case-Based Reasoning Approach to Learning Control The paper describes an application of a case-based reasoning system TA3, to a control task in robotics. It differs from previous methods in its approach to relevancy-based retrieval, which allows for greater flexibility and for improved accuracy in performance. Most successful robotic manipulators are precise, fast, smooth and have high repeatability. Their major drawback is their tendency to have only a limited repertoire of movements that can only be extended by costly inverse kinematic calculations or direct teaching. Our approach also starts with a limited repertoire of movements, but can incrementally add new positions and thus allows for higher flexibility. The proposed architecture is experimentally evaluated on two real world domains, albeit simple, and the results are compared to other machine learning algorithms applied to the same problem. Keywords: Learning control systems, case-based reasoning, flexible manufacturing, inverse kinematic task   This research was supported by...
ML
524726
Personalized Information Management for Web Intelligence Web intelligence can be defined as the process of scanning and tracking information on the World Wide Web so as to gain competitive advantages. This paper describes a system known as Flexible Organizer for Competitive Intelligence (FOCI) that transforms raw URLs returned by internet search engines into personalized information portfolios. FOCI builds information portfolios by gathering and organizing on-line information according to a user's needs and preferences. Through a novel method called User-Configurable Clustering, a user can personalize his/her portfolios in terms of the content as well as the information structure. The personalized portfolios can then be used to track new information and organize them into appropriate folders accordingly. We show a sample session of FOCI which illustrates how a user may create and personalize an information portfolio according to his/her preferences and how the system discovers novel information groupings while organizing familiar information according to the userdefined themes.
IR
caropreso00statistical
Statistical Phrases in Automated Text Categorization In this work we investigate the usefulness of n-grams for document indexing in text categorization  (TC). We call n-gram a set t k of n word stems, and we say that t k occurs in a document  d j when a sequence of words appears in d j that, after stop word removal and stemming,  consists exactly of the n stems in t k , in some order. Previous researches have investigated the  use of n-grams (or some variant of them) in the context of specific learning algorithms, and  thus have not obtained general answers on their usefulness for TC. In this work we investigate  the usefulness of n-grams in TC independently of any specific learning algorithm. We do so  by applying feature selection to the pool of all #-grams (#  #  n), and checking how many  n-grams score high enough to be selected in the top # #-grams. We report the results of  our experiments, using several feature selection functions and varying values of #, performed  on the Reuters-21578 standard TC benchmark. We also report results of making actual use  of the selected n-grams in the context of a linear classifier induced by means of the Rocchio  method.
IR
nrvag98analytical
An Analytical Study of Object Identifier Indexing The object identifier index of an object-oriented database system is typically 20% of the size of the database itself, and for large databases, only a small part of the index fits in main memory. To avoid index retrievals becoming a bottleneck, efficient buffering strategies are needed to minimize the number of disk accesses. In this report, we develop analytical cost models which we use to find optimal sizes of index page buffer and index entry cache, for different memory sizes, index sizes, and access patterns. Because existing buffer hit estimation models are not applicable for index page buffering in the case of tree based indexes, we have also developed an analytical model for index page buffer performance. The cost gain from using the results in this report is typically in the order of 200-300%. Thus, the results should be of valuable use in optimizers and tools for configuration and tuning of object-oriented database systems. 1 Introduction  In a large OODB with logical object i...
DB
kruschwitz00extracting
Extracting Semistructured Data - Lessons Learnt The Yellow Pages Assistant (YPA) is a natural language dialogue system which guides a user through a dialogue in order to retrieve addresses from the Yellow Pages. Part of the work in this project is concerned with the construction of a Backend, i.e. the database extracted from the raw input text that is needed for the online access of the addresses. Here we discuss some aspects involved in this task as well as report on experiences which might be interesting for other projects as well.
DB
285081
Reasoning With Inconsistency in Structured Text Reasoning with inconsistency involves some compromise on classical logic. There is a range  of proposals for logics (called paraconsistent logics) for reasoning with inconsistency each with  pros and cons. Selecting an appropriate paraconsistent logic for an application depends on  the requirements of the application. Here we review paraconsistent logics for the potentially  significant application area of technology for structured text. Structured text is a general  concept that is implicit in a variety of approaches to handling information. Syntactically, an  item of structured text is a number of grammatically simple phrases together with a semantic  label for each phrase. Items of structured text may be nested within larger items of structured  text. The semantic labels in a structured text are meant to parameterize a stereotypical  situation, and so a particular item of structured text is an instance of that stereotypical  situation. Much information is potentially available as st...
DB
7036
Parsing As Information Compression By Multiple Alignment, Unification And Search: SP52 This article presents and discusses examples illustrating aspects of the proposition, described in the accompanying article (Wolff, 1998), that parsing may be understood as information compression by multiple alignment, unification and search (ICMAUS). The later examples show that the multiple alignment framework as described in the accompanying article has expressive power which is comparable with other `context sensitive' systems used to represent the syntax of natural languages. In all the examples, the SP52 model, described in the accompanying article, is capable of finding an alignment which is intuitively `correct' and assigning to it a `compression score' which is higher than for any other alignment. The congruance which has been found between this range of alignments produced by a system which is dedicated to information compression and what is judged to be `correct' in terms of linguistic intuition lends support to the hypothesis that linguistic intuition is itself a product o...
IR
292524
Document Classification with Unsupervised Artificial Neural Networks Text collections may be regarded as an almost perfect application arena for unsupervised neural networks. This is because many operations computers have to perform on text documents are classification tasks based on noisy patterns. In particular we rely on self-organizing maps which produce a map of the document space after their training process. From geography, however, it is known that maps are not always the best way to represent information spaces. For most applications it is better to provide a hierarchical view of the underlying data collection in form of an atlas where, starting from a map representing the complete data collection, different regions are shown at finer levels of granularity. Using an atlas, the user can easily "zoom" into regions of particular interest while still having general maps for overall orientation. We show that a similar display can be obtained by using hierarchical feature maps to represent the contents of a document archive. These neural networks have layerd architecture where each layer consists of a number of individual self-organizing maps. By this, the contents of the text archive may be represented at arbitrary detail while still having the general maps available for global orientation.
IR
51607
Boosting and Rocchio Applied to Text Filtering We discuss two learning algorithms for text filtering: modified Rocchio and a boosting algorithm called AdaBoost. We show how both algorithms can be adapted to maximize any general utility matrix that associates cost (or gain) for each pair of machine prediction and correct label. We first show that AdaBoost significantly outperforms another highly effective text filtering algorithm. We then compare AdaBoost and Rocchio over three large text filtering tasks. Overall both algorithms are comparable and are quite effective. AdaBoost produces better classifiers than Rocchio when the training collection contains a very large number of relevant documents. However, on these tasks, Rocchio runs much faster than AdaBoost. 1 Introduction  With the explosion in the amount of information available electronically, information filtering systems that automatically send articles of potential interest to a user are becoming increasingly important. If users indicate their interests to a filtering system...
ML
67194
An Fft-Based Algorithm For Multichannel Blind Deconvolution A new update equation for the general multichannel blind deconvolution (MCBD) of a convolved mixture of source signals is derived. It is based on the update equation for blind source separation (BSS), which has been shown to be an alternative interpretation [1] of the natural gradient applied to the minimization of some mutual information criterion [2]. Computational complexity is held at a minimum by carrying out the separation/equalization task in the frequency domain. The algorithm is compared to similar known blind algorithms and its validity is demonstrated by simulations of real-world acoustic filters. In order to assess the performance of the algorithm, performance measures for multichannel blind deconvolution of signals are given in the paper. 1. INTRODUCTION Blind separation, blind deconvolution and the combination of both, multichannel blind deconvolution, are tasks that have to be carried out in an increasing number of applications, particularly in acoustics and communicati...
ML
guestrin02algorithmdirected
Algorithm-Directed Exploration for Model-Based Reinforcement Learning in Factored MDPs One of the central challenges in reinforcement  learning is to balance the exploration/exploitation  tradeoff while scaling up to large problems. Although  model-based reinforcement learning has  been less prominent than value-based methods in  addressing these challenges, recent progress has  generated renewed interest in pursuing modelbased  approaches: Theoretical work on the exploration  /exploitation tradeoff has yielded provably  sound model-based algorithms such as E    Rmax , while work on factored MDP representations  has yielded model-based algorithms that can  scale up to large problems. Recently the benefits  of both achievements have been combined in the    algorithm of Kearns and Koller. In  this paper, we address a significant shortcoming  of Factored E    : namely that it requires an oracle  planner that cannot be feasibly implemented. We  propose an alternative approach that uses a practical  approximate planner, approximate linear programming,  that maintains desirable properties. Further,  we develop an exploration strategy that is targeted  toward improving the performance of the linear  programming algorithm, rather than an oracle  planner. This leads to a simple exploration strategy  that visits states relevant to tightening the LP solution,  and achieves sample efficiency logarithmic in  the size of the problem description. Our experimental  results show that the targeted approach performs  better than using approximate planning for implementing  either Factored E    or Factored Rmax .
ML
faber98representing
Representing School Timetabling in a Disjunctive Logic Programming Language In this paper, we show how school timetabling problems with preferences originating from didactical, organisational and personal considerations can be represented in a highly declarative and natural way, using an extension of disjunctive datalog by strong and weak (integrity) constraints. 1 Introduction  Almost all people have come across school timetabling during their lives. For a long time almost all school timetables were created manually, a timeconsuming task, which often yielded suboptimal schedules. In the last thirty years, however, systems have been designed which automate timetable creation.  Timetabling in general is the problem of finding suitable combinations of two or more types of resources which have to be at the same place during several discrete periods of time and which have to satisfy various additional constraints. Some of these constraints are strict while some are not (the latter express preferences or desiderata). In the case of school timetabling problems these...
DB
palacz99isomorphism
The Isomorphism Between a Class of Place Transition Nets and a Multi-Plane State Machine Agent Model Recently we introduced a multi-plane state machine model of an agent, released an implementation  of the model, and designed several applications of the agent framework. In this paper  we address the translation from the Petri net language to the Blueprint language used for agent  description as well as the translation from Blueprint to Petri Nets. The simulation of a class  of Place Transition Nets is part of an effort to create an agent--based workflow management  system.  Contents  1 Introduction 1 2 A Multi-Plane State Machine Agent Model 3 2.1 Bond Core . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Bond Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3 Bond Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.4 Using Planes to Implement Facets of Behavior . . . . . . . . . . . . . . . . . . . . . 5 3 Simulating a Class of Place-Transition Nets on the Bond ...
DB
chagas98learning
A Learning Mobile Robot: Theory, Simulation and Practice .  This paper presents an implementation of the sins multi-strategy learning controller for  mobile robot navigation. This controller uses low-level reactive control that is modulated  on-line by a learning system based on case-based reasoning and reinforcement learning.  The case-based reasoning part captures regularities in the environment. The reinforcement  learning part gradually improves the acquired knowledge. Evaluation of the controller is  presented in a real and in a simulated mobile robot.  1 Introduction  How to specify behaviour in a robot has come a long way since the low-level languages of assembly robotics (Lozano-Perez, 1982). The classical AI approach to control  1  proved too slow and too fragile for the real world but showed that representations of the environment, however difficult to maintain, produce interesting behaviour. In nouvelle AI, e.g. (Brooks, 1985; Brooks, 1991a; Brooks, 1991b), agents merely react to the current environmental situation posed, limited ...
ML
ding01probabilistic
A Probabilistic Model for Dimensionality Reduction in Information Retrieval and Filtering Dimension reduction methods, such as Latent Semantic Indexing (LSI), when applied to semantic spaces built upon text collections, improve information retrieval, information filtering and word sense disambiguation. A new dual probability model based on similarity concepts is introduced to explain the observed success. Semantic associations can be quantitatively characterized by their statistical significance, the likelihood. Semantic dimensions containing redundant and noisy information can be separated out and should be ignored because their contribution to the overall statistical significance is negative, giving rise to LSI: LSI is the optimal solution of the model. The peak in likelihood curve indicates the existence of an intrinsic semantic dimension. The importance of LSI dimensions follows the Zipf-distribution, indicating that LSI dimensions represent latent concepts. Document frequency of words follow the Zipf distribution, and the number of distinct words follows log-normal distribution. Experiments on four standard document collections both confirm and illustrate the results and concepts presented here.
IR
li01indexing
Indexing and Querying XML Data for Regular Path Expressions With the advent of XML as a standard for data  representation and exchange on the Internet,  storing and querying XML data becomes more  and more important. Several XML query languages  have been proposed, and the common  feature of the languages is the use of regular  path expressions to query XML data. This  poses a new challenge concerning indexing and  searching XML data, because conventional approaches  based on tree traversals may not meet  the processing requirements under heavy access  requests. In this paper, we propose a  new system for indexing and storing XML data  based on a numbering scheme for elements.  This numbering scheme quickly determines the  ancestor-descendant relationship between elements  in the hierarchy of XML data. We also  propose several algorithms for processing regular  path expressions, namely, (1) ##-Join for  searching paths from an element to another,  (2) ##-Join for scanning sorted elements and  attributes to find element-attribute pairs, and  (3) ##-Join for finding Kleene-Closure on repeated  paths or elements. The ##-Join algorithm  is highly effective particularly for searching  paths that are very long or whose lengths  are unknown. Experimental results from our  prototype system implementation show that the  proposed algorithms can process XML queries  with regular path expressions by up to an or-  #  This work was sponsored in part by National Science Foundation CAREER Award (IIS-9876037) and Research Infrastructure program EIA-0080123. The authors assume all responsibility for the contents of the paper.  Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its...
DB
pavlovic97visual
Visual Interpretation of Hand Gestures for Human-Computer Interaction: A Review The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. We survey the literature on vision-based hand gesture recognition within the context of its role in HCI. The number of approaches to video-based hand gesture recognition has grown in recent years. Thus, the need for systematization and analysis of different aspects of gestural interaction has developed. We discuss a complete model of hand gestures that possesses both spatial and dynamic properties of human hand gestures and can accommodate for all their natural types. Two classes of models that have been employed for interpretation of hand gestures for HCI are considered. The first utilizes 3D models of the human hand, while the second relies on the appearance of the human hand in the image. Investigation of model parameters and analysis feat...
HCI
fiebig97raw
RAW: A Relational Algebra for the Web The main idea underlying the paper is to extend the relational algebra such that it becomes possible to process queries against the World-Wide Web. These extensions are minor in that we tried to keep them at the domain level. Additionally to the known domains (int, bool, float, string), we introduce three new domains to deal with URLs, html-documents or fragments thereof, and path expressions. Over these domains we define several functions that are accessible from the algebra within the subscripts of the relational operators. The approach allows us to reuse the operators of the relational algebra without major modifications. Indead, the only extension necessary is the introduction of a map operator. Further, two modifications to the scan and the indexscan are necessary. Finally, the indexscan which has the functionality of a typical meta-search engine is capable of computing a unified rank based on the tuple order provided by the underlying search engines. 1 Introduction  The Web [2] w...
IR
311589
About Knowledge Discovery in Texts: A Definition and an Example This paper claims that Knowledge Discovery in texts (KDT) is a new scientific topic, stemming from Knowledge Discovery in DataBases (KDD), both of them relying on a rather specific definition of what "knowledge" is (it has to be, as it is often said, understandable and useful, and even surprising). We shall rapidly explore this definition of knowledge, and apply it to a definition of what KDT is. We shall then illustrate on a detailed example what kind of new results KDT can bring, showing their interest and that they have little to do with the results of the long existing Natural Language Processing (NLP) research field.
IR
cabri98reactive
Reactive Tuple Spaces for Mobile Agent Coordination Mobile active computational entities introduce peculiar problems in the coordination of distributed application components. The paper surveys several coordination models for mobile agent applications and outlines the advantages of uncoupled coordination models based on reactive blackboards. On this base, the paper presents the design and the implementation of the MARS system, a coordination tool for Java-based mobile agents. MARS defines Linda-like tuple spaces that can be programmed to react with specific actions to the accesses made by mobile agents.  Keywords: Mobile Agents, Coordination, Reactive Tuple Spaces, Java, WWW Information Retrieval  1. Introduction  Traditional distributed applications are designed as a set of processes statically assigned to given execution environments and cooperating in a (mostly) network-unaware fashion [Adl95]. The  mobile agent paradigm, instead, defines applications composed by network-aware entities (agents) capable of changing their execution env...
IR
120432
Dynamic Logic Programming In this paper we investigate updates of knowledge bases represented by logic programs. In order to represent negative information, we use generalized logic programs which allow default negation not only in rule bodies but also in their heads.We start by introducing the notion of an update $P\oplus U$ of a logic program $P$ by another logic program $U$. Subsequently, we provide a precise semantic characterization of $P\oplus U$, and study some basic properties of program updates. In particular, we show that our update programs generalize the notion of interpretation update. We then extend this notion to compositional sequences of logic programs updates $P_{1}\oplus P_{2}\oplus \dots $, defining a dynamic program update, and thereby introducing the paradigm of \emph{dynamic logic programming}. This paradigm significantly facilitates modularization of logic programming, and thus modularization of non-monotonic reasoning as a whole. Specifically, suppose that we are given a set of logic program modules, each describing a different state of our knowledge of the world. Different states may represent different time points or different sets of priorities or perhaps even different viewpoints. Consequently, program modules may contain mutually contradictory as well as overlapping information. The role of the dynamic program update is to employ the mutual relationships existing between different modules to precisely determine, at any given module composition stage, the declarative as well as the procedural semantics of the combined program resulting from the modules.
AI
broersen01boid
The BOID Architecture - Conflicts Between Beliefs, Obligations, Intentions and Desires In this paper we introduce the so-called Beliefs-Obligations-Intentions-Desires or BOID architecture. It contains feedback loops to consider all eects of actions before committing to them, and mechanisms to resolve conflicts between the outputs of its four components. Agent types such as realistic or social agents correspond to specific types of conflict resolution embedded in the BOID architecture.
Agents
461789
Extraction of Semantic XML DTDs from Texts Using Data Mining Techniques Although composed of unstructured texts, documents contained in textual archives such as public announcements, patient records and annual reports to shareholders often share an inherent though undocumented structure. In order to facilitate efficient, structure-based search in archives and to enable information integration of text collections with related data sources, this inherent structure should be made explicit as detailed as possible. Inferring a semantic and structured XML document type definition (DTD) for an archive and subsequently transforming the corresponding texts into XML documents is a successful method to achieve this objective. The main contribution of this paper is a new method to derive structured XML DTDs in order to extend previously derived flat DTDs. We use the DIAsDEM framework to derive a preliminary, unstructured XML DTD whose components are supported by a large number of documents. However, all XML tags contained in this preliminary DTD cannot a priori be assumed to be mandatory. Additionally, there is no fixed order of XML tags and automatically tagging an archive using a derived DTD always implicates tagging errors. Hence, we introduce the notion of probabilistic XML DTDs whose components are assigned probabilities of being semantically and structurally correct. Our method for establishing a probabilistic XML DTD is based on discovering associations between, resp. frequent sequences of XML tags.  Keywords  semantic annotation, XML, DTD derivation, knowledge discovery, data mining, clustering  
DB
46840
Greedy Function Approximation: A Gradient Boosting Machine Function approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest--descent minimization. A general gradient--descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least--squares, least--absolute--deviation, and Huber--M loss functions for regression, and multi--class logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are decision trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of decision trees produces competitive, highly robust, interpretable procedures for regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire 1996, and Friedman, Has...
ML
cachin99computationally
Computationally Private Information Retrieval with Polylogarithmic Communication We present a single-database computationally private information retrieval scheme with polylogarithmic communication complexity. Our construction is based on a new, but reasonable intractability assumption, which we call the \Phi-Hiding Assumption (\PhiHA): essentially the difficulty of deciding whether a small prime divides OE(m), where m is a composite integer of unknown factorization. Keywords: Integer factorization, Euler's function, \Phi-hiding assumption, private information retrieval. 1 Introduction Private information retrieval. The notion of private information retrieval (PIR for short) was introduced by Chor, Goldreich, Kushilevitz and Sudan [CGKS95] and has already received a lot of attention. The study of PIR is motivated by the growing concern about the user's privacy when querying a large commercial database. (The problem was independently studied by Cooper and Birman [CB95] to implement an anonymous messaging service for mobile users.) Ideally, the PIR problem consists...
IR
zhang01dyda
DyDa: Dynamic Data Warehouse Maintenance in a Fully Concurrent Environment Data warehouse is an emerging technology to support high-level decision making by gathering data from several distributed information sources into one repository. In dynamic environments, data warehouses must be maintained in order to stay consistent with the underlying sources. Recently proposed view maintenance algorithms tackle the problem of data warehouse maintenance under concurrent source data updates.While the view synchronization is to handle non-concurrent source schema changes. However, the concurrency between interleaved schema changes and data updates still remain unexplored problems.  In this paper, we propose a solution framework called DyDa that successfully addresses this problem. The DyDa framework detects concurrent schema changes by a broken query scheme and conicting concurrent data updates by a local timestamp scheme. A fundamental idea of the DyDa framework is the development of a two-layered architecture that separates the concerns for concurrent data updates and concurrent schema changes handling without imposing any restrictions on the sourse update transactions. At the lower level of the framework, it employs a local compensation algorithm to handle concurrent data updates, and a metadata name mapping strategy to handle concurrent source rename operations. At the higher level, it addresses the problem of concurrent source drop operations. For the latter problem, we design a strategy for the detection and correction of such concurrency and nd an executable plan for the aected updates. We then develop a new view adaption algorithm, called Batch-VA for execution of such plan to incrementally adapt the view. Put together, these algorithms are the rst to provide a complete solution to data warehouse management in a fully concurrent environment....
DB
319242
MIND-WARPING: Towards Creating a Compelling Collaborative Augmented Reality Game Computer gaming offers a unique test-bed and market for advanced concepts in computer science, such as Human Computer Interaction (HCI), computer-supported collaborative work (CSCW), intelligent agents, graphics, and sensing technology. In addition, computer gaming is especially wellsuited for explorations in the relatively young fields of wearable computing and augmented reality (AR). This paper presents a developing multi-player augmented reality game, patterned as a cross between a martial arts fighting game and an agent controller, as implemented using the Wearable Augmented Reality for Personal, Intelligent, and Networked Gaming (WARPING) system. Through interactions based on gesture, voice, and head movement input and audio and graphical output, the WARPING system demonstrates how computer vision techniques can be exploited for advanced, intelligent interfaces.  Keywords  Augmented reality, wearable computing, computer vision  1. INTRODUCTION: WHY GAMES?  Computer gaming provides...
HCI
520409
User Modelling as an Aid for Human Web This paper explores how user modelling can work as an aid for human assistants in a user support system for web sites. Information about the user can facilitate for the assistant the tailoring of the consultation to the individual needs of the user. Such information can be represented and structured in a user model made available for the assistant. A user modelling approach has been implemented and deployed in a real web environment as part of a user support system. Following the deployment we have analysed consultation dialogue logs and answers to a questionnaire for participating assistants. The initial results show that assistants consider user modelling to be helpful and that consultation dialogues can be an important source for user model data collection.
IR
amitay98using
Using Common Hypertext Links to Identify the Best Phrasal Description of Target Web Documents This paper describes previous work which studied and compared the distribution of words in web documents with the distribution of words in "normal" flat texts. Based on the findings from this study it is suggested that the traditional IR techniques cannot be used for web search purposes the same way they are used for "normal" text collections, e.g. news articles. Then, based on these same findings, I will describe a new document description model which exploits valuable anchor text information provided on the web that is ignored by the traditional techniques.  The problem  Amitay (1997) has found, through a corpus analysis of a 1000 web pages that the lexical distribution in documents which were written especially for the web (home pages), is significantly different than the lexical distribution observed in a corpus of "normal" English language (the British National Corpus - 100,000,000 words). For example, in the web documents collection there were some HTML files which contained no v...
IR
driessens01speeding
Speeding up Relational Reinforcement Learning Through the Use of an Incremental First Order Decision Tree Learner Relational reinforcement learning (RRL) is a learning technique  that combines standard reinforcement learning with inductive logic  programming to enable the learning system to exploit structural knowledge  about the application domain.
ML
braumandl98evaluating
Evaluating Functional Joins Along Nested Reference Sets in Object-Relational and Object-Oriented Databases Previous work on functional joins was constrained in two ways: (1) all approaches we know assume references being implemented as physical object identifiers (OIDs) and (2) most approaches are, in addition, limited to single-valued reference attributes. Both are severe limitations since most object-relational and all object-oriented database systems do support nested reference sets and many object systems do implement references as location-independent (logical) OIDs. In this work, we develop a new functional join algorithm that can be used for any realization form for OIDs (physical or logical) and is particularly geared towards supporting functional joins along nested reference sets. The algorithm can be applied to evaluate joins along arbitrarily long path expressions which may include one or more reference sets. The new algorithm generalizes previously proposed partition-based pointer joins by repeatedly applying partitioning with interleaved re-merging before evaluating the next fu...
DB
howell98towards
Towards Visually Mediated Interaction using Appearance-Based Models . This paper reports initial research on supporting Visually Mediated Interaction  (VMI) by developing generic expression models and person-specific and  generic gesture models for the control of active cameras. We investigate the recognition  of both head pose and expression using simple generalisation of trained  generic models using Radial Basis Function (RBF) networks. Then we go on  to describe a time-delay variant (TDRBF) of the network and evaluate its performance  on recognising simple pointing and waving hand gestures in image sequences.  Experimental results are presented that show that high levels of performance  in gesture recognition can be obtained using these techniques, both for  particular individuals and across a set of individuals. Characteristic visual evidence  can be automatically selected and used even to recognise individuals from their  gestures, depending on the task demands.  1 Introduction  This paper reports initial research on supporting Visually Mediated ...
HCI
aberg01collection
Collection and Exploitation of Expert Knowledge in Web Assistant Systems Recent research and commercial developments have highlighted the importance of human involvement in user support for web information systems. In our earlier work a web assistant system has been introduced, which is a hybrid support system with human web assistants and computer-based support. An important issue with web assistant systems is how to make optimal use of these support resources. We use a knowledge management approach with frequently asked questions for a question-answering system that acts as a question filter for the human assistants. Knowledge is continuously collected from the assistants and exploited to augment the question-answering capabilities. Our system has been deployed and evaluated by an analysis of conversation logs and questionnaires for users and assistants. The results show that our approach is feasible and useful. Lessons learned are summarised in a set of recommendations.
IR
williams01searchable
Searchable Words on the Web In designing data structures for text databases, it is valuable to know how many different words are likely to be encountered in a particular collection. For example, vocabulary accumulation is central to index construction for text database systems; it is useful to be able to estimate the space requirements and performance characteristics of the main-memory data structures used for this task. However, it is not clear how many distinct words will be found in a text collection or whether new words will continue to appear after inspecting large volumes of data. We propose practical definitions of a word, and investigate new word occurrences under these models in a large text collection. We inspected around two billion word occurrences in 45 gigabytes of world-wide web documents, and found just over 9.74 million different words in 5.5 million documents; overall, 1 word in 200 was new. We observe that new words continue to occur, even in very large data sets, and that choosing stricter definitions of what constitutes a word has only limited impact on the number of new words found.
IR
turk00perceptual
Perceptual User Interfaces For some time, graphical user interfaces (GUIs) have been the dominant platform for human computer interaction. The GUI-based style of interaction has made computers simpler and easier to use, especially for office productivity applications where computers are used as tools to accomplish specific tasks. However, as the way we use computers changes and computing becomes more pervasive and ubiquitous, GUIs will not easily support the range of interactions necessary to meet users ’ needs. In order to accommodate a wider range of scenarios, tasks, users, and preferences, we need to move toward interfaces that are natural, intuitive, adaptive, and unobtrusive. The aim of a new focus in HCI, called Perceptual User Interfaces (PUIs), is to make human-computer interaction more like how people interact with each other and with the world. This paper describes the emerging PUI field and then reports on three PUImotivated projects: computer vision-based techniques to visually perceive relevant information about the user. 1.
HCI
landau02comparison
A Comparison between ATNoSFERES and XCSM In this paper we present ATNoSFERES, a new framework based on an indirect encoding Genetic Algorithm which builds finite-state automata controllers able to deal with perceptual aliasing. We compare it with XCSM, a memory-based extension of the most studied Learning Classifier System, XCS, through a benchmark experiment. We then discuss the assets and drawbacks of ATNoSFERES in the context of that comparison.
ML
wendler98cbr
CBR for Dynamic Situation Assessment in an Agent-Oriented Setting . In this paper, we describe an approach of using case-based reasoning in an agent-oriented setting. CBR is used in a real-time environment to select actions of soccer players based on previously collected experiences encoded as cases.  Keywords: Case-based reasoning, artificial soccer, agentoriented programming. 1 Introduction  In recent years, agent-oriented technologies have caught much attention both in research and commercial areas. To provide a testbed for developing, evaluating, and testing various agent architectures, RoboCup Federation started the Robot World Cup Initiative (RoboCup), which is an attempt to foster AI and intelligent robotics research by providing a somewhat standardized problem where wide range of technologies from AI and robotics can be integrated and examined. In particular, during the annual RoboCup championships different teams utilizing different models and methods compete with each other in the domain of soccer playing. Key issues in this domain are that...
AI
pynadath99toward
Toward Team-Oriented Programming The promise of agent-based systems is leading towards the development  of autonomous, heterogeneous agents, designed by a variety of research/industrial  groups and distributed over a variety of platforms and environments.
Agents
self01design
Design & Specification of Dynamic, Mobile, and Reconfigurable Multiagent Systems Multiagent Systems use the power of collaborative software agents to solve complex distributed problems. There are many Agent-Oriented Software Engineering (AOSE) methodologies available to assist system designers to create multiagent systems. However, none of these methodologies can specify agents with dynamic properties such as cloning, mobility or agent instantiation.  This thesis starts the process to bridge the gap between AOSE methodologies and dynamic agent platforms by incorporating mobility into the current Multiagent Systems Engineering (MaSE) methodology. Mobility was specified within all components composing a mobile agent class. An agent component was also created that integrated the behavior of the components within an agent class and was transformed to handle most of the move responsibilities for a mobile agent. Those agent component and component mobility transformations were integrated into agentTool as a proof-of-concept and a demonstration system built on the mobility specifications was implemented for execution on the Carolina mobile agent platform.  1  DESIGN & SPECIFICATION OF DYNAMIC, MOBILE, AND  RECONFIGURABLE MULTIAGENT SYSTEMS  I. 
Agents
339430
The Chromatic Structure of Natural Scenes We applied Independent Component Analysis (ICA) to hyperspectral images in  order to learn an ecient representation of color in natural scenes. In the spectra of  single pixels, the algorithm found basis functions that had broadband spectra, as well  as basis functions that were similar to natural reectance spectra. When applied to  small image patches, the algorithm found basis functions that were achromatic and  others with overall chromatic variation along lines in color space, indicating color  opponency. The directions of opponency were not strictly orthogonal. Comparison  1  with Principal Component Analysis (PCA) on the basis of statistical measures such as  average mutual information, kurtosis and entropy, shows that the ICA transformation  results in much sparser coecients and gives higher coding eciency. Our ndings  suggest that non-orthogonal opponent encoding of photoreceptor signals leads to  higher coding eciency, and that ICA may be used to reveal the underlying stati...
ML
nigam01using
Using Unlabeled Data to Improve Text Classification One key difficulty with text classification learning algorithms is that they require many hand-labeled examples to learn accurately. This dissertation demonstrates that supervised learning algorithms that use a small number of labeled examples and many inexpensive unlabeled examples can create high-accuracy text classifiers. By assuming that documents are created by a parametric generative model, Expectation-Maximization (EM) finds local maximum a posteriori models and classifiers from all the data -- labeled and unlabeled. These generative models do not capture all the intricacies of text; however on some domains this technique substantially improves classification accuracy, especially when labeled data are sparse. Two problems arise from this basic approach. First, unlabeled data can hurt performance in domains where the generative modeling assumptions are too strongly violated. In this case the assumptions can be made more representative in two ways: by modeling sub-topic class structure, and by modeling super-topic hierarchical class relationships. By doing so, model probability and classification accuracy come into correspondence, allowing unlabeled data to improve classification performance. The second problem is that even with a representative model, the improvements given by unlabeled data do not sufficiently compensate for a paucity of labeled data. Here, limited labeled data provide EM initializations that lead to low-probability models. Performance can be significantly improved by using active learning to select high-quality initializations, and by using alternatives to EM that avoid low-probability local maxima.
IR
brand00style
Style Machines We approach the problem of stylistic motion synthesis by learning motion patterns from  a highly varied set of motion capture sequences. Each sequence may have a distinct  choreography, performed in a distinct style. Learning identifies common choreographic  elements across sequences, the different styles in which each element is performed, and  a small number of stylistic degrees of freedom which span the many variations in the  dataset. The learned model can synthesize novel motion data in any interpolation or  extrapolation of styles. For example, it can convert novice ballet motions into the more  graceful modern dance of an expert. The model can also be driven by video, by scripts,  or even by noise to generate new choreography and synthesize virtual motion-capture  in many styles.  In Proceedings of SIGGRAPH 2000, July 23-28, 2000. New Orleans, Louisiana, USA.  This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in  whole o...
ML
kendall00java
A Java Application Framework for Agent Based Systems Agents are the next significant software abstraction, especially for distributed systems. Agent based systems have been developed in response to the following requirements: - Personalized and customized user interfaces that are pro-active in assisting the user - Adaptable, fault tolerant distributed systems that solve complex problems - Open systems where components come and go and new components are continually added. - Migration and load balancing across platforms, throughout a network. - New metaphors, such as negotiation, for solving distributed, multi- disciplinary problems. Agents appear in a wide range of applications, but all agent development to date has been done independently by each development team. This has led to several problems, including i) duplication of effort, ii) inability to satisfy industrial strength requirements for security and scalability, and iii) incompatibility of agent systems. The application framework described in this chapter captures and clarifies th...
Agents
beneventano00information
Information Integration: the MOMIS Project Demonstration ranted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 26th VLDB Conference,  Cairo, Egypt, 2000.  2  MOMIS is a joint project among the Universit`a di Modena e Reggio Emilia, the Universit`a di Milano, and the Universit`a di Brescia within the national research project INTERDATA, theme n.3 "Integration of Information over the Web", coordinated by V. De Antonellis, Universit`a di Brescia.  1. a common data model, ODM I 3  , which is defined according to the ODL I 3 language, to describe source schemas for integration purposes. ODM I 3 and ODL I 3<F12.24
DB
82903
The Relationlog System User Manual - Release 1.0 This document introduces the Relationlog system through the use of examples. All examples used here are available as part of the Relationlog release in the directory named demo.  2 INSTALLING RELATIONLOG 3 2 Installing Relationlog
DB
292223
Process- and Agent-Based Modelling Techniques for Dialogue Systems and Virtual Environments This text presents results of ongoing research, which is aimed at developing a framework for developing multimodal natural language dialogue systems operating within virtual environments. The aspects of multimodality and presence in a virtual environment are chosen as the main focus of this research. It may be argued that specification techniques would form the basis of such a framework. Therefore, a general overview and evaluation is given of existing specification techniques for interactive systems, based on both literature and previous research results. This includes the object-oriented model, process algebras, interactor models, and agent systems. Agent systems are further subdivided into intentional logics, production rule systems, agent communication languages, agent platforms, and agent architectures. A new agent system is proposed, which is based on update notification mechanisms as found in interactor models, and the `facilitator' function as found in some agent platfo...
HCI
horrocks98using
Using an Expressive Description Logic: FaCT or Fiction? Description Logics form a family of formalisms closely related to semantic networks but with the distinguishing characteristic that the semantics of the concept description language is formally defined, so that the subsumption relationship between two concept descriptions can be computed by a suitable algorithm. Description Logics have proved useful in a range of applications but their wider acceptance has been hindered by their limited expressiveness and the intractability of their subsumption algorithms. This paper addresses both these issues by describing a sound and complete tableaux subsumption testing algorithm for a relatively expressive Description Logic which, in spite of the logic's worst case complexity, has been shown to perform well in realistic applications. 1 INTRODUCTION Description Logics (DLs) form a family of formalisms which have grown out of knowledge representation techniques using frames and semantic networks
AI
408262
Conceptual Linking: Ontology-based Open Hypermedia This paper describes the attempts of the COHSE project to define and deploy a Conceptual Open Hypermedia Service. Consisting of  . an ontological reasoning service which is used to represent a sophisticated conceptual model of document terms and their relationships;  . a Web-based open hypermedia link service that can offer a range of different linkproviding facilities in a scalable and non-intrusive fashion;  and integrated to form a conceptual hypermedia system to enable documents to be linked via metadata describing their contents and hence to improve the consistency and breadth of linking of WWW documents at retrieval time (as readers browse the documents) and authoring time (as authors create the documents).  Introduction: concepts and metadata  Metadata is data that describes other data to enhance its usefulness. The library catalogue or database schema are canonical examples. For our purposes, metadata falls into three broad categories:  . Catalogue information: e.g. the artist ...
IR
franconi00general
A General Framework for Evolving Schemata Support . In this paper a semantic approach for the specication and the management of  databases with evolving schemata is introduced. It is shown how a general object-oriented  model for schema versioning and evolution can be formalised; how the semantics of schema  change operations can be dened; how interesting reasoning tasks can be supported, based  on an encoding in description logics.  1 Introduction  The problems of schema evolution and versioning arose in the context of long-lived database applications, where stored data were considered worth surviving changes in the database schema [23]. According to a widely accepted terminology [19], a database supports schema evolution if it permits modications of the schema without the loss of extant data; in addition, it supports schema versioning if it allows the querying of all data through user-denable version interfaces. For the sake of brevity, schema evolution can be considered as a special case of schema versioning where only the curr...
DB
wolpert99general
General Principles Of Learning-Based Multi-Agent Systems We consider the problem of how to design large decentralized multi-agent systems (MAS’s) in an automated fashion, with little or no hand-tuning. Our approach has each agent run a reinforcement learning algorithm. This converts the problem into one of how to automatically set/update the reward functions for each of the agents so that the global goal is achieved. In particular we do not want the agents to “work at cross-purposes ” as far as the global goal is concerned. We use the term artificial COllective INtelligence (COIN) to refer to systems that embody solutions to this problem. In this paper we present a summary of a mathematical framework for COINs. We then investigate the real-world applicability of the core concepts of that framework via two computer experiments: we show that our COINs perform near optimally in a difficult variant of Arthur’s bar problem [1] (and in particular avoid the tragedy of the commons for that problem), and we also illustrate optimal performance for our COINs in the leader-follower problem. 1
ML
cassell00nudge
Nudge Nudge Wink Wink: Elements of Face-to-Face Conversation for Embodied Conversational Agents Introduction  Only humans communicate using language and carry on conversations with one another. And the skills of conversation have developed in humans in such a way as to exploit all of the unique affordances of the human body. We make complex representational gestures with our prehensile hands, gaze away and towards one another out of the corners of our centrally set eyes, and use the pitch and melody of our voices to emphasize and clarify what we are saying. Perhaps because conversation is so defining of humanness and human interaction, the metaphor of face-to-face conversation has been applied to human-computer interface design for quite some time. One of the early arguments for the utility of this metaphor gave a list of features of face-to-face conversation that could be applied fruitfully to human-computer interaction, including mixed initiative, nonverbal communication, sense of presence, rules for transfer of control (Nickerson 1976). However, although these feature
HCI
tourapis01temporal
Temporal Interpolation Of Video Sequences Using Zonal Based Algorithms Temporal interpolation has been recently proposed as a solution for increasing temporal resolution or even for predicting missing or corrupted frames within a video sequence. In this paper new techniques on temporal interpolation are presented, by mainly exploiting properties of the very popular and highly efficient zonal based motion estimation algorithms, and by introducing several other techniques such as multihypothesis motion compensation, motion classification and temporal/spatial filtering. In addition we further give an analysis on when temporal interpolation should be employed, thus possibly avoiding unwanted artifacts created from this process, while at the same time significantly improving the overall performance of the interpolation.
ML
jantke93types
Types of Incremental Learning This paper is intended to introduce a closer  look at incremental learning by developing the  two concepts of informationally incremental  learning and operationally incremental learning.  These concept are applied to the problem  of learning containment decision lists for  demonstrating its relevance.  1 Introduction  The intention of the present paper is to introduce two new notions in incremental learning which allow a classification of phenomena finer than known so far in the area. These concepts are denoted by the phrases informationally incremental learning and operationally incremental learning, respectively. Roughly spoken, informationally incremental algorithms are required to work incrementally as usual, i.e. they have no permission to look back at the whole history of information presented during the learning process. Operationally incremental learning algorithms may have permission to look back, but they are not allowed to use information of the past in some effective way. ...
ML
oakley00putting
Putting the Feel in `Look and Feel' Haptic devices are now  commercially available and thus  touch has become a potentially  realistic solution to a variety of  interaction design challenges. We  reportonanexperimental  investigation of the use of touch as  a way of reducing visual overload in  the conventional desktop. In a  two-phase study, we investigated  the use of the PHANToM haptic  device as a means of interacting  with a conventional graphical user  interface. The first experiment  compared the effects of four  different haptic augmentations on  user performance in a simple  targeting task. The second  experiment involved a more  ecologically oriented searching and  scrolling task. Results indicated that  the haptic effects did not improve  users performance in terms of task  completion time. However, the  number of errors made was  significantly reduced. Subjective  workload measures showed that  participants perceived many aspects  of workload as significantly less  with haptics. The results are  described and the implications for  the use of haptics in user interface  design are discussed.  Subject  Areas:  Multimodal interaction  Augmented reality  Empirical (quantitative)  Evaluation  Input devices  Interaction technology  CHI 2000  Home Page  1of2 13/09/99 11:36  CHI 2000 Call For Papers http://msrconf.microsoft.com/sigchi/AuthorListEditProcess.asp  Tactile or gestural I/O  CLICK HERE to view the anonymous version of  your paper that we have received. Your paper  must be viewable by the standard Adobe Acrobat  viewer. If you cannot get this to work, then your  hardcopy version will be sent to reviewers.  Remember, in addition to your electronic  submission, you must submit your paper in  hardcopy form to CHI 2000 , c/o Mary  Czerwinski , Microsoft Research , One Microsoft  Way , Redmond, ...
HCI
cassell00more
More Than Just Another Pretty Face: Embodied Conversational Interface Agents this article I describe some of the features of human-human conversation that are being implemented in this new genre of embodied conversational agents. Then I describe an embodied conversational agent that is based on these features. I argue that, because conversation is such a primary skill for humans, and such an early-learned skill (practiced, in fact, between infants and mothers who take turns cooing and burbling at one another), and because the body is so well-equipped to support conversation, embodied conversational agents may turn out to be powerful ways for humans to interact with their computers. However, I claim that in order for embodied conversational agents to live up to their promise, their implementations must be based on actual study of human -- human conversation, and their architectures must reflect some of the intrinsic properties found there. Embodied conversational interfaces are not just computer interfaces represented by way of human or animal bodies. And they are not just interfaces where those human or animal bodies are lifelike or believable in their actions and their reactions to human users. Embodied conversational interfaces are specifically conversational in their behaviors, and specifically human-like in the way they use their bodies in conversation. That is, embodied conversational agents may be defined as those that have the same properties as humans in face-to-face conversation, including:
HCI
kumar98beyond
Beyond Best Effort: Router Architectures for the Differentiated Services of Tomorrow's Internet With the transformation of the Internet to a commercial infrastructure, the ability to provide differentiated services to users with widely varying requirements is rapidly becoming as important as meeting the massive increases in bandwidth demand. Hence, while deploying routers, switches, and transmission systems of ever increasing capacity, Internet Service Providers would also like toprovide customer specific differentiated services using the same shared network infrastructure. In this paper, we describe router architectures that can support the two trends of rising bandwidth demand and rising demand for differentiated services. We focus on router mechanisms that can support differentiated services at a level not being contemplated in proposals currently under consideration due to concern regarding their implementability at high-speeds. We consider the types of differentiated services that service providers may want to offer and then discuss the mechanisms needed in routers to support them. We describe plausible implementations of these mechanisms (the scalability and performance of which have been demonstrated by implementation in a prototype system) and argue that it is technologically possible to raise considerably the level of differentiated services that service providers can offer their customers, and that it is not necessary to restrict differentiated services to rudimentary offerings even in very high-speed networks.  
DB
crabbe99secondorder
Second-Order Networks for Wall-Building Agents This paper describes robust neurocontrollers for groups of agents that perform construction tasks. They enable agents to balance multiple goals, perform sequences of actions and survive while building walls, corridors, intersections, and briar patches.
Agents
resnik98parallel
Parallel Strands: A Preliminary Investigation into Mining the Web for Bilingual Text . Parallel corpora are a valuable resource for machine translation, but at present their availability and utility is limited by genreand domain-specificity, licensing restrictions, and the basic difficulty of locating parallel texts in all but the most dominant of the world's languages. A parallel corpus resource not yet explored is the World Wide Web, which hosts an abundance of pages in parallel translation, offering a potential solution to some of these problems and unique opportunities of its own. This paper presents the necessary first step in that exploration: a method for automatically finding parallel translated documents on the Web. The technique is conceptually simple, fully language independent, and scalable, and preliminary evaluation results indicate that the method may be accurate enough to apply without human intervention. 1 Introduction  In recent years large parallel corpora have taken on an important role as resources in machine translation and multilingual natural la...
IR
phan02extensible
An Extensible and Scalable Content Adaptation Pipeline Architecture to Support Heterogeneous Clients The importance of middleware and content adaptation has previously been demonstrated for pervasive use of Web-based applications. In this paper we propose a modular, extensible, and scalable middleware component called the Content Adaptation Pipeline that performs content adaptation on arbitrarily complex data types not limited to text and graphic images. Furthermore, the architecture can be used as part of many client-server applications, not just Web browsers. In our work we leverage the XML language as a uniform means to describe all the elements in our architecture, including the client device and user profiles, the data characteristics, the transcoding operations performed on the data, and the resultant adapted data. We illustrate the flexibility of our architecture to support new data types and adaptation operations by first showing its use with data from a real-world medical application and then extending its capabilities to handle animated graphics and also real-time streaming RTP data. Finally, we demonstrate scalability in our architecture by executing the Content Adaptation Pipeline over a distributed set of servers running an efficient protocol.  
HCI
andersen00what
What semiotics can and cannot do for HCI Semiotics is "the mathematics of the humanities" in the sense that it provides an abstract language covering a diversity of special sign-usages (language, pictures, movies, theatre, etc.). In this capacity, semiotics is helpful for bringing insights from older media to the task of interface design, and for defining the special characteristics of the computer medium. However, semiotics is not limited to interface design but may also contribute to the proper design of program texts and yield predictions about the interaction between computer systems and their context of use.  Keywords  Computer based signs, aesthetics, context of use.  0. The mathematics of the humanities.  In my experience, semiotics can be useful for the HCI-field, but the purely analytic character of traditional semiotics has to be supplemented by a constructive one. In addition, the semiotic community has to acquire a solid understanding of the technical possibilities and limitations of computer systems in order to b...
HCI
gajos01design
Design Principles For Resource Management Systems For Intelligent Spaces The idea of ubiquitous computing and smart environments is no longer a dream and has long become a serious area of research and soon this technology will start entering our every day lives. There are two major obstacles that prevent this technology from spreading. First, di#erent smart spaces are equipped with very di#erent kinds of devices (e.g. a projector vs. a computer monitor, vs. a TV set). Second, multiple applications running in a space at the same time inevitably contend for those devices and other scarce resources. The underlying software in a smart space needs to provide tools for self-adaptivity in that it shields the rest of the software from the physical constraints of the space, and that it dynamically adjusts the allocation of scarce resources as the number and priorities of active tasks change.
HCI
491341
Multi-Document Summarization and Visualization in the Informedia Digital Video Library The Informedia Digital Video Library project provided a technological foundation for full content indexing and retrieval of video and audio media. The library now contains over 2000 hours of video and is growing daily. A good query engine is not sufficient for information retrieval because often the candidate result sets grow in number as the library grows. Video digests summarize sets of stories from the library, providing users with a visual mechanism for interactive browsing and query refinement. These digests are generated dynamically under the direction of the user based on automatically derived metadata from the video library. Informedia Digital Video Library Foundation Work The Informedia Digital Video Library focused on the development and integration of technologies for information extraction from video and audio content to enable its full content search and retrieval. Over two terabytes (2000 hours, 5,000 segments) of online data was collected, with automatically generated metadata and indices for retrieving video segments from this library. Informedia successfully pioneered the automatic creation of multimedia abstractions, demonstrated empirical proofs of their relative benefits, and gathered usage data of different summarizations and abstractions. Fundamental research and prototyping was conducted in the following areas, shown with a sampling of references to particular work:
IR
537127
The eSleeve: A Novel Wearable Computer Configuration for the Discovery of Situated Information This paper describes work in progress on wearable computing configurations which provide audio and visual output based on the position and orientation of the user. We introduce the `eSleeve' - a wearable wrist computer with position and heading sensors combined with a user interface employing speech recognition and a small display.
HCI
dinverno97formal
A Formal Specification of dMARS   The Procedural Reasoning System (PRS) is the best established agent  architecture currently available. It has been deployed in many major industrial  applications, ranging from fault diagnosis on the space shuttle to air traffic management  and business process control. The theory of PRS-like systems has also  been widely studied: within the intelligent agents research community, the beliefdesire  -intention (BDI) model of practical reasoning that underpins PRS is arguably  the dominant force in the theoretical foundations of rational agency. Despite  the interest in PRS and BDI agents, no complete attempt has yet been made  to precisely specify the behaviour of real PRS systems. This has led to the development  of a range of systems that claim to conform to the PRS model, but which  differ from it in many important respects. Our aim in this paper is to rectify this  omission. We provide an abstract formal model of an idealised dMARS system  (the most recent implementation of the PRS...
Agents
wang01agentoriented
Agent-Oriented Requirements Engineering Using ConGolog and i* : Agent-oriented approaches are becoming popular in software engineering, both as architectural frameworks, and as modeling frameworks for requirements engineering and design. i* is an informal diagram-based language for early-phase requirements engineering that supports the modeling of social dependencies between agents and how process design choices affect the agents' goals both functional and non-functional. ConGolog is an expressive logic-based formalism for specifying processes that involves multiple agents. Tools are being developed to support the validation of ConGolog process models though simulation and verification. The two formalisms complement each other well, and in this work, we develop a methodology for their combined use in requirements engineering. The i* SR-diagram language is extended with process specification annotations, which allow the SR model of a system to be refined and then mapped into a ConGolog model. The mapping must satisfy a set of mapping rules, which ensure that it specifies which elements in the two models are related and that the models are consistent. The methodology is illustrated on a meeting scheduling application example. 1
Agents
537920
Query Evaluation for Mediators over Web Catalogs The Web catalogs like Yahoo! and Open Directory  are very useful for browsing and querying the Web. Although  they index only a fraction of the pages that are indexed by search  engines these catalogs are hand-crafted by domain experts and  are therefore of high quality. We present a model for building  mediators over Web catalogs, so as to provide users with customized   views of such catalogs. We focus on query evaluation,  specifically on the complexity of query answering by the mediator.
IR
huang01programmability
Programmability of Intelligent Agent Avatars In this paper, we propose an approach to the programmability of intelligent agent avatars, supported by the distributed logic programming language DLP. Intelligent agent avatars can be considered as one of the applications of web agents. As one of the testbeds of 3D web agents, we are developing and implementing soccer playing avatars. We discuss how the language DLP can be used to support soccer playing avatars using rules to guide their behaviors in networked virtual environments. Categories and Subject Descriptors I.2 [Computing Methodologies]: Artificial Intelligence; H.4.m [Information Systems]: Miscellaneous General Terms Intelligent Agent Keywords avatar, intelligent agent, distributed logic programming, networked virtual environment 1.
Agents
kubiatowicz00oceanstore
OceanStore: An Architecture for Global-Scale Persistent Storage OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development. 1
HCI
457806
An Expert System for Analyzing Firewall Rules When deploying firewalls in an organization, it is essential to verify that the firewalls are configured properly. The problem of finding out what a given firewall configuration does occurs, for instance, when a new network administrator takes over, or a third party performs a technical security audit for the organization. While the problem can be approached via testing, non-intrusive techniques are often preferred. Existing tools for analyzing firewall configurations usually rely on hard-coded algorithms for analyzing access lists. In this paper we present a tool based on constraint logic programming (CLP) which allows the user to write higher level operations for, e.g., detecting common configuration mistakes. Our tool understands Cisco router access lists, and it is implemented using Eclipse, a constraint logic programming language. The problem of analyzing firewall configurations lends itself quite naturally to be solved by an expert system. We found it surprisingly easy to use logic statements to express knowledge on networking, firewalls, and common configuration mistakes, for instance. Using an existing generic inference engine allowed us to focus on defining the core concepts and relationships in the knowledge base. 1
AI
marsic01adaptive
Adaptive Collaboration for Wired and Wireless Platforms - A data-centric architecture for collaboration environments uses XML to adapt shared data dynamically between devices with widely disparate capabilities. This article begins by introducing a data-centric  architecture that abstracts collaborative tasks as  editing of data repositories, followed by descriptions  of the role of XML in managing heterogeneity  and intelligent software agents in discovering  network and computing environment conditions
HCI
eijk98informationpassing
Information-Passing and Belief Revision in Multi-Agent Systems We define a programming language for multi-agent systems in which agents interact with a common environment and cooperate by exchanging their individual beliefs on the environment. In handling the information they acquire, the agents employ operations to expand, remove and update their individual belief bases. The overall framework, which generalizes traditional concurrent programming concepts, is parameterized by an information system of constraints. Such a system is used to represent the environment as well as the beliefs of the agents. We give the syntax of the programming language and develop an operational semantics in terms of a transition system.  1. Introduction and syntax  A lot of effort has been made in the development of programming languages for multi-agent systems that cover typical agent concepts as beliefs, desires, intentions, commitments, speech acts, communication, cooperation and so on. However, in our opinion, most of the concurrency aspects of the existing multi-a...
Agents
200824
Logic-based Genetic Programming with Definite Clause Translation Grammars DCTG-GP is a genetic programming system that uses definite clause translation grammars. A DCTG is a logical version of an attribute grammar that supports the definition of context--free languages, and it allows semantic information associated with a language to be easily accomodated by the grammar. This is useful in genetic programming for defining the interpreter of a target language, or incorporating both syntactic and semantic problem--specific contraints into the evolutionary search. The DCTG-GP system improves on other grammar--based GP systems by permitting non--trivial semantic aspects of the language to be defined with the grammar. It also automatically analyzes grammar rules in order to determine their minimal depth and termination characteristics, which are required when generating random program trees of varied shapes and sizes. An application using DCTG-GP is described. 1 INTRODUCTION  Genetic programming (GP) implementations have benefitted from simple program denotations....
ML
94069
An Intelligent Multi-Agent Architecture for Information Retrieval from the Internet The World Wide Web (WWW) offers an uncountable number of documents which deal with information from a neverending list of topics. Thus the question of whether to find information turned into a question of how to find relevant information. Search engines with crawler based indexes vary in recall and offer a very bad precision. Meta search engines try to overcome these lacks based upon a specialised monolithic architecture for information extraction, information filtering and integration of heterogenous information resources. Few search engines employ intelligent techniques in order to increase precision. On the other hand, user modeling techniques become more and more popular. Many personalized agent based system for web browsing are currently developed. It is a straightforward idea to incorporate the idea of user modeling with machine learning methods into web search services. We propose an abstract prototype which is being developed at the University of Osnabruck and which incorporat...
IR
294039
Designing StoryRooms: Interactive Storytelling Spaces for Children Limited access to space, costly props, and complicated authoring technologies are among the many reasons why children can rarely enjoy the experience of authoring roomsized interactive stories. Typically in these kinds of environments, children are restricted to being story participants, rather than story authors. Therefore, we have begun the development of "StoryRooms," room-sized immersive storytelling experiences for children. With the use of low-tech and high-tech storytelling elements, children can author physical storytelling experiences to share with other children. In the paper that follows, we will describe our design philosophy, design process with children, the current technology implementation and example StoryRooms.  KEYWORDS  Augmented Environments, Storytelling, Children, Educational Applications, Participatory Design, Cooperative Inquiry  INTRODUCTION  A child sits in a playroom. She tells a story to her dolls about her family. Another child sits at the dinner table wit...
HCI
menczer02topicdriven
Topic-Driven Crawlers: Machine Learning Issues Topic driven crawlers are increasingly seen as a way to address the scalability limitations of universal  search engines, by distributing the crawling process across users, queries, or even client computers.
IR
holliday00database
Database Replication Using Epidemic Update Due to severe performance penalties associated with synchronous replication, there is an increasing  interest in asynchronous replica management protocols in which database transactions are executed locally,  and the effects of these transactions are incorporated asynchronously on remote database copies.  However, the asynchronous protocols currently in use either do not guarantee consistency and serializability  as needed by transactional semantics or they impose restrictions on placement of data and on which  data objects can be updated. In this paper we investigate an epidemic update protocol that guarantees  consistency and serializability in spite of a write-anywhere capability. We conducted experiments on a detailed  simulation of a distributed, replicated database to evaluate this protocol. Our results establish that  this epidemic approach is indeed a viable alternative to traditional eager update protocols for a distributed  database environment where consistency and full seri...
DB
pascoe98adding
Adding Generic Contextual Capabilities to Wearable Computers Context-awareness has an increasingly important role to play in the development of wearable computing systems. In order to better define this role we have identified four generic contextual capabilities: sensing, adaptation, resource discovery, and augmentation. A prototype application has been constructed to explore how some of these capabilities could be deployed in a wearable system designed to aid an ecologist's observations of giraffe in a Kenyan game reserve. However, despite the benefits of context-awareness demonstrated in this prototype, widespread innovation of these capabilities is currently stifled by the difficulty in obtaining the contextual data. To remedy this situation the Contextual Information Service (CIS) is introduced. Installed on the user's wearable computer, the CIS provides a common point of access for clients to obtain, manipulate and model contextual information independently of the underlying plethora of data formats and sensor interface mechanisms.  1. Int...
HCI
amin02towards
Towards Resource Efficient and Scalable Routing: An Agent-based Approach Mobile Agents are being proposed for an increasing variety of applications. Agent mobility can be exploited to implement a scalable system level solutions. Network routing is one such domain that can benefit from an agent-based approach. Shortest Path Routing algorithms enjoy a widespread use in most communication networks. However, large amounts of routing data exchanged in these algorithms consume substantial bandwidth, making conventional routing schemes less scalable.
Agents
pelikan99bivariate
The Bivariate Marginal Distribution Algorithm The paper deals with the Bivariate Marginal Distribution Algorithm (BMDA). BMDA is an extension of the Univariate Marginal Distribution Algorithm (UMDA). It uses the pair gene dependencies in order to improve algorithms that use simple univariate marginal distributions. BMDA is a special case of the Factorization Distribution Algorithm, but without any problem specic knowledge in the initial stage. The dependencies are being discovered during the optimization process itself. In this paper BMDA is described in detail. BMDA is compared to dierent algorithms including the simple genetic algorithm with dierent crossover methods and UMDA. For some tness functions the relation between problem size and the number of tness evaluations until convergence is shown. 1. Introduction  Genetic algorithms work with populations of strings of xed length. In this paper binary strings will be considered. From current population better strings are selected at the expense of worse ones. New strings ar...
ML
ware96evaluating
Evaluating Stereo and Motion Cues for Visualizing Information Nets in Three Dimensions This article concerns the benefits of presenting abstract data in 3D. Two experiments show that motion cues combined with stereo viewing can substantially increase the size of tbe graph that can be perceived. The first experiment was designed to provide quantitative measurements of how much more (or less) can be understood in 3D than in 2D. Tbe 3D display used was configured so that the image on the monitor was coupled to the user’s actual eye positions (and it was updated in real-time as the user moved) as well as being in stereo. Thus the effect was like a local “virtual reality ” display located in the vicinity of the computer monitor. The results from this study show that head-coupled stereo viewing can increase the size of an abstract graph that can be understood by a factor of three; using stereo alone provided an increase by a factor of 1.6 and bead coupling alone produced an increase by a factor of 2.2, Tbe second experiment examined a variety of motion cues provided by head-coupled perspective (as in virtual reality displays), hand-guided motion and automatic rotation, respectively, both with and without stereo in each case. The results show that structured 3D motion and stereo viewing both help in understanding, but that the kind of motion is not particularly important; all improve performance, and all are more significant than stereo cues. These results provide strong reasons for using advanced 3D graphics for interacting with a large variety of information structures.
HCI
sen98individual
Individual Learning of Coordination Knowledge Social agents, both human and computational, inhabiting a world con-taining multiple active agents, need to coordinate their activities. This is because agents share resources, and without proper coordination or “rules of the road”, everybody will be interfering with the plans of others. As such, we need coordination schemes that allow agents to effectively achieve local goals without adversely affecting the problem-solving capabilities of other agents. Researchers in the field of Distributed Artificial Intelligence (DAI) have de-veloped a variety of coordination schemes under different assumptions about agent capabilities and relationships. Whereas some of these research have been motivated by human cognitive biases, others have approached it as an engineering problem of designing the most effective coordination architec-ture or protocol. We evaluate individual and concurrent learning by mul-tiple, autonomous agents as a means for acquiring coordination knowledge. We show that a uniform reinforcement learning algorithm suffices as a coor-dination mechanism in both cooperative and adversarial situations. Using a number of multiagent learning scenarios with both tight and loose coupling between agents and with immediate as well as delayed feedback, we demon-strate that agents can consistently develop effective policies to coordinate their actions without explicit information sharing. We demonstrate the vi-ability of using both the Q-learning algorithm and genetic algorithm based classifier systems with different payoff schemes, namely the bucket brigade algorithm (BBA) and the profit sharing plan (PSP), for developing agent coordination on two different multi-agent domains. In addition, we show that a semi-random scheme for action selection is preferable to the more traditional fitness proportionate selection scheme used in classifier systems. 1 1
ML
bonifati01pushing
Pushing Reactive Services to XML Repositories using Active Rules Push technology, i.e., the ability of sending relevant information to clients in reaction to new events, is a fundamental aspect of modern information systems; XML is rapidly emerging as the widely adopted standard for information exchange and representation and hence, several XML-based protocols have been defined and are the object of investigation at W3C and throughout commercial organizations. In this paper, we propose the new concept of active XML rules for "pushing" reactive services to XML-enabled repositories. Rules operate on XML documents and deliver information to interested remote users in reaction to update events occurring at the repository site. The proposed mechanism assumes the availability of XML repositories supporting a standard XML query language, such as XQuery that is being developed by the W3C; for the implementation of the reactive components, it capitalizes on the use of standard DOM events and of the SOAP interchange standard to enable the remote installation of active rules. A simple protocol is proposed for subscribing and unsubscribing remote rules.
DB
calvanese02lossless
Lossless Regular Views If the only information we have on a certain database is through a set of views, the question arises of whether this is sufficient to answer completely a given query. We say that the set of views is lossless with respect to the query, if, no matter what the database is, we can answer the query by solely relying on the content of the views. The question of losslessness has various applications, for example in query optimization, mobile computing, data warehousing, and data integration. We study this problem in a context where the database is semistructured, and both the query and the views are expressed as regular path queries. The form of recursion present in this class prevents us from applying known results to our case.
DB
272797
Querying the Physical World Data Type (ADT) objects that are single attribute values encapsulating a collection of related data [S98]. Note that there are natural parallels between devices and ADTs. Both ADTs and devices provide controlled access to encapsulated data through a well-defined interface. We build upon this observation by modeling each type of device in the network as an ADT. The public interface of the ADT corresponds to the specific functions supported by the device. An actual ADT object in the database corresponds to a physical device in the real world.  Let us model the database schema corresponding to the flood detection example from the  introduction. We consider a simplified schema that consists of the following relations  . RFSensors(Sensor, X, Y)  . Areas (Name, X1, Y1, X2, Y2)  A record in the RFSensors relation has three attributes. The first attribute, called Sensor, is an ADT  that represents the physical rainfall sensor. The actual Sensor data is located on the rainfall sensor;  the ADT ...
DB
35997
Forming Neural Networks through Efficient and Adaptive Coevolution This article demonstrates the advantages of a cooperative, coevolutionary search in difficult control problems. The SANE system coevolves a population of neurons that cooperate to form a functioning neural network. In this process, neurons assume different but overlapping roles, resulting in a robust encoding of control behavior. SANE is shown to be more efficient, more adaptive, and maintain higher levels of diversity than the more common network-based population approaches. Further empirical studies illustrate the emergent neuron specializations and the different roles the neurons assume in the population. 1 Introduction  Artificial evolution has become an increasingly popular method for forming control policies in difficult decision problems (Grefenstette, Ramsey, & Schultz, 1990; Moriarty & Miikkulainen, 1996a; Whitley, Dominic, Das, & Anderson, 1993). Such applications are very different from the function optimization tasks to which evolutionary algorithms (EA) have been tradition...
ML
parker00current
Current State of the Art in Distributed Autonomous Mobile Robotics . As research progresses in distributed robotic systems, more and more aspects of multi-robot systems are being explored. This article surveys the current state of the art in distributed mobile robot systems. Our focus is principally on research that has been demonstrated in physical robot implementations. We have identi ed eight primary research topics within multi-robot systems | biological inspirations, communication, architectures, localization/mapping/exploration, object transport and manipulation, motion coordination, recon gurable robots, and learning { and discuss the current state of research in these areas. As we describe each research area, we identify some key open issues in multi-robot team research. We conclude by identifying several additional open research issues in distributed mobile robotic systems. 1 Introduction The eld of distributed robotics has its origins in the late-1980's, when several researchers began investigating issues in multiple mobile robot systems....
ML
449447
Speech and Hand Transcribed Retrieval This paper describes the issues and preliminary work involved in the creation of an information retrieval system that will manage the retrieval from collections composed of both speech recognised and ordinary text documents. In previous work, it has been shown that because of recognition errors, ordinary documents are generally retrieved in preference to recognised ones. Means of correcting or eliminating the observed bias is the subject of this paper. Initial ideas and some preliminary results are presented.  General Terms  Measurement, Experimentation.  Keywords  Information Retrieval, Spoken Document Retrieval, Mixed Collections.  1. 
IR
ultis01adaboost
AdaBoost for Query-by-Example in Text This paper describes an implementation of query-by-example, or relevance  feedback, for text. The implementation uses Google's search engine  to perform a keyword query as requested by the user. If the user  requires more information, the user may score documents in the result  set as relevant or irrelevant. An implementation of the AdaBoost algorithm  is then used choose words that separate the relevant documents  from a random document set. Examples of negative document sets are  also tested. An example query and refinements of the query is presented.  The results seem promising. The system seems to propose new keywords  that are sensible to the requested context. Many of the keywords prove  useful in constructing new queries. However, refinement using exactly  the new terms predicted by the system does not seem to return noticeably  better or worse results. This may be the result of an inexact fit  between the design of AdaBoost and the capabilities of Google as a back  end engine. ...
IR
han99automated
Automated Robot Behavior Recognition Applied to Robotic Soccer Automated recognition of the behavior of robots is increasingly needed in a variety of tasks, as we develop more autonomous robots and general information processing agents. For example, in environments with multiple autonomous robots, a robot may need to make decisions based on the behavior of the other robots. As another interesting example, an intelligent narrator agent observing a robot will need to automatically identify the robot's behaviors. In this paper, we introduce a novel framework for using Hidden Markov Models (HMMs) to represent and recognize strategic behaviors of robotic agents. We first introduce and characterize the perceived signal in terms of behavioral-relevant state features. We then show how several HMMs capture different defined robot behaviors. Finally we present the HMM-based recognition algorithm which orchestrates and selects the appropriate HMMs in real time. We use the multi-robot robotic soccer domain as the substrate of our empirical validation, both in...
ML
ciravegna01lp
(LP)2, an Adaptive Algorithm for Information Extraction from Web-related Texts (LP)  2  is an algorithm for adaptive Information  Extraction from Web-related text that induces  symbolic rules by learning from a corpus tagged  with SGML tags. Induction is performed by  bottom-up generalisation of examples in a  training corpus. Training is performed in two  steps: initially a set of tagging rules is learned;  then additional rules are induced to correct  mistakes and imprecision in tagging. Shallow  NLP is used to generalise rules beyond the flat  word structure. Generalization allows a better  coverage on unseen texts, as it limits data  sparseness and overfitting in the training phase.  In experiments on publicly available corpora the  algorithm outperforms any other algorithm  presented in literature and tested on the same  corpora. Experiments also show a significant  gain in using NLP in terms of (1) effectiveness  (2) reduction of training time and (3) training  corpus size. In this paper we present the  machine learning algorithm for rule induction.  In particular we focus on the NLP-based  generalisation and the strategy for pruning both  the search space and the final rule set.  1. 
IR
208646
State-Based SHOSLIF for Indoor Visual Navigation In this paper, we investigate vision-based navigation using the Self-organizing Hierarchical Optimal Subspace Learning and Inference Framework (SHOSLIF) that incorporates states and a visual attention mechanism. The problem is formulated as an observation-driven Markov model (ODMM) which is realized through recursive partitioning regression. A stochastic recursive partition tree (SRPT), which maps an preprocessed current input raw image and the previous state into the current state and the next control signal, is used for efficient recursive partitioning regression. The SRPT learns incrementally: each learning sample is learned or rejected "onthe -fly". The proposed scheme has been successfully applied to indoor navigation. Keywords: Vision-based navigation, incremental learning, eigen-subspace method, content-based retrieval, observation-driven Markov model, nearest neighbor regression.  1 1 Introduction  Much progress has been made in autonomous navigation of mobile robots, both ind...
ML
agichtein01learning
Learning Search Engine Specific Query Transformations for Question Answering We introduce a method for learning query transformations that improves the ability to retrieve answers to questions from an information retrieval system. During the training stage the method involves automatically learning phrase features for classifying questions into different types, automatically generating candidate query transformations from a training set of question/answer pairs, and automatically evaluating the candidate transforms on target information retrieval systems such as real-world general purpose search engines. At run time, questions are transformed into a set of queries, and re-ranking is performed on the documents retrieved. We present a prototype search engine, Tritus, that applies the method to web search engines. Blind evaluation on a set of real queries from a web search engine log shows that the method significantly outperforms the underlying web search engines as well as a commercial search engine specializing in question answering. Keywords Web search, quer...
IR
howell98learning
Learning Gestures for Visually Mediated Interaction This paper reports initial research on supporting Visually Mediated Interaction  (VMI) by developing person-specific and generic gesture models  for the control of active cameras. We describe a time-delay variant of the  Radial Basis Function (TDRBF) network and evaluate its performance on  recognising simple pointing and waving hand gestures in image sequences.
HCI
orovas00cellular
A Cellular Neural Associative Array for Symbolic Vision . A system which combines the descriptional power of symbolic representations with the parallel and distributed processing model of cellular automata and the speed and robustness of connectionist symbol processing is described. Following a cellular automata based approach, the aim of the system is to transform initial symbolic descriptions of patterns to corresponding object level descriptions in order to identify patterns in complex or noisy scenes. A learning algorithm based on a hierarchical structural analysis is used to learn symbolic descriptions of objects. The underlying symbolic processing engine of the system is a neural based associative memory (AURA) which use enables the system to operate in high speed. In addition, the use of distributed representations allow both efficient inter-cellular communications and compact storage of rules. 1 Introduction  One of the basic features of syntactic and structural pattern recognition systems is the use of the structure of the patterns...
ML
oyama01keyword
Keyword Spices: A New Method for Building Domain-Specific Web Search Engines This paper presents a new method for building domain-specific web search engines. Previous methods eliminate irrelevant documents from the pages accessed using heuristics based on human knowledge about the domain in question. Accordingly, they are hard to build and can not be applied to other domains. The keyword spice method, in contrast, improves search performance by adding
IR
116087
Greedy strikes back: Improved Facility Location Algorithms A fundamental facility location problem is to choose the location of facilities, such as industrial plants and warehouses, to minimize the cost of satisfying the demand for some commodity. There are associated costs for locating the facilities, as well as transportation costs for distributing the commodities. We assume that the transportation costs form a metric. This problem is commonly referred to as the uncapacitated facility location (UFL) problem. Applications to bank account location and clustering, as well as many related pieces of work, are discussed by Cornuejols, Nemhauser and Wolsey [?]. Recently, the first constant factor approximation algorithm for this problem was obtained by Shmoys, Tardos and Aardal [?]. We show that a simple greedy heuristic combined with the algorithm by Shmoys, Tardos and Aardal, can be used to obtain an approximation guarantee of 2.408. We discuss a few variants of the problem, demonstrating better approximation factors for restricted versions of the...
IR
schuldt99transactions
Transactions and Electronic Commerce . Electronic Commerce is a rapidly growing area that is gaining more and more importance not only in the interrelation of businesses (business--to--business Electronic Commerce) but also in the everyday consumption of individuals performed via the Internet (business--to-- customer Electronic Commerce). Since Electronic Commerce is a very interdisciplinary area, it has a lot of impacts to various communities. The goal of this paper is to identify and to summarize the impact of Electronic Commerce from a database transaction point of view and to highlight open problems in transaction management arising in Electronic Commerce applications by reflecting the discussions of the working group "Transactions and Electronic Commerce" held at the TDD Workshop. 1 Motivation The exchange of electronic data between companies has been an important issue in business interactions for quite a while. However, the recent proliferation of the Internet together with the rapid propagation of pers...
Agents
277650
Content-Based Video Indexing Of TV Broadcast News Using Hidden Markov Models This paper presents a new approach to content-based video indexing using Hidden Markov Models (HMMs). In this approach one feature vector is calculated for each image of the video sequence. These feature vectors are modeled and classified using HMMs. This approach has many advantages compared to other video indexing approaches. The system has automatic learning capabilities. It is trained by presenting manually indexed video sequences. To improve the system we use a video model, that allows the classification of complex video sequences. The presented approach works three times faster than real-time. We tested our system on TV broadcast news. The rate of 97.3 % correctly classified frames shows the efficiency of our system. 1. INTRODUCTION The increasing amount of digital video in multimedia databases results in a demand for techniques for automatic content-based access to video data. In the last years there have been many different approaches to content-based video indexing. A rough ...
ML
urhan98costbased
Cost-based Query Scrambling for Initial Delays Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scramblin...
DB
24318
Consistent Query Answers in Inconsistent Databases In this paper we consider the problem of the logical characterization of the notion of consistent answer in a relational database that may violate given integrity constraints. This notion is captured in terms of the possible repaired versions of the database. A method for computing consistent answers is given and its soundness and completeness (for some classes of constraints and queries) proved. The method is based on an iterative procedure whose termination for several classes of constraints is proved as well. 1 Introduction  Integrity constraints capture an important normative aspect of every database application. However, it is often the case that their satisfaction cannot be guaranteed, allowing for the existence of inconsistent database instances. In that case, it is important to know which query answers are consistent with the integrity constraints and which are not. In this paper, we provide a logical characterization of consistent query answers in relational databases that may...
DB
allen99control
Control States and Motivated Agency One of the challenges faced by researchers in the behaviour modelling of life-like  characters is the need to develop a systematic framework in which to ask  questions about the types of internal state life-like characters might possess, and  how those different states interact. We propose a solution based on a cognitively  inspired multi-layered agent architecture (composed of reactive, deliberative and  meta-management layers), and a recursive "design-based" research methodology  -- wherein each new design gradually increases our explanatory power and allows  us to account for more and more of the phenomena of interest. By describing a  variety of "broad but shallow" complete agents at the information-level, and  showing how these designs realise mental states and processes, we aim to provide  a rich and deep explanatory framework from which to explore motivated  autonomous agency. Early experiments have concentrated on: (a) the  requirements of goal-processing; (b) the emergence of ...
Agents
marques02componentbased
A Component-Based Approach for Integrating Mobile Agents Into the Existing Web Infrastructure Mobile agents provide a new abstraction for deploying functionality over the existing internet infrastructure. During the last two years, we have been working on a project that tries to overcome some of the limitations found in terms of programmability and usability of the mobile agent paradigm in real applications. In the M&M framework there are no agent platforms. Instead applications become agent-enabled by using simple JavaBeans components. In this paper we present an architecture that allows currently available web servers to become capable of sending and receiving agents in an easy way. By using this approach, existing web infrastructure can be maintained, while gaining a whole new potential by being able to make use of agent technology. Our approach involves wrapping the components inside a Java servlet that can be included in any web server supporting the Servlet Specification. This servlet enables the servers to receive and send agents that can query local information, and also enables the agents to behave as servlets themselves.
Agents
mertz00influence
The influence of design techniques on user interfaces: the DigiStrips experiment for air traffic control Graphical user interfaces have limitations in terms of the information bandwidth they provide between users and systems. This can impede the redesign of systems previously based on more physical media: information may be less appropriately displayed, and shared cognition between users can be reduced. However, in parallel with research on new user interaction techniques, a more systematic use of visual design  techniques can relieve those limitations. This article  explores some of those techniques and how they can be applied, through a design experiment. Virtuosi and DigiStrips are two user interface prototypes developed within a research program on air traffic control workstations, which make use of touch screens and served as a basis for research on the use of graphical design techniques in user interfaces. This paper describes the lessons learnt in that experience and  argues that techniques such as animation, font design, careful use of graphical design techniques can augment the p...
HCI
su00prediction
A prediction system for multimedia pre-fetching in Internet The rapid development of Interact has resulted in more and more multimedia in Web content. However, due to the limitation in the bandwidth and huge size of the multimedia data, users always suffer from long time waiting. On the other hand, if we can predict the web object or page that the user most likely will view next while the user is viewing the current page, and pre-fetch the content, then the perceived network latency can be significantly reduced. In this paper, we present an n-gram based model to utilize path profiles of users from very large web log to predict the users ' future requests. Our model is based on a simple extension of existing point-based models for such predictions, but our results show that by sacrificing the applicability somewhat one can gain a great deal in prediction precision. Also we present an efficient method to compress the prediction model size so that it can be fitted into the main memory. Our result can potentially be applied to a wide range of applications on the web, including pre-fetehing, enhancement of recommendation systems as well as web caching policies. The experiments based on three realistic web logs have proved the effectiveness of the proposed scheme.
DB
156949
Systematic Change Management in Dimensional Data Warehousing With the widespread and increasing use of data warehousing in industry, the design of effective data warehouses and their maintenance has become a focus of attention. Independently of this, the area of temporal databases has been an active area of research for well beyond a decade. This article identifies shortcomings of so-called star schemas, which are widely used in industrial warehousing, in their ability to handle change and subsequently studies the application of temporal techniques for solving these shortcomings. Star schemas represent a new approach to database design and have gained widespread popularity in data warehousing, but while they have many attractive properties, star schemas do not contend well with so-called slowly changing dimensions and with state-oriented data. We study the use of so-called temporal star schemas that may provide a solution to the identified problems while not fundamentally changing the database design approach. More specifically, we study the rel...
DB
finn01fact
Fact or fiction: Content classification for digital libraries The World-Wide Web (WWW) is a vast repository of information, much of which is valuable  but very often hidden to the user. The anarchic nature of the WWW presents unique challenges  when it comes to information extraction and categorization. We view the WWW as a valuable  resource for the gathering of information for Digital Libraries. In this paper we will describe the  process of extracting and classifying information from the WWW for the purpose of integrating  it into digital libraries. Our eorts focus on ways to automatically classify news articles according  to whether they present opinions or reported facts. We describe and evaluate a system in  development that automatically classies and recommends Web news articles from sports and  politics domains.  1 
IR
schmill99learned
Learned Models for Continuous Planning We are interested in the nature of activity -- structured behavior of nontrivial duration -- in intelligent agents. We believe that the development of activity is a continual process in which simpler activities are composed, via planning, to form more sophisticated ones in a hierarchical fashion. The success or failure of a planner depends on its models of the environment, and its ability to implement its plans in the world. We describe an approach to generating dynamical models of activity from real-world experiences and explain how they can be applied towards planning in a continuous state space. 1 Introduction  We are interested in the problem of how activity emerges in an intelligent agent. We believe that activity plays a critical role in the development of many high-level cognitive structures: classes, concepts, and language, to name a few. Thus, it is our goal to derive and implement a theory of the development of activity in intelligent agents and implement it using the Pioneer...
AI
goldman98interactive
Interactive Query and Search in Semistructured Databases Semistructured graph-based databases have been proposed as well-suited stores for World-Wide Web data. Yet so far, languages for querying such data are too complex for casual Web users. Further, proposed query approaches do not take advantage of the interactivity of typical Web sessions---users are proficient at iteratively refining their Web explorations. In this paper we propose a new model for interactively querying and searching semistructured databases. Users can begin with a simple keyword search, dynamically browse the structure of the result, and then submit further refining queries. Enabling this model exposes new requirements of a semistructured database that are not apparent under traditional database uses. We demonstrate the importance of efficient keyword search, structural summaries of query results, and support for inverse pointers. We also describe some preliminary solutions to these technical issues. 1 Introduction Querying the Web has understandably gathered much att...
IR
grishman01adaptive
Adaptive Information Extraction and Sublanguage Analysis Introduction  1  Information extraction (IE) has made significant progress in the last decade. We have developed practical, efficient approaches to IE which have yielded modest levels of performance on general texts and quite good performance on restricted, `semi-structured' texts. More notably, over the last few years there has been a blossoming of work in adaptive IE --- the topic of this and other recent workshops --- IE systems which can be rapidly and automatically (or semi-automatically) moved to new extraction tasks.  To date, these developments have been relatively little influenced by linguistic studies of the texts. In fact, the trend has been towards less linguistic analysis. Some early IE systems used full parsing and in a few cases relatively deep semantic analysis. Because of limitations of full parsing methods (particularly a decade ago) this gave way to a common methodology based on limited parsing and simple pattern matching. Adaptive IE systems have in
IR
menczer99adaptive
Adaptive Retrieval Agents: Internalizing Local Context and Scaling up to the Web . This paper focuses on two machine learning abstractions springing from ecological models: (i) evolutionary adaptation by local selection, and (ii) selective query expansion by internalization of environmental signals. We first outline a number of experiments pointing to the feasibility and performance of these methods on a general class of graph environments. We then describe how these methods have been applied to the intelligent retrieval of information distributed across networked environments. In particular, the paper discusses a novel distributed evolutionary algorithm and representation used to construct populations of adaptive Web agents. These InfoSpiders search on-line for information relevant to the user, by traversing hyperlinks in an autonomous and intelligent fashion. They can adapt to the spatial and temporal regularities of their local context. Our results suggest that InfoSpiders could complement current search engine technology by starting up where search engines stop...
IR
kallmann99behavioral
A Behavioral Interface to Simulate Agent-Object Interactions in Real Time This paper shows a new approach to model and control interactive objects for simulations with virtual human  agents when real time interactivity is essential. A general conceptualization is made to model objects with behaviors that can provide: information about their functionality, changes in appearance from parameterized deformations, and a complete plan for each possible interaction with a virtual human. Such behaviors are  described with simple primitive commands, following the actual trend of many standard scene graph file formats that connects language with movements and events to create interactive animations. In our case, special attention is given to correctly interpret object behaviors in parallel: situation that arrives when many human  agents interact at the same time with one same object.  Keywords: Virtual Humans, Virtual Environments, Object Modeling, Object Interaction, Script Languages, Parameterized Deformations.  1 Introduction  The necessity to have interactive obje...
Agents
chiueh99charm
Charm: An I/O-Driven Execution Strategy for High-Performance Transaction Processing The performance of a transaction processing system whose database is not completely  memory-resident critically depends on the amount of physical disk I/O required. This  paper describes a high-performance transaction processing system called Charm, which  aims to reducing the concurrency control overhead by minimizing the performance impacts  of disk I/O on lock contention delay. In existing transaction processing systems, a  transaction blocked by lock contention is forced to wait while the transaction currently  holding the contended lock is performing physical disk I/O. A substantial portion of a  transaction's lock contention delay is thus attributed to disk I/Os performed by other  transactions. Charm implements a two-stage transaction execution (TSTE) strategy,  which makes sure that all the data pages that a transaction needs be memory-resident  before it is allowed to lock database pages. Moreover, Charm supports an optimistic  version of the TSTE strategy (OTSTE), which furth...
DB
bonnet00query
Query Processing over Device Networks In the next decade, millions of sensors and small-scale mobile devices will integrate processors, memory, and communication capabilities.  Networks of devices will be widely deployed for monitoring applications. In these new applications, users need to query very large collections of  devices in an ad hoc manner. Most existing systems rely on a centralized system for collecting device data. These systems lack flexibility because  data is extracted in a predefined way. Also, they do not scale to a large number of devices because large volumes of raw data are transferred.  In our new concept of a device database system, distributed query execution techniques are applied to leverage the computing capabilities of  devices, and to reduce communication. In this article, we define an abstraction that allows us to represent a device network as a database  and we describe how distributed query processing techniques are applied in this new context.  Praveen Seshradi is currently on leave at Micr...
DB
obst01specifying
Specifying Rational Agents with Statecharts and Utility Functions Abstract. To aid the development of the robotic soccer simulation league team RoboLog-2000, a method for the specification of multi-agent teams by statecharts has been introduced. The results in the last years competitions showed that though the team was competitive, it did not behave adaptive in unknown situations. The design of adaptive agents with this method is possible, but not in a straightforward manner. The purpose of this paper is to extend the approach by a more adaptive action selection mechanism and to facilitate a more explicit representation of goals of an agent. 1
Agents
100157
Semantics for an Agent Communication Language . We address the issue of semantics for an agent communication language. In particular, the semantics of Knowledge Query Manipulation Language (KQML) is investigated. KQML is a language and protocol to support communication between (intelligent) software agents. Based on ideas from speech act theory, we present a semantic description for KQML that associates "cognitive" states of the agent with the use of the language's primitives (performatives). We have used this approach to describe the semantics for the whole set of reserved KQML performatives. Our research offers a method for a speech act theory-based semantic description of a language of communication acts. Languages of communication acts address the issue of communication between software applications at a level of abstraction that could prove particularly useful to the emerging software agents paradigm of software design and development. 1 Introduction This research is concerned with communication between software agents [13]...
Agents
532537
Self-Organization in Multiagent Systems: From Agent Interaction to Agent Organization In this paper we suggest a new sociological concept to the study of (self-)  organization in multiagent systems. First, we discuss concepts of (self-) organization  typically used in DAI. From a sociological point of view all these concepts are missing  the special quality of organizations as self-organizing social entities. Therefore we  present a concept of organization based on the habitus-field theory of Pierre Bourdieu.
Agents
roth00snowbased
A SNoW-Based Face Detector A novel learning approach for human face detection using a network of linear units is presented. The SNoW learning architecture is a sparse network of linear functions over a pre-defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of features. A wide range of face images in different poses, with different expressions and under different lighting conditions are used as a training set to capture the variations of human faces. Experimental results on commonly used benchmark data sets of a wide range of face images show that the SNoW-based approach outperforms methods that use neural networks, Bayesian methods, support vector machines and others. Furthermore, learning and evaluation using the SNoW-based method are significantly more efficient than with other methods.
ML
widyantoro99dynamic
Dynamic Modeling and Learning User Profile In Personalized News Agent Finding relevant information effectively on the Internet is a challenging task. Although the information is widely available, exploring Web sites and finding information relevant to a user's interest can be a time-consuming and tedious task. As a result, many software agents have been employed to perform autonomous information gathering and filtering on behalf of the user. One of the critical issues in such an agent is the capability of the agent to model its users and adapt itself over time to changing user interests. In this thesis, a novel scheme is proposed to learn user profile. The proposed scheme is designed to handle multiple domains of long-term and short-term users' interests simultaneously, which are learned through positive and negative user feedback. A 3-descriptor interest category representation approach is developed to achieve this objective. Using such a representation, the learning algorithm is derived by imitating human personal assistants doing the same task. Based on experimental evaluation, the scheme performs very well and adapts quickly to significant changes in user interest.
IR
singletary01learning
Learning Visual Models of Social Engagement We introduce a face detector for wearable computers that exploits constraints in face scale and orientation imposed by the proximity of participants in near social interactions. Using this method we describe a wearable system that perceives "social engagement," i.e., when the wearer begins to interact with other individuals. Our experimental system proved > 90% accurate when tested on wearable video data captured at a professional conference. Over 300 individuals were captured during social engagement, and the data was separated into independent training and test sets. A metric for balancing the performance of face detection, localization, and recognition in the context of a wearable interface is discussed.  Recognizing social engagement with a user's wearable computer provides context data that can be useful in determining when the user is interruptible. In addition, social engagement detection may be incorporated into a user interface to improve the quality of mobile face recognition software. For example, the user may cue the face recognition system in a socially graceful way by turning slightly away and then toward a speaker when conditions for recognition are favorable.  1 
HCI
feris00detection
Detection and Tracking of Facial Features in Video Sequences . This work presents a real time system for detection and tracking of facial  features in video sequences. Such system may be used in visual communication  applications, such as teleconferencing, virtual reality, intelligent interfaces, humanmachine  interaction, surveillance, etc. We have used a statistical skin-color model to  segment face-candidate regions in the image. The presence or absence of a face in  each region is verified by means of an eye detector, based on an efficient template  matching scheme . Once a face is detected, the pupils, nostrils and lip corners are  located and these facial features are tracked in the image sequence, performing real  time processing.  1 
HCI
decker01extending
Extending a Multi-Agent System for Genomic Annotation . The explosive growth in genomic (and soon, expression and proteomic)  data, exemplified by the Human Genome Project, is a fertile domain for the application  of multi-agent information gathering technologies. Furthermore, hundreds  of smaller-profile, yet still economically important organisms are being studied  that require the efficient and inexpensive automated analysis tools that multiagent  approaches can provide. In this paper we give a progress report on the use  of the DECAF multi-agent toolkit to build reusable information gathering systems  for bioinformatics. We will briefly summarize why bioinformatics is a classic  application for information gathering, how DECAF supports it, and recent  extensions underway to support new analysis paths for genomic information.  1 
IR
364516
Report on the TREC-8 Experiment: Searching on the Web and in Distributed Collections this paper verifies whether or not hyperlinks improve retrieval effectiveness. In the second chapter, we describe experiments on the ad hoc track. In this case, we acknowledge that currently it is becoming more and more difficult to store and manage the growing document collections within a single computer. Recent advances in network technology do however allow us to disseminate information sources by partitioning a single huge corpus (or distributing heterogeneous collections) over a local-area network (Intranet). Most retrieval mechanisms currently proposed however are based on conventional IR models [Salton 89], and where a centralized document collection is assumed.
IR
13423
Reasoning with Examples: Propositional Formulae and Database Dependencies For humans, looking at how concrete examples behave is an intuitive way of deriving conclusions. The drawback with this method is that it does not necessarily give the correct results. However, under certain conditions example-based deduction can be used to obtain a correct and complete inference procedure. This is the case for Boolean formulae (reasoning with models) and for certain types of database integrity constraints (the use of Armstrong relations). We show that these approaches are closely related, and use the relationship to prove new results about the existence and sizes of Armstrong relations for Boolean dependencies. Furthermore, we exhibit close relations between the questions of finding keys in relational databases and that of finding abductive explanations. Further applications of the correspondence between these two approaches are also discussed. 1 Introduction  One of the major tasks in database systems as well as artificial intelligence systems is to express some know...
AI
brandt00task
Task Assignment in Multiagent Systems based on Vickrey-type Auctioning and Leveled Commitment Contracting . A key problem addressed in the area of multiagent systems  is the automated assignment of multiple tasks to executing agents. The  automation of multiagent task assignment requires that the individual  agents (i) use a common protocol that prescribes how they have to interact  in order to come to an agreement and (ii) x their nal agreement  in a contract that species the commitments resulting from the assignment  on which they agreed. The work reported in this paper is part of a  broader research eort aiming at the design and analysis of approaches to  automated multiagent task assignment that combine auction protocols  and leveled commitment contracts. The primary advantage of such approaches  is that they are applicable in a broad range of realistic scenarios  in which knowledge-intensive negotiation among agents is not feasible  and in which unforeseeable future environmental changes may require  agents to breach their contracts. Examples of standard auction protocols  are the...
Agents
dorohonceanu00novel
A Novel User Interface for Group Collaboration Flexible user interfaces that can be customized to meet the needs of the task at hand are particularly important for real-time group collaboration. This paper presents the user interface of the DISCIPLE (DIstributed System for Collaborative Information Processing and LEarning) system for synchronous groupware along with the multimodal human-computer interface enhancement. DISCIPLE supports sharing of JavaBeans-compliant components [17], i.e., beans and applets, which at runtime get imported into the shared workspace and can be interconnected into more complex components. As a result, importing various components allows user tailoring of the human-computer interface. We present a software architecture for customization of both grouplevel and application-level interfaces. The applicationlevel interface includes a management system for sharing multiple modalities across concurrent applications. This multimodal management system is loadable on demand yet strongly embedded in the DISCIPLE f...
HCI
322562
Improved Algorithms for Optimal Winner Determination in Combinatorial Auctions and Generalizations Combinatorial auctions, i.e. auctions where bidders can bid on com-binations of items, tend to lead to more e cient allocations than tra-ditional auctions in multi-item auctions where the agents ' valuations of the items are not additive. However, determining the winners so as to maximize revenue is NP-complete. First, existing approaches for tackling this problem are reviewed: exhaustive enumeration, dynamic programming, approximation algorithms, and restricting the allow-able combinations. Then we present our search algorithm for optimal winner determination. Experiments are shown on several bid distri-butions. The algorithm allows combinatorial auctions to scale up to signi cantly larger numbers of items and bids than prior approaches to optimal winner determination by capitalizing on the fact that the space of bids is necessarily sparsely populated in practice. The algo-rithm does this by provably su cient selective generation of children in the search tree, by using a secondary search for fast child genera-tion, by heuristics that are accurate and optimized for speed, and by four methods for preprocessing the search space. Patent pending. A highly optimized implementation of the algorithm is available for licensing both for research and commercial purposes. Please contact the author. 1 1
Agents
332789
Machine Learning in Automated Text Categorisation The automated categorisation (or classification) of texts into topical categories has a long history, dating back at least to the early ’60s. Until the late ’80s, the most effective approach to the problem seemed to be that of manually building automatic classifiers by means of knowledgeengineering techniques, i.e. manually defining a set of rules encoding expert knowledge on how to classify documents under a given set of categories. In the ’90s, with the booming production and availability of on-line documents, automated text categorisation has witnessed an increased and renewed interest, prompted by which the machine learning paradigm to automatic classifier construction has emerged and definitely superseded the knowledge-engineering approach. Within the machine learning paradigm, a general inductive process (called the learner) automatically builds a classifier (also called the rule, or the hypothesis) by “learning”, from a set of previously classified documents, the characteristics of one or more categories. The advantages of this approach are a very good effectiveness, a considerable savings in terms of expert manpower, and domain independence. In this survey we look at the main approaches that have been taken towards automatic text categorisation within the general machine learning paradigm. Issues pertaining to document indexing, classifier construction, and classifier evaluation, will be discussed in detail. A final section will be devoted to the techniques that have specifically been devised for an emerging application such as the automatic classification of Web pages into “Yahoo!-like ” hierarchically structured sets of categories. Categories and Subject Descriptors: H.3.1 [Information storage and retrieval]: Content analysis and indexing—Indexing methods; H.3.3 [Information storage and retrieval]: Information search and retrieval—Information filtering; H.3.3 [Information storage and retrieval]: Systems and software—Performance evaluation (efficiency and effectiveness); I.2.3 [Artificial
ML
525378
Optimal Camera Parameter Selection for State Estimation with Applications in Object Recognition In this paper we introduce a formalism for optimal camera parameter selection for iterative state estimation. We consider a framework based on Shannon 's information theory and select the camera parameters that maximize the mutual information, i.e. the information that the captured image conveys about the true state of the system. The technique explicitly takes into account the a priori probability governing the computation of the mutual information. Thus, a sequential decision process can be formed by treating the a posteriori probability at the current time step in the decision process as the a priori probability for the next time step. The convergence of the decision process can be proven.
ML
choppy00control
Control and Datatypes using the View Formalism . We herein deal with mixed specification formalisms, i.e. formalisms with both a static (data types) and a dynamic (behaviour) part. Our formalism is based on symbolic transition systems (STS) [9], that allow one to specify systems at an abstract level and to avoid state explosion. STS are a kind of guarded finite state/transition diagrams where states and transitions are labelled with open terms. Both dynamic and static parts of objects are specified, in a unifying approach, as formal structures that we call views. These components interpretation structures use STS, and we show how these may be derived from their view structures. The system is structured by means of collections of objects (with identities) . A temporal logic is used to glue the components altogether and expresses a generalized form of synchronous product [1]. We then show how a view structure and its interpretation structure may be obtained. The formalism is explained using a simplified phone service example.  Keywor...
DB
witten01power
Power to the people: End-user building of digital library collections Naturally, digital library systems focus principally on the reader: the consumer of the material that constitutes the library. In contrast, this paper describes an interface that makes it easy for people to build their own library collections. Collections may be built and served locally from the user's own web server, or (given appropriate permissions) remotely on a shared digital library host. End users can easily build new collections styled after existing ones from material on the Web or from their local files---or both, and collections can be updated and new ones brought on-line at any time. The interface, which is intended for non-professional end users, is modeled after widely used commercial software installation packages. Lest one quail at the prospect of end users building their own collections on a shared system, we also describe an interface for the administrative user who is responsible for maintaining a digital library installation.
IR
dantsin99complexity
Complexity and Expressive Power of Logic Programming . This paper surveys various complexity and expressiveness results on different forms of logic programming. The main focus is on decidable forms of logic programming, in particular, propositional logic programming and datalog, but we also mention general logic programming with function symbols. Next to classical results on plain logic programming (pure Horn clause programs), more recent results on various important extensions of logic programming are surveyed. These include logic programming with different forms of negation, disjunctive logic programming, logic programming with equality, and constraint logic programming. 1  Computing Science Department, Uppsala University, Box 311, S 751 05 Uppsala, Sweden. Email: dantsin@pdmi.ras.ru  2  Institut und Ludwig Wittgenstein Labor fur Informationssysteme, Technische Universitat Wien, Treitlstraße 3, A-1040 Wien, Austria. E-mail: eiter@kr.tuwien.ac.at  3  Institut und Ludwig Wittgenstein Labor fur Informationssysteme, Technische Universitat ...
AI
70265
Relational Learning Techniques for Natural Language Information Extraction The recent growth of online information available in the form of natural language documents creates a greater need for computing systems with the ability to process those documents to simplify access to the information. One type of processing appropriate for many tasks is information extraction, a type of text skimming that retrieves specific types of information from text. Although information extraction systems have existed for two decades, these systems have generally been built by hand and contain domain specific information, making them difficult to port to other domains. A few researchers have begun to apply machine learning to information extraction tasks, but most of this work has involved applying learning to pieces of a much larger system. This paper presents a novel rule representation specific to natural language and a learning system, Rapier, which learns information extraction rules. Rapier takes pairs of documents and filled templates indicating the information to be ext...
ML
240825
The Global Dimensionality of Face Space Low-dimensional representations of sensory signals are key to solving many of the computational problems encountered in high-level vision. Principal Component Analysis (PCA) has been used in the past to derive such compact representations for the object class of human faces. Here, with an interpretation of PCA as a probabilistic model, we employ two objective criteria to study its generalization properties in the context of large frontal-pose face databases. We find that the eigenfaces, the eigenspectrum, and the generalization depend strongly on the ensemble composition and size, with statistics for populations as large as 5500, still not stationary. Further, the assumption of mirror symmetry of the ensemble improves the quality of the results substantially in the low-statistics regime, and is also essential in the high-statistics regime. We employ a perceptual criterion and argue that, even with large statistics, the dimensionality of the PCA subspace necessary for adequate represent...
ML
323955
Hierarchical Memory-Based Reinforcement Learning A key challenge for reinforcement learning is scaling up to large  partially observable domains. In this paper, we show how a hierarchy  of behaviors can be used to create and select among variable  length short-term memories appropriate for a task. At higher levels  in the hierarchy, the agent abstracts over lower-level details  and looks back over a variable number of high-level decisions in  time. We formalize this idea in a framework called Hierarchical  Sux Memory (HSM). HSM uses a memory-based SMDP learning  method to rapidly propagate delayed reward across long decision  sequences. We describe a detailed experimental study comparing  memory vs. hierarchy using the HSM framework on a realistic  corridor navigation task.  1 Introduction  Reinforcement learning encompasses a class of machine learning problems in which an agent learns from experience as it interacts with its environment. One fundamental challenge faced by reinforcement learning agents in real-world problems is that ...
ML
sintek01using
Using Information Extraction Rules for Extending Domain Ontologies - nt, we lay special emphasis on considerations and methods which are necessary to realize such a scenario in industrial practice. In each industrial environment, besides the questions of smooth introduction of new technology regarding human factors and organizational processes, and besides the question of modeling tools and method support for knowledge (in particular ontologies for structuring OMs or parts of OMs) acquisition, at least two other factors are of utmost importance:  One is the predominance of informal, i.e. essentially textbased, representations of knowledge. This is not only just a matter of fact, but really useful, because the cost of formalization is often not in the right relation to the potential benefits such that many informal parts of the scenario are economically reasonable [5]. One implication is that also methods for building formal models must be affordable.  The other is the fact that ontologies are not a stand-alone component built once and then remaining unt
IR
veloso99cmunited
The CMUnited-98 Champion Small-Robot Team Abstract. In this chapter, we present the main research contributions of our champion CMUnited-98 small robot team. The team is a multiagent robotic system with global perception, and distributed cognition and action. We describe the main features of the hardware design of the physical robots, including di erential drive, robust mechanical structure, and a kicking device. We brie y review the CMUnited-98 global vision processing algorithm, which is the same as the one used by the previous champion CMUnited-97 team. We introduce our new robot motion algorithm which reactively generates motion control to account for the target point, the desired robot orientation, and obstacle avoidance. Our robots exhibit successful collision-free motion in the highly dynamic robotic soccer environment. At the strategic and decision-making level, we present the role-based behaviors of the CMUnited-98 robotic agents. Team collaboration is remarkably achieved through a new algorithm that allows for team agents to anticipate possible collaboration opportunities. Robots position themselves strategically in open positions that increase passing opportunities. The chapter terminates with a summary of the results of the RoboCup-98 games in which the CMUnited-98 small robot team scored a total of 25 goals and su ered 6 goals in the 5 games that it played. 1
AI
baerentzen97learning
Learning Network Designs for Asynchronous Teams . An asynchronous team (A-Team) is a network of agents (workers) and memories (repositories for the results of work). It is possible to design A-Teams to be effective in solving difficult computational problems. The main design issues are: "What structure should the network have?" and "What should be the complement of agents?" In the past, the structure-issue was resolved by intuition and experiment. This paper describes a procedure by which good structures can be learned from experience. The procedure is based on the use of regular expressions for encoding the capabilities of networks. 1 Introduction  An Asynchronous Team (A-Team) is a problem solving architecture consisting of collections of agents and memories connected into a strongly cyclic directed network. The memories form the nodes of the network, the agents form the arcs. Figure 1 below shows such a network. Each memory holds a population of trial solutions. The solutions are not necessarily solutions to the overall problem t...
Agents
108573
Relational Transducers for Electronic Commerce Electronic commerce is emerging as one of the major Websupported applications requiring database support. We introduce and study high-level declarative specifications of business models, using an approach in the spirit of active databases. More precisely, business models are specified as  relational transducers that map sequences of input relations into sequences of output relations. The semantically meaningful trace of an input-output exchange is kept as a sequence of log relations. We consider problems motivated by electronic commerce applications, such as log validation, verifying temporal properties of transducers, and comparing two relational transducers. Positive results are obtained for a restricted class of relational transducers called Spocus transducers (for semi-positive outputs and cumulative state). We argue that despite the restrictions, these capture a wide range of practically significant business models.  1 Introduction  Electronic commerce is emerging as a major Web-s...
DB
78173
Graph Structured Views and Their Incremental Maintenance We study the problem of maintaining materialized views of graph structured data. The base data consists of records containing identifiers of other records. The data could represent traditional objects (with methods, attributes, and a class hierarchy), but it could also represent a lower level data structure. We define simple views and materialized views for such graph structured data, analyzing options for representing record identity and references in the view. We develop incremental maintenance algorithms for these views.  1 Introduction  Relational views are useful for controlling data access, specifying contents of caches (or remote copies), and other data management tasks. In this paper we study how to extend this view concept and the associated maintenance algorithms to what we call a graph structured database (GSDB). Informally, a GSDB is a collection of "objects" that may contain "pointers" (graph edges) to other objects. A GSDB can represent Web pages, Lotus Notes documents, o...
DB
iyer00boosting
Boosting for Document Routing RankBoost is a recently proposed algorithm for learning ranking functions. It is simple to implement and has strong justifications from computational learning theory. We describe the algorithm and present initial experimental results on applying it to the document routing problem. The first set of results applies RankBoost to a text representation produced using modern term weighting methods. Performance of RankBoost is somewhat inferior to that of a state-of-the-art routing algorithm which is, however, more complex and less theoretically justified than RankBoost. RankBoost achieves comparable performance to the state-of-the-art algorithm when combined with feature or example selection heuristics. Our second set of results examines the behavior of RankBoost when it has to learn not only a ranking function but also all aspects of term weighting from raw data. Performance is usually, though not always, less good here, but the term weighting functions implicit in the resulting ranking fun...
IR
493671
sView - Architecture Overview and System Description This report presents an architecture overview and a system description of the sView system. The system provides developers, service providers, and users of electronic services with an open and extendible service infrastructure that allows far-reaching user control. This is accomplished by collecting the services of an individual user in a virtual briefcase. Services come in the form of self-contained service components (i.e. including both logic and data), and the briefcase is mobile to allow it to follow as the user moves between different hosts and terminals. A specification of how to build such service components and the infrastructure for handling briefcases is presented. A reference implementation of the specification as well as extensions in the form of service components is also described. The purpose of the report is to serve as a technical reference for developers of sView services and software infrastructure that builds on sView technology. Keywords. Electronic services, personal service environments, user interfaces, mobility, personalization, service collaboration, component-based software engineering. May 2001 SICS Technical Report T2001/06 ISSN 1100-3154 ISRN: SICS-T--2001/06-SE  2 Bylund  1. 
HCI
roy01toward
Toward Optimal Active Learning through Sampling Estimation of Error Reduction This paper presents an active learning method that directly  optimizes expected future error. This is in contrast  to many other popular techniques that instead  aim to reduce version space size. These other methods  are popular because for many learning models,  closed form calculation of the expected future error is  intractable. Our approach is made feasible by taking a  sampling approach to estimating the expected reduction  in error due to the labeling of a query. In experimental  results on two real-world data sets we reach  high accuracy very quickly, sometimes with four times  fewer labeled examples than competing methods.  1. 
IR
458208
Configuration Management for Multi-Agent Systems As heterogeneous distributed systems, multi-agent systems present some challenging configuration management issues. There are the problems of knowing how to allocate agents to computers, launch them on remote hosts, and once the agents have been launched, how to monitor their runtime status so as to manage computing resources effectively.  In this paper, we present the RETSINA Configuration Manager, RECoMa. We describe its architecture, how it uses agent infrastructure such as service discovery, to assist the multi-agent system administrator in allocating, launching, and monitoring a heterogeneous distributed agent system in a distributed and networked computing environment.  1  1. 
Agents
brass98semantics
Semantics Of (disjunctive) Logic Programs Based On Partial Evaluation SEMANTICS AND TRANSFORMATIONS  In this paper, we consider allowed disjunctive DATALOG  :  programs over some fixed function-free finite signature \Sigma. In fact, in the semantical part of this paper, we consider only the ground instantiation of the programs, because we claim that any sensible semantics should assign the same meaning to a program P and its instantiation ground(P ). So the variables are seen only as a shorthand for denoting ground programs in a more compact way. This means that in the semantical part, we could as well have worked with propositional programs. However, in the computational part, it would be very inefficient to compute first the ground instantiation of the given program. Here we make use of the allowedness condition: every variable of the rule must occur also in a positive body literal. This guarantees that in every rule application, all variables are bound to a constant. It is  5 true that in this way we again manage to consider only ground programs. But...
DB
schallehn02extensible
Extensible and Similarity-based Grouping for Data Integration Data integration as required in a variety of applications like data warehousing, information system  integration etc. makes great demands regarding features to deal with overlapping and inconsistent  data. Object-relational and other data management systems available today provide only limited concepts  to deal with these requirements. The general concept of grouping and aggregation appears to  be a fitting paradigm for various of the current issues in data integration, but in its common form of  equality-based grouping a number of problems remain unsolved. Various extensions to this concept  have been introduced over the last years regarding user-defined functions for aggregation and grouping.
DB
532227
Automatic translation of PARADIGM models into PLTL-based Software systems have evolved from monolythic programs to systems constructed from parallel, cooperative components, as can be currently found in objectoriented applications. Although powerfull, these cooperative systems are also more di#cult to verify.
Agents
bowman01introduction
An Introduction to 3-D User Interface Design Three-dimensional user interface design is a critical component of any virtual environment (VE) application. In this paper, we present a broad overview of 3-D interaction and user interfaces. We discuss the effect of common VE hardware devices on user interaction, as well as interaction techniques for generic 3-D tasks and the use of traditional 2-D interaction styles in 3-D environments. We divide most userinteraction tasks into three categories: navigation, selection/manipulation, and system control. Throughout the paper, our focus is on presenting not only the available techniques but also practical guidelines for 3-D interaction design and widely held myths. Finally, we briefly discuss two approaches to 3-D interaction design and some example applications with complex 3-D interaction requirements. We also present an annotated online bibliography as a reference companion to this article.
HCI
marsh01guiding
Guiding User Navigation in Virtual Environments Using Awareness of Virtual Off-Screen Space Navigation in virtual environments can be difficult. One contributing factor is the problem of user disorientation. Two major causes of this are the lack of navigation cues in the environment and problems with navigating too close to or through virtual world objects. Previous work has developed guidelines, informed by cinematography conventions, for the construction of virtual environments to aid user comprehension of virtual "space" to reduce user disorientation. This paper describes the validation of these guidelines via a user study involving a navigation task in a virtual "maze". Results suggest that the use of the guidelines can help reduce the incidences of user disorientation. However, the guidelines seemed to have little impact on users' abilities to construct 'cognitive maps' of the environment.
HCI
khanna01computing
On Computing Functions with Uncertainty We study the problem of computing a function f(x1;:::; xn) giv en that the actual values of the variables xi's are kno wn only with some uncertainty. F or each variable xi, aninterval Ii is kno wn such that the value of xi is guaranteed to fall within this interval. Any such interval can be probed to obtain the actual value of the underlying variable; how ever, there is a cost associated with each suc h probe. The goal is to adaptively iden tify a minimum cost sequence of probes suc h that regardless of the actual values tak en b y the unprobed xi's, the v alue of the functionf can be computed to within a speci ed precision. We design online algorithms for this problem when f is either the selection function or an aggregation function such as sum or average. We consider three natural models of precision and give algorithms for each model. We analyze our algorithms in the framework of competitive analysis and sho w that our algorithms are asymptotically optimal. Finally, we also study online algorithms for functions that are obtained by composing together selection and aggregation functions. 1.
IR
226643
A Semantics of Contrast and Information Structure for Specifying Intonation in Spoken Language Generation In this dissertation I present a model for the determination of intonation contours from context and provide two implemented systems which apply this theory to the problem of generating spoken language with appropriate intonation from high-level semantic representations. The theory and implementations presented here are based on an information structure framework that mediates between intonation and discourse, and encodes the proper level of semantic information to account for both contextually-bound accentuation patterns and intonational phrasing. The structural similarities among these linguistic levels of representation are the basis for selecting Combinatory Categorial Grammar #CCG, Steedman 1985,1990a# as the model for spoken language production. This model licenses congruent syntactic, prosodic and information structural constituents and consequently represents a simpli#cation over models of prosody developed in syntactically more traditional frameworks.  The previous mention heu...
HCI
streitz99iland
i-LAND: An interactive Landscape for Creativity and Innovation We describe the i-LAND environment which constitutes an example of our vision of the workspaces of the future, in this case supporting cooperative work of dynamic teams with changing needs. i-LAND requires and provides new forms of human-computer interaction and new forms of computer-supported cooperative work. Its design is based on an integration of information and architectural spaces, implications of new work practices and an empirical requirements study informing our design. i-LAND consists of several ‘roomware ’ components, i.e. computer-augmented objects integrating room elements with information technology. We present the current realization of i-LAND in terms of an interactive electronic wall, an interactive table, two computer-enhanced chairs, and two “bridges ” for the Passage-mechanism. This is complemented by the description of the creativity support application and the technological infrastructure. The paper is accompanied by a video figure in the CHI’99 video program.
HCI
likas99training
Training Reinforcement Neurocontrollers Using The Polytope Algorithm A new training algorithm is presented for delayed reinforcement learning problems that does not assume the existence of a critic model and employs the polytope optimization algorithm to adjust the weights of the action network so that a simple direct measure of the training performance is maximized. Experimental results from the application of the method to the pole balancing problem indicate improved training performance compared with critic-based and genetic reinforcement approaches. Keywords: reinforcement learning, neurocontrol, optimization, polytope algorithm, pole balancing, genetic reinforcement.  TRAINING REINFORCEMENT NEUROCONTROLLERS USING THE POLYTOPE ALGORITHM  Abstract  A new training algorithm is presented for delayed reinforcement learning problems that does not assume the existence of a critic model and employs the polytope optimization algorithm to adjust the weights of the action network so that a simple direct measure of the training performance is maximized. Exper...
ML
domingos99metacost
MetaCost: A General Method for Making Classifiers Cost-Sensitive Research in machine learning, statistics and related fields has produced a wide variety of algorithms for classification. However, most of these algorithms assume that all errors have the same cost, which is seldom the case in KDD prob- lems. Individually making each classification learner costsensitive is laborious, and often non-trivial. In this paper we propose a principled method for making an arbitrary classifier cost-sensitive by wrapping a cost-minimizing procedure around it. This procedure, called MetaCost, treats the underlying classifier as a black box, requiring no knowledge of its functioning or change to it. Unlike stratification, MetaCost is applicable to any number of classes and to arbitrary cost matrices. Empirical trials on a large suite of benchmark databases show that MetaCost almost always produces large cost reductions compared to the cost-blind classifier used (C4.5RULES) and to two forms of stratification. Further tests identify the key components of MetaCost and those that can be varied without substantial loss. Experiments on a larger database indicate that MetaCost scales well.
ML
22863
Digital Libraries and Autonomous Citation Indexing The World Wide Web is revolutionizing the way that researchers access scientific information. Articles are increasingly being made available on the homepages of authors or institutions, at journal Web sites, or in online archives. However, scientific information on the Web is largely disorganized. This article introduces the creation of digital libraries incorporating Autonomous Citation Indexing (ACI). ACI autonomously creates citation indices similar to the Science Citation Index R . An ACI system autonomously locates articles, extracts citations, identifies identical citations that occur in different formats, and identifies the context of citations in the body of articles. ACI can organize the literature and provide most of the advantages of traditional citation indices, such as literature search using citation links, and the evaluation of articles based on citation statistics. Furthermore, ACI can provide significant advantages over traditional citation indices. No manual effort is required for indexing, which should result in a reduction in cost and an increase in the availability of citation indices. An ACI system can also provide more comprehensive and up-to-date indices of the literature by indexing articles on the Web, technical reports, conference papers, etc. Furthermore, ACI makes it easy to browse the context of citations to given articles, allowing researchers to quickly and easily see what subsequent researchers have said about a given article. digital libraries incorporating ACI may significantly improve scientific dissemination and feedback.
IR
goldberg99optimizing
Optimizing Global-Local Search Hybrids This paper develops a framework for optimizing global-local hybrids of search or optimization procedures. The paper starts by idealizing the search problem as a search by a global algorithm G for either (1) acceptable targets---solutions that meet a specified criterion---or for (2) basins of attraction that then lead to acceptable targets under a specified local search algorithm L. The paper continues by abstracting two sets of parameters, probabilities of successfully hitting targets and basins and time-to-criterion coefficients. With these parameters, equations may be written to account for the total time of search and for the probabilistic success (reliability) in reaching an acceptable solution. Thereafter, optimization problems are formulated in which the division of local versus global search time is optimized so that solution time to acceptable reliability is minimized or reliability under specified solution time is maximized. A two-basin optimality criterion is derived and appl...
ML
apte01ai
AI at IBM Research IBM has played an active role in AI research since the field's inception more than 50 years ago. In a trend that reflects the increasing demand for applications that behave intelligently, IBM today carries out most AI research in an interdisciplinary fashion by combining AI techniques with other computing techniques to solve difficult technical problems.  1 
AI
382561
Query Expansion Using an Interactive Concept Hierarchy Query expansion is the process of supplementing an original query with additional terms in order to refine a search and increase retrieval effectiveness. If the query expansion is interactive, then the user and the system work together to expand the query. The system usually suggests possible expansion terms and the user selects those they wish to add to the query. Studies have shown that interactive query expansion has the potential to improve retrieval effectiveness, but that it rarely succeeds in achieving its potential. It has been shown that users desire some control over the expansion process. In order to achieve this, the functionality of the system must be represented on the user interface in a comprehensible way. The main aim of this study is to focus on a small aspect of the interface and investigate whether the method used to present potential query expansion terms has any effect on retrieval effectiveness. The tool tested in this study automatically generates a hierarchical...
HCI
540845
Comparing Top k Lists Motivated by several applications, we introduce various distance measures between “top k lists.” Some of these distance measures are metrics, while others are not. For each of these latter distance measures, we show that they are “almost ” a metric in the following two seemingly unrelated aspects: (i) they satisfy a relaxed version of the polygonal (hence, triangle) inequality, and (ii) there is a metric with positive constant multiples that bound our measure above and below. This is not a coincidence—we show that these two notions of almost being a metric are formally identical. Based on the second notion, we define two distance measures to be equivalent if they are bounded above and below by constant multiples of each other. We thereby identify a large and robust equivalence class of distance measures. Besides the applications to the task of identifying good notions of (dis-)similarity between two top k lists, our results imply polynomial-time constant-factor approximation algorithms for the rank aggregation problem [DKNS01] with respect to a large class of distance measures. To appear in SIAM J. on Discrete Mathematics. Extended abstract to appear in 2003 ACM-SIAM Symposium on Discrete Algorithms (SODA ’03).
IR
hayashi00comparison
A Comparison Between Two Neural Network Rule Extraction Techniques for the Diagnosis of Hepatobiliary Disorders Neural networks have been widely used as tools for prediction in medicine. We  expect to see even more applications of neural networks for medical diagnosis as recently  developed neural network rule extraction algorithms make it possible for the  decision process of a trained network to be expressed as classification rules. These  rules are more comprehensible to a human user than the classification process of the  networks which involves complex nonlinear mapping of the input data. This paper  reports the results from two neural network rule extraction techniques, NeuroLinear  and NeuroRule applied to the diagnosis of hepatobiliary disorders. The data set consists  of nine measurements collected from patients in a Japanese hospital and these  measurements have continuous values. NeuroLinear generates piece-wise linear discriminant  functions for this data set. The continuous measurements have previously  been discretized by domain experts. NeuroRule is applied to the discretized data...
AI
rosenfeld00two
Two Decades Of Statistical Language Modeling: Where Do We Go From Here? Statistical Language Models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies. Since the first significant model was proposed in 1980, many attempts have been made to improve the state of the art. We review them here, point to a few promising directions, and argue for a Bayesian approach to integration of linguistic theories with data. 1. OUTLINE Statistical language modeling (SLM) is the attempt to capture regularities of natural language for the purpose of improving the performance of various natural language applications. By and large, statistical language modeling amounts to estimating the probability distribution of various linguistic units, such as words, sentences, and whole documents. Statistical language modeling is crucial for a large variety of language technology applications. These include speech recognition (where SLM got its start), machine translation, document classification and routing, optical character recognition, information retrieval, handwriting recognition, spelling correction, and many more. In machine translation, for example, purely statistical approaches have been introduced in [1]. But even researchers using rule-based approaches have found it beneficial to introduce some elements of SLM and statistical estimation [2]. In information retrieval, a language modeling approach was recently proposed by [3], and a statistical/information theoretical approach was developed by [4]. SLM employs statistical estimation techniques using language training data, that is, text. Because of the categorical nature of language, and the large vocabularies people naturally use, statistical techniques must estimate a large number of parameters, and consequently depend critically on the availability of large amounts of training data.
IR
485564
Steps Toward Accommodating Variable Position Tracking Accuracy in a Mobile Augmented Reality System The position-tracking accuracy of a location-aware mobile system can change dynamically as a function of the user's location and other variables specific to the tracker technology used. This is especially problematic for mobile augmented reality systems, which ideally require extremely precise position tracking for the user's head, but which may not always be able to achieve the necessary level of accuracy. While it is possible to ignore variable positional accuracy in an augmented reality user interface, this can make for a confusing system; for example, when accuracy is low, virtual objects that are nominally registered with real ones may be too far off to be of use.
HCI
rosenthal01administering
Administering Permissions for Distributed Data: Factoring and Automated Inference We extend SQL's grant/revoke model to handle all administration of permissions in a distributed database. The key idea is to "factor" permissions into simpler decisions that can be administered separately, and for which we can devise sound inference rules. The model enables us to simplify administration via separation of concerns (between technical DBAs and domain experts), and to justify fully automated inference for some permission factors. We show how this approach would coexist with current practices based on SQL permissions. Keywords: Access permissions, derived data, view, federation, warehouse 1 
DB
buneman98path
Path Constraints on Semistructured and Structured Data We present a class of path constraints of interest in connection with both structured and semi-structured databases, and investigate their associated implication problems. These path constraints are capable of expressing natural integrity constraints that are not only a fundamental part of the semantics of the data, but are also important in query optimization. We show that in semistructured databases, despite the simple syntax of the constraints, their associated implication problem is r.e. complete and finite implication problem is co-r.e. complete. However, we establish the decidability of the implication problems for several fragments of the path constraint language, and demonstrate that these fragments suffice to express important semantic information such as inverse relationships and local database constraints commonly found in object-oriented databases. We also show that in the presence of types, the analysis of path constraint implication becomes more delicate. We demonstrate so...
DB
68306
Employing EM and Pool-Based Active Learning for Text Classification This paper shows how a text classifier's need for labeled training data can be reduced by a combination of active learning and Expectation Maximization (EM) on a pool of unlabeled data. Query-by-Committee is used to actively select documents for labeling, then EM with a naive Bayes model further improves classification accuracy by concurrently estimating probabilistic labels for the remaining unlabeled documents and using them to improve the model. We also present a metric for better measuring disagreement among committee members; it accounts for the strength of their disagreement and for the distribution of the documents. Experimental results show that our method of combining EM and active learning requires only half as many labeled training examples to achieve the same accuracy as either EM or active learning alone. Keywords:  text classification active learning unsupervised learning information retrieval  1 Introduction  In many settings for learning text classifiers, obtaining lab...
ML
koubarakis99querying
Querying Temporal Constraint Networks in PTIME We start with the assumption that temporal knowledge usually captured by constraint networks can be represented and queried more effectively by using the scheme of indefinite constraint databases proposed by Koubarakis. Although query evaluation in this scheme is in general a hard computational problem, we demonstrate that there are several interesting cases where query evaluation can be done in PTIME. These tractability results are original and subsume previous results by van Beek, Brusoni, Console and Terenziani. Introduction  When temporal constraint networks are used in applications, their nodes represent the times when certain facts are true, or when certain events take place, or when events start or end. By labeling nodes with appropriate natural language expressions (e.g., breakfast  or walk) and arcs by temporal relations, temporal constraint networks can be queried in useful ways. For example the query "Is it possible (or certain) that event walk happened after event breakfast...
DB
43702
Autonomous Learning of Sequential Tasks: Experiments and Analyses : This paper presents a novel learning model Clarion, which is a hybrid model based on the twolevel approach proposed in Sun (1995). The model integrates neural, reinforcement, and symbolic learning methods to perform on-line, bottom-up learning (i.e., learning that goes from neural to symbolic representations). The model utilizes both procedural and declarative knowledge (in neural and symbolic representations respectively), tapping into the synergy of the two types of processes. It was applied to deal with sequential decision tasks. Experiments and analyses in various ways are reported that shed light on the advantages of the model.  obstacles agent target Figure 1: Navigating Through A Minefield  1 Introduction  This paper presents a model that unifies neural, symbolic, and reinforcement learning. It addresses the following three issues: (1) It deals with autonomous learning: It allows a situated agent to learn autonomously and continuously, from on-going experience in the world, w...
ML
shehory99feasible
Feasible Formation of Coalitions Among Autonomous Agents in Non-Super-Additive Environments Cooperating and sharing resources by creating coalitions of agents are an important way for autonomous agents to execute tasks and to maximize payoff. Such coalitions will form only if each member of a coalition gains more if it joins the coalition than it could gain otherwise. There are several ways of creating such coalitions and dividing the joint payoff among the members. In this paper we present algorithms for coalition formation and payoff distribution in non-super-additive environments. We focus on a low-complexity kernel-oriented coalition formation algorithm. The properties of this algorithm were examined via simulations. These have shown that the model increases the benefits of the agents within a reasonable time period, and more coalition formations provide more benefits to the agents.  Key Words Distributed AI, Coalition Formation, Multi-Agent Systems.   This material is based upon work supported in part by the NSF under grant No. IRI-9423967, ARPA/Rome Labs contract F30602...
Agents
kotagiri01transaction
Transaction Oriented Computational Models For Multi-Agent Systems BDI (Belief, Desire, Intention) is a mature and commonly adopted architecture  for Intelligent Agents. However, the current computational model adopted  by BDI has a number of problems with concurrency control, recoverability and  predictability. This has hindered the construction of agents having robust and predictable  behaviour. Indeed the conceptual and practical tools needed for building  dependable agent systems, resilient to faults and other unexpected situations, are  still at an early stage of research.
Agents
lomuscio00classification
A Classification Scheme for Negotiation in Electronic Commerce In the last few years we have witnessed a surge of business-to-consumer and business-to-business commerce operated on the Internet. However many of these systems are often nothing more than electronic catalogues on which the user can choose a product which is made available for a fixed price. This modus operandi is clearly failing to exploit the full potential of electronic commerce. Against this background, we argue here that in the next few years we will see a new generation of systems emerge, based on automatic negotiation. In this paper we identify the main parameters on which any automatic negotiation depends. This classification schema is then used to categorise the subsequent papers in this book that focus on automatic negotiation.
Agents
crabtree98wearable
Wearable Computing and the Remembrance Agent This paper gives an overview of the field of wearable computing. It covers the key differences between wearables and other portable computers, and discusses issues with the design and application for wearables. There then follows a specific example, the wearable remembrance agent --- a proactive memory aid. The paper concludes with discussion of future directions for research and applications inspired by using the prototype. 1. Introduction This scenario may sound some way off in practical terms, but it is not. All the technologies needed to support it are available. It is the aim of this paper to describe some of these technologies in more detail and give an indication of their current status. The paper will start by describing features available in wearable computers that are not available in current laptops or personal digital assistants (PDAs). It will then go on to describe a number of other general application areas for wearables and current wearable technologies and design needs. This is followed by a description of the remembrance agent (RA), a wearable memory aid that continually reminds the wearer of potentially relevant information based on the wearer's current physical and virtual context. Finally, the paper discusses extensions that are being added to the current prototype system. 2. What are wearable computers? The fuzzy definition of a wearable computer is that it is a computer that is always with the user, is comfortable and easy to keep and use, and is as unobtrusive as clothing. However, this `smart clothing' definition is unsatisfactory when the details are considered. Most importantly, it does not convey how a wearable computer is any different from a very small palm-top. A more specific definition is that wearable computers have many of the followin...
HCI
ian99kea
KEA: Practical Automatic Keyphrase Extraction Keyphrases provide semantic metadata that summarize and characterize documents. This paper describes Kea, an algorithm for automatically extracting keyphrases from text. Kea identifies candidate keyphrases using lexical methods, calculates feature values for each candidate, and uses a machine -learning algorithm to predict which candidates are good keyphrases. The machine learning scheme first builds a prediction model using training documents with known keyphrases, and then uses the model to find keyphrases in new documents. We use a large test corpus to evaluate Kea's effectiveness in terms of how many author-assigned keyphrases are correctly identified. The system is simple, robust, and publicly available.  INTRODUCTION  Keyphrases provide a brief summary of a document's contents. As large document collections such as digital libraries become widespread, the value of such summary information increases. Keywords and keyphrases  1  are particularly useful because they can be interpret...
IR
kurki99agents
Agents in Delivering Personalized Content Based on Semantic Metadata In the SmartPush project professional editors add semantic  metadata to information flow when the content is created.  This metadata is used to filter the information flow to  provide the end users with a personalized news service.  Personalization and delivery process is modeled as software  agents, to whom the user delegates the task of sifting  through incoming information. The key components of the  SmartPush architecture have been implemented, and the  focus in the project is shifting towards a pilot  implementation and testing the ideas in practice.  Introduction  Internet and online distribution are changing the rules of professional publishing. Physical constraints of media products are not any more the key factors in the publishing process. In newspaper publishing, the bottleneck was what could fit in the paper. Today publishers are capable of creating more content, but they need new methods for personalized distribution. Incremental price for publishing on the Internet is neg...
IR
benford00designing
Designing Storytelling Technologies to Encourage Collaboration Between Young Children We describe the iterative design of two collaborative storytelling technologies for young children, KidPad and the Klump. We focus on the idea of designing interfaces to subtly encourage collaboration so that children are invited to discover the added benefits of working together. This idea has been motivated by our experiences of using early versions of our technologies in schools in Sweden and the UK. We compare the approach of encouraging collaboration with other approaches to synchronizing shared interfaces. We describe how we have revised the technologies to encourage collaboration and to reflect design suggestions made by the children themselves.  Keywords  Children, Single Display Groupware (SDG), Computer Supported Cooperative Work (CSCW), Education, Computer Supported Collaborative Learning (CSCL).  INTRODUCTION  Collaboration is an important skill for young children to learn. Educational research has found that working in pairs or small groups can have beneficial effects on l...
HCI
470087
Towards a Living Lab research facility and a ubiquitous computing research programme Introduction  My interest in the topic of this workshop stems from my current involvement in setting up a new research facility at the Eindhoven University of Technology. This facility is called the 'Living Lab', and is quite similar to related projects around the globe in that it aims to study how people experience a ubiquitous computing environment, when they will inhabit it and use it for prolonged periods of time. The slogan of this development, is 'Vacation on Campus'.  The project is currently at the initiation phase. We have proposed a white paper [1] describing the concept and the research programme, and we are currently working to involve stakeholders from different departments of the TU/e, e.g., Architecture, Electrical Engineering, and Technology Management, and of the local industry. (E.g., Philips).  In the remaining of this position paper, I outline our research concept and our programme.  Concept - Vacation on Campus  The Living Lab, will be a cros
HCI
halevy00theory
Theory of Answering Queries Using Views The problem of answering queries using views is to nd ecient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has recently received signicant attention because of its relevance to a wide variety of data management problems, such as query optimization, the maintenance of physical data independence, data integration and data warehousing. This article surveys the theoretical issues concerning the problem of answering queries using views.  1 
DB
vanlaerhoven01combining
Combining the Self-Organizing Map and K-Means Clustering for On-line Classification of Sensor Data Many devices, like mobile phones, use contextual profiles like "in the car" or "in a meeting" to quickly switch between behaviors. Achieving automatic context detection, usually by analysis of small hardware sensors, is a fundamental problem in human-computer interaction. However, mapping the sensor data to a context is a difficult problem involving near real-time classification and training of patterns out of noisy sensor signals. This paper proposes an adaptive approach that uses a Kohonen Self-Organizing Map, augmented with on-line k-means clustering for classification of the incoming sensor data. Overwriting of prototypes on the map, especially during the untangling phase of the Self-Organizing Map, is avoided by a refined k-means clustering of labeled input vectors.
HCI
yen01cast
CAST: Collaborative Agents for Simulating Teamwork Psychological studies on teamwork have shown that an effective team often can anticipate information needs of teammates based on a shared mental model. Existing multi-agent models for teamwork are limited in their ability to support proactive information exchange among teammates. To address this issue, we have developed and implemented a multi-agent architecture called CAST that simulates teamwork and supports proactive information exchange in a dynamic environment. We present a formal model for proactive information exchange. Knowledge regarding the structure and process of a team is described in a language called MALLET. Beliefs about shared team processes and their states are represented using Petri Nets. Based on this model, CAST agents offer information proactively to those who might need it using an algorithm called DIARG. Empirical evaluations using a multi-agent synthetic testbed application indicate that CAST enhances the effectiveness of teamwork among agents without sacrificing a high cost for communications. 1
Agents
ferhatosmanoglu01approximate
Approximate Nearest Neighbor Searching in Multimedia Databases In this paper, we develop a general framework for approximate nearest neighbor queries. We categorize the current approaches for nearest neighbor query processing based on either their ability to reduce the data set that needs to be examined, or their ability to reduce the representation size of each data object. We first propose modifications to wellknown techniques to support the progressive processing of approximate nearest neighbor queries. A user may therefore stop the retrieval process once enough information has been returned. We then develop a new technique based on clustering that merges the benefits of the two general classes of approaches. Our cluster-based approach allows a user to progressively explore the approximate results with increasing accuracy. We propose a new metric for evaluation of approximate nearest neighbor searching techniques. Using both the proposed and the traditional metrics, we analyze and compare several techniques with a detailed performance evaluation. We demonstrate the feasibility and efficiency of approximate nearest neighbor searching. We perform experiments on several real data sets and establish the superiority of the proposed cluster-based technique over the existing techniques for approximate nearest neighbor searching. 1
DB
benerecetti97model
Model Checking Multiagent Systems Model checking is a very successful technique which has been applied in the design and verification of finite state concurrent reactive processes. In this paper we show how this technique can be lifted to be applicable to multiagent systems. Our approach allows us to reuse the technology and tools developed in model checking, to design and verify multiagent systems in a modular and incremental way, and also to have a very efficient model checking algorithm. 1 Introduction  Model checking is a very successful automatic technique which has been devised for the design and verification of finite state reactive systems, e.g., sequential circuit designs, communication protocols, and safety critical control systems (see, e.g., [2]). There is evidence that model checking, when applicable, is far more successful than the other approaches to formal methods and verification (e.g., first order or inductive theorem proving, tableau based reasoning about modal satisfiability). Nowadays many very eff...
Agents
250204
Communicating Agents We study the problem of endowing logic-based agents that can reason about their own beliefs as well as the beliefs of other agents with communication skills. We show how communication performatives from existing agent communication languages as well as their preconditions and eects can be expressed within logic-based agents in terms of the agents' beliefs. We illustrate the resulting language for programming logic-based agents by means of examples. 1 Introduction  In an earlier paper [6], we propose an approach to logic-based agents by combining the approach to agents by Kowalski and Sadri [11] and the approach to meta-reasoning by Costantini et al. [5, 4]. Similarly to Kowalski and Sadri's agents, the agents in [6] are hybrid in that they exhibit both rational  (or deliberative) and reactive behaviour. The reasoning core of the agents is a proof procedure that combines forward and backward reasoning. Backward reasoning is used primarily for planning, problem solving and other deliber...
Agents
126894
The Open Agent Architecture: A Framework for Building Distributed Software Systems The Open Agent Architecture (OAA), developed and used for several years at SRI International, makes it possible for software services to be provided through the cooperative efforts of distributed collections of autonomous agents. Communication and cooperation between agents are brokered by one or more facilitators, which are responsible for matching requests, from users and agents, with descriptions of the capabilities of other agents. Thus, it is not generally required that a user or agent know the identities, locations, or number of other agents involved in satisfying a request. OAA is structured so as to minimize the effort involved in creating new agents and "wrapping" legacy applications, written in various languages and operating on various platforms; to encourage the reuse of existing agents; and to allow for dynamism and flexibility in the makeup of agent communities. Distinguishing features of OAA as compared with related work include extreme flexibility in using facilitator-b...
HCI
liechti00digital
A Digital Photography Framework Enabling Affective Awareness in Home Communication By transforming the personal computer into a communication appliance, the Internet has initiated the true home computing revolution. As a result, Computer Mediated Communication (CMC) technologies are increasingly used in domestic settings, and are changing the way people keep in touch with their relatives and friends. This article first looks at how CMC tools are currently used in the home, and points at some of their benefits and limitations. Most of these tools support explicit interpersonal communication, by providing a new medium for sustaining conversations. The need for tools supporting implicit interaction between users, in more natural and effortless ways, is then argued for. The idea of affective awareness is introduced as a general sense of being in touch with one's family and friends. Finally, the KAN-G framework, which enables affective awareness through the exchange of digital photographs, is described. Various components, which make the capture, distribution, observation...
HCI
230265
Methodologies for PVC Configuration in Heterogeneous ATM Environments Using Intelligent Mobile Agents Traditionally, the functionality enabling the configuration of Permanent Virtual Connections (PVCs) in heterogeneous Asynchronous Transfer Mode (ATM) environments has been accomplished through the use of client/server technologies. However, this approach suffers from a number of problems in the areas of flexibility, extensibility and efficiency, and as networks become increasingly complex and dynamic, the need for a new paradigm has become apparent. This paper is concerned with the analysis of a proposed solution wherein intelligent mobile agents are utilized to provide autonomous network configuration management functionality. Configuration methodologies based on a single mobile agent acting serially and multiple mobile agents acting in parallel are compared in terms of time duration and network load. The use of a small expert system to provide the intelligence needed to handle error situations is investigated. A test environment composed of three simulated ATM switch nodes is develop...
Agents
heidemann01building
Building Efficient Wireless Sensor Networks with Low-Level Naming In most distributed systems, naming of nodes for low-level communication leverages topological location (such as node addresses)  and is independent of any application. In this paper, we investigate an emerging class of distributed systems where low-level communication does not rely on network topological location. Rather, low-level communication is based on attributes that are external to the network topology and relevant to the application. When combined with dense deployment of nodes, this kind of named data enables in-network processing for data aggregation, collaborative signal processing, and similar problems. These approaches are essential for emerging applications such as sensor networks where resources such as bandwidth and energy are limited. This paper is the first description of the software architecture that supports  named data and in-network processing in an operational, multi-application sensor-network. We show that approaches such as in-network aggregation and nested queries can significantly affect network traffic. In one experiment aggregation reduces traffic by up to 42% and nested queries reduce loss rates by 30%. Although aggregation has been previously studied in simulation, this paper demonstrates nested queries as another form of in-network processing, and it presents the first evaluation of these approaches over an operational testbed.  
IR
michaud98learning
Learning from History for Behavior-Based Mobile Robots in Non-Stationary Conditions . Learning in the mobile robot domain is a very challenging task, especially in nonstationary conditions. The behavior-based approach has proven to be useful in making mobile robots work in real-world situations. Since the behaviors are responsible for managing the interactions between the robots and its environment, observing their use can be exploited to model these interactions. In our approach, the robot is initially given a set of "behavior-producing" modules to choose from, and the algorithm provides a memory-based approach to dynamically adapt the selection of these behaviors according to the history of their use. The approach is validated using a vision- and sonar-based Pioneer I robot in non-stationary conditions, in the context of a multirobot foraging task. Results show the effectiveness of the approach in taking advantage of any regularities experienced in the world, leading to fast and adaptable specialization for the learning robot.  Keywords: Multi-robot learning, histor...
AI
ciravegna00bringing
Bringing Information Extraction out of the Labs: the Pinocchio Environment . Pinocchio is an environment for developing Information Extraction applications. New applications and languages can be covered by just writing declarative resources. Information is represented uniformly throughout the architecture: all the modules use the same input structure and the same type of declarative resources. Modules are implemented via the same basic processors and share a common environment for resource development and debugging. The result is an environment easy to use with limited training and skills.  1 INTRODUCTION  The exponential increase in the quantity of information held in digital archives has fueled growing interest in techniques for Information Extraction. Given that the vast majority of available information is textual (e.g., web pages, electronic newspapers, agency news), the role of Information Extraction from text (IE) is becoming more and more central. An IE system extracts pieces of information that are salient to the user's needs. The typical output is a...
IR
27008
The Persistent Cache: Improving OID Indexing in Temporal Object-Oriented Database Systems In a temporal OODB, an OID index (OIDX) is needed to map from OID to the physical location of the object. In a transaction time temporal OODB, the OIDX should also index the object versions. In this case, the index entries, which we call object descriptors (OD), also include the commit timestamp of the transaction that created the object version. The OIDX in a non-temporal OODB only needs to be updated when an object is created, but in a temporal OODB, the OIDX have to be updated every time an object is updated. We have in a previous study shown that this can be a potential bottleneck, and in this report, we present the Persistent Cache (PCache), a novel approach which reduces the index update and lookup costs in temporal OODBs. In this report, we develop a cost model for the PCache, and use this to show that the use of a PCache can reduce the average access cost to only a fraction of the cost when not using the PCache. Even though the primary context of this report is OID indexing in ...
DB
sadri99abduction
Abduction with Negation as Failure for Active Databases and Agents Recent work has suggested abductive logic programming as a suitable formalism to represent active databases and intelligent agents. In particular, abducibles and integrity constaints in abductive logic programs can be used respectively to represent actions and active/reactive rules. One would expect that, in this approach, abductive proof procedures could provide the engine underlying active database management systems and agents. We analyse existing abductive proof procedures and argue that they are inadequate in handling these applications. The inadequacy is due to the "incorrect" treatment of negative literals in integrity constraints. We propose a new abductive proof procedure and give examples of how this proof procedure can be used to achieve active behaviour in (deductive) databases and proactivity and reactivity in agents.
DB
duch00eliminators
Eliminators and Classifiers Classification may not be reliable for several reasons: noise in the data, insufficient input information, overlapping distributions and sharp definition of classes. Faced with K possibilities a decision support system may still be useful in such cases if instead of classification elimination of improbable classes is done. Eliminators may be constructed using classifiers assigning new cases to a pool of several classes instead of just one winning class. Elimination may be done with the help of several classifiers using modified error functions. A real life medical example is presented illustrating the usefulness of elimination.
IR
536996
Identifying the Subject of Documents in Digital Libraries Automatically Using Contemporary information databases contain millions of electronic documents. The immense number of documents makes it difficult to conduct efficient searches on the Internet. Several studies have found that associating documents with a subject or list of topics can make them easier to locate online [5] [6] [7]. Effective cataloging of information is performed manually, requiring extensive resources. Consequently, at present most information is not cataloged. This paper will present the findings of a study based on a software tool (TextAnalysis) that automatically identifies the subject of a document. We tested documents in two subject categories: geography and family studies. The present study follows an earlier one that examined the subject categories of industrial management and general management.
IR
chantemargue99collective
A Collective Robotics Application Based On Emergence And Self-Organization This paper presents a collective robotics application, which consists of making a pool of robots regroup objects that are distributed in their environment. The innovative aspect in our approach rests on a system integrating operationally autonomous robots that make the global task achieved by virtue of emergence and self-organization. 1 INTRODUCTION Our work fits in the framework of Bottom-Up Artificial Intelligence (Bottom-Up AI in short) and more particularly in that of Autonomous Agents. We are concerned with collective phenomena and their issues and more precisely the way to carry out solutions that allow a multi-robot system to achieve global tasks by virtue of emergence and self-organization. Our work is supported by two types of experiments, namely those involving multi-agent simulations and those involving real robots. Our paper precisely presents a collective robotics application, which consists of making a pool of autonomous robots regroup objects that are distributed in thei...
Agents
mcroy98achieving
Achieving Robust Human-Computer Communication This paper describes a computational approach to robust human-computer interaction. The approach relies on an explicit, declarative representation of the content and structure of the interaction that a computer system builds over the course of the interaction. In this paper, we will show how this representation allows the system to recognize and repair misunderstandings between the human and the computer. We demonstrate the utility of the representations by showing how they facilitate the repair process.  1 Introduction  In dialogs between people or between people and machines, understanding is an uncertain process. If the goals or beliefs of two discourse  1  participants differ, one of them might interpret an event in the dialog in a way that she believes is complete and correct, although her interpretation is not the one that the other one had intended. When this happens, we as analysts (or observers) would say that a misunderstanding has occurred. The participants themselves might...
AI
3602
The CMUnited-97 Small Robot Team Abstract. Robotic soccer is a challenging research domain which involves multiple agents that need to collaborate in an adversarial environment toachieve speci c objectives. In this paper, we describe CMUnited, the team of small robotic agents that we developed to enter the RoboCup-97 competition. We designed and built the robotic agents, devised the appropriate vision algorithm, and developed and implemented algorithms for strategic collaboration between the robots in an uncertain and dynamic environment. The robots can organize themselves in formations, hold speci c roles, and pursue their goals. In game situations, they have demonstrated their collaborative behaviors on multiple occasions. The robots can also switch roles to maximize the overall performance of the team. We present anoverview of the vision processing algorithm which successfully tracks multiple moving objects and predicts trajectories. The paper then focusses on the agent behaviors ranging from low-level individual behaviors to coordinated, strategic team behaviors. CMUnited won the RoboCup-97 small-robot competition at IJCAI-97 in Nagoya, Japan. 1
AI
estlin98using
Using Multi-Strategy Learning to Improve Planning Efficiency and Quality  
ML
hunter98ramification
Ramification Analysis Using Causal Mapping To operate in the real-world, intelligent agents constantly need to absorb new information, and to consider the ramifications of it. This raises interesting questions for knowledge representation and reasoning. Here we consider ramification analysis in which we wish to determine both the likely outcomes from events occuring and the less likely, but very significant outcomes, from events occuring. To formalize ramification analysis, we introduce the notion of causal maps for modelling "causal relationships" between events. In particular, we consider existential event classes, for example presidential-election, with instances being true, false,  or unknown, and directional events classes, for example inflation, with instances being increasing, decreasing or unchanging. Using causal maps, we can propagate new information to determine possible ramifications. These ramifications are also described in terms of events. Whilst causal maps offer a lucid view on ramifications, we also want to su...
AI
440022
DAB: Interactive Haptic Painting with 3D Virtual Brushes We present a novel painting system with an intuitive haptic interface, which serves as an expressive vehicle for interactively creating painterly works. We introduce a deformable, 3D brush model, which gives the user natural control of complex brush strokes. The force feedback enhances the sense of realism and provides tactile cues that enable the user to better manipulate the paint brush. We have also developed a bidirectional, two-layer paint model that, combined with a palette interface, enables easy loading of complex blends onto our 3D virtual brushes to generate interesting paint effects on the canvas. The resulting system, DAB, provides the user with an artistic setting, which is conceptually equivalent to a real-world painting environment. Several users have tested DAB and were able to start creating original art work within minutes.
HCI
457393
Context Awareness and Mobile Phones : This paper investigates some aspects of how context-awareness can support users of mobile phones, in particular the calling party. The use of mobile and stationary phones is discussed in relation to situational properties of a phone conversation, especially with regards to who might benefit from context-awareness in this context. An initial hypothesis is that mobile phone users communicate context information to each other (verbally) to a much higher degree than do stationary phone users. Mobile phone users could benefit much from context awareness technology, in particular when about to make a call, if they can receive context information regarding the person they are trying to reach prior to establishing the call. We argue that such technology should require low amounts of explicit user interaction, and could lead to less disrupting calls in inappropriate moments, as well as less frustration for the calling party when a call is not answered.  Keywords: Computer mediated communicati...
HCI
ali00incremental
Incremental Maintenance of Materialized OQL Views The importance of materialized views has grown significantly with the advent of data warehousing and OLAP technology. This increases the relevance of solutions to the problem of incrementally maintaining materialized views. So far, most work on this problem has been confined to relational settings. Proposals that apply to object databases have either used non-standard models or fallen short of providing a comprehensive framework. This paper contributes a solution to the incremental view maintenance problem for a large class of views expressed in OQL, the query language of the ODMG standard for object databases. The solution applies to immediate update propagation, and works for any update operation on views de ned over a substantial subset of ODMG types. The approach presented has been fully implemented and preliminary performance results are reported.
DB
449876
Why Unary and Binary Operations in Logic: General Result Motivated by Interval-Valued Logics Traditionally, in logic, only unary and binary operations are used as basic ones -- e.g., "not", "and", "or" -- while the only ternary (and higher order) operations are the operations which come from a combination of unary and binary ones. For the classical logic, with the binary set of truth values f0; 1g, the possibility to express an arbitrary operation in terms of unary and binary ones is well known: it follows, e.g., from the well known possibility to express an arbitrary operation in DNF form. A similar representation result for [0; 1]-based logic was proven in our previous paper. In this paper, we expand this result to finite logics (more general than classical logic) and to multi-D analogues of the fuzzy logic -- both motivated by interval-valued fuzzy logics.  1. 
ML
brown98iadea
IaDEA: A Development Environment Architecture for Building Generic Intelligent User Interface Agents The need exists in the work force for generic intelligent user interface agents to address the problem of increasing taskload that is overwhelming the human user. Interface agents could help alleviate user taskload by providing abstractions and intelligent assistance in a self-contained software agent that communicates with the user through the existing user interface and also adapts to user needs and behaviors. The benefits of a generic intelligent user interface agent environment is it can be applied to any highly interactive and information intensive software system from freight and parcel management systems to Wall Street financial investment and analysis. We desire to address the two following difficulties with developing interface agents: (1) The extensive number of existing computer systems makes it impractical to build these agents by hand for each system; (2) Any such agent must be compliant with existing user interface standards and business practices (e.g., the United States...
AI
grimm00revisiting
Revisiting Structured Storage: A Transactional Record Store An increasing number of applications, such as electronic mail servers, web servers, and personal information managers, handle large amounts of homogeneous data. This data can be effectively represented as records and manipulated through simple operations, e.g., record reading, writing, and searching. Unfortunately, modern storage systems are inappropriate for the needs of these applications. On one side, file systems store only unstructured data (byte strings) with very limited reliability guarantees. On the other side, relational databases store structured data and provide both concurrency control and transactions; but relational databases are often too slow, complex, and difficult to manage for many applications. This paper presents a transactional record store that directly addresses the needs of modern applications. The store combines the simplicity and manageability of the file system interface with a select few features for managing record-oriented data. We describe the principles guiding the design of our transactional record store as well as its design. We also present a prototype implementation and its performance evaluation. 1
DB
hoque98information
An Information Search Cost Perspective for Designing Interfaces for Electronic Commerce This research helps web developers apply knowledge on information search costs to the design of a web site for selling consumer products or services. The goal of the research is to predict how subtle changes in the user interface design influence information search costs. An empirical study compared 1,411 choices subjects made regarding a business to patronize from paper and electronic telephone directories. The choices were contingent upon information search costs imposed by the media. By providing a theoretical basis for predicting differences in information search costs, this research helps designers create more effective web sites for achieving their marketing objectives.  1  An Information Search Cost Perspective for Designing Interfaces for Electronic Commerce INTRODUCTION  Now more than ever, the promise of electronic commerce and on-line shopping will depend to a great extent upon the user interface and how people interact with the computer. In particular, success will depend ...
HCI
banko99generating
Generating Extraction-Based Summaries from Hand-Written Summaries by Aligning Text Spans Human-quality text summarization systems based on sentence extraction are difficult to design because documents can differ along several dimensions, such as length, writing style and lexical usage. The lack of suitable corpora of extraction-based summaries makes it difficult to evaluate and improve existing algorithms. However, there are a large number of hand-written (not extraction-based) summaries available for news-wire stories. This paper discusses our work on generating a corpus of approximately 25,000 extractionbased summaries from hand-written summaries. We discuss how text-span alignment can be applied to this problem and how this problem differs from previous work on aligning parallel texts. In addition, we briefly analyze differences between handwritten and extracted summaries. 1 Introduction  Human-quality text summarization systems based on sentence extraction are difficult to design, and even more difficult to evaluate, because documents can differ along several dimension...
IR
rafter00automated
Automated Collaborative Filtering Applications for Online Recruitment Services . Online recruitment services suffer from shortcomings due to traditional search techniques. Most users fail to construct queries that provide an adequate and accurate description of their (job) requirements, leading to imprecise search results. We investigate one potential solution that combines implicit profiling methods and automated collaborative filtering (ACF) techniques to build personalised query-less job recommendations. Two ACF strategies are implemented and evaluated in the JobFinder domain. 1 Introduction Online recruitment services have emerged as one of the most successful and popular information services on the Internet, providing job seekers with a comprehensive database of jobs and a dedicated search engine. For example, the award-winning Irish site, JobFinder (www.jobfinder.ie). However, like many similar Internet applications JobFinder suffers from shortcomings, due to its reliance on traditional database technology and the client-pull information access mode...
IR
meng99estimating
Estimating the Usefulness of Search Engines In this paper, we present a statistical method to estimate the usefulness of a search engine for any given query. The estimates can be used by a metasearch engine to choose local search engines to invoke. For a given query, the usefulness of a search engine in this paper is defined to be a combination of the number of documents in the search engine that are sufficiently similar to the query and the average similarity of these documents. Experimental results indicate that the proposed estimation method is quite accurate. 1 Introduction  Many search engines have been created on the Internet to help ordinary users find desired data. Each search engine has a corresponding database that defines the set of documents that can be searched by the search engine. Usually, an index for all documents in the database is created and stored in the search engine to speed up query processing. The amount of data in the Internet is huge (it is believed that by the end of 1997, there were more than 300 mil...
IR
458600
A Framework for Ontology Integration One of the basic problems in the development of techniques for the semantic  web is the integration of ontologies. Indeed, the web is constituted by a variety of  information sources, each expressed over a certain ontology, and in order to extract  information from such sources, their semantic integration and reconciliation in terms  of a global ontology is required. In this paper, we address the fundamental problem  of how to specify the mapping between the global ontology and the local ontologies.  We argue that for capturing such mapping in an appropriate way, the notion of query  is a crucial one, since it is very likely that a concept in one ontology corresponds  to a view (i.e., a query) over the other ontologies. As a result query processing in  ontology integration systems is strongly related to view-based query answering in data  integration.  1 
DB
harik99linkage
Linkage Learning via Probabilistic Modeling in the ECGA The goal of linkage learning, or building block identification, is the creation of a more effective genetic algorithm (GA). This paper explores the relationship between the linkage-learning problem and that of learning probability distributions over multi-variate spaces. Herein, it is argued that these problems are equivalent. Using a simple but effective approach to learning distributions, and by implication linkage, this paper reveals the existence of GA-like algorithms that are potentially orders of magnitude faster and more accurate than the simple GA. I. Introduction Linkage learning in genetic algorithms (GAs) is the identification of building blocks to be conserved under crossover. Theoretical studies have shown that if an effective linkage-learning GA were developed, it would hold significant advantages over the simple GA (2). Therefore, the task of developing such an algorithm has drawn significant attention. Past approaches to developing such an algorithm have focused on ev...
ML
myers97amulet
The Amulet Environment: New Models for Effective User Interface Software Development Abstract—The Amulet user interface development environment makes it easier for programmers to create highly-interactive, graphical user interface software for Unix, Windows and the Macintosh. Amulet uses new models for objects, constraints, animation, input, output, commands, and undo. The object system is a prototype-instance model in which there is no distinction between classes and instances or between methods and data. The constraint system allows any value of any object to be computed by arbitrary code and supports multiple constraint solvers. Animations can be attached to existing objects with a single line of code. Input from the user is handled by “interactor ” objects which support reuse of behavior objects. The output model provides a declarative definition of the graphics and supports automatic refresh. Command objects encapsulate all of the information needed about operations, including support for various ways to undo them. A key feature of the Amulet design is that all graphical objects and behaviors of those objects are explicitly represented at run-time, so the system can provide a number of high-level built-in functions, including automatic display and editing of objects, and external analysis and control of interfaces. Amulet integrates these capabilities in a flexible and effective manner. Index Terms—Toolkits, user interface tools, user interface development environments, user interface management systems (UIMSs). 1
HCI
449833
XML with Data Values: Typechecking Revisited We investigate the typechecking problem for XML queries: statically verifying that every answer to a query conforms to a given output DTD, for inputs satisfying a given input DTD. This problem had been studied by a subset of the authors in a simplified framework that captured the structure of XML documents but ignored data values. We revisit here the typechecking problem in the more realistic case when data values are present in documents and tested by queries. In this extended framework, typechecking quickly becomes undecidable. However, it remains decidable for large classes of queries and DTDs of practical interest. The main contribution of the present paper is to trace a fairly tight boundary of decidability for typechecking with data values. The complexity of typechecking in the decidable cases is also considered.  1. 
DB
turney99learning
Learning to Extract Keyphrases from Text Many academic journals ask their authors to provide a list of about five to fifteen key words, to appear on the first page of each article. Since these key words are often phrases of two or more words, we prefer to call them keyphrases. There is a surprisingly wide variety of tasks for which keyphrases are useful, as we discuss in this paper. Recent commercial software, such  as Microsoft's Word 97 and Verity's Search 97, includes algorithms that automatically extract keyphrases from documents. In this paper, we approach the problem of automatically extracting keyphrases from text as a supervised learning task. We treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Our first set of experiments applies the C4.5 decision tree induction algorithm to this learning task. The second set of experiments applies the GenEx algorithm to the task. We developed the GenEx algorithm specifically for this task. T...
ML
ordonez99discovering
Discovering Association Rules based on Image Content Our focus for data mining in this paper is concerned with knowledge discovery in image databases. We present a data mining algorithm to find association rules in 2-dimensional color images. The algorithm has four major steps: feature extraction, object identification, auxiliary image creation and object mining. Our emphasis is on data mining of image content without the use of auxiliary domain knowledge. The purpose of our experiments is to explore the feasibility of this approach. A synthetic image set containing geometric shapes was generated to test our initial algorithm implementation. Our experimental results show that there is promise in image mining based on content. We compare these results against the rules obtained from manually identifying the shapes. We analyze the reasons for discrepancies. We also suggest directions for future work. 1. Introduction  Discovering knowledge from data stored in typical alphanumeric databases, such as relational databases, has been the focal p...
ML
460612
Routing Through the Mist: Privacy Preserving Communication in Ubiquitous Computing Environments Ubiquitous computing is poised to revolutionize the way we compute and interact with each other. However, unless privacy concerns are taken into account early in the design process, we will end up creating a very effective distributed surveillance system, which would be a dream come true for electronic stalkers and "big brothers. " We present a protocol, which preserves the privacy of users and keeps their communication anonymous. In effect, we create a "mist" that conceals users from the system and other users. Yet, users will still be able to enjoy seamless interaction with services and other entities that wander within the ubiquitous computing environment. Keywords  Ubiquitous computing, privacy, Mist Routers, anonymous communication, authentication, security  1. 
HCI
538938
StorHouse/Relational Manager (RM) - Active Storage Hierarchy Database System and Applications This paper describes how database systems can use and exploit a cost-effective active storage hierarchy. By active storage hierarchy we mean a database system that uses all storage media (i.e. optical, tape, and disk) to store and retrieve data and not just disk. We describe and emphasize the active part, whereby all storage types are used to store raw data that is converted to strategic business information. We describe an evolution to the Data Warehouse concept, called Atomic Data Store, whereby atomic data is stored in the database system. Atomic data is defined as storing all the historic data values and executing queries against the historic queries. We also describe a Data Warehouse information collection, flow and central data store Hub-and-Spoke architecture, used to feed data into Data Marts. We also describe a commercial product; StorHouse/Relational Manager (RM). RM is a commercial relational database system that executes SQL queries directly against data stored on the storage hierarchy (i.e. tape, optical, disk). We conclude with a brief overview of a real world AT&T Call Detail Warehouse (CDW) case study.
DB
quek95eyes
Eyes in the Interface Computer vision has a significant role to play in the human-computer interaction (HCI) devices of the future. All computer input devices serve one essential purpose. They transduce some motion or energy from a human agent into machine useable signals. One may therefore think of input devices as the `perceptual organs' by which computers sense the intents of their human users. We outline the role computer vision will play, highlight the impediments to the development of vision-based interfaces, and propose an approach for overcoming these impediments. Prospective vision research areas for HCI include human face recognition, facial expression interpretation, lip reading, head orientation detection, eye gaze tracking, three-dimensional finger pointing, hand tracking, hand gesture interpretation, and body pose tracking. For vision-based interfaces to make any impact, we will have to embark on an expansive approach which begins with the study of the interaction modality we seek to implement...
HCI
gaudiano99neurobotic
Neurobotics Lab Research: Learning, Vision and Sonar Recognition with Mobile Robots This article provides an overview of research projects undertaken in the Neurobotics Laboratory at Boston University. We focus on applications of neural networks and other biomimetic techniques in sensory processing, navigation, and other tasks using mobile robots. These applications share some central themes: the inclusion of minimal assumptions about the robots and the environment; cross-validation of modules on a variety of robotics platforms and environments; and real-time operation using real robots.  Keywords: Mobile robots, looming, mobile robots, robot learning, Neural networks, ARTMAP, sensor fusion 1 Introduction  The Neurobotics Laboratory was founded in 1996 with the goal of applying neural networks and other biomimetic techniques to the control and guidance of wheeled mobile robot. Research in the lab covers various problems in the general area of autonomous mobile robotics, with an emphasis on navigation and control using biomimetic algorithms that operate in real-time wi...
ML
hupfeld00spatially
Spatially aware local communication in the RAUM system In this paper, we propose a new paradigm for local communication between devices in Ubiquitous Computing environments, assuming a multitude of computerized everyday appliances communicating with each other to solve tasks. This paradigm is based on the concept that the location of devices is central for the communication in such a scenario. Devices define their communication scope by spatial criteria. In our paradigm no explicit addressing or identification of communication partners is used. In comparison to traditional communication methods the approach eases routing and discovery problems and can be deployed in a highly dynamic environment without centralized services. We use the term local communication as inter-device communication in a physically restricted local area. This is well distinguish from the terms telecommunication as communication over distance where location information is explicitly hidden. The communication model (RAUM) introduced is based on the observ...
HCI
kutulakos96affine
Affine Object Representations for Calibration-Free Augmented Reality We describe the design and implementation of a videobased augmented reality system capable of overlaying threedimensional graphical objects on live video of dynamic environments. The key feature of the system is that it is completely uncalibrated: it does not use any metric information about the calibration parameters of the camera or the 3D locations and dimensions of the environment’s objects. The only requirement is the ability to track across frames at least four feature points that are specified by the user at system initialization time and whose world coordinates are unknown. Our approach is based on the following observation: Given a set of four or more non-coplanar 3D points, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points. We exploit this observation by (1) tracking lines and fiducial points at frame rate, and (2) representing virtual objects in a non-Euclidean, affine frame of reference that allows their projection to be computed as a linear combination of the projection of the fiducial points. 1.
HCI
14331
Point- Versus Interval-based Temporal Data Models The association of timestamps with various data items such as tuples or attribute values is fundamental to the management of time-varying information. Using intervals in timestamps, as do most data models, leaves a data model with a variety of choices for giving a meaning to timestamps. Specifically, some such data models claim to be point-based while other data models claim to be interval-based. The meaning chosen for timestamps is important---it has a pervasive effect on most aspects of a data model, including database design, a variety of query language properties, and query processing techniques, e.g., the availability of query optimization opportunities. This paper precisely defines the notions of point-based and interval-based temporal data models, thus providing a new, formal basis for characterizing temporal data models and obtaining new insights into the properties of their query languages. Queries in point-based models treat snapshot equivalent argument relations identically....
DB
miller99genetic
The Genetic Algorithm as a Discovery Engine: Strange Circuits and New Principles This paper examines the idea of a genetic or evolutionary algorithm being an inspirational or discovery engine. This is illustrated in the particular context of designing electronic circuits. We argue that by connecting pieces of logic together and testing them to see if they carry out the desired function it may be possible to discover new principles of design, and new algebraic techniques. This is illustrated in the design of binary circuits, particularly arithmetic functions, where we demonstrate that by evolving a hierarchical series of examples, it becomes possible to re-discover the well known ripple-carry principle for building adder circuits of any size. We also examine the much harder case of multiplication. We show also that extending the work into the field of multiple-valued logic, the genetic algorithm is able to produce fully working circuits that lie outside conventional algebra. In addition we look at the issue of principle extraction from evolved data.  1 Introduction ...
ML
469172
Mindless Visualisations The wonder and, unfortunately, to the detriment of visualisation for the representation and  comprehension of complex data sets is that to be most successful requires that they are tailored to suit  the task and underlying data. Such a restriction enables visualisations to be well designed for the tasks  to which they are known to be applied to, and also to accommodate the style and range of data to be  expected as normal. The problem with this repeated redesign of visualisations is that the interface is  often neglected, and can even be solely dependent on the implementing technology used for the  visualisation. It is important to add such issues as the interface to visualisation considerations, and to  provide reusable concepts that will integrate with a range of metaphors and displays. This position  paper examines the issues surrounding such visualisation interfaces and presents a discussion of those  issues.  1. 
HCI
vandertorre01contextual
Contextual Deontic Logic: Normative Agents, Violations and Independence this paper we discuss when and how to use deontic logic in multi agent systems
Agents
ockerman98preliminary
Preliminary Investigation of Wearable Computers for Task Guidance in Aircraft Inspection This paper describes a preliminary investigation of how the capabilities of wearable computers may be used to provide task guidance in mobile environments. Specifically, this study examined how the capabilities of wearable computers may be used to aid a user in an inspection task, using as a case study the procedural task of preflight inspection of a general aviation aircraft. Two different configurations of a computer-based, voiceactivated task guidance system and the current method of preflight inspection were compared and evaluated. Initial results demonstrate an over reliance on the computer by the pilots and indicate the importance of the user interface design to the performance of the inspectors. The paper concludes with recommendations on promising directions of research.  Keywords  task guidance, procedural tasks, aircraft inspection, computerized procedures, decision aiding, wearable computers  1. Introduction  Wearable computers combine portable, voiceactivated, wireless-netw...
HCI
oliboni01temporal
Temporal Aspects of Semistructured Data In many applications, information about the history of data and their dynamic aspects are just as important as static information. During the last years the increasing amount of information accessible through the Web has presented new challenges to academic and industrial research on database. In this context, data are either structured, when coming from relational or object-oriented databases, or partially or completely unstructured, when they consist of simple collections of text or image files. In the context of semistructured data, model and query languages must be extended in order to consider dynamic aspects. We present a model based on labeled graphs for representing changes in semistructured data and a SQL-like query language for querying it.
DB
moon00scalable
Scalable Algorithms for Large Temporal Aggregation The ability to model time-varying natures is essential to many database applications such as data warehousing and mining. However, the temporal aspects provide many unique characteristics and challenges for query processing and optimization. Among the challenges is computing temporal aggregates, which is complicated by having to compute temporal grouping. In this paper, we introduce a variety of temporal aggregation algorithms that overcome major drawbacks of previous work. First, for small-scale aggregations, both the worst-case and average-case processing time have been improved significantly. Second, for large-scale aggregations, the proposed algorithms can deal with a database that is substantially larger than the size of available memory.  
DB
wooldridge98intention
Intention Reconsideration Reconsidered Abstract. In this paper, we consider the issue of designing agents that successfully balance the amount of time spent in reconsidering their intentions against the amount of time spent acting to achieve them. Following a brief review of the various ways in which this problem has previously been analysed, we motivate and introduce a simple formal model of agents, which is closely related to the well-known belief-desire-intention model. In this model, an agent is explicitly equipped with mechanisms for deliberation and action selection, as well as a meta-level control function, which allows the agent to choose between deliberation and action. Using the formal model, we define what it means for an agent to be optimal with respect to a task environment, and explore how various properties of an agent’s task environment can impose certain requirements on its deliberation and meta-level control components. We then show how the model can capture a number of interesting practical reasoning scenarios, and illustrate how our notion of meta-level control can easily be extended to encompass higherorder meta-level reasoning. We conclude with a discussion and pointers to future work. 1
Agents
50203
What can Knowledge Representation do for Semi-Structured Data? The problem of modeling semi-structured data is important in many application areas such as multimedia data management, biological databases, digital libraries, and data integration. Graph schemas (Buneman et al. 1997) have been proposed recently as a simple and elegant formalism for representing semistructured data. In this model, schemas are represented as graphs whose edges are labeled with unary formulae of a theory, and the notions of conformance of a database to a schema and of subsumption between two schemas are defined in terms of a simulation relation. Several authors have stressed the need of extending graph schemas with various types of constraints, such as edge existence and constraints on the number of outgoing edges. In this paper we analyze the appropriateness of various knowledge representation formalisms for representing and reasoning about graph schemas extended with constraints. We argue that neither First Order Logic, nor Logic Programming nor Frame-based languages ...
AI
bazzan99moral
Moral Sentiments in Multi-Agent Systems We present a simulation of a society of agents where some  of them have "moral sentiments" towards the agents that belong to the  same social group, using the Iterated Prisoner's Dilemma as a metaphor  for the social interactions. Besides the well-understood phenomenon of  short-sighted, self-interested agents performing well in the short-term  but ruining their chances of such performance in the long run in a world  of reciprocators, the results suggest that, where some agents are more  generous than that, these agents have a positive impact on the social  group to which they belong, without compromising too much their individual  performance (i.e., the group performance improves). The inspiration  for this project comes from a discussion on Moral Sentiments by  M.Ridley. We describe various simulations where conditions and parameters  over determined dimensions were arranged to account for different  types and compositions of societies. Further, we indicate several lessons  that arise from the analysis of the results and comparison of the different  experiments. We also relate this work to our previous anthropological  approach to the adaptation of migrant agents, and argue that allowing  agents to possess suitably-chosen emotions can have a decisive impact on  Multi-Agent Systems. This implies that some common notions of agent  autonomy (and related concepts) should be reexamined.
Agents
482071
Using Process Algebras to Formally Specify Mobile Agent Data Integrity Properties: a Case Study This paper shows how cryptographic protocols for mobile agent data integrity properties can be formally specified by using spi calculus, an extension of # calculus with cryptographic properties. In particular, by means of a case study, it is shown how a specification technique initially conceived only for classical cryptographic protocols can be used in the context of mobile agents as well. Our case study includes the spi calculus specification of a sample mobile agent data integrity protocol and of its security property.
Agents
christel99interactive
Interactive Maps for a Digital Video Library The Informedia Digital Video Library contains over 1200 hours of video. Through automatic processing, descriptors are derived for the video to improve library access. A new extension to the video processing is the extraction of geographic references from these descriptors. The operational library interface shows the geographic entities addressed in a given story, highlighting the regions discussed at any point in the video through a map display synchronized with the video playback. The map can also be used as a query mechanism, allowing users to search the terabyte library for stories taking place in a selected area of interest.  1. Introduction  The Informedia Project at Carnegie Mellon University investigates the utility of speech recognition, image processing, and natural language processing techniques for improving search and discovery in the video medium. Since 1994, the project has been digitizing, in MPEG-1 format, news video from CNN as well as documentary/educational video fro...
HCI
22778
Incremental Maintenance for Materialized Views over Semistructured Data Semistructured data is not strictly typed like relational or object-oriented data and may be irregular or incomplete. It often arises in practice, e.g., when heterogeneous data sources are integrated or data is taken from the World Wide Web. Views over semistructured data can be used to filter the data and to restructure (or provide structure to) it. To achieve fast query response time, these views are often materialized. This paper studies incremental maintenance techniques for materialized views over semistructured data. We use the graph-based data model OEM and the query language Lorel, developed at Stanford, as the framework for our work. We propose a new algorithm that produces a set of queries that compute the changes to the view based upon a change to the source. We develop an analytic cost model and compare the cost of executing our incremental maintenance algorithm to that of recomputing the view. We show that for nearly all types of database updates, it is more efficient to a...
DB
jonsson00planning
Planning in Interplanetary Space: Theory and Practice On May 17th 1999, NASA activated for the first time  an AI-based planner/scheduler running on the flight  processor of a spacecraft. This was part of the Remote  Agent Experiment (RAX), a demonstration of closedloop  planning and execution, and model-based state inference  and failure recovery. This paper describes the  RAX Planner/Scheduler (RAX-PS), both in terms of  the underlying planning framework and in terms of the  fielded planner. RAX-PS plans are networks of constraints,  built incrementally by consulting a model of  the dynamics of the spacecraft. The RAX-PS planning  procedure is formally well defined and can be  proved to be complete. RAX-PS generates plans that  are temporally flexible, allowing the execution system  to adjust to actual plan execution conditions without  breaking the plan. The practical aspect, developing a  mission critical application, required paying attention  to important engineering issues such as the design of  methods for programmable search contr...
DB
staab00from
From Binary Temporal Relations to Non-Binary Ones and Back In this paper a new approach towards temporal reasoning is presented that scales up from the temporal relations commonly used in Allen's qualitative interval calculus and in quantitative temporal constraint satisfaction problems to include interval relations with distances, temporal rules and other non-binary relations into the reasoning scheme. For this purpose, we generalize well-known methods for constraint propagation, determination of consistency and computation of the minimal network from simpler schemes that only allow for binary relations. Thereby, we nd that levels of granularity play a major role for applying these techniques in our more expressive framework. Indeed, the technical preliminaries we provide are especially apt to investigate the switching between dierent granularities of representation, hence illucitating and exploiting the tradeo between expressiveness and eciency of temporal reasoning schemes on the one side and between expressiveness and understandability ...
DB
128773
Answering Queries by Semantic Caches There has been growing interest in semantic query caches to  aid in query evaluation. Semantic caches are simply the results of previously  asked queries, or selected relational information chosen by an evaluation  strategy, that have been cached locally. For complex environments  such as distributed, heterogeneous databases and data warehousing, the  use of semantic caches promises to help optimize query evaluation, increase  turnaround for users, and reduce network load and other resource  usage. We present a general logical framework for semantic caches. We  consider the use of all relational operations across the caches for answering  queries, and we consider the various ways to answer, and to partially  answer, a query by cache. We address when answers are in cache, when  answers in cache can be recovered, and the notions of semantic overlaps,  semantic independence, and semantic query remainder.  While there has been much work relevant to the use of semantic caches,  no one has addressed in conjunction the issues pertinent to the effective  use of semantic caches to evaluate queries. In some cases, this is due to  overly simplified assumptions, and in other cases to the lack of a formal  framework. We attempt to establish some of that framework here. Within  that framework, we illustrate the issues involved in using semantic caches  for query evaluation. We show various applications for semantic caches,  and relate the work to relevant areas.  1 
DB
524282
Phidgets: Easy Development of Physical Interfaces through Physical Widgets Physical widgets or phidgets are to physical user interfaces what widgets are to graphical user interfaces. Similar to widgets, phidgets abstract and package input and output devices: they hide implementation and construction details, they expose functionality through a well-defined API, and they have an (optional) on-screen interactive interface for displaying and controlling device state. Unlike widgets, phidgets also require: a connection manager to track how devices appear on-line; a way to link a software phidget with its physical counterpart; and a simulation mode to allow the programmer to develop, debug and test a physical interface even when no physical device is present. Our evaluation shows that everyday programmers using phidgets can rapidly develop physical interfaces.
HCI
cluet99using
Using LDAP Directory Caches this paper, we consider the problem of reusing cached LDAP directory entries for answering (declarative) LDAP queries. We develop a suite of query transformations that capture the semantics of LDAP queries, and design a sound and complete algorithm for determining whether a conjunctive LDAP query is cacheanswerable  using positive query templates. We demonstrate the practicality of our algorithm for real applications with a preliminary performance evaluation, based on sample queries from a directory enabled application at AT&T Labs. Cache-answerability is related to the problem of finding complete rewritings of a query using views (see, e.g., [8]). A key conceptual difference arises due to the fact that we do not seek a rewriting of the original query in terms of the (views in the) semantic cache description, but want to evaluate the original query against the cached directory instance. Further, the differences of the LDAP data model and query language make the previous results inapplicable; for details, see Section 2.
DB
13839
Mixtures of Probabilistic Principal Component Analysers Principal component analysis (PCA) is one of the most popular techniques for processing, compressing and visualising data, although its effectiveness is limited by its global linearity. While nonlinear variants of PCA have been proposed, an alternative paradigm is to capture data complexity by a combination of local linear PCA projections. However, conventional PCA does not correspond to a probability density, and so there is no unique way to combine PCA models. Previous attempts to formulate mixture models for PCA have therefore to some extent been ad hoc. In this paper, PCA is formulated within a maximum-likelihood framework, based on a specific form of Gaussian latent variable model. This leads to a well-defined mixture model for probabilistic principal component analysers, whose parameters can be determined using an EM algorithm. We discuss the advantages of this model in the context of clustering, density modelling and local dimensionality reduction, and we demonstrate its applicatio...
ML
salles98development
Development of a Computer Aided Geographic Database Design System This paper presents a prototype being developed at IC-UNICAMP to help environmental planners specify their application databases. The ultimate goal of the system presented is to reduce the impedance mismatch between the end-user's view of the geographic reality and its implementation in Geographic Information Systems (gis). The prototype offers users the possibility of specifying their application databases using concepts closer to their view of the world, by means of an object oriented geographic data model. This specification is mapped by the system to an intermediate object oriented schema, which can then be transformed into different underlying target geographic DBMS. The prototype was implemented using C and O2C on the O2 object oriented DBMS. 1 Introduction  A Geographic Information System -- gis, for short -- is a software that performs data management and retrieval operations for georeferenced data. The term refers to data about geographic phenomena associated with its location...
DB
piater99toward
Toward Learning Visual Discrimination Strategies Humans learn strategies for visual discrimination through interaction with their environment. Discrimination skills are refined as demanded by the task at hand, and are not a priori determined by any particular feature set. Tasks are typically incompletely specified and evolve continually. This work presents a general framework for learning visual discrimination that addresses some of these characteristics. It is based on an infinite combinatorial feature space consisting of primitive features such as oriented edgels and texture signatures, and compositions thereof. Features are progressively sampled from this space in a simple-to-complex manner. A simple recognition procedure queries learned features one by one and rules out candidate object classes that do not sufficiently exhibit the queried feature. Training images are presented sequentially to the learning system, which incrementally discovers features for recognition. Experimental results on two databases of geometric objects ill...
ML
diao00toward
Toward Learning Based Web Query Processing In this paper, we describe a novel Web query processing approach with learning capabilities. Under this approach, user queries are in the form of keywords and search engines are employed to find URLs of Web sites that might contain the required information. The first few URLs are presented to the user for browsing. Meanwhile, the query processor learns both the information required by the user and the way that the user navigates through hyperlinks to locate such information. With the learned knowledge, it processes the rest URLs and produces precise query results in the form of segments of Web pages without user involvement. The preliminary experimental results indicate that the approach can process a range of Web queries with satisfactory performance. The architecture of such a query processor, techniques of modeling HTML pages, and knowledge for query processing are discussed. Experiments on the effectiveness of the approach, the required knowledge, and the training strategies are presented.
Agents
254597
A Mediation Infrastructure for Digital Library Services Digital library mediators allow interoperation between diverse information services. In this paper we describe a flexible and dynamic mediator infrastructure that allows mediators to be composed from a set of modules ("blades"). Each module implements a particular mediation function, such as protocol translation, query translation, or result merging. All the information used by the mediator, including the mediator logic itself, is represented by an RDF graph. We illustrate our approach using a mediation scenario involving a Dienst and a Z39.50 server, and we discuss the potential advantages and weaknesses of our framework.  KEYWORDS: mediator, wrapper, interoperability, component design  1 INTRODUCTION  Heterogeneity is one of the main challenges faced by digital libraries. Too often documents are stored in different formats, collections are searched with disparate query languages, search services are accessed with incompatibleprotocols, intellectual property protection and access sche...
IR
pal01rough
Rough Fuzzy MLP: Modular Evolution, Rule Generation and Evaluation A methodology is described for evolving a Rough-fuzzy multi layer perceptron with modular concept using a genetic algorithm to obtain a structured network suitable for both classification and rule extraction. The modular concept, based on "divide and conquer" strategy, provides accelerated training and a compact network suitable for generating a minimum number of rules with high certainty values. The concept of variable mutation operator is introduced for preserving the localized structure of the constituting knowledge based subnetworks, while they are integrated and evolved. Rough set dependency rules are generated directly from the real valued attribute table containing fuzzy membership values. Two new indices viz., `certainty' and `confusion' in a decision are defined for evaluating quantitatively the quality of rules. The effectiveness of the model and the rule extraction algorithm is extensively demonstrated through experiments alongwith comparisons. 
ML
sherrah00fusion
Fusion of Perceptual Cues for Robust Tracking of. . . The paradigm of perceptual fusion provides robust solutions to computer vision problems. By combining the outputs of multiple vision modules, the assumptions and constraints of each module are factored out to result in a more robust system overall. The integration of dierent modules can be regarded as a form of data fusion. To this end, we propose a framework for fusing dierent information sources through estimation of covariance from observations. The framework is demonstrated in a face and 3D pose tracking system that fuses similarity-to-prototypes measures and skin colour to track head pose and face position. The use of data fusion through covariance introduces constraints that allow the tracker to robustly estimate head pose and track face position simultaneously.  Key words: data fusion, pose estimation, similarity representation, face  recognition  1 Introduction  The approach we have taken to computer vision, referred to as perceptual fusion, involves the integration of multip...
ML
449529
An Agent-Based Framework for Financial Transactions Ever-changing Internet technologies are creating revolutions in the way people interact with each other. In particular, business interactions are rapidly transforming and evolving toward more dynamic and automated solutions. Among various on-line commercial activities, financial services represent a fundamental component for developing and supplying many other e-businesses. The main idea behind efinance  is to provide support by deploying software instruments that enable to automate many of B2B and B2C transactions. The current degree of automation and personalisation of on-line financial services is still very limited: Web interfaces or ad hoc tools still require a lots of human interactions. Agent technology seems to be one of the most promising approaches for evolving toward more flexible and dynamic solutions. Autonomous, intelligent, social and selfinterested software entities would act on behalf of final endusers and/or business operators without the need for direct human involvement. This paper describes an agent-based system supporting automated business transactions. The aim is to evaluate the main potential and the major limits of supplying financial services by deploying agents in a software environment.  Keywords  agent-based interactions, e-finance, electronic payments, ontology. 1. 
Agents
giunchiglia01tropos
The Tropos Software Development Methodology: Processes, Models And Diagrams Abstract. Tropos is a novel agent-oriented software development methodology founded on two key features: (i) the notions of agent, goal, plan and various other knowledge level concepts are fundamental primitives used uniformly throughout the software development process; and (ii) a crucial role is assigned to requirements analysis and specification when the system-to-be is analyzed with respect to its intended environment. This paper provides a (first) detailed account of the Tropos methodology. In particular, we describe the basic concepts on which Tropos is founded and the types of models one builds out of them. We also specify the analysis process through which design flows from external to system actors through a goal analysis and delegation. In addition, we provide an abstract syntax for Tropos diagrams and other linguistic constructs. 1
Agents
gibbins01scalability
Scalability Issues for Query Routing Service Discovery In this paper, we discuss the relationship between mediatorbased systems for service discovery in multi-agent systems, and the technique of query routing used for resource discovery in distributed information systems. We then construct a model of the query routing task which we use to examine the complexity and scalability characteristics of a number of commonly encountered architectures for resource or service discovery.
Agents
rabarijaona99building
Building a XML-based Corporate Memory . This paper emphasizes the interest of XML meta-language for corporate knowledge management.  Taking into account the advantages of the World Wide Web and of ontologies for knowledge  management, we present OSIRIX, a tool enabling enterprise-ontology- guided search in XML documents  that may consitute a part of a corporate memory.  Keywords: XML, World Wide Web, knowledge management, document-based corporate memory,  enterprise ontology, information retrieval.  1 Introduction  Extending the definitions proposed by [28] [20], we consider a corporate memory as an explicit, disembodied, persistent representation of knowledge and information in an organization, in order to facilitate its access and reuse by members of the organization, for their tasks. We consider its building as relying on the following steps [11]: (1) Detection of needs in corporate memory, (2) Construction of the corporate memory, (3) Diffusion of the corporate memory, (4) Use of the corporate memory, (5) Evaluation of...
IR
nahm00using
Using Information Extraction to Aid the Discovery of Prediction Rules from Text Text mining and Information Extraction (IE) are both topics of significant recent interest. Text mining concerns applying data mining, a.k.a. knowledge discovery from databases (KDD) techniques to unstructured text. Information extraction (IE) is a form of shallow text understanding that locates specific pieces of data in natural language documents, transforming unstructured text into a structured database. This paper describes a system called DiscoTEX, that combines IE and KDD methods to perform a text mining task, discovering prediction rules from natural-language corpora. An initial version of DiscoTEX is constructed by integrating an IE module based on Rapier and a rule-learning module, Ripper. We present encouraging results on applying these techniques to a corpus of computer job postings from an Internet newsgroup.
ML
denti99from
From Tuple Spaces to Tuple Centres A tuple centre is a Linda-like tuple space whose behaviour can be programmed by means of transactional reactions to the standard communication events. This paper defines the notion of tuple centre, and shows the impact of its adoption as a coordination medium for a distributed multi-agent system on both the system design and the overall system efficiency.
Agents
tari00cache
Cache Management in CORBA Distributed Object Systems For many distributed data intensive applications, the default remote invocation of CORBA objects by clients is not acceptable because of performance degradation. Caching enables clients to invoke operations locally on distributed objects instead of fetching them from remote servers. This paper addresses the design and implementation of a specific caching approach for CORBA-based systems. We propose a new removal algorithm which uses a double linked structure and a hash table for eviction. We also present a new variation of optimistic two phase locking for consistency control, which does not require a lock at the client side by using a perprocess caching design. With the experiments we have performed, we demonstrate that the proposed caching approach provides an important performance gain: the caching with half buffer saves up to 45% of access time and the caching with full buffer saves up to 50% of access time. The Common Object Request Broker Architecture (CORBA) provides several adv...
DB
474192
Battery-aware Static Scheduling for Distributed Real-time Embedded Systems This paper addresses battery-aware static scheduling in batterypowered  distributed real-time embedded systems. As suggested by  previous work, reducing the discharge current level and shaping  its distribution are essential for extending the battery lifespan. We  propose two battery-aware static scheduling schemes. The first  one optimizes the discharge power profile in order to maximize  the utilization of the battery capacity. The second one targets  distributed systems composed of voltage-scalable processing  elements (PEs). It performs variable-voltage scheduling via  efficient slack time re-allocation, which helps reduce the average  discharge power consumption as well as flatten the discharge  power profile. Both schemes guarantee the hard real-time  constraints and precedence relationships in the real-time  distributed embedded system specification. Based on previous  work, we develop a battery lifespan evaluation metric which is  aware of the shape of the discharge power profile. Our  experimental results show that the battery lifespan can be  increased by up to 29% by optimizing the discharge power file  alone. Our variable-voltage scheme increases the battery lifespan  by up to 76% over the non-voltage-scalable scheme and by up to  56% over the variable-voltage scheme without slack-time reallocation.  1. 
HCI
420817
MRML: An Extensible Communication Protocol for Interoperability and Benchmarking of Multimedia Information Retrieval Systems While in the area of relational databases interoperability is ensured by common communication protocols (e.g. ODBC/JDBC using SQL), Content Based Image Retrieval Systems (CBIRS) and other multimedia retrieval systems are lacking both a common query language and a common communication protocol.  Besides its obvious short term convenience, interoperability of systems is crucial for the exchange and analysis of user data. In this paper, we present and describe an extensible XML-based query markup language, called MRML (Multimedia Retrieval Markup Language). MRML is primarily designed so as to ensure interoperability between dierent content{based multimedia retrieval systems. Further, MRML allows researchers to preserve their freedom in extending their system as needed.  MRML encapsulates multimedia queries in a way that enables multimedia (MM) query languages, MM content descriptions, MM query engines, and MM user interfaces to grow independently from each other, reaching a maximum of in...
HCI
moreno96limited
Limited Logical Belief Analysis . The process of rational inquiry can be defined as the evolution of a rational agent's belief set as a consequence of its internal inference procedures and its interaction with the environment. These beliefs can be modelled in a formal way using doxastic logics. The possible worlds model and its associated Kripke semantics provide an intuitive semantics for these logics, but they seem to commit us to model agents that are logically omniscient and perfect reasoners. These problems can be avoided with a syntactic view of possible worlds, defining them as arbitrary sets of sentences in a propositional doxastic logic. In this paper this syntactic view of possible worlds is taken, and a dynamic analysis of the agent's beliefs is suggested in order to model the process of rational inquiry in which the agent is permanently engaged. One component of this analysis, the logical one, is summarily described. This dimension of analysis is performed using a modified version of the analytic tableaux...
Agents
clarke01exploiting
Exploiting Redundancy in Question Answering Our goal is to automatically answer brief factual questions of the form "When was the Battle of Hastings?" or "Who wrote The Wind in the Willows?". Since the answer to nearly any such question can now be found somewhere on the Web, the problem reduces to finding potential answers in large volumes of data and validating their accuracy. We apply a method for arbitrary passage retrieval to the first half of the problem and demonstrate that answer redundancy can be used to address the second half. The success of our approach depends on the idea that the volume of available Web data is large enough to supply the answer to most factual questions multiple times and in multiple contexts. A query is generated from a question and this query is used to select short passages that may contain the answer from a large collection of Web data. These passages are analyzed to identify candidate answers. The frequency of these candidates within the passages is used to "vote" for the most likely answer. The ...
IR
paton99active
Active Database Systems , Exception, Clock, Externalg Granularity ae fMember, Subset, Setg Type ae fPrimitive, Composite g Operators ae for, and, seq, closure, times, not g Consumption mode ae fRecent, Chronicle, Cumulative, Continuous g Role 2 fMandatory, Optional, Noneg Condition Role 2 fMandatory, Optional, Noneg Context ae fDB T , BindE , DBE , DBC g Action Options ae fStructure Operation, Behavior Invocation, Update-Rules, Abort Inform, External, Do Instead g Context ae fDB T , BindE , BindC , DBE , DBC , DBA g ---behavior invocation, in which case the event is raised by the execution of some user-defined operation (e.g. the message display is sent to an object of type widget). It is common for event languages to allow events to be raised before or after an operation has been executed. ---transaction, in which case the event is raised by transaction commands (e.g. abort, commit, begin-transaction) ---abstract or user-defined, in which case a programming mechanism is used that allows an appli...
DB
446538
Exploiting Context to Make Delivered Information Relevant To Tasks and Users Building truly "context-aware" environments presents a greater challenge than using data transmitted by ubiquitous computing devices: it requires shared understanding between humans and their computational environments. This essay articulates some specific problems that can be addressed by representing context. It explores the unique possibilities of design environments that model and represent domains, tasks, design guidelines, solutions and their rationale, and the larger context of such environments embedded in the physical world. Context in design is not a fixed entity sensed by devices, but it is emerging and it is unbounded. Context-aware environments must address these challenges to be more supportive to all stakeholders who design and evolve complex design artifacts.  
HCI
30500
Design Issues for Mixed-Initiative Agent Systems This paper addresses the effect of mixed-initiative systems on multiagent systems design. A mixed-initiative system is one in which humans interact directly with software agents in a collaborative approach to problem solving. There are two main levels at which multiagent systems are designed: the domain level and the individual agent level. At the domain level, there are few unique challenges to mixedinitiative system design. However, at the individual agent level, the agent itself must be designed to interact with the human and the agent system, integrating the two into a single system. Introduction  Much of the current research related to intelligent agents has focused on the capabilities and structure of individual agents. However, in order to solve complex problems, these agents must work cooperatively with other agents in a heterogeneous environment. This is the domain of  Multiagent Systems. In multiagent systems, we are interested in the coordinated behavior of a system of indiv...
Agents
bradford99characterization
Characterization and Parallelization of Decision Tree Induction This paper examines the performance and memory-access behavior of the C4.5 decision tree induction program, a representative example of data mining applications, for both uniprocessor and parallel implementations. The goals of this paper are to characterize C4.5, in particular its memory hierarchy usage, and to decrease the runtime of C4.5 by algorithmic improvement and parallelization. Performance is studied via RSIM, an execution driven simulator, for three uniprocessor models that exploit instruction level parallelism to varying degrees. This paper makes the following four contributions. First it presents a complete characterization of the C4.5 decision tree induction program. It was found that the with the exception of the input dataset, the working set fits into an 8-kB data cache; the instruction working set also fits into an 8-kB instruction cache. For datasets smaller than the L2 cache, performance is limited by accesses to main memory. It was also found that multiple-issue can...
DB
532291
Reconciling Co-Operative Planning and Automated Co-Ordination in Multiagent Systems In the context of cooperative work, the coordination of activities is provided essentially by two opposite classes of approaches, based on the notion of situated action and planning. Actually, given the complexity and dynamism of current cooperative scenarios, the need of model supporting both the approaches has emerged. A similar situation can be recognised in multiagent system coordination, where two classes of approaches are clearly distinguishable, with properties similar to the situated action and planning cases: the one, defined in literature as subjective coordination, accounts for realising coordination exclusively by means of the skills and the situated actions of the individual agents, exploiting their intelligence and communication capability. Among the other properties, these approaches promote autonomy, flexibility and intelligence in coordination processes. The other, defined in literature as objective coordination, accounts for realising coordination exploiting coordination media, which mediate agent interactions and govern them according to laws which reflect social goals and norms. Among the other properties, these approaches promote automation, efficiency, and prescriptiveness in coordination processes. In this work, the importance to support both approaches in the same coordination context is remarked, and a conceptual framework -- derived from Activity Theory -- is proposed, where both approaches effectively co-exist and work at different collaborative levels, exploiting both the flexibility and intelligence of the subjective approaches, and the prescription and automation of the objective ones.
Agents
500641
Application of ART2 Networks and Self-Organizing Maps to Collaborative Filtering Since the World Wide Web has become widespread, more and more  applications exist that are suitable for the application of social information  filtering techniques. In collaborative filtering, preferences of a user are  estimated through mining data available about the whole user population,  implicitly exploiting analogies between users that show similar characteristics.
IR
sandholm01winner
Winner Determination in Combinatorial Auction Generalizations Combinatorial markets where bids can be submitted on bundles of items can be economically desirable coordination mechanisms in multiagent systems where the items exhibit complementarity and substitutability. There has been a surge of recent research on winner determination in combinatorial auctions. In this paper we study a wider range of combinatorial market designs: auctions, reverse auctions, and exchanges, with one or multiple units of each item, with and without free disposal. We first theoretically characterize the complexity. The most interesting results are that reverse auctions with free disposal can be approximated, and in all of the cases without free disposal, even finding a feasible solution is ##-complete. We then ran experiments on known benchmarks as well as ones which we introduced, to study the complexity of the market variants in practice. Cases with free disposal tended to be easier than ones without. On many distributions, reverse auctions with free disposal were easier than auctions with free disposal -- as the approximability would suggest -- but interestingly, on one of the most realistic distributions they were harder. Single-unit exchanges were easy, but multi-unit exchanges were extremely hard.
Agents
lam99feature
Feature Reduction for Neural Network Based Text Categorization In a text categorization model using an artificial neural network as the text classifier, scalability is poor if the neural network is trained using the raw feature space since textural data has a very high-dimension feature space. We proposed and compared four dimensionality reduction techniques to reduce the feature space into an input space of much lower dimension for the neural network classifier. To test the effectiveness of the proposed model, experiments were conducted using a subset of the Reuters-22173 test collection for text categorization. The results showed that the proposed model was able to achieve high categorization effectiveness as measured by precision and recall. Among the four dimensionality reduction techniques proposed, Principal Component Analysis was found to be the most effective in reducing the dimensionality of the feature space. 1. Introduction  Text categorization is the classification of text documents into a set of one or more categories. In this paper, ...
ML
459109
Face-to-Face With Your Assistant - Realization Issues of Animated User Interface Agents for Home Appliances With the introduction of software agents and assistants, the concept of so-called social user interfaces evolved, incorporating natural language interaction, context awareness and anthropomorphic representations of visuals, scales, and degrees of freedom for interactions. Today's challenge is to build a suitable visualization architecture for anthropomorphic conversational user interfaces, and to design for  the believable and appropriate inclusion of  human attributes (such as emotions) in a face-toface  interaction. Integrated approaches to these tasks are presented here.  1 
HCI
papadias99constraintbase
Constraint-based Processing of Multiway Spatial Joins . A multiway spatial join combines information found in three or more spatial relations with respect to some spatial predicates. Motivated by their close correspondence with constraint satisfaction problems (CSPs), we show how multiway spatial joins can be processed by systematic search algorithms traditionally used for CSPs. This paper describes two different strategies, window reduction and synchronous traversal, that take advantage of underlying spatial indexes to effectively prune the search space. In addition, we provide cost models and optimization methods that combine the two strategies to compute more efficient execution plans. Finally, we evaluate the efficiency of the proposed techniques and the accuracy of the cost models through extensive experimentation with several query and data combinations.  Key Words. Spatial Databases, Spatial Joins, Constraint Satisfaction, R-trees  1. INTRODUCTION  Spatial DBMSs and GISs store large amounts of multi-dimensional data, such as points...
DB
mistry01materialized
Materialized View Selection and Maintenance Using Multi-Query Optimization Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.  In this paper, we show how to find an efficient plan for the maintenance of a set of materialized views, by exploiting common subexpressions between different view maintenance expressions. In particular, we show how to efficiently select (a) expressions and indices that can be effectively shared, by transient materialization; (b) additional expressions and indices for permanent materialization; and (c) the best maintenance plan -- incremental or recomputation -- for each view. These three decisions are highly interdependent, and the choice of...
DB
hetzler98multifaceted
Multi-faceted Insight through Interoperable Visual Information Analysis Paradigms To gain insight and understanding of complex information collections, users must be able to visualize and explore many facets of the information. This paper presents several novel visual methods from an information analyst's perspective. We present a sample scenario, using the various methods to gain a variety of insights from a large information collection. We conclude that no single paradigm or visual method is sufficient for many analytical tasks. Often a suite of integrated methods offers a better analytic environment in today's emerging culture of information overload and rapidly changing issues. We also conclude that the interactions among these visual paradigms are equally as important as, if not more important than, the paradigms themselves.  Keywords: information visualization, user scenario, information analysis, document analysis  1 Introduction  Information analysts face significant challenges dealing with the growing amount of information available and how to gain needed i...
HCI
82834
Realising the Full Potential of Workflow Modelling: A Practical Perspective In a broad sense, a workflow provides a partial or complete automation of a process at a level above traditional implementation platforms. Much of the emphasis of workflow specifications, as deployed by workflow management systems, focuses on process execution semantics. This includes a process's pre- and post-conditions and the sequence, repetition, choice, parallelism and synchronisation which characterises inter-process triggering. The successful development of workflows requires not only this but, in general, a fuller conceptualisation  of a domain's processing, including process semantics. Given the diversity of business processing configurations, which in absence of a complete theoretical foundation, follow experience and intuition, the characterisations - indeed the cognition - of workflows still appears coarse. In this regard, a wellspring of modelling techniques, paradigms and informal-formal method extensions which address the analysis of organisational processing structures ...
HCI
329890
A Realistic (Non-Associative) Logic And a Possible Explanations of 7&plusmn;2 Law When we know the subjective probabilities (degrees of belief) p1 and  p2 of two statements S1 and S2 , and we have no information about the relationship between these statements, then the probability of S1 &S2 can take any value from the interval [max(p1 + p2 \Gamma 1; 0); min(p1 ; p2 )]. If we must select a single number from this interval, the natural idea is to take its midpoint. The corresponding "and" operation p1 & p2  def  = (1=2) \Delta (max(p1 +p2 \Gamma 1; 0)+min(p1 ; p2)) is not associative. However, since the largest possible non-associativity degree j(a & b) & c \Gamma a & (b & c)j is equal to 1/9, this non-associativity is negligible if the realistic "granular" degree of belief have granules of width  1=9. This may explain why humans are most comfortable with  9 items to choose from (the famous "7 plus minus 2" law). We also show that the use of interval computations can simplify the (rather complicated) proofs. 1  1 In Expert Systems, We Need Estimates for the Degree of...
AI
529138
Managing Data Quality in Cooperative Information Systems (Extended Abstract) Massimo Mecella 1, Monica Scannapieco 1'2, Antonino Virgillito 1, Roberto  Baldoni I , Tiziana Catarci 1, and Carlo Batini 3  i Universirk di Roma "La Sapienza"  Dipartimento di Informatica e Sistemistica  {mecella, monscan, virgi, baldoni, catarci}dis. uniromal. it  2 Consiglio Nazionale delle Ricerche  Istituto di Analisi dei Sistemi ed Informatica (IASI-CNR)  3 Universirk di Milano "Bicocca"  Dipartimento di Informatica, Sistemistica e Comunicazione  batinidisco. unimib. it  Abstract. Current approaches to the development of cooperative information  systems are based on services to be offered by cooperating organizations,  and on the opportunity of building coordinators and brokers  on top of such services. The quality of data exchanged and provided by  different services hampers such approaches, as data of low quality can  spread all over the cooperative system. At the same time, improvement  can be based on comparing data, correcting them and disseminating high  quality data. In this paper, a service-based framework for managing data  quality in cooperative information systems is presented. An XML-based  model for data and quality data is proposed, and the design of a broker  for data, which selects the best available data from different services, is  presented. Such a broker also supports the improvement of data based on  feedbacks to source services.
DB
169000
Learning Comprehensible Descriptions of Multivariate Time Series Supervised classification is one of the most active areas of machine learning research. Most work has focused on classification in static domains, where an instantaneous snapshot of attributes is meaningful. In many domains, attributes are not static; in fact, it is the way they vary temporally that can make classification possible. Examples of such domains include speech recognition, gesture recognition and electrocardiograph classification. While it is possible to use ad-hoc, domain-specific techniques for "attening" the time series to a learner-friendly representation, this fails to take into account both the special problems and special heuristics applicable to temporal data and often results in unreadable concept descriptions. Though traditional time series techniques can sometimes produce accurate classi ers, few can provide comprehensible descriptions. We propose a general architecture for classification and description of multivariate time series. It employs event primitives to ana...
DB
kaski98dimensionality
Dimensionality Reduction by Random Mapping: Fast Similarity Computation for Clustering When the data vectors are high-dimensional it is computationally infeasible to use data analysis or pattern recognition algorithms which repeatedly compute similarities or distances in the original data space. It is therefore necessary to reduce the dimensionality before, for example, clustering the data. If the dimensionality is very high, like in the WEBSOM method which organizes textual document collections on a Self-Organizing Map, then even the commonly used dimensionality reduction methods like the principal component analysis may be too costly. It will be demonstrated that the document classiøcation accuracy obtained after the dimensionality has been reduced using a random mapping method will be almost as good as the original accuracy if the ønal dimensionality is sufficiently large (about 100 out of 6000). In fact, it can be shown that the inner product (similarity) between the mapped vectors follows closely the inner product of the original vectors. 
ML
sahin98visual
Visual Looming as a range sensor for mobile robots This paper describes and evaluates visual looming as a method for monocular range estimation. The looming algorithm is based on the relationship between displacements of the observer relative to an object, and the resulting change in the size of the object's image on the focal plane of the camera. Though the looming algorithm has been described in detail in prior reports, its usefulness for inexpensive, robust ranging has not been realized widely. In this paper we analyze visual looming as a visual range sensor for autonomous mobile robots. Systematic experiments with a Pioneer 1  mobile robot show that visual looming can be used to extract ranging information much as with sonar. The accuracy of the looming algorithm is found to be significantly more robust than sonar when the object whose distance is being measured is slanted relative to the robot's line of sight. On the other hand, sonar is better suited for objects that cannot be visually segmented from their background, or objects ...
AI
rehg99visionbased
Vision-Based Speaker Detection Using Bayesian Networks The development of user interfaces based on vision and speech requires the solution of a challenging statistical inference problem: The intentions and actions of multiple individuals must be inferred from noisy and ambiguous data. We argue that Bayesian network models are an attractive statistical framework for cue fusion in these applications. Bayes nets combine a natural mechanism for expressing contextual information with efficient algorithms for learning and inference. We illustrate these points through the development of a Bayes net model for detecting when a user is speaking. The model combines four simple vision sensors: face detection, skin color, skin texture, and mouth motion. We present some promising experimental results.  1 Introduction  Human-centered user-interfaces based on vision and speech present challenging sensing problems in which multiple sources of information must be combined to infer the user's actions and intentions. Statistical inference techniques therefore...
AI
riely99trust
Trust and Partial Typing in Open Systems of Mobile Agents . We present a partially-typed semantics for Dp, a distributed p-calculus. The semantics is designed for mobile agents in open distributed systems in which some sites may harbor malicious intentions. Nonetheless, the semantics guarantees traditional type-safety properties at  good locations by using a mixture of static and dynamic type-checking. We show how the semantics can be extended to allow trust between sites, improving performance and expressiveness without compromising type-safety. 1 Introduction  In [13] we presented a type system for controlling the use of resources in a distributed system, or network. The type system guarantees two properties:  ffl resource access is always safe, e.g. integer resources are always accessed with integers and string resources are always accessed with strings, and  ffl resource access is always authorized, i.e. resources may only be accessed by agents that have been granted permission to do so. While these properties are desirable, they are prop...
Agents
455679
Incremental Document Clustering for Web Page Classification Introduction  We consider document clustering for Web pages. Traditionally, the document classification task is carried out manually. In order to assign a document to an appropriate class, people would analyze the contents of the document first. Therefore a large amount of human effort would be required. There has been some research work conducted on automatic text classification. One approach is to learn the text classifiers by using the machine learning techniques. However, these algorithms are based on a set of positive and negative training examples for learning the text classifiers. The quality of the resulting classifiers highly depends on the fitness of the training examples. There are many terms and classes in the World Wide Web (or just the Web), and many new terms and concepts are created everyday. It is quite impossible to have domain experts to identify training examples to learn a classifier for each text class in the above manner.  In order to make the document cl
IR
443668
Instant Messaging and Awareness of Presence in WebWho This is a study of how awareness of presence affects content of instant messages via an awareness tool, WebWho. The awareness tool is an easily accessible web based system that visualises a large university computer lab. The instant messaging system is one of the functions of the tool, which allows students to virtually locate one another and to communicate via the instant messaging system. As WebWho is there to be accessed through any web browser, it requires no programming skills or special software. It may also be used from outside the computer lab by students located elsewhere.
HCI
neumann99augmented
Augmented Reality Tracking in Natural Environments Tracking, or camera pose determination, is the main technical challenge in creating augmented realities. Constraining the degree to which the environment may be altered to support tracking heightens the challenge. This paper describes several years of work at the USC Computer Graphics and Immersive Technologies (CGIT) laboratory to develop self-contained, minimally intrusive tracking systems for use in both indoor and outdoor settings. These hybrid-technology tracking systems combine vision and inertial sensing with research in fiducial design, feature detection, motion estimation, recursive filters, and pragmatic engineering to satisfy realistic application requirements.
HCI
295735
An Object Oriented Multidimensional Data Model for OLAP . Online Analytical Processing (OLAP) data is frequently organized in the form of multidimensional data cubes each of which is used to examine a set of data values, called measures, associated with multiple dimensions and their multiple levels. In this paper, we first propose a conceptual multidimensional data model, which is able to represent and capture natural hierarchical relationships among members within a dimension as well as the relationships between dimension members and measure data values. Hereafter, dimensions and data cubes with their operators are formally introduced. Afterward, we use UML (Unified Modeling Language) to model the conceptual multidimensional model in the context of object oriented databases. 1. Introduction  Data warehouses and OLAP are essential elements of decision support [5], they enable business decision makers to creatively approach, analyze and understand business problems [16]. While data warehouses are built to store very large amounts of integrat...
DB
amato98supporting
Supporting Image Search on the Web While pages on the Web contain more and more multimedia information, such as images, videos and  audio, today search engines are mostly based on textual information. There is an emerging need of a  new generation of search engines that try to exploit the full multimedia information present on the  Web. The approach presented in this paper is based on a multimedia model intended to describe the  various multimedia components, their structure and their relationships with a pre-defined  taxonomy of concepts, in order to support the information retrieval process. A prototype of an image  search engine, based on this approach, is presented as a first step in this direction, and results are  discussed.  This research has been funded by the EC ESPRIT Long Term Research program, project no. 9141, HERMES (Foundations of High Performance  Multimedia Information Management Systems).  1. INTRODUCTION  The wide use of the World-Wide Web (WWW) across Internet is making of vital importance the proble...
IR
507954
Applying A New Multidimentional Framework To The Evaluation Of Multiagent System Methodologies Because of the great interest in using multiagent systems (MAS) in a wide variety of applications in recent years, agentoriented methodologies and related modeling techniques have become a priority for the development of large scale agentbased systems. The work we present here belongs to the disciplines of Software Engineering and Distributed Artificial Intelligence. More specifically, we are interested in software engineering aspects involved in the development of multiagent systems (MAS). Several methodologies have been proposed for the development of MAS. For the most part, these methodologies remain incomplete: they are either an extension of object-oriented methodologies or an extension of knowledge-based methodologies. In addition, too little effort has gone into the standardization of MAS methodologies, platforms and environments. It seems obvious, therefore, that software engineering aspects of the development of MAS still remain an open field. The success of the agent paradigm requires systematic methodologies for the specification, analysis and design of "non toy" MAS applications. We here present a framework called MUCCMAS, which stands for MUltidimensional framework of Criteria for the Comparison of MAS methodologies, that enabled us to make a comparative analysis of existing main MAS methodologies. The overall results of this comparative analysis are presented here. Results from this work have also lead us to propose a unification scheme, much in the same spirit as that of UML, for MAS  methodologies. Our goal is to make MAS design and development more systematic and to contribute to the standardisation of MAS methodologies and platforms.
Agents
devaney98needles
Needles in a Haystack: Plan Recognition in Large Spatial Domains Involving Multiple Agents While plan recognition research has been applied to a wide variety of problems, it has largely made identical assumptions about the number of agents participating in the plan, the observability of the plan execution process, and the scale of the domain. We describe a method for plan recognition in a real-world domain involving large numbers of agents performing spatial maneuvers in concert under conditions of limited observability. These assumptions differ radically from those traditionally made in plan recognition and produce a problem which combines aspects of the fields of plan recognition, pattern recognition, and object tracking. We describe our initial solution which borrows and builds upon research from each of these areas, employing a pattern-directed approach to recognize individual movements and generalizing these to produce inferences of large-scale behavior.  Introduction  Plan recognition, the problem of inferring goals, intentions, or future actions given observations of ...
ML
hofmann99probabilistic
Probabilistic Latent Semantic Analysis Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two--mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.
IR
534400
Tinmith-Metro: New Outdoor Techniques for Creating City Models with an Augmented Reality Wearable Computer This paper presents new techniques for capturing and viewing on site 3D graphical models for large outdoor objects. Using an augmented reality wearable computer, we have developed a software system, known as TinmithMetro. Tinmith-Metro allows users to control a 3D constructive solid geometry modeller for building graphical objects of large physical artefacts, for example buildings, in the physical world. The 3D modeller is driven by a new user interface known as Tinmith-Hand, which allows the user to control the modeller using a set of pinch gloves and hand tracking. These techniques allow user to supply their AR renderers with models that would previously have to be captured with manual, time-consuming, and/or expensive methods.
HCI
diaconescu99componentbased
Component-based Algebraic Specification and Verification in CafeOBJ . We present a formal method for component-based system specification  and verification which is based on the new algebraic specification language  CafeOBJ, which is a modern successor of OBJ incorporating several new developments  in algebraic specification theory and practice.  We first give an overview of the main features of CafeOBJ, including its logical  foundations, and then we focus on the behavioural specification paradigm in  CafeOBJ, surveying the object-oriented CafeOBJ specification and verification  methodology based on behavioural abstraction.  The last part of this paper further focuses on a component-based behavioural  specification and verification methodology which features high reusability of  both specification code and verification proof scores. This methodology constitutes  the basis for an industrial strength formal method around CafeOBJ.  1 Introduction  CafeOBJ (whose definition is given by [7])is a modern successor of the OBJ language incorporating several ne...
DB
oard01evaluating
Evaluating Interactive Cross-Language Information Retrieval: Document selection . The problem of finding documents that are written in a language  that the searcher cannot read is perhaps the most challenging  application of Cross-Language Information Retrieval (CLIR) technology.  The first Cross-Language Evaluation Forum (CLEF) provided an  excellent venue for assessing the performance of automated CLIR techniques,  but little is known about how searchers and systems might interact  to achieve better cross-language search results than automated  systems alone can provide. This paper explores the question of how interactive  approaches to CLIR might be evaluated, suggesting an initial  focus on evaluation of interactive document selection. Important evaluation  issues are identified, the structure of an interactive CLEF evaluation  is proposed, and the key research communities that could be brought together  by such an evaluation are introduced.  1 Introduction  Cross-language information retrieval (CLIR) has somewhat uncharitably been referred to as "the problem ...
HCI
kennedy99distributed
Distributed Reflective Architectures The autonomy of a system can be defined as its capability to recover from unforeseen  difficulties without any user intervention. This thesis proposal addresses a small part of  this problem, namely the detection of anomalies within a system's own operation by the  system itself. It is a response to a challenge presented by immune systems which can  distinguish between "self" and "nonself", i.e. they can recognise a "foreign" pattern (due  to a virus or bacterium) as different from those associated with the organism itself, even  if the pattern was not previously encountered. The aim is to apply this requirement to  an artificial system, where "nonself" may be any form of deliberate intrusion or random  anomalous behaviour due to a fault.  When designing reflective architectures or self-diagnostic systems, it is simpler to rely  on a single coordination mechanism to make the system work as intended. However, such  a coordination mechanism cannot be inspected or repaired by the system i...
Agents
gratch00emile
Emile: Marshalling Passions in Training and Education Emotional reasoning can be an important contribution to auto- mated tutoring and training systems. This paper describes mile, a model of emotional reasoning that builds upon existing approaches and significantly generalizes and extends their capabilities. The main contribution is to show how an explicit planning model allows a more general treatment of several stages of the reasoning process. The model supports educational applications by allowing agents to appraise the emotional significance of events as they relate to students' (or their own) plans and goals, model and predict the emotional state of others, and alter behavior accordingly. 1 INTRODUCTION Emotional computers may seem an oxymoron but recent years have seen a flurry of computation accounts of emotion in a variety of applications. This paper describes mile, a model of emotional reasoning that extends and significantly generalizes prior work. mile illustrates how an explicit planning model supports a more general treatme...
Agents
jennings95controlling
Controlling Cooperative Problem Solving in Industrial Multi-Agent Systems using Joint Intentions One reason why Distributed AI (DAI) technology has been deployed in relatively few real-size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex, dynamic and unpredictable environments. As a consequence of the experience gained whilst building a number of DAI systems for industrial applications, a new principled model of cooperation has been developed. This model, called Joint Responsibility, has the notion of joint intentions at its core. It specifies pre-conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty. The theoretical model has been used to guide the implementation of a general-purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparativ...
Agents
412941
Towards the Standardization of Multi-Agent Systems Architectures: An Overview This article briefly describes these groups' efforts toward the standardization of multi-agent systems architectures, and sketches early works to define a multi-agent systems architecture at the University of Calgary. However, the main objective of this article is to give the reader a basic overview of the background and terminology in this exciting area of research.
Agents
65816
Distributional Clustering of Words for Text Classification This paper applies Distributional Clustering (Pereira  et al. 1993) to document classification. The approach clusters words into groups based on the distribution of class labels associated with each word. Thus, unlike some other unsupervised dimensionality-reduction techniques, such as Latent Semantic Indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy. Experimental results obtained on three real-world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2% accuracy---significantly better than Latent Semantic Indexing (Deerwester et al. 1990), class-based clustering (Brown et al. 1992), feature selection by mutual information (Yang and Pederson 1997), or Markovblanket -based feature selection (Koller and Sahami 1996). We also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clusteri...
IR
marin01hybrid
A Hybrid Approach to Profile Creation and Intrusion Detection Anomaly detection involves characterizing the behaviors of individuals or systems and recognizing behavior that is outside the norm. This paper describes some preliminary results concerning the robustness and generalization capabilities of machine learning methods in creating user profiles based on the selection and subsequent classification of command line arguments. We base our method on the belief that legitimate users can be classified into categories based on the percentage of commands they use in a specified period. The hybrid approach we employ begins with the application of expert rules to reduce the dimensionality of the data, followed by an initial clustering of the data and subsequent refinement of the cluster locations using a competitive network called Learning Vector Quantization. Since Learning Vector Quantization is a nearest neighbor classifier, and new record presented to the network that lies outside a specified distance is classified as a masquerader. Thus, this system does not require anomalous records to be included in the training set. 1.
ML
roobaert99viewbased
View-Based 3d Object Recognition With Support Vector Machines . Support Vector Machines have demonstrated excellent results in pattern recognition tasks and 3D object recognition. In this contribution, we confirm some of the results in 3D object recognition and compare it to other object recognition systems. We use di#erent pixel-level representations to perform the experiments, while we extend the setting to the more challenging and practical case when only a limited number of views of the object are presented during training. We report high correct classification of unseen views, especially considering that no domain knowledge is including into the proposed system. Finally, we suggest an active learning algorithm to reduce further the required number of training views. INTRODUCTION  Humans are able to recognize everyday 3D objects when shown previously only one - or at most a few - views of the object. In contrast, artificial systems must either been shown many views of an object (e.g. [8]) or either a lot of knowledge of object structure must ...
ML
edwards97systematic
Systematic Output Modification in a 2D User Interface Toolkit In this paper we present a simple but general set of techniques for modifying output in a 2D user interface toolkit. We use a combination of simple subclassing, wrapping, and collusion between parent and output objects to produce arbitrary sets of composable output transformations. The techniques described here allow rich output effects to be added to most, if not all, existing interactors in an application, without the knowledge of the interactors themselves. This paper explains how the approach works, discusses a number of example effects that have been built, and describes how the techniques presented here could be extended to work with other toolkits. We address issues of input by examining a number of extensions to the toolkit input subsystem to accommodate transformed graphical output. Our approach uses a set of "hooks" to undo output transformations when input is to be dispatched. KEYWORDS: User Interface Toolkits, Output, Rendering, Interactors, Drawing Effects. INTRODUCTION ...
HCI
chaudhri98okbc
OKBC: A Programmatic Foundation for Knowledge Base Interoperability The technology for building large knowledge bases (KBs) is yet to witness a breakthrough so that a KB can be constructed by the assembly of prefabricated knowledge components. Knowledge components include both pieces of domain knowledge (for example, theories of economics or fault diagnosis) and KB tools (for example, editors and theorem provers). Most of the current KB development tools can only manipulate knowledge residing in the knowledge representation system (KRS) for which the tools were originally developed. Open Knowledge Base Connectivity (OKBC) is an application programming interface for accessing KRSs, and was developed to enable the construction of reusable KB tools. OKBC improves upon its predecessor, the Generic Frame Protocol (GFP), in several signi cant ways. OKBC can be used with a much larger range of systems because its knowledge model supports an assertional view of a KRS. OKBC provides an explicit treatment ofentities that are not frames, and it has a much better way of controlling inference and specifying default values. OKBC can be used on practically any platform because it supports network transparency and has implementations for multiple programming languages. In this paper, we discuss technical design issues faced in the development of OKBC, highlight how OKBC improves upon GFP, and report on practical experiences in using it.
AI
533891
Multi-Sensor Context Aware Clothing Inspired by perception in biological systems, distribution of a massive amount of simple sensing devices is gaining more support in detection applications. A focus on fusion of sensor signals instead of strong analysis algorithms, and a scheme to distribute sensors, results in new issues. Especially in wearable computing, where sensor data continuously changes, and clothing provides an ideal supporting structure for simple sensors, this approach may prove to be favourable. Experiments with a body-distributed sensor system investigate the influence of two factors that affect classification of what has been sensed: an increase in sensors enhances recognition, while adding new classes or contexts depreciates the results. Finally, a wearable computing related scenario is discussed that exploits the presence of many sensors.
HCI
lobo99extended
Extended Compact Genetic Algorithm in C++ This report tells you how to download, compile, and run the extended compact genetic algorithm (ECGA) described in Harik's paper (Harik, 1999). It also explains how to modify the objective function that comes with the distribution of the code. The source is written in C++ but a knowledge of the C programming language is sufficient to modify the objective function so that you can try the ECGA on your own problems. 2 How to download the code
ML
turk98moving
Moving from GUIs to PUIs For some time, graphical user interfaces (GUIs) have been the dominant platform for human computer interaction. The GUI-based style of interaction has made computers simpler and easier to use, especially for office productivity applications. However, as the way we use computers changes and computing becomes more pervasive and ubiquitous, GUIs will not easily support the range of interactions necessary to  meet users' needs. In order to accommodate a wider range of scenarios, tasks, users, and preferences, we need to move toward interfaces that are natural, intuitive, adaptive, and unobtrusive. The aim of a new focus in HCI, called Perceptual User Interfaces (PUIs), is to make human-computer interaction more like how people interact with each other and with the world. This paper describes the emerging PUI field and then reports on three PUI-motivated components: computer vision-based techniques to visually perceive relevant information about the user.  1. Introduction  Recent research i...
HCI
gustafson00adapt
AdApt - a multimodal conversational dialogue system in an apartment domain A general overview of the AdApt project and the research that is performed within the project is presented. In this project various aspects of human-computer interaction in a multimodal conversational dialogue systems are investigated. The project will also include studies on the integration of user/system/dialogue dependent speech recognition and multimodal speech synthesis. A domain in which multimodal interaction is highly useful has been chosen, namely, finding available apartments in Stockholm. A Wizard-of-Oz data collection within this domain is also described. 1.
HCI
19462
Array-Based Evaluation of Multi-Dimensional Queries in Object-Relational Database Systems Since multi-dimensional arrays are a natural data structure for supporting multi-dimensional queries, and object-relational database systems support multi-dimensional array ADTs, it is natural to ask if a multi-dimensional array-based ADT can be used to improve O/R DBMS performance on multi-dimensional queries. As an initial step toward answering this question, we have implemented a multi-dimensional array in the Paradise ObjectRelational DBMS. In this paper we describe the implementation of this compressed-array ADT, and explore its performance for queries including star-join consolidations and selections. We show that in many cases the array ADT can provide significantly higher performance than can be obtained by applying techniques such as bitmap indices and star-join algorithms to relational tables. 1 Introduction  Multi-dimensional data analysis has been around for at least twenty years [OR95], but has recently taken the spotlight in the context of OLAP (On-Line Analytical Process...
DB
obitko01ontologies
Ontologies Description and Applications The word "ontology" has gained a good popularity within the AI  community. Ontology is usually viewed as a high-level description consisting  of concepts that organize the upper parts of the knowledge base.
AI
cardie97improving
Improving Minority Class Prediction Using Case-Specific Feature Weights This paper addresses the problem of handling skewed class distributions within the case-based learning (CBL) framework. We rst present as a baseline an informationgain-weighted CBL algorithm and apply it to three data sets from natural language processing (NLP) with skewed class distributions. Although overall performance of the baseline CBL algorithm is good, we show that the algorithm exhibits poor performance on minority class instances. We then present two CBL algorithms designed to improve the performance of minority class predictions. Each variation creates test-case-speci c feature weights by rst observing the path taken by the test case in a decision tree created for the learning task, and then using pathspeci c information gain values to create an appropriate weight vector for use during case retrieval. When applied to the NLP data sets, the algorithms are shown to signi cantly increase the accuracy of minority class predictions while maintaining or improving overall classi cation accuracy. 1
ML
overeinder02multiagent
Multi-Agent Support for Internet-Scale Grid Management Internet-scale computational grids are emerging from various research projects. Most notably are the US National Technology  Grid and the European Data Grid projects. One specific problem in realizing wide-area distributed computing  environments as proposed in these projects, is effective management of the vast amount of resources that are made  available within the grid environment. This paper proposes an agent-based approach to resource management in grid  environments, and describes an agent infrastructure that could be integrated with the grid middleware layer. This agent  infrastructure provides support for mobile agents that is scalable in the number of agents and the number of resources.
Agents
kon00using
Using Dynamic Configuration to Manage A Scalable Multimedia Distribution System Multimedia applications and interfaces will change radically the way computer systems will look like in the coming years. Radio and TV broadcasting will assume a digital format and their distribution networks will be integrated to the Internet. Existing hardware and software infrastructures, however, are unable to provide all the scalability, flexibility, and quality of service that these applications require. We present a framework for building scalable and flexible multimedia distribution systems that greatly improves the possibilities for the provision of quality of service in large-scale networks. We show how to use architectural-awareness, mobile agents, and a CORBA-based framework to support dynamic (re)configuration, efficient code distribution, and fault-tolerance. This approach can be applied not only for multimedia distribution, but also for any QoS-sensitive distributed application.
HCI
bauer01design
Design of a Component-Based Augmented Reality Framework We propose a new approach to building augmented reality (AR) systems using a component-based software framework. This has advantages for all parties involved with AR systems. Our proposed framework consists of reusable distributed services for key subproblems of AR, the middleware to combine them, and an extensible software architecture.
HCI
junker98learning
Learning Complex Patterns for Document Categorization Knowledge-based approaches to document categorization make use of well elaborated and powerful pattern languages for manual writing of classification rules. Although such classification patterns have proven useful in many practical applications, algorithms for learning classifiers from examples mostly rely on much simpler representations of classification knowledge. In this paper, we describe a learning algorithm which employs a pattern language similar to languages used for manual rule editing. We focus on the learning of three specific constructs of this pattern language, namely phrases, tolerance matches of words and substring matches of words. Introduction  Manually writing document categorization rules is labor intensive and requires much expertise. This caused a growing research interest in learning systems for document categorization. Most of these systems transform pre-classified example documents into a propositional attribute-value representation. Simple attributes indicate w...
ML
291837
Designing Synchronous User Interface for Collaborative Applications Synchronous User interface is a medium where all objects being shared on it can be  viewed indifferently from the geographical location and its users can interact with each other  in real-time. Designing such an interface for users working collaboratively requires to deal  with a number of issues. Herein, our concerns lies on the design of control component  of Human-Computer Interaction (HCI) and corresponding User Interface (UI) software that  implements it. We make use of our approach to interactive system development based on the  MPX - Mapping from PAN (Protagonist Action Notation) into Xchart (eXtended Statechart)  - and illustrate it by presenting the case study of a collaborative application.  Keywords: PAN, MPX, HCI design, UI software design.  1 INTRODUCTION  To survive, human beings need to organize themselves into a society. Differently from other animals that are able to live separately in reasonable manner, human beings are endowed with physical and cognitive abilities ne...
HCI
luyten01xmlbased
An XML-based runtime user interface description language for mobile computing devices In a time where mobile computing devices and embedded  systems gain importance, too much time is spent to reinventing user interfaces  for each new device. To enhance future extensibility and reusability  of systems and their user interfaces we propose a runtime user interface  description language, which can cope with constraints found in  embedded systems and mobile computing devices. XML seems to be a  suitable tool to do this, when combined with Java. Following the evolution  of Java towards XML, it is logical to introduce the concept applied  to mobile computing devices and embedded systems.  1 
HCI
tzouramanis99overlapping
Overlapping B+-trees: an Implementation of a Transaction Time Access Method A new variation of Overlapping B+-trees is presented, which provides efficient indexing of transaction time and keys in a two dimensional key-time space. Modification operations (i.e. insertions, deletions and updates) are allowed at the current version, whereas queries are allowed to any temporal version, i.e. either in the current or in past versions. Using this structure, snapshot and range-timeslice queries can be answered optimally. However, the fundamental objective of the proposed method is to deliver efficient performance in case of a general pure-key query (i.e. "history of a key"). The trade-off is a small increase in time cost for version operations and storage requirements.
DB
cicirello01ant
Ant Colony Control for Autonomous Decentralized Shop Floor Routing In this paper, we introduce a new approach to autonomous decentralized shop floor routing. Our system, which we call Ant Colony Control (AC  2  ), applies the analogy of a colony of ants foraging for food to the problem of dynamic shop floor routing. In this system, artificial ants use only indirect communication to make all shop routing decisions by altering and reacting to their dynamically changing common environment through the use of simulated pheromone trails. For simple factory layouts, we show that the emergent behavior of the colony is comparable to using the optimal routing strategy. Furthermore, as the complexity of the factory layout is increased, we show that the adaptive behavior of AC  2  evolves local decision making policies that lead to near-optimal solutions from the standpoint of global performance.  1. Introduction  The factory is a complex dynamical environment often plagued by unexpected events. Machines may break down. An unexpected urgent job may suddenly be re...
ML
18124
Domain-Specific Keyphrase Extraction Keyphrases are an important means of document  summarization, clustering, and topic  search. Only a small minority of documents  have author-assigned keyphrases, and manually  assigning keyphrases to existing documents is  very laborious. Therefore it is highly desirable  to automate the keyphrase extraction process.  This paper shows that a simple procedure for  keyphrase extraction based on the naiveBayes  learning scheme performs comparably to the  state of the art. It goes on to explain how  this procedure's performance can be boosted by  automatically tailoring the extraction process  to the particular document collection at hand.  Results on a large collection of technical reports  in computer science show that the quality of  the extracted keyphrases improves signi#cantly  when domain-speci#c information is exploited.  1 Introduction  Keyphrases give a high-level description of a document's contents that is intended to make it easy for prospective readers to decide whether or no...
IR
plaisant99design
The Design of History Mechanisms and their Use in Collaborative Educational Simulations Reviewing past events has been useful in many domains. Videotapes and flight data recorders provide invaluable technological help to sports coaches or aviation engineers. Similarly, providing learners with a readable recording of their actions may help them monitor their behavior, reflect on their progress, and experiment with revisions of their experiences. It may also facilitate active collaboration among dispersed learning communities. Learning histories can help students and professionals make more effective use of digital library searching, word processing tasks, computer-assisted design tools, electronic performance support systems, and web navigation. This paper describes the design space and discusses the challenges of implementing learning histories. It presents guidelines for creating effective implementations, and the design tradeoffs between sparse and dense history records. The paper also presents a first implementation of learning histories for a simulation-based engineer...
HCI
hoadley99between
Between Information and Communication: Middle Spaces in Computer Media for Learning In this paper, we identify two categories of media that are common in computer-supported collaborative learning and software in general: communication media, and information media. These two types of media map easily on to two types of social activities in which learning is grounded: dialogue and monologue. Drawing on literature in learning theory, we suggest the need for interfaces that helpstudents transition from dialogue to monologue and back again. This "middle space" between communication and information interfaces is illustrated with several examples from CSCL. We advocate filling in this middle space with software and activities that transcend some of the traditional design tradeoffs associated with information and communication interfaces. Keywords: Collaboration, Interaction & Design Tradeoffs Introduction: computers, communication & learning Computer mediated acts of communication are becoming more commonplace in today's classroom. Like all media, particular computer techn...
HCI
busi01some
Some Thoughts on Transiently Shared Dataspaces Transiently Shared Dataspaces (TSD), recently introduced in the coordination infrastructure Lime, represent an emerging technology to enable the use of dataspaces for the coordination of mobile agents: data-sharing is allowed among agents running on hosts belonging to the same federation (i.e., currently connected). In this paper we present a formalization of the TSD technology in order to provide a framework for reasoning about systems based on this infrastructure. In particular, we concentrate on alternative design choices related to data migration, host connectivity, and reactive programming. 1 
Agents
shkapenyuk01design
Design and Implementation of a High-Performance Distributed Web Crawler Broad web search engines as well as many more specialized search tools rely on web  crawlers to acquire large collections of pages for indexing and analysis. Such a web crawler  may interact with millions of hosts over a period of weeks or months, and thus issues of robustness,  flexibility, and manageability are of major importance. In addition, I/O performance,  network resources, and OS limits must be taken into account in order to achieve high performance  at a reasonable cost.  In this paper, we describe the design and implementation of a distributed web crawler that  runs on a network of workstations. The crawler scales to (at least) several hundred pages per  second, is resilient against system crashes and other events, and can be adapted to various  crawling applications. We present the software architecture of the system, discuss the performance  bottlenecks, and describe efficient techniques for achieving high performance. We also  report preliminary experimental results based on a crawl of  million pages on  million  hosts.    Work supported by NSF CAREER Award NSF CCR-0093400, Intel Corporation, and the New York State Center for Advanced Technology in Telecommunications (CATT) at Polytechnic University, and by equipment grants from Intel Corporation and Sun Microsystems.  1  1 
DB
wittner00network
Network Management by Knowledge Distribution using Mobile Agents d to each other. Functionality provided by the different agents and managers are located at given places in the system. Such a system may have difficulties coping with a highly dynamic network environment.  By introducing mechanisms for moving programcode. a more flexible system can be designed. A specific part (e.g. an agent) having a certain set of abilities can be moved or move itself closer to (or even into) the NE in need of management. Bandwidth can be saved and "expert agents" can be utilised. Autonomous moving program code is known as mobile agents [3].  3 Collective behavior  When designing agents for complex problem solving you can either create a few highly advanced agents [2] or try to divide and distribute the problem solving task between a number of less complex agents. Mother nature can give several examples of successful implementations based on the latter strategy. An ant colony is such an example where a high number of less inte
Agents
494575
Towards Group Communication for Mobile Participants Group communication will undoubtedly be a useful paradigm for many applications of wireless networking in which reliability and timeliness are requirements. Moreover, location awareness is clearly central to mobile applications such as traffic management and smart spaces. In this paper, we introduce our definition of proximity groups in which group membership depends on location and then discuss some requirements for a group membership management service suitable for proximity groups. We describe a novel approach to efficient coverage estimation, giving applications feedback on the proportion of the area of interest covered by a proximity group, and also discuss our approach to partition anticipation.
HCI
goldstein00creating
Creating and Evaluating Multi-Document Sentence Extract Summaries This paper discusses passage extraction approaches to multidocument summarization that use available information about  the document set as a whole and the relationships between the documents to build on single document summarization methodology. Multi-document summarization differs from single in that the issues of compression, speed, redundancy and passage selection are critical in the formation of useful summaries, as well as the user's goals in creating the summary. Our approach addresses these issues by using domain-independent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements. We examined howhumans create multi-document summaries as well as the characteristics of such summaries and use these summaries to evaluate the performance of various multidocument summarization algorithms.  1. 
IR
lau99privacy
Privacy Interfaces for Information Management this article, we propose a set of guidelines for designing privacy interfaces that facilitate the creation, inspection, modification, and monitoring of privacy policies. These guidelines are based on our experience with COLLABCLIO---a system that supports automated sharing of Web browsing histories. COLLAB- CLIO stores a person's browsing history and makes it searchable by content, keyword, and other attributes. A typical COLLABCLIO query might be: "Show me all the pages Tessa has visited in the .edu domain that contain the phrase `direct manipulation.' " Since a COLLABCLIO user can make queries regarding the browsing history of other users, there are obvious privacy concerns.
IR
bieman01finding
Finding Code on the World Wide Web: A Preliminary Investigation To find out what kind of design structures programmers really use, we need to examine a wide variety of programs. Unfortunately most program source code is proprietary and is unavailable for analysis. The World Wide Web (Web) potentially can provide a rich source of programs for study. The freely available code on the Web, if in sufficient quality and quantity, can provide a window into software design as it is practiced today. In a preliminary study of source code availability on the Web, we estimate that 4% of URLs contain object-oriented source code, and 9% of URLs contain executable code --- either binary or class files. This represents an enormous resource for program analysis. We can, with some risk of inaccuracy, conservatively project our sampling results to the entire Web. Our estimate is that the Web contains at least 3.4 million files containing either Java, C++, or Perl source code, 20.3 million files containing C source code, and 8.7 million files containing executable code.  Keywords: Design, source code analysis, World Wide Web estimation, code on the World Wide Web.  1. 
IR
34291
Lower Bounds for High Dimensional Nearest Neighbor Search and Related Problems In spite of extensive and continuing research, for various geometric search problems (such as nearest neighbor search), the best algorithms known have performance that degrades exponentially in the dimension. This phenomenon is sometimes called the curse of dimensionality. Recent results [33, 32, 35] show that in some sense it is possible to avoid the curse of dimensionality for the approximate nearest neighbor search problem. But must the exact nearest neighbor search problem suffer this curse? We provide some evidence in support of the curse. Specifically we investigate the exact nearest neighbor search problem and the related problem of exact partial match within the asymmetric communication model first used by Miltersen [38] to study data structure problems. We derive non-trivial asymptotic lower bounds for the exact problem that stand in contrast to known algorithms for approximate nearest neighbor search.   Department of Computer Science, University of Toronto. Part of this work ...
IR
rungsarityotin00finding
Finding Location Using Omnidirectional Video on a Wearable Computing Platform In this paper we present a framework for a navigation system in an indoor environment using only omnidirectional video. Within a Bayesian framework we seek the appropriate place and image from the training data to describe what we currently see and infer a location. The posterior distribution over the state space conditioned on image similarity is typically not Gaussian. The distribution is represented using sampling and the location is predicted and verified over time using the Condensation algorithm. The system does not require complicated feature detection, but uses a simple metric between two images. Even with low resolution input, the system may achieve accurate results with respect to the training data when given favorable initial conditions.  1. Introduction and Previous Work  Recognizing location is a difficult but often essential part of identifying a wearable computer user's context. Location sensing may be used to provide mobility aids for the blind [13], spatially-based not...
HCI
bowling01rational
Rational and Convergent Learning in Stochastic Games This paper investigates the problem of policy learning  in multiagent environments using the stochastic  game framework, which we briefly overview. We  introduce two properties as desirable for a learning  agent when in the presence of other learning agents,  namely rationality and convergence. We examine  existing reinforcement learning algorithms according  to these two properties and notice that they fail  to simultaneously meet both criteria. We then contribute  a new learning algorithm, WoLF policy hillclimbing,  that is based on a simple principle: "learn  quickly while losing, slowly while winning." The  algorithm is proven to be rational and we present  empirical results for a number of stochastic games  showing the algorithm converges.  1 
ML
wood00overview
An Overview of the Multiagent Systems Engineering Methodology . To solve complex problems, agents work cooperatively with other  agents in heterogeneous environments. We are interested in coordinating the  local behavior of individual agents to provide an appropriate system-level  behavior. The use of intelligent agents provides an even greater amount of  flexibility to the ability and configuration of the system itself. With these new  intricacies, software development is becoming increasingly difficult. Therefore,  it is critical that our processes for building the inherently complex distributed  software that must run in this environment be adequate for the task. This paper  introduces a methodology for designing these systems of interacting agents.  1. 
Agents
flach00abduction
Abduction And Induction. Essays On Their Relation And Integration. Bibliography  Charles S. Peirce Society, 22(2):145--164.  Anderson, D. (1987). Creativity and the Philosophy of C.S. Peirce, volume 27 of Philosophy  Library. Martinus Nijhoff.  Andreasen, T. and Christiansen, H. (1996). Counterfactual exceptions in deductive database queries. In Proceedings of the Twelfth European Conference on Artificial Intelligence, pages 340--344.  i  ii BIBLIOGRAPHY  Andreasen, T. and Christiansen, H. (1998). A practical approach to hypothetical database  queries. In Freitag, B., Decker, H., Kifer, M., and Voronkov, A., editors, Transactions and Change in Logic Databases, volume 1472 of Lecture Notes in Computer Science, Berlin. Springer-Verlag.  Angluin, D. (1980). Inductive inference of formal languages from positive data. Information and Control, 45:117--135.  Angluin, D., Frazier, M., and Pitt, L. (1992). Learning conjunctions of horn clauses.<F11
DB
goguen99social
Social and Semiotic Analyses for Theorem Prover User Interface Design : We describe an approach to user interface design based on ideas from cognitive science, social science, especially the theory of stories, and a new area tentatively called algebraic semiotics. Social analysis helps to identify coherent classes of users and their requirements, and suggests some ways to make proofs more understandable, while algebraic semiotics, which combines semiotics with algebraic specification, provides a rigorous theory of interface functionality and quality. We apply these techniques to designing user interfaces for a distributed cooperative theorem proving system, whose main component is a website generation and proof assistance tool called Kumo. This interface integrates formal proving, proof browsing, animation, informal explanation, and online background tutorials. Experience with using the interface is reported, and some conclusions are drawn. 1 Introduction  Recent large advances in performance have made it arguable that the most pressing open problems in ...
HCI
rosenstein99continuous
Continuous Categories For a Mobile Robot Autonomous agents make frequent use of knowledge in the form of categories --- categories of objects, human gestures, web pages, and so on. This paper describes a way for agents to learn such categories for themselves through interaction with the environment. In particular, the learning algorithm transforms raw sensor readings into clusters of time series that have predictive value to the agent. We address several issues related to the use of an uninterpreted sensory apparatus and show specific examples where a Pioneer 1 mobile robot interacts with objects in a cluttered laboratory setting.  Introduction  "There is nothing more basic than categorization to our thought, perception, action, and speech" (Lakoff 1987). For autonomous agents, categories often appear as abstractions of raw sensor readings that provide a means for recognizing circumstances and predicting effects of actions. For example, such categories play an important role for a mobile robot that navigates around obstacles ...
ML
7785
Fuzzy Finite-state Automata Can Be Deterministically Encoded into Recurrent Neural Networks There has been an increased interest in combining fuzzy systems with neural networks because fuzzy neural systems merge the advantages of both paradigms. On the one hand, parameters in fuzzy systems have clear physical meanings and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models. However, most of the proposed combined architectures are only able to process static input-output relationships, i.e. they are not able to process temporal input sequences of arbitrary length. Fuzzy finite-state automata (FFAs) can model dynamical processes whose current state depends on the current input and previous states. Unlike in the case of deterministic finite-state automata (DFAs), FFAs are not in one particular state, rather each state is occupied to some degree defined by a membership function. Based on previous work on encoding DFAs in discrete-tim...
ML
202510
Embodied Evolution: A Response to Challenges in Evolutionary Robotics We introduce Embodied Evolution (EE), a new methodology for conducting evolutionary robotics (ER). Embodied evolution uses a population of physical robots that evolve by reproducing with one another in the task environment. EE addresses several issues identified by researchers in the evolutionary robotics community as problematic for the development of ER. We review results from our first experiments and discuss the advantages and limitations of the EE methodology.
ML
532672
Efficient Learning of Reactive Robot Behaviors with a Neural-Q Learning Approach The purpose of this paper is to propose a Neural-Q_learning approach designed for online learning of simple and reactive robot behaviors. In this approach, the Q_function is generalized by a multi-layer neural network allowing the use of continuous states and actions. The algorithm uses a database of the most recent learning samples to accelerate and guarantee the convergence. Each Neural-Q_learning function represents an independent, reactive and adaptive behavior which maps sensorial states to robot control actions. A group of these behaviors constitutes a reactive control scheme designed to fulfill simple missions. The paper centers on the description of the Neural-Q_learning based behaviors showing their performance with an autonomous underwater vehicle (AUV) in a target following mission. Simulated experiments demonstrate the convergence and stability of the learning system, pointing out its suitability for online robot learning. Advantages and limitations are discussed.
ML
16892
Routing Through Networks with Hierarchical Topology Aggregation Abstract In the future, global networks will consist of a hierarchy of subnetworks called domains. For reasons of both scalability and security, domains will not reveal details of their internal structure to outside nodes. Instead, these domains will advertise only a summary, or aggregated view, of their internal structure, e.g., as proposed by the ATM PNNI standard. This work compares, by simulation, the performance of several different aggregation schemes in terms of network throughput (the fraction of attempted connections that are realized), and network control load (the average number of crankbacks per realized connection.) Our main results are: ffl Minimum spanning tree is a good aggregation scheme; ffl Exponential link cost functions perform better than min-hop routing; ffl Our suggested logarithmic update scheme that determine when re-aggregation should be computed can significantly reduce the computational overhead due to re-aggregation with a negligible decrease in performance. 1.
DB
lam01overview
The Overview of Web Search Engines The World Wide Web allows people to share information globally. The amount of information grows without bound. In order to extract information that we are interested in, we need a tool to search the Web. The tool is called a search engine. This survey covers different components of the search engine and how the search engine really works. It provides a background understanding of information retrieval. It discusses different search engines that are commercially available. It investigates how the search engines find information in the Web and how they rank its pages to the give n query. Also, the paper provides guidelines for users on how to use search engines.
IR
529980
Agent Oriented Specification for Patient-Scheduling Systems in Hospitals Introduction  Patient-scheduling in hospitals is a complex task which requires new computational methods, e.g. market mechanisms and enhanced support by software agents. These demands are addressed by the MedPAge-Project (Medical Path Agents) which covers the development of a multiagent -system for which an agent oriented specification will be presented.  Firstly, based on field studies in five German hospitals, the hospital domain is analysed (c.f. [9]). In this domain analysis, a generic hospital structure is derived and the relevant co-ordination objects for patient-scheduling are identified. Secondly, hospital specific scheduling problems are discussed.  On the foundation of this domain analysis, the architecture of the MedPAge multi-agent-system is developed, taking actual agent-oriented methodologies into account. The agents, consisting of an individual schedule and utility function, are modeled and the co-ordination mechanism, determining  the agent interactions, is described. F
Agents
472493
Search History for User Support in Information-Seeking Interfaces The research overview described focuses on the design of search history displays to support information seeking (IS). It examines users' IS activities, current and potential use of histories, and building on this theoretical framework, assesses prototype interfaces that integrate these histories into search systems. Preliminary results described indicate search history use in coordinated work, mental model building, and end user IS strategies. Searchers create and use external records of their actions and the corresponding results by writing/typing notes, using copy and paste functions, and making printouts. Recording user actions and results in computerized systems automates this process, and enables the creation of search history displays that support users in their IS. Existing systems provide search history capabilities, however these often do not offer enough flexibility for users. Legal information has been selected as the domain for the research.  Keywords  History, Information-...
HCI
pinheirodasilva00umli
UMLi: The unified modeling language for interactive applications User interfaces (UIs) are essential components of most software systems, and significantly affect the effectiveness of installed applications. In addition, UIs often represent a significant proportion of the code delivered by a development activity. However, despite this, there are no modelling languages and tools that support contract elaboration between UI developers and application developers. The Unified Modeling Language (UML) has been widely accepted by application developers, but not so much by UI designers. For this reason, this paper introduces the notation of the Unified Modelling Language for Interactive Applications (UMLi), that extends UML, to provide greater support for UI design. UI elements elicited in use cases and their scenarios can be used during the design of activities and UI presentations. A diagram notation for modelling user interface presentations is introduced. Activity diagram notation is extended to describe collaboration between interaction and domain objects. Further, a case study using UMLi notation and method is presented.  
HCI
bailey02eventconditionaction
An Event-Condition-Action Language for XML XML repositories are now a widespread means for storing and exchanging information on the Web. As these repositories become increasingly used in dynamic applications such as e-commerce, there is a rapidly growing need for a mechanism to incorporate reactive functionality in an XML setting. Event-condition-action (ECA) rules are a technology from active databases and are a natural method for supporting such functionality. ECA rules can be used for activities such as automatically enforcing document constraints, maintaining repository statistics, and facilitating publish/subscribe applications. An important question associated with the use of a ECA rules is how to statically predict their run-time behaviour. In this paper, we de ne a language for ECA rules on XML repositories. We theninvestigate methods for analysing the behaviour of a set of ECA rules, ataskwhich has added complexity in this XML setting compared with conventional active databases. Keywords: Event-condition-action rules, XML, XML repositories, reactive functionality, rule analysis. 1
DB
orr00smart
The Smart Floor: A Mechanism for Natural User Identification and Tracking We have created a system for identifying people based on their footstep force profiles and have tested its accuracy against a large pool of footstep data. This floor system may be used to identify users transparently in their everyday living and working environments. We have created user footstep models based on footstep profile features and have been able to achieve a recognition rate of 93%. We have also shown that the effect of footwear is negligible on recognition accuracy.  Keywords  Interaction technology, ubiquitous computing, user identification, biometrics, novel input.  INTRODUCTION  In the Smart Floor project, we have created and validated a system for biometric user identification based on footstep profiles. We have outfitted a floor tile with force measuring sensors and are using the data gathered as users walk over the tile to identify them. We rely on the uniqueness of footstep profiles within a small group of people to provide recognition accuracy similar to other biome...
HCI
meretakis00scalable
Scalable Association-based Text Classification Nave Bayes (NB) classifier has long been considered a core methodology in text classification mainly due to its simplicity and computational efficiency. There is an increasing need however for methods that can achieve higher classification accuracy while maintaining the ability to process large document collections. In this paper we examine text categorization methods from a perspective that considers the tradeoff between accuracy and scalability to large data sets and large feature sizes. We start from the observation that Support Vector Machines, one of the best text categorization methods cannot scale up to handle the large document collections involved in many real word problems. We then consider bayesian extensions to NB that achieve higher accuracy by relaxing its strong independence assumptions. Our experimental results show that LB, an association-based lazy classifier can achieve a good tradeoff between high classification accuracy and scalability to large document collections...
IR
parunak01erims
ERIM's Approach to Fine-Grained Agents Traditional software agents, an extension of Artificial Intelligence,  seek human-level intelligence in each agent. For over 15 years, inspired by Artificial  Life, ERIM has been devising architectures in which useful intelligence  emerges at the system level from interactions of fine-grained agents. We have  applied such architectures to a wide variety of domains, including business, industrial,  and military. This white paper outlines three major principles that  characterize our approach. For each we discuss what the principle is, why it is  important, and how it works in practical implementations.
Agents
schneider99distributed
Distributed Value Functions Many interesting problems, such as power grids, network switches, and traffic flow, that are candidates for solving with reinforcement learning (RL), also have properties that make distributed solutions desirable. We propose an algorithm for distributed reinforcement learning based on distributing the representation of the value function across nodes. Each node in the system only has the ability to sense state locally, choose actions locally, and receive reward locally (the goal of the system is to maximize the sum of the rewards over all nodes and over all time). However each node is allowed to give its neighbors the current estimate of its value function for the states it passes through. We present a value function learning rule, using that information, that allows each node to learn a value function that is an estimate of a weighted sum of future rewards for all the nodes in the network. With this representation, each node can choose actions to improve the performance of the overall...
ML
ljungstrand99webwho
WebWho: Support for Student Awareness and Coordination this paper, WEBWHO, is a lightweight, value-adding service that relies on readily available server status information, which is refined and visualized in a way that is easily accessible to individuals from any workstation with a web browser.
HCI
wang00nonmonotonic
Nonmonotonic Reasoning In LDL++ Deductive database systems have made major advances on efficient  support for nonmonotonic reasoning. A first generation of deductive  database systems supported the notion of stratification for programs  with negation and set aggregates. Stratification is simple to understand  and efficient to implement but it is too restrictive; therefore, a second generation of systems seeks efficient support for more powerful  semantics based on notions such as well-founded models and stable  models. In this respect, a particularly powerful set of constructs is provided by the recently enhanced LDL++ system that supports (i) monotonic user-defined aggregates, (ii) XY-stratified programs, and (iii)  the nondeterministic choice constructs under stable model semantics.  This integrated set of primitives supports a terse formulation and efficient implementation for complex computations, such as greedy algorithms and data mining functions, yielding levels of expressive power unmatched by other deductive...
DB
liu00threedimensional
Three-dimensional PC: toward novel forms of human-computer interaction The ongoing integration of IT systems is offering computer users a wide range of new (networked) services and the access to an explosively growing host of multimedia information. Consequently, apart from the currently established computer tasks, the access and exchange of information is generally considered as the driving computer application of the next decade. Major advances are required in the development of the human-computer interface, in order to enable end-users with the most varied backgrounds to master the added functionalities of their computers with ease, and to make the use of computers in general an enjoyable experience. Research efforts in computer science are concentrated on user interfaces that support the highly evolved human perception and interaction capabilities better than today's 2D graphic user interfaces with a mouse and keyboard. Thanks to the boosting computing power of general purpose computers, highly interactive interfaces are becoming feasible supporting a...
HCI
tzouramanis00overlapping
Overlapping Linear Quadtrees and Spatio-Temporal Query Processing indexing in spatio-temporal databases by using the technique of overlapping is investigated. Overlapping has been previously applied in various access methods to combine consecutive structure instances into a single structure, without storing identical sub-structures. In this way, space is saved without sacrificing time performance. A new access method, overlapping linear quadtrees is introduced. This structure is able to store consecutive historical raster images, a database of evolving images. Moreover, it can be used to support query processing in such a database. Five such spatio-temporal queries along with the respective algorithms that take advantage of the properties of the new structure are introduced. The new access method was implemented and extensive experimental studies for space efficiency and query processing performance were conducted. A number of results of these experiments are presented. As far as space is concerned, these results indicate that, in the case of similar consecutive images, considerable storage is saved in comparison to independent linear quadtrees. In the case of query processing, the results indicate that the proposed algorithmic approaches outperform the respective straightforward algorithms, in most cases. The region data sets used in experiments were real images of meteorological satellite views and synthetic random images with specified aggregation
DB
nichols01using
Using Handhelds as Controls for Everyday Appliances: A Paper Prototype Study Everyday appliances, including telephones, copiers, and home stereos, increasingly contain embedded computers which enable greater functionality. If the interfaces to these appliances were easy to use, people might benefit from these new functions. Unfortunately, it is rare to find a well-designed appliance interface. This study shows that existing appliance interfaces could be improved by using a remote control interface on a handheld computer.  Keywords: Handheld computers, remote control, appliance, Pebbles  INTRODUCTION  The problem with many appliances is that they are too complex. Some appliances need thirty or more buttons to cover all of their functions. This complexity can even make relatively simple tasks, like setting the clock on a VCR, so difficult that people avoid them.  Most appliances do not provide unambiguous feedback to users. Indicators of appliance state can be confusing. For example, on a stereo that combines a CD and tape player, it may be difficult to decide wh...
HCI
goguen00overview
An Overview of the Tatami Project This paper describes the Tatami project at UCSD, which is developing a system to support distributed cooperative software development over the web, and in particular, the validation of concurrent distributed software. The main components of our current prototype are a proof assistant, a generator for documentation websites, a database, an equational proof engine, and a communication protocol to support distributed cooperative work. We believe behavioral specification and verification are important for software development, and for this purpose we use first order hidden logic with equational atoms. The paper also briefly describes some novel user interface design methods that have been developed and applied in the project
HCI
dipasquo98using
Using HTML Formatting to Aid in Natural Language Processing on the World Wide Web Because of its magnitude and the fact that it is not computer understandable, the World Wide Web has become a prime candidate for automatic natural language tasks. This thesis argues that there is information in the layout of a web page, and that by looking at the HTML formatting in addition to the text on a page, one can improve performance in tasks such as learning to classify segments of documents. A rich representation for web pages, the HTML Struct Tree, is described. A parsing algorithm for creating Struct Trees is presented, as well as a set of experiments that use Struct Trees as a feature set for learning to extract a company's name and location from its Web pages. Through these experiments we found that it is useful to consider the layout of a page for these tasks.  Contents 1. Acknowledgements 2 2. Motivations 3 3. Related Work 3 4. HTML Struct Trees 4  4.1. The Representation : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 4.2. The Parsing Algorithm : : :...
IR
sma98theory
Theory of Neuromata A finite automaton --- the so-called neuromaton, realized by a finite discrete recurrent neural network, working in parallel computation mode, is considered. Both the size of neuromata (i.e., the number of neurons) and their descriptional complexity (i.e., the number of bits in the neuromaton representation) are studied. It is proved that a constant time delay of the neuromaton output does not play a role within a polynomial descriptional complexity. It is shown that any regular language given by a regular expression of length n  is recognized by a neuromaton with \Theta(n) neurons. Further, it is proved that this network size is, in the worst case, optimal. On the other hand, generally there is not an equivalent polynomial length regular expression for a given neuromaton. Then, two specialized constructions of neural acceptors of the optimal descriptional complexity \Theta(n) for a single n--bit string recognition are described. They both require O(n  1 2 ) neurons and either O(n) con...
ML
528932
Inverted files and dynamic signature files for optimisation of Web Directories Web directories are taxonomies for the classification of Web documents. This kind of IR systems present a specific type of search where the document collection is restricted to one area of the category graph. This paper introduces a specific data architecture for Web directories which improves the performance of restricted searches. That architecture is based on a hybrid data structure composed of an inverted file with multiple embedded signature files. Two variants based on the proposed model are presented: hybrid architecture with total information and hybrid architecture with partial information. The validity of this architecture has been analysed by means of developing both variants to be compared with a basic model. The performance of the restricted queries was clearly improved, specially the hybrid model with partial information, which yielded a positive response under any load of the search system.
IR
wooldridge00computationally
Computationally Grounded Theories of Agency In this paper, I motivate, define, and illustrate the notion of computationally grounded theories of agency. A theory of agency is said to be computationally grounded if we can give the theory an interpretation in terms of some concrete computational model. This requirement is essential if we are to claim that the theories we develop can be understood as expressing properties of real multiagent systems. After introducing and formally defining the concept of a computationally grounded theory of agency, I illustrate the idea with reference to VSK logic, a formalism for reasoning about agent systems that has a semantics defined with respect to an automata-like model of agents. VSK logic is an extension of modal epistemic logic, which allows us to represent what information is visible to an agent, what it sees, and what it knows. We are able to prove that formulae of VSK logic correspond directly to properties of agents.  1 Introduction  Artificial Intelligence (AI) is a broad church, whic...
Agents
251444
Probabilistic Object Bases There are many applications where an object oriented data model is a good way of representing  and querying data. However, current object database systems are unable to handle the  case of objects whose attributes are uncertain. In this paper, extending previous pioneering work  by Kornatzky and Shimony, we develop an extension of the relational algebra to the case of object  bases with uncertainty. We propose concepts of consistency for such object bases, together  with an NP-completeness result, and classes of probabilistic object bases for which consistency  is polynomially checkable. In addition, as certain operations involve conjunctions and disjunctions  of events, and as the probability of conjunctive and disjunctive events depends both on the  probabilities of the primitive events involved as well as on what is known (if anything) about the  relationship between the events, we show how all our algebraic operations may be performed  under arbitrary probabilistic conjunction and ...
DB
jagadish99querying
Querying Network Directories Hierarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal pro les, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way far superior to what conventional relational or object-oriented databases o er. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated \queries &quot; involve navigational access. In this paper, we develop the core of a formal data model for network directories, and propose a sequence of e ciently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the e ciency of each algorithm in terms of its I/O complexity. Our data model and query languages share the exibility and utility of the recent proposals for semi-structured data models, while at the same time e ectively addressing the speci c needs of network directory applications, which we demonstrate by means of a representative real-life example. This work was done when the authors were at AT&T Labs{
DB
539420
Learning Reactive Robot Behaviors with a Neural-Q Learning Approach The purpose of this paper is to propose a Neural-Q_learning approach designed for online learning of simple and reactive robot behaviors. In this approach, the Q_function is generalized by a multi-layer neural network allowing the use of continuous states and actions. The algorithm uses a database of the most recent learning samples to accelerate and guarantee the convergence. Each Neural-Q_learning function represents an independent, reactive and adaptive behavior which maps sensorial states to robot control actions. A group of these behaviors constitutes a reactive control scheme designed to fulfill simple missions. The paper centers on the description of the Neural-Q_learning based behaviors showing their performance with an autonomous underwater vehicle (AUV) in a target following mission. Simulated experiments demonstrate the convergence and stability of the learning system, pointing out its suitability for online robot learning. Advantages and limitations are discussed.
ML
pollack99adjustable
Adjustable Autonomy for a Plan Management Agent The Plan Management Agent (PMA) is an intelligent software system that is intended to aid a user in managing a potentially large and complex set of plans. Currently under development, PMA applies AI technology for modeling and reasoning about plans and processes to the development of automated support for work activities. We have developed and implemented algorithms for reasoning about richly expressive plans, which include explicit temporal constraints, temporal uncertainty, and observation actions and conditional branches. We have also developed and implemented an approach to computing the cost of a new plan in the context of existing commitments. The current version of PMA has a low level of autonomy: it makes suggestions to its user, but it does not directly act on her behalf. In this paper, we first describe the PMA system, and then briefly raise some design questions we will need to address as we increase the level of PMA's autonomy, and have it vary with the situation.  Introduc...
AI
sullivan98tribeca
Tribeca: A System for Managing Large Databases of Network Traffic The engineers who analyze tra c on high bandwidth networks must lter and aggregate either recorded traces of network packets or live tra c from the network itself. These engineers perform operations similar to database queries, but cannot use conventional data managers because of performance concerns and a semantic mismatch between the analysis operations and the operations supported by commercial DBMSs. Tra c analysis does not require fast random access, transactional update, or relational joins. Rather, it needs fast sequential access to a stream of tra c records and the ability to lter, aggregate, de ne windows, demultiplex, and remultiplex the stream. Tribeca is an extensible, stream-oriented DBMS designed to support network tra c analysis. It combines ideas from temporal and sequence databases with an implementation optimized for databases stored on high speed ID-1 tapes or arriving in real time from the network. The paper describes Tribeca's query language, executor and optimizer as well as performance measurements of a prototype implementation. 1
DB
squire99improving
Improving Response Time by Search Pruning in a Content-Based Image Retrieval System, Using Inverted File Techniques This paper describes several methods for improving query evaluation speed in a content-based image retrieval system (CBIRS). Response time is an extremely important factor in determining the usefulness of any interactive system, as has been demonstrated by human factors studies over the past thirty years. In particular, response times of less than one second are often specified as a usability requirement. It is shown that the use of inverted files facilitates the reduction of query evaluation time without significantly reducing the accuracy of the response. The performance of the system is evaluated using precision vs. recall graphs, which are an established evaluation method in information retrieval (IR), and are beginning to be used by CBIR researchers.  KEYWORDS: content-based image retrieval, search pruning, inverted file, response time 1 Introduction  Response times in the interaction between computer systems and human users are of great importance to user satisfaction. At present...
IR
104129
Improving Retrieval on Imperfect Speech Transcriptions This paper presents the results from adding several forms of query expansion to our retrieval system running on transcriptions of broadcast news from the 1997 TREC-7 spoken document retrieval track. 1  Introduction  Retrieving documents which originated as speech is complicated by the presence of errors in the transcriptions. If some method of increasing retrieval performance despite these errors could be found, then even low-accuracy recognisers could be used as part of a successful spoken document retrieval (SDR) system. This paper presents results using four query expansion techniques described in [3] on 8 different sets of transcriptions generated for the 1997 TREC-7 SDR evaluation. The baseline retrieval system and the techniques used for query expansion are described in section 2, the transcriptions on which the experiments were performed in section 3 and results and further discussion are offered in section 4. 2  Retrieval Systems  2.1  Baseline System (BL)  Our baseline system ...
IR
125291
User-Centered Design and Evaluation of a Real-Time Battlefield Visualization Virtual Environment The ever-increasing power of computers and hardware rendering systems has, to date, primarily motivated the creation of visually rich and perceptually realistic virtual environment (VE) applications. Comparatively very little effort has been expended on the user interaction components of VEs. As a result, VE user interfaces are often poorly designed and are rarely evaluated with users. Although usability engineering is a newly emerging facet of VE development, user-centered design and usability evaluation in VEs as a practice still lags far behind what is needed. This paper presents a structured, iterative approach for the user-centered design and evaluation of VE user interaction. This approach consists of the iterative use of expert heuristic evaluation, followed by formative usability evaluation, followed by summative evaluation. We describe our application of this approach to a real-world VE for battlefield visualization, describe the resulting series of design iterations, and present evidence that this approach provides a cost-effective strategy for assessing and iteratively improving user interaction design in VEs. This paper is among the first to report applying an iterative, structured, user-centered design and evaluation approach to VE user interaction design.
HCI
windhouwer01flexible
Flexible and Scalable Digital Library Search In this report the development of a specialised search engine for a digital library is described. The proposed  system architecture consists of three levels: the conceptual, the logical and the physical level. The conceptual  level schema enables by its exposure of a domain specific schema semantically rich conceptual search. The  logical level provides a description language to achieve a high degree of flexibility for multimedia retrieval. The  physical level takes care of scalable and e#cient persistent data storage. The role, played by each level, changes  during the various stages of a search engine's lifecycle: (1) modeling the index, (2) populating and maintaining  the index and (3) querying the index. The integration of all this functionality allows the combination of both  conceptual and content-based querying in the query stage. A search engine for the Australian Open tennis  tournament website is used as a running example, which shows the power of the complete architecture and its  various components.
IR
schimkat01maintaining
On Maintaining Code Mobility We introduce the aspect of maintenance to code mobility with its major problem of keeping track of code migrating through computer networks. Our approach introduces the concept of mobile, lightweight knowledge repositories to support the maintenance of applications deploying mobile code in highly distributed computing environments. Our proposed system establishes a virtual global database with information about the mobile application and its surrounding environment based on distributed structured XML-based knowledge repositories. These distributed databases provide well-defined structural query and retrieval capabilities and ensure the abstraction from programming languages and proprietary hardware platforms. Finally, we point out several future research issues in the field of maintaining mobile code with respect to automatic maintenance of applications and automatic quality checking among others.
Agents
cohen98joins
Joins that Generalize: Text Classification Using WHIRL WHIRL is an extension of relational databases that can perform "soft joins" based on the similarity of textual identifiers; these soft joins extend the traditional operation of joining tables based on the equivalence of atomic values. This paper evaluates WHIRL on a number of inductive classification tasks using data from the World Wide Web. We show that although WHIRL is designed for more general similaritybased reasoning tasks, it is competitive with mature inductive classification systems on these classification tasks. In particular, WHIRL generally achieves lower generalization error than C4.5, RIPPER, and several nearest-neighbor methods. WHIRL is also fast---up to 500 times faster than C4.5 on some benchmark problems. We also show that WHIRL can be efficiently used to select from a large pool of unlabeled items those that can be classified correctly with high confidence. Introduction  Consider the problem of exploratory analysis of data obtained from the Internet. Assuming that o...
DB
mohan01efficient
An Efficient Method for Performing Record Deletions and Updates Using Index Scans We present a method for efficiently performing deletions and updates of records when the records to be deleted or updated are chosen by a range scan on an index. The traditional method involves numerous unnecessary lock calls and traversals of the index from root to leaves, especially when the qualifying records' keys span more than one leaf page of the index. Customers have suffered performance losses from these inefficiencies and have complained about them. Our goal was to minimize the number of interactions with the lock manager, and the number of page fixes, comparison operations and, possibly, I/Os. Some of our improvements come from increased synergy between the query planning and data manager components of a DBMS. Our patented method has been implemented in DB2 V7 to address specific customer requirements. It has also been done to improve performance on the TPC-H benchmark. 1.
DB
basili00language
Language Sensitive Text Classification It is a traditional belief that in order to scale-up to more effective retrieval and access methods modern Information Retrieval has to consider more the text content. The modalities and techniques to fit this objectives are still under discussion. More empirical evidence is required to determine the suitable linguistic levels for modeling each IR subtask (e.g. information zoning, parsing, feature selection for indexing,...) and the corresponding use of this information. In this paper an original classification model sensitive to document syntactic information and characterized by a novel inference method is described. Extensive experimental evidence has been derived on real test data and also from well-established academic test sets. The results show that a significant improvement can be derived using the proposed inference model. Also the role of linguistic preprocessing seems to provide positive effects on the performance. POS tagging and recognition of Proper Nouns received a specific experimental attention and provided significant effects on measured accuracy. 1.
IR
goldberg99coordinating
Coordinating Mobile Robot Group Behavior Using a Model of Interaction Dynamics In this paper we show how various levels of coordinated behavior may be achieved in a group of mobile robots by using a model of the interaction dynamics between a robot and its environment. We present augmented Markov models  (AMMs) as a tool for capturing such interaction dynamics on-line and in real-time, with little computational and storage overhead. We begin by describing the structure of AMMs and the algorithm for generating them, then verify the approach utilizing data from physical mobile robots performing elements of a foraging task. Finally, we demonstrate the application of the model for resolving group coordination issues arising from three sources: individual performance, group affiliation, and group performance. Corresponding respectively to these are the three experimental examples we present --- fault detection, group membership based on ability and experience, and dynamic leader selection.  1 Introduction  Learning models of the environment, other robots, and interact...
AI
valencia98hitch
Hitch hiker's Guide to ESQIMO - Computational model for analogy solving in IQ-tests ESQIMO is a computational model for analogy solving based on a topological formalism of representation. The source and the target analogs are represented as  simplexes and the analogy solving is modeled as a topological deformation of these simplexes along a polygonal chain and according to some constraints. We apply this framework to the resolution of IQ-tests typically presented as igiven A, B and C, ønd D such that A is to B what C is to Dj. Keywords : Knowledge Representation, Diagrammatic Reasoning, Simplicial complexes, Analogy solving, CAT.  4  Introduction ESQIMO is a system that automatically solves the following type of IQ-tests:  Given three øgures A, B and C, ønd a øgure D such that D is to C what B is to  A. These IQ-tests are a paradigmatic example of analogy solving problems. ESQIMO is thus a computational model for analogy solving. The framework used by ESQIMO to solve analogies is called CAT for Combinatorial Algebraic Topology. More precisely, ESQIMO models knowledg...
AI
bergamaschi99intelligent
Intelligent Techniques for the Extraction and Integration of Heterogeneous Information Developing intelligent tools for the integration of information extracted from multiple heterogeneous sources is a challenging issue to effectively exploit the numerous sources available on-line in global information systems. In this paper, we propose intelligent, tool-supported techniques to information extraction and integration which take into account both structured and semistructured data sources. An object-oriented language called ODL I  3 , derived from the standard ODMG, with an underlying Description Logics, is introduced for information extraction. ODL I 3 descriptions of the information sources are exploited first to set a shared vocabulary for the sources. Information integration is performed in a semiautomatic way, by exploiting ODL I 3 descriptions of source schemas with a combination of Description Logics and clustering techniques. Techniques described in the paper have been implemented in the MOMIS system, based on a conventional mediator architecture. Keywords - Hetero...
DB
isokoski01model
Model for Unistroke Writing Time Unistrokes are a viable form of text input in pen-based user interfaces. However, they are a very heterogeneous group of gestures the only common feature being that all are drawn with a single stroke. Several unistroke alphabets have been proposed including the original Unistrokes, Graffiti, Allegro, T-Cube and MDITIM. Comparing these methods usually requires a lengthy study with many writers and even then the results are biased by the earlier handwriting experience that the writers have. Therefore, a simple descriptive model can make these comparisons easier. In this paper we propose a model for predicting the writing time for an expert user on any given unistroke alphabet thus enabling sounder argumentation on the properties of different writing methods.  Keywords  Modeling of motor performance, handwriting, pen input  INTRODUCTION  Unistrokes were introduced as a text input method for penbased user interfaces by Goldberg and Richardson in their 1993 paper [8]. Unistrokes are an alte...
HCI
nrvag98optimizing
Optimizing OID Indexing Cost in Temporal Object-Oriented Database Systems In object-oriented database systems (OODB) with logical OIDs, an OID index (OIDX) is needed to map from OID to the physical location of the object. In a transaction time temporal OODB, the OIDX should also index the object versions. In this case, the index entries, which we call object descriptors (OD), also include the commit timestamp of the transaction that created the object version. In this report, we develop an analytical model for OIDX access costs in temporal OODBs. The model includes the index page buffer as well as an OD cache. We use this model to study access cost and optimal use of memory for index page buffer and OD cache, with different access patterns. The results show that 1) the OIDX access cost can be high, and can easy become a bottleneck in large temporal OODBs, 2) the optimal OD cache size can be relatively large, and 3) the gain from using an optimal size is considerable, and because access pattern in a database system can be very dynamic, the system should be ab...
DB
279508
Towards a Highly-Scalable Metasearch Engine The World Wide Web has been expanding in a very fast rate. The coverage of the Web by each of the major search engines has been steadily decreasing despite their effort to index more web pages. Worse yet, as these search engines get larger, higher percentages of their indexed information are becoming obsolete. More and more people are having doubt about the scalability of centralized search engine technology. A more scalable alternative to search the Web is the metasearch engine approach. A metasearch engine can be considered as an interface on top of multiple local search engines to provide uniform access to many local search engines. Database selection is one of the main challenges in building a large-scale metasearch engine. The problem is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query. In order to enable accurate selection, metadata that reflect the content of each search engine need to be co...
IR
cassell99requirements
Requirements for an Architecture for Embodied Conversational Characters In this paper we describe the computational and architectural requirements for systems which support real-time multimodal interaction with an embodied conversational character. We argue that the three primary design drivers are real-time multithreaded entrainment, processing of both interactional and propositional information, and an approach based on a functional understanding of human face-toface conversation. We then present an architecture which meets these requirements and an initial conversational character that we have developed who is capable of increasingly sophisticated multimodal input and output in a limited application domain.  1 Introduction  Research in computational linguistics, multimodal interfaces, computer graphics, and autonomous agents has led to the development of increasingly sophisticated autonomous or semi-autonomous virtual humans over the last five years. Autonomous self-animating characters of this sort are important for use in production animation, interfa...
Agents
44892
Using Probabilistic Knowledge and Simulation to Play Poker Until recently, artificial intelligence researchers who use games as their experimental testbed have concentrated on games of perfect information. Many of these games have been amenable to brute-force search techniques. In contrast, games of imperfect information, such as bridge and poker, contain hidden information making similar search techniques impractical. This paper describes recent progress in developing a high-performance pokerplaying program. The advances come in two forms. First, we introduce a new betting strategy that returns a probabilistic betting decision, a probability triple, that gives the likelihood of a fold, call or raise occurring in a given situation. This component unifies all the expert knowledge used in the program, does a better job of representing the type of decision making needed to play strong poker, and improves the way information is propagated throughout the program. Second, real-time simulations are used to compute the expected values of betting decisions. The program generates an instance of the missing data, subject to any constraints that have been learned, and then simulates the rest of the game to determine a numerical result. By repeating this a sufficient number of times, a statistically meaningful sample is used in the program’s decision–making process. Experimental results show that these enhancements each represent major advances in the strength of computer poker programs. 1.
AI
266823
On the Scalability of Simple Genetic Algorithms Scalable evolutionary computation has become an intensively studied research topic in recent years. The issue of scalability is predominant in any eld of algorithmic design, but it became particularly relevant for the design of competent genetic algorithms once the scalability problems of simple genetic algorithms were understood. Here we present some of the work that has aided in getting a clear insight in the scalability problems of simple genetic algorithms. Particularly, we discuss the important issue of building block mixing and show how the need for mixing places a boundary in the GA parameter space that together with the boundary from the schema theorem delimits the region where the GA converges reliably to the optimum of problems of bounded diculty. This region - or sweet spot as it has been called - shrinks unfortunately very rapidly with increasing problem size unless the building blocks are tightly linked in the problem-coding structure. In addition we look how straightforw...
ML
honavar98distributed
Distributed Knowledge Networks Distributed Knowledge Networks (DKN) provide some of the key enabling technologies for translating recent advances in automated data acquisition, digital storage, computers and communications into fundamental advances in organizational decision support, data analysis, and related applications. DKN include computational tools for accessing, organizing, transforming, and analyzing the contents of heterogeneous, distributed data and knowledge sources and for distributed problem solving and decision making under tight time, resource, and performance constraints. This paper presents an overview of the DKN project in the Iowa State University Artificial Intelligence Laboratory.  I. Introduction  Advanced scientific research (e.g., the genome project), military applications (e.g., intelligence data handling, situation assessment, command and control) , law enforcement (e.g., terrorism prevention), crisis management, design and manufacturing systems, and medical information infrastructure, pow...
DB
vonhardenberg01barehand
Bare-Hand Human-Computer Interaction In this paper, we describe techniques for barehanded interaction between human and computer. Barehanded means that no device and no wires are attached to the user, who controls the computer directly with the movements of his/her hand. Our approach is centered on the needs of the user. We therefore define requirements for real-time barehanded interaction, derived from application scenarios and usability considerations. Based on those requirements a finger-finding and hand-posture recognition algorithm is developed and evaluated. To demonstrate the strength of the algorithm, we build three sample applications. Finger tracking and hand posture recognition are used to paint virtually onto the wall, to control a presentation with hand postures, and to move virtual items on the wall during a brainstorming session. We conclude the paper with user tests, which were conducted to prove the usability of bare-hand human computer interaction. Categories and Subject Descriptors H.5.2 [Information interfaces and presentation]: User interfaces -Input devices and strategies; I.5.5 [Pattern recognition]: Implementation - Interactive systems. General Terms Algorithms, Design, Experimentation, Human Factors Keywords Computer Vision, Human-computer Interaction, Real-time, Finger Tracking, Hand-posture Recognition, Bare-hand Control. 1.
HCI
brown98utility
Utility Theory-Based User Models for Intelligent Interface Agents . An underlying problem of current interface agent research is the failure to adequately address effective and efficient knowledge representations and associated methodologies suitable for modeling the users' interactions with the system. These user models lack the representational complexity to manage the uncertainty and dynamics involved in predicting user intent and modeling user behavior. A utility theory-based approach is presented for effective user intent prediction by incorporating the ability to explicitly model users' goals, the uncertainty in the users' intent in pursuing these goals, and the dynamics of users' behavior. We present an interface agent architecture, CIaA, that incorporates our approach and discuss the integration of CIaA with three disparate domains --- a probabilistic expert system shell, a natural language input database query system, and a virtual space plane ---that are being used as test beds for our interface agent research. Keywords: cognitive modeling,...
HCI
fuhr98hyspirit
HySpirit - a Probabilistic Inference Engine for Hypermedia Retrieval in Large Databases . HySpirit is a retrieval engine for hypermedia retrieval integrating concepts from information retrieval (IR) and deductive databases. The logical view on IR models retrieval as uncertain inference, for which we use probabilistic reasoning. Since the expressiveness of classical IR models is not sufficient for hypermedia retrieval, HySpirit is based on a probabilistic version of Datalog. In hypermedia retrieval, different nodes may contain contradictory information; thus, we introduce probabilistic four-valued Datalog. In order to support fact queries as well as contentbased retrieval, HySpirit is based on an open world assumption, but allows for predicate-specific closed world assumptions. For performing efficient retrieval on large databases, our system provides access to external data. We demonstrate the application of HySpirit by giving examples for retrieval on images, structured documents and large databases. 1 Introduction  Due to the advances in hardware, processing of multimed...
IR
moreno97dynamic
Dynamic Belief Analysis The process of rational inquiry can be defined as the evolution of the beliefs of a rational agent as a consequence of its internal inference procedures and its interaction with the environment. These beliefs can be modelled in a formal way using belief logics. The possible worlds model and its associated Kripke semantics provide an intuitive semantics for these logics, but they commit us to model agents that are  logically omniscient and perfect reasoners. These problems can be avoided with a syntactic view of possible worlds, defining them as arbitrary sets of sentences in a propositional belief logic. In this article this syntactic view of possible worlds is taken, and a dynamic analysis of the beliefs of the agent is suggested in order to model the process of rational inquiry in which the agent is permanently engaged. 1 INTRODUCTION  The aim of this work  1  is to model the process of rational inquiry, i.e. the (rationally controlled) transformation of the beliefs of an intelligent...
Agents
menzies00issues
Issues with Meta-Knowledge This article approaches these questions from our di#erent viewpoints  a  :  The Constructors: Motta and Kalfoglou focus on the construction of knowledge base systems (KBS) using two special kinds of meta-knowledge: ontologies [14,33] and problem solving methods (PSMs) [26,6]. PSMs model the useful inference patterns seen in previous applications. Such patterns, it is argued, simplify and clarify future implementations. Ontologies model common domain terminology. This terminology might include the data structures required by a PSM. Using a good ontology, it is argued, can guide developers in the construction of new systems. For more information on ontologies, see http://www.dai.ed.ac.uk/daidb/people/homes/yannisk/seke99panelhtml.html.  The Maintainers: Altho# and Menzies focus on the maintenance and modification of KBS using case-based reasoning (CBR) [3,1] and continual testing [10,22]. CBR researchers argue that people 
AI
diao00comparative
A Comparative Study of Classification Based Personal E-mail Filtering . This paper addresses personal E-mail filtering by casting it in the  framework of text classification. Modeled as semi-structured documents, Email  messages consist of a set of fields with predefined semantics and a number  of variable length free-text fields. While most work on classification either  concentrates on structured data or free text, the work in this paper deals with  both of them. To perform classification, a naive Bayesian classifier was  designed and implemented, and a decision tree based classifier was  implemented. The design considerations and implementation issues are  discussed. Using a relatively large amount of real personal E-mail data, a  comprehensive comparative study was conducted using the two classifiers. The  importance of different features is reported. Results of other issues related to  building an effective personal E-mail classifier are presented and discussed. It is  shown that both classifiers can perform filtering with reasonable accuracy.  While the decision tree based classifier outperforms the Bayesian classifier  when features and training size are selected optimally for both, a carefully  designed naive Bayesian classifier is more robust.  1 
IR
leung01towards
Towards Web-Scale Web Archeology Web-scale Web research is difficult. Information on the Web is vast in quantity, unorganized and uncatalogued, and available only over a network with varying reliability. Thus, Web data is difficult to collect, to store, and to manipulate efficiently. Despite these difficulties, we believe performing Web research at Web-scale is important. We have built a suite of tools that allow us to experiment on collections that are an order of magnitude or more larger than are typically cited in the literature. Two key components of our current tool suite are a fast, extensible Web crawler and a highly tuned, in-memory database of connectivity information. A Web page repository that supports easy access to and storage for billions of documents would allow us to study larger data sets and to study how the Web evolves over time.
IR
brown01switch
Switch Packet Arbitration via Queue-Learning In packet switches, packets queue at switch inputs and contend for outputs. The contention arbitration policy directly affects switch performance. The best policy depends on the current state of the switch and current traffic patterns. This problem is hard because the state space, possible transitions, and set of actions all grow exponentially with the size of the switch. We present a reinforcement learning formulation of the problem that decomposes the value function into many small independent value functions and enables an efficient action selection. 1
ML
tews02gday
G'day Mate. Let me Introduce you to Everyone: An Infrastructure for Scalable Human-System Interaction We are exposed to physical and virtual systems every day. They consist of computers, PDAs, wireless devices and increasingly, robots. Each provides services to individual or groups of users whether they are local or remote to the system. Services offered by these systems may be useful beyond these users to others, however connecting many of these systems to more users presents a challenging problem. The primary goal of the research presented in this paper is to demonstrate a scalable approach for connecting multiple users to the services provided by multiple systems. Such an approach must be simple, robust and general to contend with the heterogeneous capabilities of the services. An infrastructure is presented that addresses these scalability requirements and establishes the foundation for contending with heterogeneous services. Additionally, it allows services to be linked to form higher-level abstractions. The infrastructure is demonstrated in simulation on several similar multirobot systems with multiple users. The results propose it as a solution for large-scale human-system interaction.
Agents
nodine99active
Active Information Gathering in InfoSleuth . InfoSleuth  1  is an agent-based system that can be configured to perform many different information management activities in a distributed environment. InfoSleuth agents provide a number of complex query services that require resolving ontology-based queries over dynamically changing, distributed, heterogeneous resources. These include distributed query processing, location-independent single-resource updates, event and information monitoring, statistical or inferential data analysis, and trend discovery in complex event streams. It has been used in numerous applications, including the Environmental Data Exchange Network and the Competitive Intelligence System. 1 Introduction  In the past 15-20 years, numerous products and prototypes have regularly appeared to provide uniform access to heterogeneous data sources. As a result, that access to heterogeneous sources is now taken as a "given" by customers. Current MCC studies indicate that, given the availability of products that achieve...
DB
bergamaschi99semantic
Semantic Integration of Semistructured and Structured Data Sources this paper is to describe the MOMIS [4, 5] (Mediator envirOnment for Multiple Information Sources) approach to the integration and query of multiple, heterogeneous information sources, containing structured and semistructured data. MOMIS has been conceived as a joint collaboration between University of Milano and Modena in the framework of the INTERDATA national research project, aiming at providing methods and tools for data management in Internet-based information systems. Like other integration projects [1, 10, 14], MOMIS follows a "semantic approach" to information integration based on the conceptual schema, or metadata, of the information sources, and on the following architectural elements: i) a common object-oriented data model, defined according to the ODL I 3 language, to describe source schemas for integration purposes. The data model and ODL I 3 have been defined in MOMIS as subset of the ODMG-93 ones, following the proposal for a standard mediator language developed by the I
DB
kargupta01mobimine
MobiMine: Monitoring the Stock Market from a PDA This paper describes an experimental mobile data mining system that allows intelligent monitoring of time-critical financial data from a hand-held PDA. It presents the overall system architecture and the philosophy behind the design. It explores one particular aspect of the system-automated construction of personalized focus area that calls for user's attention. This module works using data mining techniques. The paper describes the data mining component of the system that employs a novel Fourier analysis-based approach to efficiently represent, visualize, and communicate decision trees over limited bandwidth wireless networks. The paper also discusses a quadratic programming-based personalization module that runs on the PDAs and the multi-media based user-interfaces. It reports experimental results using an ad hoc peer-to-peer IEEE 802.11 wireless network.
HCI
christophides00community
Community Webs (C-Webs): Technological Assessment and System Architecture this paper, our presentation mainly relies on examples taken from one of the potential C-Web applications, namely C-Web Portals for cultural communities.
DB
299008
Transformation-Based Bottom-Up Computation of the Well-Founded Model We present a bottom-up algorithm for the computation of the well-founded model of non-disjunctive logic programs. Our method is based on the notion of conditional facts and elementary program transformations studied by Bras and Dix [BD94, BD97, BD98b]. However, their "residual program" can grow to exponential size, whereas for function free programs our "program remainder" is always polynomial in the size of the extensional database (EDB). Our approach is also closely related to the alternating fixpoint procedure [VG89, VG93]. However, the alternating fixpoint procedure is known to have inefficiencies due to redundant recomputations of possible facts. By using conditional facts that can be deleted directly instead of recomputing the complement our approach is guaranteed to need not more work than the alternating fixpoint procedure and to be much more efficient in many cases. The magic set transformation as a method for bottom-up query answering is known to have problems with undefined magic facts in the context of the well-founded semantics. However, our approach treats magic set transformed programs correctly, i.e. it always computes a relevant part of the well-founded model of the original program. We show that our approach is guaranteed to need not more work than the well-founded magic sets method [KSS95] or the magic alternating fixpoint procedure [Mor96] and is much more efficient for many programs. Again the use of conditional facts gives valuable information to select the right magic facts to use and to avoid redundant recomputations.
DB
hellerstein00recognizing
Recognizing End-User Transactions in Performance Management Providing good quality of service (e.g., low response times) in distributed computer systems requires measuring end-user perceptions of performance. Unfortunately, in practice such measures are often expensive or impossible to obtain. Herein, we propose a machine learning approach to recognizing end-user transactions consisting of sequences of remote procedure calls (RPCs) received at a server. Two problems are addressed. The first is labeling previously segmented transaction instances with the correct transaction type. This is akin to work done in document classification. The second problem is segmenting RPC sequences into transaction instances. This is a more difficult problem, but it is similar to segmenting sounds into words as in speech understanding. Using Naive Bayes, we tackle the labeling problem with four combinations of feature vectors and probability distributions: RPC occurrences with the Bernoulli distribution and RPC counts with the multinomial, geometric, and shifted ge...
IR
benitez98contentbased
A Content-Based Image Meta-Search Engine using Relevance Feedback Search engines are the most powerful resources for finding information on the rapidly expanding World-Wide Web. Finding the desired search engines and learning how to use them, however, can be very time consuming. Metasearch engines, which integrate a group of such search tools, enable users to access information across the world in a transparent and more efficient manner. The recent emergence of visual information retrieval (VIR) systems on the Web is leading to the same efficiency problem. This paper describes MetaSEEk, a meta-search engine used for retrieving images based on their visual content on the Web. MetaSEEk is designed to intelligently select and interface with multiple on-line image search engines by ranking their performance for different classes of user queries. User feedback is also integrated in the ranking refinement. MetaSEEk has been developed to explore the issues involved in querying large, distributed, on-line visual information system sources. We compare MetaSEE...
IR
dey01distributed
Distributed and Disappearing User Interfaces in . . . f expression, will often dominate the pure functional aspect to a user. We are already witnessing this transition with mobile phones.  Novel computational artifacts based on these technologies may become `invisible', or reside in the background, in several different ways:  Truly invisible - when the actual computer and its interface is almost totally integrated with an environment that is familiar to the user. The user interaction model is implicit and perhaps even unnoticeable to the user. Such a system truly resides in the background of the user's attention at all times.  Transparent - here the `invisible' UI is not invisible in the literal sense. Rather, it is transparent in the same sense as a very familiar tool is transparent to its user, to the point where it almost acts as an extension of one's body. Every- Distributed and Disappearing User Interfaces in  Ubiquitous Computing  Keywords  context-awareness, disappearing user interfaces
HCI
arampatzis00evaluation
An Evaluation of Linguistically-motivated Indexing Schemes In this article, we describe a number of indexing experiments based on indexing terms other than simple keywords. These experiments were conducted as one step in validating a linguistically-motivated indexing model. The problem is important but not new. What is new in this approach is the variety of schemes evaluated. It is important since it should not only help to overcome the well-known problems of bag-of-words representations, but also the difficulties raised by non-linguistic text simplification techniques such as stemming, stop-word deletion, and term selection. Our approach in the selection of terms is based on part-of-speech tagging and shallow parsing. The indexing schemes evaluated vary from simple keywords to nouns, verbs, adverbs, adjectives, adjacent word-pairs, and head-modifier pairs. Our findings apply to Information Retrieval and most of related areas.
IR
bailey01engineering
Engineering a multi-purpose test collection for Web retrieval experiments Past research into text retrieval methods for the Web has been restricted by the lack of a  test collection capable of supporting experiments which are both realistic and reproducible.  The 1.69 million document WT10g collection is proposed as a multi-purpose testbed for  experiments with these attributes, in distributed IR, hyperlink algorithms and conventional  ad hoc retrieval.  WT10g was constructed by selecting from a superset of documents in such a way that  desirable corpus properties were preserved or optimised. These properties include: a high  degree of inter-server connectivity, integrity of server holdings, inclusion of documents related  to a very wide spread of likely queries, and a realistic distribution of server holding sizes. We  conrm that WT10g contains exploitable link information using a site (homepage) nding  experiment. Our results show that, on this task, Okapi BM25 works better on propagated  link anchor text than on full text.  Keywords: Web retrieval; Link-based ranking; Distributed information retrieval; Test  collections  1 
IR
wetzel98semantic
Semantic Query Optimization through Abduction and Constraint Handling . The use of integrity constraints to perform Semantic Query Optimization (SQO) in deductive databases can be formalized in a way similar to the use of integrity constraints in Abductive Logic Programming (ALP) and the use of Constraint Handling Rules in Constraint Logic Programming (CLP). Based on this observation and on the similar role played by, respectively, extensional, abducible and constraint predicates in SQO, ALP and CLP, we present a unified framework from which (variants of) SQO, ALP and CLP can be obtained as special instances. The framework relies on a proof procedure which combines backward reasoning with logic programming clauses and forward reasoning with integrity constraints. 1 Introduction  Semantic Query Optimization (SQO) in deductive databases uses implicit knowledge coded in Integrity Constraints (ICs) to transform queries into new queries that are easier to evaluate and ideally contain only atoms of extensional predicates. SQO sometimes allows for unsatisfiable...
DB
david01agentbased
Agent-Based Social Simulation with Coalitions in Social Reasoning There is a growing belief that the agents' cognitive structures play a central role on the enhancement of predicative capacities of decision-making strategies. This paper analyses and simulates the construction of cognitive social structures in the process of decision making with multiple actors. In this process it is argued that the agent's rational choices may be assessed by its motivations, according to different patterns of social interactions. We first construct an abstract model of social dependence between agents, and define a set of social structures that are easily identifiable according to potential interactions. We then carry out a set of experiments at micro-social levels of analysis, where the agents' cognitive structures are explicitly represented. These experiments indicate that different social dependence structures imply distinct structural patterns of negotiation proposals, which appear to have diverse patterns of complexity in the search space. It is subsequently shown that this observation emerges as an issue of ambiguity in the regulation of different decision-making criteria, relative to motivation-oriented and utility-oriented choices. In the scope of this ambiguity, we finally make some conjectures relative to further analytical and empirical analysis around the relation between patterns of complexity of social structures and decision-making.
Agents
munetomo99identifying
Identifying Linkage Groups by Nonlinearity/Non-monotonicity Detection This paper presents and discusses direct  linkage identification procedures based  on nonlinearity/non-monotonicity detection.  The algorithm we propose checks arbitrary  nonlinearity/non-monotonicity of fitness  change by perturbations in a pair of  loci to detect their linkage. We first discuss  condition of the linkage identification by  nonlinearity check (LINC) procedure (Munetomo  & Goldberg, 1998) and its allowable  nonlinearity. Then we propose another condition  of the linkage identification by nonmonotonicity  detection (LIMD) and prove its  equality to the LINC with allowable nonlinearity  (LINC-AN). The procedures can identify  linkage groups for problems with at most  order-k difficulty by checking O(2  k  ) strings  and the computational cost for each string is  O(l  2  ) where l is the string length.  1 Introduction  The definition of linkage in genetics is `the tendency for alleles of different genes to be passed together from one generation to the next' (Winter, Hickey...
ML
245254
Continuous Conceptual Set Covering: Learning Robot Operators From Examples Continuous Conceptual Set Covering (CCSC) is  an algorithm that uses engineering knowledge to  learn operator effects from training examples.  The program produces an operator hypothesis  that, even in noisy and nondeterministic  domains, can make good quantitative  predictions. An empirical evaluation in the traytilting  domain shows that CCSC learns faster  than an alternative case-based approach. The  best results, however, come from integrating  CCSC and the case-based approach.  Figure 1. Experimental Set Up  1. INTRODUCTION  Initially, the robot knows how to physically execute the tilt_operator. It does not, however, know the effects of the operator. When the tray-tilt operator is executed, the robot tips the tray down 30 from the horizontal in the direction of tilt. The new position of the puck is hard to predict because of uncertainty in the initial conditions (the initial position of the puck and the tilting angle are continuous values subject to measurement error). In additio...
ML
mackay98augmented
Augmented Reality: Linking real and virtual worlds - A new paradigm for interacting with computers A revolution in computer interface design is changing the way we think about computers. Rather than typing on a keyboard and watching a television monitor, Augmented Reality lets people use familiar, everyday objects in ordinary ways. The difference is that these objects also  provide a link into a computer network. Doctors can examine patients while viewing superimposed medical images; children can program their own LEGO constructions; construction engineers can use ordinary paper engineering drawings to communicate with distant colleagues. Rather than immersing people in an artificiallycreated virtual world, the goal is to augment objects in the physical world by enhancing them with a wealth of digital information and communication capabilities.  KEYWORDS: Augmented Reality, Interactive Paper, Design Space Exploration, Participatory Design  INTRODUCTION  Computers are everywhere: in the past several decades they have transformed our work and our lives. But the conversion from traditi...
AI
298894
Problem solving in ID-logic with aggregates: some experiments The goal of the LP+ project at the K.U.Leuven is  to design an expressive logic, suitable for declarative  knowledge representation, and to develop intelligent  systems based on Logic Programming technology for  solving computational problems using the declarative  specications. The ID-logic is an integration of typed  classical logic and a denition logic. Dierent abductive  solvers for this language are being developed. This  paper is a report of the integration of high order aggregates  into ID-logic and the consequences on the solver  SLDNFA.  
DB
441527
The relations between Technologies for Human Learning and Agents . In this position paper we review the historical emergence of Agents
HCI
paris00common
Common Sense and Maximum Entropy This paper concerns the question of how to draw inferences common sensically from uncertain knowledge. Since the early work of Shore and Johnson, [10], Paris and Vencovsk a, [6], and Csiszár, [1], it has been known that the Maximum Entropy Inference Process is the only inference process which obeys certain common sense principles of uncertain reasoning. In this paper we consider the present status of this result and argue that within the rather narrow context in which we work this complete and consistent mode of uncertain reasoning is actually characterised by the observance of just a single common sense principle (or slogan).
AI
cardie99guest
Guest Editors' Introduction: Machine Learning and Natural Language Introduction:  Machine Learning and Natural Language  CLAIRE CARDIE cardie@cs.cornell.edu Department of Computer Science, Cornell University, Ithaca, NY 14853-7501  RAYMOND J. MOONEY mooney@cs.utexas.edu Department of Computer Sciences, Taylor Hall 2.124, University of Texas, Austin, TX 787121188 The application of machine learning techniques to natural language processing (NLP) has increased dramatically in recent years under the name of "corpus-based," "statistical," or "empirical" methods. However, most of this research has been conducted outside the traditional machine learning research community. This special issue attempts to bridge this divide by assembling an interesting variety of recent research papers on various aspects of natural language learning -- many from authors who do not generally publish in the traditional machine learning literature -- and presenting them to the readers of Machine Learning.  In the last five to ten y
ML
340855
Decomposition of Object-Oriented Database Schemas Based on F-logic, we specify an advanced data model with object-oriented and  logic-oriented features. For this model we study the decomposition of a class, the  counterpart to the well-known decomposition of a relation scheme under functional  dependencies. For this decomposition of a class, the transformation pivoting is  used. Pivoting transplants some attributes of the class to a newly generated class.  This new class is a subclass of the result class of the so-called pivot attribute. The  pivot attribute maintains the link between the original class and the new subclass.  We identify the conditions for the output of pivoting being equivalent with its  input. Additionally, we show under which conditions a schema with functional  dependencies can be recursively transformed into an equivalent one without nonkey  functional dependencies.  1 Introduction  The theory of database schema design aims at formally characterising \good" schemas and at inventing algorithms to measure and to im...
DB
bharat98improved
Improved Algorithms for Topic Distillation in a Hyperlinked Environment Abstract This paper addresses the problem of topic distillation on the World Wide Web, namely, given a typ-ical user query to find quality documents related to the query topic. Connectivity analysis has been shown to be useful in identifying high quality pages within a topic specific graph of hyperlinked documents. The essence of our approach is to augment a previous connectivity anal-ysis based algorithm with content analysis. We identify three problems with the existing approach and devise al-gorithms to tackle them. The results of a user evaluation are reported that show an improvement of precision at 10 documents by at least 45 % over pure connectivity anal-ysis. 1
IR
castelfranchi99deliberate
Deliberate Normative Agents: Principles and Architecture . In this paper norms are assumed to be useful in agent societies. It is claimed that not only following norms, but also the possibility of `intelligent' norm violation can be useful. Principles for agents that are able to behave deliberately on the basis of explicitly represented norms are identified and an architecture is introduced. Using this agent architecture, norms can be communicated, adopted and used as meta-goals on the agent's own processes. As such they have impact on deliberation about goal generation, goal selection, plan generation and plan selection. 1 Introduction Besides autonomy, an important characteristic of agents is that they can react to a changing environment. However, if the protocols that they use to react to (at least some part of) the environment are fixed, they have no ways to respond to impredictable changes. For instance, if an agent notices that another agent is cheating it cannot switch to another protocol to protect itself. (At least this is ...
Agents
miller00training
Training Teams with Collaborative Agents . Training teams is an activity that is expensive, time-consuming,  hazardous in some cases, and can be limited by availability of equipment and  personnel. In team training, the focus is on optimizing interactions, such as  efficiency of communication, conflict resolution and prioritization, group  situation awareness, resource distribution and load balancing, etc. This paper  presents an agent-based approach to designing intelligent team training systems.  We envision a computer-based training system in which teams are trained by  putting them through scenarios, which allow them to practice their team skills.  There are two important roles that intelligent agents can play; these are as  virtual team members and as coach. To carry out these functions, these agents  must be equipped with an understanding of the task domain, the team structure,  the selected decision-making process and their belief about other team  members' mental states.  1 Introduction  An integral element of large c...
Agents
243029
Combining Positional Information with Visual Media By integrating visual media with positioning information obtained with our wearable computer, we create new opportunities for using visuals both in the field and at the workstation. The position information we store with each visual is direction, pitch, roll, location, focal length, and zoom. This information allows any system to reconstruct the frustum of the visual, and, if height data is available to reconstruct which parts of the earth are visible in the visual. This enables position based lookup and 3D mosaicing of visuals to reconstruct a 3D model.  1 Introduction  We are interested in the association of media files with contextual information gathered by our wearable computer. We have previously explored the combination of audio notes, or reminders, with locations [1]. In this paper we turn our attention to visuals.  Classic photographs and videos store only visual information. Recent advances in photo and video technology have increased the amount of information which can be st...
HCI
490051
Learning of Kick in Artificial Soccer Soccer simulation is a suitable domain for research in artificial intelligence. This paper describes a new ball kicking skill, that uses case based learning. In many situations a single kick command is not sufficient to reach the desired ball movement. Hence a skill is needed, that finds a suitable sequence of kicks. The new kicking skill was developed for the AT Humboldt artificial soccer team.
ML
32521
A Web-based Information System that Reasons with Structured Collections of Text The degree to which information sources are pre-processed by Web-based information systems varies greatly. In search engines like Altavista, little pre-processing is done, while in "knowledge integration" systems, complex site-specific "wrappers" are used integrate different information sources into a common database representation. In this paper we describe an intermediate between these two models. In our system, information sources are converted into a highly structured collection of small fragments of text. Databaselike queries to this structured collection of text fragments are approximated using a novel logic called WHIRL, which combines inference in the style of deductive databases with ranked retrieval methods from information retrieval. WHIRL allows queries that integrate information from multiple Web sites, without requiring the extraction and normalization of object identifiers that can be used as keys; instead, operations that in conventional databases require equality tests...
DB
aha91casebased
Case-Based Learning Algorithms Abstract. Storing and using specific instances improves the performance of several supervised learning algorithms. These include algorithms that learn decision trees, classification rules, and distributed networks. However, no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this paper, we describe a framework and methodology, called instance-based learning, that generates classification predictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions derived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage requirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices in learning rate and classification accuracy. While the storage-reducing algorithm performs well on several realworld databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore, we extended it with a significance test to distinguish noisy instances. This extended algorithm's performance degrades gracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm.
ML
yokoo02secure
Secure Multi-agent Dynamic Programming based on Homomorphic Encryption and its Application to Combinatorial Auctions This paper presents a secure dynamic programming protocol that utilizes homomorphic encryption. By using this method, multiple agents can solve a combinatorial optimization problem among them without leaking their private information. More specifically, in this method, multiple servers cooperatively perform dynamic programming procedures for solving a combinatorial optimization problem by using the private information sent from agents as inputs. Although the severs can compute the optimal solution correctly, the inputs are kept secret even from the servers.
Agents
weng00incremental
An Incremental Learning Algorithm with Automatically Derived Discriminating Features We propose a new technique which incrementally derive discriminating features in the input space. This technique casts both classification problems (class labels as outputs) and regression problems (numerical values as outputs) into a unified regression problem. The virtual labels are formed by clustering in the output space. We use these virtual labels to extract discriminating features in the input space. This procedure is performed recursively. We organize the resulting discriminating subspace in a coarse-to-fine fashion and store the information in a decision tree. Such an incrementally hierarchical discriminating regression (IHDR) decision tree can be realized as a hierarchical probability distribution model. We also introduce a sample size dependent negativelog -likelihood (NLL) metric to deal with large-sample size cases, small-sample size cases, and unbalanced-sample size cases. This is very essential since the number of training samples per class are different at each internal node of the IHDR tree. We report experimental results for two types of data: face image data along with comparison with some major appearance-based method and decision trees, hall way images with driving directions as outputs for the automatic navigation problem -- a regression application.
ML
343493
Inducing Conceptual User Models Usually, performance is the primary objective in systems that make use of user modeling (Um) techniques. But since machine learning (Ml) in user modeling addresses several issues in the context of human computer interaction (Hci), the requirements on "performance" are manifold. Thus, machine learning for user modeling (Ml4Um) has to meet several demands in order to satisfy the aims of involved disciplines. In this article we describe the application of a rather unusual Ml method to Um, namely inductive logic programming (Ilp). Though not primarily associated with efficient learning methods, we motivate the use of Ilp by showing translucency of derived user models and the explanatory potential of such models during a user adapted filtering process. In course of the OySTER project our goal is to induce conceptual user models that allow for a transparent query refinement and information filtering in the domain of Www meta--search. 1 Introduction: Why Conceptual User Models?...
IR
pelikan99boa
BOA: The Bayesian Optimization Algorithm In this paper, an algorithm based on the concepts of genetic algorithms that uses an estimation of a probability distribution of promising solutions in order to generate new candidate solutions is proposed. To estimate the distribution, techniques for modeling multivariate data by Bayesian networks are used. The proposed algorithm identifies, reproduces and mixes building blocks up to a specified order. It is independent of the ordering of the variables in the strings representing the solutions. Moreover, prior information about the problem can be incorporated into the algorithm. However, prior information is not essential. Preliminary experiments show that the BOA outperforms the simple genetic algorithm even on decomposable functions with tight building blocks as a problem size grows. 1 INTRODUCTION Recently, there has been a growing interest in optimization methods that explicitly model the good solutions found so far and use the constructed model to guide the fu...
ML
hawking01measuring
Measuring Search Engine Quality The effectiveness of twenty public search engines is evaluated using TREC-inspired methods  and a set of 54 queries taken from real Web search logs. The World Wide Web is taken as the  test collection and a combination of crawler and text retrieval system is evaluated. The engines  are compared on a range of measures derivable from binary relevance judgments of the first  seven live results returned. Statistical testing reveals a significant difference between engines  and high inter-correlations between measures. Surprisingly, given the dynamic nature of the  Web and the time elapsed, there is also a high correlation between results of this study and  a previous study by Gordon and Pathak. For nearly all engines, there is a gradual decline  in precision at increasing cutoff after some initial fluctuation. Performance of the engines as  a group is found to be inferior to the group of participants in the TREC-8 Large Web task,  although the best engines approach the median of those systems. Shortcomings of current  Web search evaluation methodology are identified and recommendations are made for future  improvements. In particular, the present study and its predecessors deal with queries which are  assumed to derive from a need to find a selection of documents relevant to a topic. By contrast,  real Web search reflects a range of other information need types which require different judging  and different measures.  The authors wish to acknowledge that this work was carried out partly within the Cooperative Research Centre for Advanced Computational Systems established under the Australian Government's Cooperative Research Centres Program.  1  1 
IR
scheuermann98data
Data Partitioning and Load Balancing in Parallel Disk Systems Parallel disk systems provide opportunities for exploiting I/O parallelism in two possible ways, namely via inter-request and intra-request parallelism. In this paper we discuss the main issues in performance tuning of such systems, namely striping and load balancing, and show their relationship to response time and throughput. We outline the main components of an intelligent, self-reliant file system that aims to optimize striping by taking into account the requirements of the applications, and performs load balancing by judicious file allocation and dynamic redistributions of the data when access patterns change. Our system uses simple but effective heuristics that incur only little overhead. We present performance experiments based on synthetic workloads and real-life traces.  Keywords: parallel disk systems, performance tuning, file striping, data allocation, load balancing, disk cooling. 1 Introduction: Tuning Issues in Parallel Disk Systems  Parallel disk systems are of great imp...
DB
adams99stable
Stable Haptic Interaction with Virtual Environments A haptic interface is a kinesthetic link between a human operator and a virtual environment. This paper addresses fundamental stability and performance issues associated with haptic interaction. It generalizes and extends the concept of a virtual coupling network, an artificial link between the haptic display and a virtual world, to include both the impedance and admittance models of haptic interaction. A benchmark example exposes an important duality between these two cases. Linear circuit theory is used to develop necessary and sufficient conditions for the stability of a haptic simulation, assuming the human operator and virtual environment are passive. These equations lead to an explicit design procedure for virtual coupling networks which give maximum performance while guaranteeing stability. By decoupling the haptic display control problem from the design of virtual environments, the use of a virtual coupling network frees the developer of haptic-enabled virtual reality models fr...
HCI
311175
OBSERVER: An Approach for Query Processing in Global Information Systems based on Interoperation across Pre-existing Ontologies . There has been an explosion in the types, availability and volume of data accessible in an information system, thanks to the World Wide Web (the Web) and related inter-networking technologies. In this environment, there is a critical need to replace or complement earlier database integration approaches and current browsing and keyword-based techniques with concept-based approaches. Ontologies are increasingly becoming accepted as an important part of any concept or semantics based solution, and there is increasing realization that any viable solution will need to support multiple ontologies that may be independently developed and managed. In particular, we consider the use of concepts from pre-existing real world domain ontologies for describing the content of the underlying data repositories. The most challenging issue in this approach is that of vocabulary sharing, which involves dealing with the use of different terms or concepts to describe similar information. In this paper, we ...
DB
diehl00vrml
VRML with Constraints In this paper we discuss the benefits of extending VRML by constraints and present a new way based on prototypes and scripting to implement this extension. Our approach is easy-to-use, extensible and it considerably increases the expressivity of VRML. Our implementation supports one-way equational and finite domain constraints. We demonstrate the use of these constraints by means of several examples. Finally we argue that in the long run constraints should become an integral part of VRML.  CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism---Virtual Reality; D.3.3 [Programming Languages ]: Language Constructs and Features---Constraints  Keywords: VRML, Animation, Programming  1 
HCI
starner00gesture
The Gesture Pendant: A Self-illuminating, Wearable, Infrared Computer Vision System for Home Automation Control and Medical Monitoring In this paper we present a wearable device for control of home automation systems via hand gestures. This solution has many advantages over traditional home automation interfaces in that it can be used by those with loss of vision, motor skills, and mobility. By combining other sources of context with the pendant we can reduce the number and complexity of gestures while maintaining functionality. As users input gestures, the system can also analyze their movements for pathological tremors. This information can then be used for medical diagnosis, therapy, and emergency services.Currently, the Gesture Pendant can recognize control gestures with an accuracy of 95% and user defined gestures with an accuracy of 97% It can detect tremors above 2HZ within /- Hz.
HCI
meeden98trend
Trends In Evolutionary Robotics A review is given on the use of evolutionary techniques for the automatic design of adaptive robots. The focus is on methods which use neural networks and have been tested on actual physical robots. The chapter also examines the role of simulation and the use of domain knowledge in the evolutionary process. It concludes with some predictions about future directions in robotics.   Appeared in Soft Computing for Intelligent Robotic Systems, edited by L.C. Jain and T. Fukuda, PhysicaVerlag, New York, NY, 215--233, 1998.  1 Introduction  To be truly useful, robots must be adaptive. They should have a collection of basic abilities that can be brought to bear in tackling a variety of tasks in a wide range of environments. These fundamental abilities might include navigation to a goal location, obstacle avoidance, object recognition, and object manipulation. However, to date, this desired level of adaptability has not been realized. Instead, robots have primarily been successful when deploye...
AI
moreau00mobile
Mobile Objects in Java Mobile Objects in Java provides support for object mobility in Java. Similarly to the RMI technique, a notion of client-side stub, called startpoint, is used to communicate transparently with a server-side stub, called endpoint. Objects and associated endpoints are allowed to migrate. Our approach takes care of routing method calls using an algorithm that we studied in [22]. The purpose of this paper is to present and evaluate the implementation of this algorithm in Java. In particular, two different strategies for routing method invocations are investigated, namely call forwarding and referrals. The result of our experimentation shows that the latter can be more efficient by up to 19%.  1 
Agents
chen98webmate
WebMate: A Personal Agent for Browsing and Searching The World-Wide Web is developing very fast. Currently, #nding useful information  on the Web is a time consuming process. In this paper, we presentWebMate, an  agent that helps users to e#ectively browse and search the Web. WebMate extends  the state of the art in Web-based information retrieval in manyways. First, it uses  multiple TF-IDF vectors to keep track of user interests in di#erent domains. These  domains are automatically learned byWebMate. Second, WebMate uses the Trigger  Pair Model to automatically extract keywords for re#ning document search. Third,  during search, the user can provide multiple pages as similarity#relevance guidance  for the search. The system extracts and combines relevantkeywords from these relevant  pages and uses them for keyword re#nement. Using these techniques, WebMate  provides e#ective browsing and searching help and also compiles and sends to users  personal newspaper by automatically spiding news sources. Wehave experimentally  evaluated the performance of the system.
IR
192987
Image Retrieval: Past, Present, And Future This paper provides a comprehensive survey of the technical achievements in the research area of Image Retrieval, especially Content-Based Image Retrieval, an area so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multi-dimensional indexing, and system design, three of the fundamental bases of Content-Based Image Retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified, and future promising research directions are suggested. 1. INTRODUCTION Recent years have seen a rapid increase of the size of digital image collections. Everyday, both military and civilian equipment generates giga-bytes of images. Huge amount of information is out there. However, we can not access to or make use of the information unless it is organized so as to allow efficient browsing, searching and retriev...
IR
busi00expressiveness
On the Expressiveness of Event Notification in Data-driven Coordination Languages JavaSpaces and TSpaces are two coordination middlewares for distributed Java programming recently proposed by Sun and IBM, respectively. They extend the data-driven coordination model of Linda with an event notification mechanism: a process can register interest in the incoming arrivals of a particular kind of data and receive communication of the occurrences of these events. In [2, 3] we introduce a process calculus L, based on the Linda coordination model, and we prove that if processes are synchronous with the dataspace (ordered interpretation) L is Turing powerful, while this is not the case if they are asynchronous (unordered interpretation). Here, we introduce a new calculus Ln , obtained by extending L with the event noti cation mechanism, and we prove two main results contrasting with what has been shown for L: (i) Ln is Turing powerful also under the unordered interpretation and (ii) it allows a faithful encoding of the ordered semantics on top of the unordered one.
HCI
heydon01vesta
The Vesta Approach to Software Configuration Management Vesta is a system for software configuration management. It stores collections of source files, keeps track of which versions of which files go together, and automates the process of building a complete software artifact from its component pieces. Vesta's novel approach gives it three important properties. First, every build is repeatable, because its component sources and build tools are stored immutably and immortally, and its configuration description completely specifies what components and tools are used and how they are put together. Second, every build is incremental, because results of previous builds are cached and reused. Third, every build is consistent, because all build dependencies are automatically captured and recorded, so that a cached result from a previous build is reused only when doing so is certain to be correct. In addition, Vesta's flexible language for writing configuration descriptions makes it easy to describe large software configurations in a modular fashion and to create variant configurations by customizing build parameters. This paper gives a brief overview of Vesta, outlining Vesta's advantages over traditional tools, how those benefits are achieved, and the system's overall performance.
HCI
clark00location
Location and Recovery of Text on Oriented Surfaces We present a method for extracting text from images where the text plane is not necessarily fronto-parallel to the camera. Initially, we locate local image features such as borders and page edges. We then use perceptual grouping on these features to find rectangular regions in the scene. These regions are hypothesised to be pages or planes that may contain text. Edge distributions are then used for the assessment of these potential regions, providing a measure of confidence. It will be shown that the text may then be transformed to a fronto-parallel view suitable, for example, for an OCR system or other higher level recognition. The proposed method is scale independent (of the size of the text). We illustrate the algorithm using various examples.  Keywords: Oriented Text, Perspective Recovery of Text, Edge Angle Distribution  1. INTRODUCTION  Location and recovery of text in a scene would be useful in the context of wearable computing, desk computing, or unguided robotic motion. Such a...
HCI
257383
A Business Process Agent The architecture of a process agent is a three-layer  BDI, hybrid, multi-agent architecture. These process  agents are intended to deal with corporate  cultural, or political, sensitivities as well as with  corporate rules. These agents adapt their behaviour  on the basis of the likelihood of plan success, and  on estimates of the time, cost and value of choosing  a plan.  1 Introduction  An intelligent multi-agent system is a society of autonomous cooperating components each of which maintains an ongoing interaction with its environment. Intelligent agents should be autonomous, cooperative and adaptive. A process agent architecture is designed specifically for business process applications. Typically the cost of bringing a business process to its conclusion is substantially due to the cost of the human processing involved. If this is so then a process management system should make its decisions in a thorough and considered way, and should have no reason, based on cost, for not doing s...
Agents
leung98probabilistic
Probabilistic Affine Invariants for Recognition Under a weak perspective camera model, the image plane coordinates in different views of a planar object are related by an affine transformation. Because of this property, researchers have attempted to use affine invariants for recognition. However, there are two problems with this approach: (1) objects or object classes with inherent variability cannot be adequately treated using invariants; and (2) in practice the calculated affine invariants can be quite sensitive to errors in the image plane measurements. In this paper we use probability distributions to address both of these difficulties. Under the assumption that the feature positions of a planar object can be modeled using a jointly Gaussian density, we have derived the joint density over the corresponding set of affine coordinates. Even when the assumptions of a planar object and a weak perspective camera model do not strictly hold, the results are useful because deviations from the ideal can be treated as deformability in the ...
ML
krink00parameter
Parameter Control Using the Agent Based Patchwork Model The setting of parameters in Evolutionary Algorithms (EA) has crucial influence on their performance. Typically, the best choice depends on the optimization task. Some parameters yield better results when they are varied during the run. Recently, the so-called TerrainBased Genetic Algorithm (TBGA) was introduced, which is a self-tuning version of the traditional Cellular Genetic Algorithm (CGA). In a TBGA, the individuals of the population are placed in a two-dimensional grid, where only neighbored individuals can mate with each other. The position of an individual in this grid is interpreted as its offspring 's specific mutation rate and number of crossover points. This approach allows to apply GA parameters that are optimal for (i) the type of optimization task and (ii) the current state of the optimization process. However, only a few individuals can apply the optimal parameters simultaneously due to their fixed position in the grid lattice. In this paper, we substituted the fixed s...
Agents
distefano01coordinating
Coordinating Mobile Agents by means of Communicators This paper proposes a coordination model, for both static and mobile agents, based on abstract structures called Communicators, entities which handle agent dialogue performed through ACL speech act exchanging. Such structures are designed based on a need to model agent dialogue in a human-like style, offering a set of coordination primitives of general validity, able to provide both a direct and indirect interaction model. Since a Communicator handles messages exchanged within a well-defined multi-agent application, it is fully programmable, i.e. it is possible to specify what messages can be exchanged and how these message have to be handled. In detail, a Communicator performs a syntactic and semantic routing, allowing the exchange and forwarding of a message according to the programmed coordination laws.
Agents
ginsburg98annotate
Annotate - A Web-based Knowledge Management Support System for Document Collections Difficulties with web-based full text information retrieval (IR) systems include spurious matches, manually intensive document sifting, and the absence of communication or coordination between users. Furthermore, it is not clear how to best design a Computer-Supported Collaborative Work (CSCW) system which possesses coordination mechanisms in the inherently stateless Web environment. Document collections in an Intranet are a particularly interesting problem; electronic documents contain the potential for improving information throughput and knowledge transfer in organizations. The emergence of Intranet infrastructure creates new IR demands as all users can easily publish documents online. The associated problems include inefficient discovery and proliferation of documents regardless of quality, which in turn implies the mismanagement of the authors' expertise. This thesis investigates how to design an IR system to support KM in organizations. I propose an architecture and implement a n...
IR
hayden99architectural
Architectural Design Patterns for Multiagent Coordination This paper presents our first step towards agent-oriented software engineering, focusing on the area of coordinated multi-agent systems. In multi-agent systems, the interactions between the agents are crucial in determining the effectiveness of the system. Hence the adoption of an appropriate coordination mechanism is pivotal in the design of multi-agent system architectures. This paper does not focus on agent theory, rather on the development of an agent-oriented software engineering methodology, collaboration architectures and design patterns for collaboration. A catalog of coordination patterns inherent in multi-agent architectures is presented. Such patterns may be utilized in the architectural design for multiagent systems, allowing researchers and practitioners to improve the integrability and reusability properties of their systems. 
Agents
hussain98attribute
Attribute Grammars for Genetic Representations of Neural Networks and Syntactic Constraints of Genetic Programming context-free grammar augmented by the assignment of semantic attributes to the symbols of the grammar. A production rule specifies not only the replacement of symbols, but also the evaluation of the symbol’s attributes. In our research, an attribute grammar is used to specify classes of neural network structures with explicit representation of their functional organization. These representations provide useful constraints upon a genetic optimization that guarantee the preservation of syntactically correct genetic trees with semantically meaningful sub-trees. In this paper, we give a broad overview of our research into attribute grammar representations, from the basic and known capabilities, to the current ideas being addressed, to the future directions of our research. Attribute Grammars and Neural Networks
ML
dourish01seeking
Seeking a Foundation for Context-Aware Computing Context-aware computing is generally associated with elements of the Ubiquitous Computing program, and the opportunity to distribute computation and interaction through the environment rather than concentrating it at the desktop computer. However, issues of context have also been important in other areas of HCI research. I argue that the scope of context-based computing should be extended to include not only Ubiquitous Computing, but also recent trends in tangible interfaces as well as work on sociological investigations of the organization of interactive behavior. By taking a view of contextaware computing that integrates these different perspectives, we can begin to understand the foundational relationships that tie them all together, and that provide a framework for understanding the basic principles behind these various forms of embodied interaction. In particular, I point to phenomenology as a basis for the development of a new framework for design and evaluation of context-aware ...
HCI
marsh99cooperative
Co-operative Evaluation of a Desktop Virtual Reality System A summative usability evaluation of a desktop virtual reality (VR) system was developed and a preliminary study then conducted. The purpose of the study was twofold. Firstly, to test whether the traditional evaluation technique, co-operative evaluation, is effective in the evaluation of desktop VR systems. Co-operative evaluation is a variation on a `think-aloud' verbal protocol, whereby, in addition to concurrently 'thinking-aloud', users are encouraged to ask any questions about an evaluation, relating to the computer-based system, the application, or the tasks that they are required to perform during the evaluation. As well as this, the evaluator may ask questions of the user at any time during the evaluation. Results from the study indicate that this additional probing technique enables an evaluator to elicit further usability problems that may not have otherwise been exteriorized by the user. Additionally, a method is developed which attempts to turn round the qualitative 'think-aloud' type data into quantitative data. This provides a way of evaluating empirical 'think-aloud' evaluation methods and will be useful for comparing their effectiveness to evaluate 3D virtual reality systems.
HCI
zheng98stochastic
Stochastic Attribute Selection Committees Classifier committee learning methods generate multiple classifiers to form a committee by repeated application of a single base learning algorithm. The committee members vote to decide the final classification. Two such methods, Bagging and Boosting, have shown great success with decision tree learning. They create different classifiers by modifying the distribution of the training set. This paper studies a different approach: Stochastic Attribute Selection Committee learning of decision trees. It generates classifier committees by stochastically modifying the set of attributes but keeping the distribution of the training set unchanged. An empirical evaluation of a variant of this method, namely Sasc, in a representative collection of natural domains shows that the SASC method can significantly reduce the error rate of decision tree learning. On average Sasc is more accurate than Bagging and less accurate than Boosting, although a one-tailed sign-test fails to show that these differences are significant at a level of 0.05. In addition, it is found that, like Bagging, Sasc is more stable than Boosting in terms of less frequently obtaining significantly higher error rates than C4.5 and, when error is raised, producing lower error rate increases. Moreover, like Bagging, Sasc is amenable to parallel and distributed processing while Boosting is not.
ML
szabo95virtual
A Virtual Reality based System Environment for Intuitive Walk-Throughs and Exploration of Large-Scale Tourist Information This paper describes the concept and prototype architecture for Virtual Reality (VR) based Information Systems (ViRXIS). ViRXIS may serve as a base architecture for different kind of IS domains, such as a VR based Tourist Information System (ViRTIS) or a VR based Geographic Information System (ViRGIS). Finally, potential application scenarios of ViRTIS will be presented. Keywords Tourist Information Systems, Virtual Reality, virtual worlds, real-time interactive 3D simulation, information systems, man-machine-interface, object-oriented database management systems, spatial data access structures. 1. Introduction The ever increasing computing power and storage capacity of low-cost computer systems enables the implementation of multimedia applications that integrate different media such as text, image, graphics, voice, music, computer animation, or video for the presentation and manipulation of tourist information. At present, the userinterface of  such multimedia-based Tourist Informati...
HCI
ijspeert99evolution
Evolution and Development of a Central Pattern Generator for the Swimming of a Lamprey This paper describes the design of neural control architectures for locomotion using an evolutionary approach. Inspired by the central pattern generators found in animals, we develop neural controllers which can produce the patterns of oscillations necessary for the swimming of a simulated lamprey. This work is inspired by Ekeberg's neuronal and mechanical model of a lamprey [11], and follows experiments in which swimming controllers were evolved using a simple encoding scheme [26, 25]. Here, controllers are developed using an evolutionary algorithm based on the SGOCE encoding [31, 32] in which a genetic programming approach is used to evolve developmental programs which encode the growing of a dynamical neural network. The developmental programs determine how neurons located on a 2D substrate produce new cells through cellular division and how they form efferent or afferent interconnections. Swimming controllers are generated when the growing networks eventually create connections to ...
ML
klinkenberg00detecting
Detecting Concept Drift with Support Vector Machines For many learning tasks where data is collected over an extended period of time, its underlying distribution is likely to change. A typical example is information filtering, i.e. the adaptive classification of documents with respect to a particular user interest. Both the interest of the user and the document content change over time. A filtering system should be able to adapt to such concept changes. This paper proposes a new method to recognize and handle concept changes with support vector machines. The method maintains a window on the training data. The key idea is to automatically adjust the window size so that the estimated generalization error is minimized. The new approach is both theoretically well-founded as well as effective and efficient in practice. Since it does not require complicated parameterization, it is simpler to use and more robust than comparable heuristics. Experiments with simulated concept drift scenarios based on real-world text data com...
IR
weiss99multiagent
A Multiagent Framework for Planning, Reacting, and Learning .  There are two main approaches to activity coordination in multiagent systems, plan-based coordination and reactive coordination, each having its specic advantages and disadvantages. This paper describes a framework called M-Dyna-Q that aims at combining these two approaches such that their advantages are retained while their disadvantages are avoided. The key idea behind M-Dyna-Q, which is a multiagent variant of a single-agent framework known as Dyna-Q, is to achieve this combination through equipping the agents with the capability to learn information that is relevant to their planning and reacting activities. M-Dyna-Q thus oers an integrated view of planning, reacting, and learning in multiagent systems.  Keywords.  (M-)Dyna-Q, multiagent systems, reactive coordination, plan-based coordination, learning.  1 Introduction  The past years have witnessed a rapidly growing interest in multiagent systems (MAS), that is, in systems in which several interacting, intelligent and autono...
Agents
fischer00user
User Modeling in Human-Computer Interaction A fundamental objective of human-computer interaction research is to make systems more usable, more useful, and to provide users with experiences fitting their specific background knowledge and objectives. The challenge in an information-rich world is not only to make information available to people at any time, at any place, and in any form, but specifically to say the right thing at the right time in the right way. Designers of collaborative humancomputer systems face the formidable task of writing software for millions of users (at design time) while making it work as if it were designed for each individual user (only known at use time). User modeling research has attempted to address these issues. In this article, I will first review the objectives, progress, and unfulfilled hopes that have occurred over the last ten years, and illustrate them with some interesting computational environments and their underlying conceptual frameworks. A special emphasis is given to high-functionali...
HCI
yu01progressive
Progressive KNN Search Using B+-trees In high-dimensional databases, nearest neighbor (NN) search is computationally expensive. However, for many applications where small errors can be tolerated, determining approximate answers quickly has become an acceptable alternative. In this paper, we present a new algorithm that exploits the iMinMax( ) scheme [16] to facilitate nearest neighbor processing. The proposed algorithm distinguishes itself from existing algorithms in two ways. First, it evaluates KNN queries progressively, i.e., it begins by examining a small amount of data to produce approximate answers quickly; as more data are examined, the approximate answers are refined. Second, unlike existing progressive algorithms that access data randomly, the algorithm searches the data space in a systematic and `optimal' manner. Our performance study shows that the proposed scheme can produce approximate answers very quickly. More importantly, the quality of the answers are not sacrified significantly. Such an extension is applicable to other relevant methods such as the Pyramid Technique[2].
DB
sandholm01cabob
CABOB: A Fast Optimal Algorithm for Combinatorial Auctions Combinatorial auctions where bidders can bid  on bundles of items can lead to more economical  allocations, but determining the winners is  NP-complete and inapproximable. We present  CABOB, a sophisticated search algorithm for  the problem. It uses decomposition techniques,  upper and lower bounding (also across components)  , elaborate and dynamically chosen bid ordering  heuristics, and a host of structural observations.  Experiments against CPLEX 7.0 show that  CABOB is usually faster, never drastically slower,  and in many cases with special structure, drastically  faster. We also uncover interesting aspects of  the problem itself. First, the problems with short  bids that were hard for the first-generation of specialized  algorithms are easy. Second, almost all of  the CATS distributions are easy, and become easier  with more bids. Third, we test a number of random  restart strategies, and show that they do not help on  this problem because the run-time distribution does  not have a heavy tail (at least not for CABOB).  1 
Agents
fegaras98new
A New Heuristic for Optimizing Large Queries There is a number of OODB optimization techniques proposed recently, such as the translation of path expressions into joins and query unnesting, that may generate a large number of implicit joins even for simple queries. Unfortunately, most current commercial query optimizers are still based on the dynamic programming approach of System R, and cannot handle queries of more than ten tables. There is a number of recent proposals that advocate the use of combinatorial optimization techniques, such as iterative improvement and simulated annealing, to deal with the complexity of this problem. These techniques, though, fail to take advantage of the rich semantic information inherent in the query specification, such as the information available in query graphs, which gives a good handle to choose which relations to join each time. This paper presents a polynomial-time algorithm that generates a good quality order of relational joins. It can also be used with minor modifications to sort OODB a...
DB
483769
Coordinated Reinforcement Learning We present several new algorithms for multiagent  reinforcement learning. A common feature of these  algorithms is a parameterized, structured representation  of a policy or value function. This structure  is leveraged in an approach we call coordinated reinforcement   learning, by which agents coordinate  both their action selection activities and their parameter  updates. Within the limits of our parametric  representations, the agents will determine  a jointly optimal action without explicitly considering  every possible action in their exponentially  large joint action space. Our methods differ from  many previous reinforcement learning approaches  to multiagent coordination in that structured communication  and coordination between agents appears  at the core of both the learning algorithm and  the execution architecture. Our experimental results,  comparing our approach to other RL methods,  illustrate both the quality of the policies obtained  and the additional benefits of coordination.
ML
67963
An Experimental Comparison of Localization Methods Localization is the process of updating the pose of a robot in an environment, based on sensor readings. In this experimental study, we compare two recent methods for localization of indoor mobile robots: Markov localization, which uses a probability distribution across a grid of robot poses; and scan matching, which uses Kalman filtering techniques based on matching sensor scans. Both these techniques are dense matching methods, that is, they match dense sets of environment features to an a priori map. To arrive at results for a range of situations, we utilize several different types of environments, and add noise to both the dead-reckoning and the sensors. Analysis shows that, roughly, the scan-matching techniques are more efficient and accurate, but Markov localization is better able to cope with large amounts of noise. These results suggest hybrid methods that are efficient, accurate and robust to noise.  1. Introduction  To carry out tasks, such as delivering objects, an indoor ro...
AI
kagal01highly
A Highly Adaptable Infrastructure for Service Discovery and Management in Ubiquitous Computing In an age where wirelessly networked appliances and devices are becoming commonplace, there is a necessity for providing a standard interface to them that is easily accessible by any mobile user. The design outlined in this paper provides an infrastructure and communication protocol for presenting services to heterogeneous mobile clients in a physical space via some short range wireless links. This system uses a Communication Manager to communicate with the client devices. The Communication Manager can be modified easily to work with any type of communication medium, including TCP/IP, Infrared, CDPD and Bluetooth. All the components in our model use a language based on Extensible Markup Language (XML) giving it a uniform and easily adaptable interface. We explain our trade-offs in implementation and through experiments we show that the design is feasible and that it indeed provides a flexible structure for providing services. Centaurus defines a uniform infrastructure for heterogeneous services, both hardware and software , to be made available to diverse mobile users within a confined space.  1 
HCI
lawrence99searching
Searching the Web: General and Scientific Information Access he World Wide Web is revolutionizing the way people access information, and has opened up new possibilities in areas such as digital libraries, general and scientific information dissemination and retrieval, education, commerce, entertainment, government, and health care. The amount of publicly available information on the Web is increasing rapidly [1]. The Web is a gigantic digital library, a searchable 15 billion word encyclopedia [2]. It has stimulated research and development in information retrieval and dissemination, and fostered search engines such as AltaVista. These new developments are not limited to the Web, and can enhance access to virtually all forms of digital libraries. The revolution the Web has brought to information access is not so much due to the availability of information (huge amounts of information has long been available in libraries and elsewhere), but rather the increased efficiency of accessing information, which can make previously impractical tasks practical. There are many avenues for improvement in the efficiency of accessing information on the Web, for example, in the areas of locating and organizing information. This article discusses general and scientific information access on the Web, and many of our comments are applicable to digital libraries in general. The effectiveness of Web search engines is discussed, including results that show that the major search engines cover only a fraction of the “publicly indexable Web ” (the part of the Web which is considered for indexing by the major engines, which excludes pages hidden behind search forms, pages with authorization requirements, etc.). Current research into improved searching of the Web is discussed, including new techniques for ranking the relevance of results, and new techniques in metasearch that can improve the efficiency and effectiveness of Web search. The amount of scientific information and the number of electronic journals on the Internet continues to increase. Researchers are increasingly making their work available online. This article also discusses the creation of digital libraries of the scientific literature, incorporating autonomous citation indexing. The autonomous creation of citation indices
IR
raman01potters
Potter's Wheel: An Interactive Data Cleaning System Cleaning data of errors in structure and content is important  for data warehousing and integration. Current  solutions for data cleaning involve many iterations of  data "auditing" to find errors, and long-running transformations  to fix them. Users need to endure long  waits, and often write complex transformation scripts.  We present Potter's Wheel, an interactive data cleaning  system that tightly integrates transformation and  discrepancy detection. Users gradually build transformations  to clean the data by adding or undoing  transforms on a spreadsheet-like interface; the effect  of a transform is shown at once on records visible on  screen. These transforms are specified either through  simple graphical operations, or by showing the desired  effects on example data values. In the background,  Potter's Wheel automatically infers structures  for data values in terms of user-defined domains, and  accordingly checks for constraint violations. Thus  users can gradually build a transformation as discrepancies  are found, and clean the data without writing  complex programs or enduring long delays.  1 
DB
merro01bisimulation
Bisimulation Congruences in Safe Ambients We study a variant of Levi and Sangiorgi's Safe Ambients (SA) enriched with passwords (SAP). In SAP by managing passwords, for example generating new ones and distributing them selectively, an ambient may now program who may migrate into its computation space, and when. Moreover in SAP an ambient may provide different services depending on the passwords exhibited by its incoming clients. We give an lts based operational semantics for SAP and a labelled bisimulation based equivalence which is proved to coincide with barbed congruence. Our notion of bisimulation is used to prove a set of algebraic laws which are subsequently exploited to prove more significant examples. 1 
Agents
schwabacher96use
The Use Of Artificial Intelligence To Improve The Numerical Optimization Of Complex Engineering Designs Gradient-based numerical optimization of complex engineering designs promises to produce better designs rapidly. However, such methods generally assume that the objective function and constraint functions are continuous, smooth, and defined everywhere. Unfortunately, realistic simulators tend to violate these assumptions. We present several artificial intelligence-based techniques for improving the numerical optimization of complex engineering designs in the presence of such pathologies in the simulators. We have tested the resulting system in several realistic engineering domains, and have found that using our techniques can greatly decrease the cost of design space search, and can also increase the quality of the resulting designs. 
ML
murphy01bayes
The Bayes Net Toolbox for MATLAB The Bayes Net Toolbox (BNT) is an open-source Matlab package for directed graphical models. BNT supports many kinds of nodes (probability distributions), exact and approximate inference, parameter and structure learning, and static and dynamic models. BNT is widely used in teaching and research: the web page has received over 28,000 hits since May 2000. In this paper, we discuss a broad spectrum of issues related to graphical models (directed and undirected), and describe, at a high-level, how BNT was designed to cope with them all. We also compare BNT to other software packages for graphical models, and to the nascent OpenBayes effort.
AI
liu98statistical
A Statistical Method for Estimating the Usefulness of Text Databases Searching desired data in the Internet is one of the most common ways the Internet is used. No single  search engine is capable of searching all data in the Internet. The approach that provides an interface for  invoking multiple search engines for each user query has the potential to satisfy more users. When the  number of search engines under the interface is large, invoking all search engines for each query is often  not cost effective because it creates unnecessary network traffic by sending the query to a large number  of useless search engines and searching these useless search engines wastes local resources. The problem  can be overcome if the usefulness of every search engine with respect to each query can be predicted. In  this paper, we present a statistical method to estimate the usefulness of a search engine for any given  query. For a given query, the usefulness of a search engine in this paper is defined to be a combination of  the number of documents in the search engine that are sufficiently similar to the query and the average  similarity of these documents. Experimental results indicate that our estimation method is much more  accurate than existing methods.  
IR
higgins99integrated
An Integrated Vision Sensor for the Computation of Optical Flow Singular Points A robust, integrative algorithm is presented for computing the position of the focus of expansion or axis of rotation (the singular point) in optical flow fields such as those generated by self-motion. Measurements are shown of a fully parallel CMOS analog VLSI motion sensor array which computes the direction of local motion (sign of optical flow) at each pixel and can directly implement this algorithm. The flow field singular point is computed in real time with a power consumption of less than 2 mW . Computation of the singular point for more general flow fields requires measures of field expansion and rotation, which it is shown can also be computed in real-time hardware, again using only the sign of the optical flow field. These measures, along with the location of the singular point, provide robust real-time self-motion information for the visual guidance of a moving platform such as a robot. 1 INTRODUCTION  Visually guided navigation of autonomous vehicles requires robust measures...
AI
rowley98rotation
Rotation Invariant Neural Network-Based Face Detection In this paper, we present a neural network-based face detection system. Unlike similar systems which are limited to detecting upright, frontal faces, this system detects faces at any degree of rotation in the image plane. The system employs multiple networks; the first is a "router" network which processes each input window to determine its orientation and then uses this information to prepare the window for one or more "detector" networks. We present the training methods for both types of networks. We also perform sensitivity analysis on the networks, and present empirical results on a large test set. Finally, we present preliminary results for detecting faces which are rotated out of the image plane, such as profiles and semi-profiles. This work was partially supported by grants from Hewlett-Packard Corporation, Siemens Corporate Research, Inc., the Department of the Army, Army Research Office under grant number DAAH04-94-G-0006, and by the Office of Naval Research under grant number...
ML
90208
Natural-Sounding Speech Synthesis Using Variable-Length Units The goal of this work was to develop a speech synthesis system which concatenates variable-length units to create naturalsounding speech. Our initial work in this area showed that by careful design of system responses to ensure consistent intonation contours, natural-sounding speech synthesis was achievable with word- and phrase-level concatenation. In order to extend the flexibility of this framework, we focused on the problem of generating novel words from a corpus of sub-word units. The design of the sub-word units was motivated by perceptual studies that investigated where speech could be spliced with minimal audible distortion and what contextual constraints were necessary to maintain in order to produce natural sounding speech. The sub-word corpus is searched during synthesis using a Viterbi search which selects a sequence of units based on how well they individually match the input specification and on how well they sound as an ensemble. This concatenative speech synthesis syste...
IR
kobsa99adapting
Adapting Web Information to Disabled and Elderly Users : Substantial research and standardization efforts already exist to make it easier for  people with physical impairments to perceive and interact with web pages. This paper describes  work aimed at catering the content of web pages to the needs of different users, including  elderly people and users with vision and motor impairments. The AVANTI system and related  efforts in the AVANTI project will be discussed and experiences reported.  1. Introduction  The World Wide Web is currently the most frequently visited electronic resource and is likely to become the access ramp to the electronic information highway of the next millennium. Web access should therefore ideally be available to everyone in order not to create yet another informational, and hence economical and social, disparity in society. Special efforts must be put into making the access to the web available to those who so far have been at a disadvantage, including people with disabilities and elderly people who until recently...
HCI
liebig99event
Event Composition in Time-dependent Distributed Systems Many interesting application systems, ranging from workflow  management and CSCW to air traffic control, are eventdriven  and time-dependent and must interact with heterogeneous  components in the real world. Event services are used  to glue together distributed components. They assume a virtual  global time base to trigger actions and to order events.  The notion of a global time that is provided by synchronized  local clocks in distributed systems has a fundamental impact  on the semantics of event-driven systems, especially the composition  of events. The well studied 2g-precedence model,  which assumes that the granularity of global time-base g can  be derived from a priori known and bounded precision of  local clocks may not be suitable for the Internet where the  accuracy and external synchronization of local clocks is best  effort and cannot be guaranteed because of large transmission  delay variations and phases of disconnection. In this  paper we introduce a mechanism based on...
DB
307133
Incremental Computation and Maintenance of Temporal Aggregates Abstract. We consider the problems of computing aggregation queries in temporal databases and of maintaining materialized temporal aggregate views efficiently. The latter problem is particularly challenging since a single data update can cause aggregate results to change over the entire time line. We introduce a new index structure called the SB-tree, which incorporates features from both segment-trees and B-trees. SB-trees support fast lookup of aggregate results based on time and can be maintained efficiently when the data change. We extend the basicSB-tree index to handle cumulative (also called moving-window) aggregates, considering separately cases when the window size is or is not fixed in advance. For materialized aggregate views in a temporal database or warehouse, we propose building and maintaining SB-tree indices instead of the views themselves.
DB
jourlin99general
General Query Expansion Techniques For Spoken Document Retrieval This paper presents some developments in query expansion and document representation of our Spoken Document Retrieval (SDR) system since the 1998 Text REtrieval Conference (TREC-7). We have shown that a modification of the document representation combining several techniques for query expansion can improve Average Precision by 17 % relative to a system similar to that which we presented at TREC-7 [1]. These new experiments have also confirmed that the degradation of Average Precision due to a Word Error Rate (WER) of 25 % is relatively small (around 2 % relative). We hope to repeat these experiments when larger document collections become available to evaluate the scalability of these techniques. 1.
IR
zheng98integrating
Integrating Boosting and Stochastic Attribute Selection Committees for Further Improving the Performance of Decision Tree Learning Techniques for constructing classifier committees including Boosting and Bagging have demonstrated great success, especially Boosting for decision tree learning. This type of technique generates several classifiers to form a committee by repeated application of a single base learning algorithm. The committee members vote to decide the final classification. Boosting and Bagging create different classifiers by modifying the distribution of the training set. SASC (Stochastic Attribute Selection Committees) uses an alternative approach to generating classifier committees by stochastic manipulation of the set of attributes considered at each node during tree induction, but keeping the distribution of the training set unchanged. In this paper, we propose a method for improving the performance of Boosting. This technique combines Boosting and SASC. It builds classifier committees by manipulating both the distribution of the training set and the set of attributes available during induction. In...
ML
328242
Implicit Human Computer Interaction Through Context In this paper the term implicit human computer interaction is defined. It is discussed how the availability of processing power and advanced sensing technology can enable a shift in HCI from explicit interaction, such as direct manipulation GUIs, towards a more implicit interaction based on situational context. In the paper an algorithm that is based on a number of questions to identify applications that can facilitate implicit interaction is given. An XMLbased language to describe implicit HCI is proposed. The language uses contextual variables that can be grouped using different types of semantics as well as actions that are called by triggers. The term of perception is discussed and four basic approaches are identified that are useful when building context-aware applications. Providing two examples, a wearable context awareness component and a sensor-board, it is shown how sensor-based perception can be implemented. It is also discussed how situational context can be exploited to im...
HCI
sun99symbol
Symbol Grounding: A New Look At An Old Idea Symbols should be grounded, as has been argued before. But we insist that they should be  grounded not only in subsymbolic activities, but also in the interaction between the agent and the  world. The point is that concepts are not formed in isolation (from the world), in abstraction, or  \objectively". They are formed in relation to the experience of agents, through their perceptual  /motor apparatuses, in their world and linked to their goals and actions.  In this paper, we will take a detailed look at this relatively old issue, using a new perspective,  aided by our new work of computational cognitive model development. To further our understanding,  we also go back in time to link up with earlier philosophical theories related to this issue. The  result is an account that extends from computational mechanisms to philosophical abstractions.  1  Symbol Grounding: A New Look At An Old Idea  Symbols should be grounded, as has been argued before. But we insist that they should be ground...
ML
wooldridge02model
Model Checking Multi-Agent Systems with MABLE MABLE is a language for the design and automatic verification of multi-agent systems. MABLE is essentially a conventional imperative programming language, enriched by constructs from the agent-oriented programming paradigm. A MABLE system contains a number of agents, programmed using the MABLE imperative programming language. Agents in MABLE have a mental state consisting of beliefs, desires and intentions. Agents communicate using request and inform performatives, in the style of the FIPA agent communication language. MABLE systems may be augmented by the addition of formal claims about the system, expressed using a quantified, linear temporal belief-desire-intention logic. MABLE has been fully implemented, and makes use of the SPIN model checker to automatically verify the truth or falsity of claims.
Agents
clark00what
What Do We Want From a Wearable User Interface? This document outlines the author's ultimate goal and suggests one technology that can be exploited by applications writers to make them fit neatly into future application integration frameworks.
HCI
503542
Using Category-Based Collaborative Filtering in the Active Webmuseum Collaborative filtering is an important technology for creating useradapting Web sites. In general the efforts of improving filtering algorithms and using the predictions for the presentation of filtered objects are decoupled. Therefore, common measures (or metrics) for evaluating collaborative filtering (recommender) systems focus mainly on the prediction algorithm. It is hard to relate the classic measurements to actual user satisfaction because of the way the  user interacts with the recommendations, determined by their representation,  influences the benefits for the user. We propose an abstract access paradigm, which can be applied to the design of filtering systems, and at the same time formalizes the access to filtering results via multi-corridors (based on content-based categories) . This leads to new measures which better relate to the user satisfaction. We use these measures to evaluate the use of various  kinds of multi-corridors for our prototype user-adapting Web site the: Active WebMuseum.
IR
vitek99secure
Secure Mobile Code: The JavaSeal experiment Mobile agents are programs that move between sites during execution to benefit from the services and information present at each site. To gain wide acceptance, strong security guarantees must be given: sites must be protected from malicious agents and agents must be protected from each other. Software based protection is widely viewed as the most efficient way of enforcing agent security. In the first part of the paper, we review programming language support for security. This review also helps to highlight weaknesses in the Java security model. In the second part of the paper, we make good on the lessons learned in the review to design a security architecture for the JavaSeal agent platform.  1 Introduction  Wide area networks such as the Internet hold the promise of a brave new wired world of global computing. The hope is to have distributed applications that scale to the size of the Internet and seamlessly provide access to massive amounts of information and value added services. Bu...
Agents
442965
Data-Driven Generation of Decision Trees for Motif-Based Assignment of Protein Sequences to Functional Families This paper describes an approach to data-driven discovery of sequence motif-based models in the form of decision trees for assigning
IR
lien98automated
Automated Facial Expression Recognition Based on FACS Action Units Automated recognition of facial expression is an important addition to computer vision research because of its relevance to the study of psychological phenomena and the development of human-computer interaction (HCI). We developed a computer vision system that automatically recognizes individual action units or action unit combinations in the upper face using Hidden Markov Models (HMMs). Our approach to facial expression recognition is based on the Facial Action Coding System (FACS), which separates expressions into upper and lower face action. In this paper, we use three approaches to extract facial expression information: (1) facial feature point tracking, (2) dense flow tracking with principal component analysis (PCA), and (3) high gradient component detection (i.e., furrow detection). The recognition results of the upper face expressions using feature point tracking, dense flow tracking, and high gradient component detection are 85%, 93%, and 85%, respectively.  1. Introduction  Fa...
ML
535112
Using Extreme Programming for Knowledge Transfer This paper presents the application of eXtreme Programming to the software agents research and development group at TRLabs Regina. The group had difficulties maintaining its identity due to a very rapid turnover and lack of strategic polarization. The application of eXtreme Programming resulted in a complete reorientation of the development culture, which now forms a continuous substrate in every individual.
HCI
471147
Monotonic and Residuated Logic Programs In this paper we define the rather general framework of Monotonic Logic Programs, where the main results of (definite) logic programming are validly extrapolated. Whenever defining new logic programming extensions, we can thus turn our attention to the stipulation and study of its intuitive algebraic properties within the very general setting. Then, the existence of a minimum model and of a monotonic immediate consequences operator is guaranteed, and they are related as in classical logic programming. Afterwards we study the more restricted class of residuated logic programs which is able to capture several quite distinct logic programming semantics. Namely: Generalized Annotated Logic Programs, Fuzzy Logic Programming, Hybrid Probabilistic Logic Programs, and Possibilistic Logic Programming. We provide the embedding of possibilistic logic programming.
ML
lukasiewicz00probabilistic
Probabilistic Logic Programming with Conditional Constraints . We introduce a new approach to probabilistic logic programming in which probabilities are defined over a set of possible worlds. More precisely, classical program clauses are extended by a subinterval of [0; 1] that describes a range for the conditional probability of the head of a clause given its body. We then analyze the complexity of selected probabilistic logic programming tasks. It turns out that probabilistic logic programming is computationally more complex than classical logic programming. More precisely, the tractability of special cases of classical logic programming generally does not carry over to the corresponding special cases of probabilistic logic programming. Moreover, we also draw a precise picture of the complexity of deciding and computing tight logical consequences in probabilistic reasoning with conditional constraints in general. We then present linear optimization techniques for deciding satisfiability and computing tight logical consequences of probabilistic...
DB
surmann01learning
Learning Feed-Forward and Recurrent Fuzzy Systems: A Genetic Approach In this paper we present a new learning method for rule-based feed-forward and recurrent fuzzy systems. Recurrent fuzzy systems have hidden fuzzy variables and can approximate the temporal relation embedded in dynamic processes of unknown order. The learning method is universal i.e. it selects optimal width and position of Gaussian like membership functions and it selects a minimal set of fuzzy rules as well as the structure of the rules. A Genetic Algorithm is used to estimate the Fuzzy Systems which capture low complexity and minimal rule base. Optimization of the "entropy" of a fuzzy rule base leads to a minimal number of rules, of membership functions and of sub-premises together with an optimal input/output behavior. Most of the resulting Fuzzy Systems are comparable to systems designed by an expert but offers a better performance. The approach is compared to others by a standard benchmark (a system identification process). Different results for feed-forward and first order recurrent Fuzzy Systems with symmetric and non-symmetric membership functions are presented. Key words: Fuzzy logic controller, recurrent fuzzy systems, genetic algorithm, entropy of fuzzy rule, machine learning, dynamic processes. 1 
ML
buyukkokten00seeing
Seeing the Whole in Parts: Text Summarization for Web Browsing on Handheld Devices We introduce five methods for summarizing parts of Web pages on handheld devices, such as personal digital assistants (PDAs), or cellular phones. Each Web page is broken into text units that can each be hidden, partially displayed, made fully visible, or summarized. The methods accomplish summarization by different means. One method extracts significant keywords from the text units, another attempts to find each text unit's most significant sentence to act as a summary for the unit. We use information retrieval techniques, which we adapt to the World-Wide Web context. We tested the relative performance of our five methods by asking human subjects to accomplish single-page information search tasks using each method. We found that the combination of keywords and single-sentence summaries provides significant improvements in access times and number of pen actions, as compared to other schemes.  
HCI
helvik01using
Using the Cross-Entropy Method to Guide/Govern Mobile Agent's Path Finding in Networks The problem of finding paths in networks is general and many faceted with a wide range of engineering applications in communication networks. Finding the optimal path or combination of paths usually leads to NP-hard combinatorial optimization problems. A recent and promising method, the cross-entropy method proposed by Rubinstein, manages to produce optimal solutions to such problems in polynomial time. However this algorithm is centralized and batch oriented. In this paper we show how the cross-entropy method can be reformulated to govern the behaviour of multiple mobile agents which act independently and asynchronously of each other. The new algorithm is evaluate on a set of well known Travelling Salesman Problems. A simulator, based on the Network Simulator package, has been implemented which provide realistic simulation environments. Results show good performance and stable convergence towards near optimal solution of the problems tested.
Agents
531526
Qualitative Velocity and Ball Interception Abstract. In many approaches for qualitative spatial reasoning, navigation of an agent in a more or less static environment is considered (e.g. in the double-cross calculus [12]). However, in general, real environment are dynamic, which means that both the agent itself and also other objects and agents in the environment may move. Thus, in order to perform spatial reasoning, not only (qualitative) distance and orientation information is needed (as e.g. in [1]), but also information about (relative) velocity of objects (see e.g. [2]). Therefore, we will introduce concepts for qualitative and relative velocity: (quick) to left, neutral, (quick) to right. We investigate the usefulness of this approach in a case study, namely ball interception of simulated soccer agents in the RoboCup [10]. We compare a numerical approach where the interception point is computed exactly, a strategy based on reinforcement learning, a method with qualitative velocities developed in this paper, and the naïve method where the agent simply goes directly to the actual ball position. Key words: cognitive robotics; multiagent systems; spatial reasoning. 1
ML
thies02searching
Searching the World Wide Web in Low-Connectivity Communities The Internet has the potential to deliver information to communities around the world that have no other information resources. High telephone and ISP fees- in combination with lowbandwidth connections- make it unaffordable for many people to browse the Web online. We are developing the TEK system to enable users to search the Web using only email. TEK stands for &quot;Time Equals Knowledge, &quot; since the user exchanges time (waiting for email) for knowledge. The system contains three components: 1) the client, which provides a graphical interface for the end user, 2) the server, which performs the searches from MIT, and 3) a reliable email-based communication protocol between the client and the server. The TEK search engine differs from others in that it is designed to return low-bandwidth results, which are achieved by special filtering, analysis, and compression on the server side. We believe that TEK will bring Web resources to people who otherwise would not be able to afford them.
IR
337690
On Decidability of Boundedness Property for Regular Path Queries The paper studies the evaluation of regular path queries on semi-structured data, i.e. path queries of the form find all objects reachable by path whose labels form a word in r where r is a regular expression. We use local information expressed in the form of path constraints in the optimization of path expression queries. These constraints are of the form r ae w where r is a regular language and w is a word
DB
266462
An Overview of the Tatami Project This paper describes the Tatami project at UCSD, which is developing a system to support distributed cooperative software development over the web, and in particular, the validation of concurrent distributed software. The main components of our current prototype are a proof assistant, a generator for documentation websites, a database, an equational proof engine, and a communication protocol to support distributed cooperative work. We believe behavioral specification and verification are important for software development, and for this purpose we use first order hidden logic with equational atoms. The paper also briefly describes some novel user interface design methods that have been developed and applied in the project
ML
1625
Constructive Theory Refinement in Knowledge Based Neural Networks Knowledge based artificial neural networks offer an approach for connectionist theory refinement. We present an algorithm for refining and extending the domain theory incorporated in a knowledge based neural network using constructive neural network learning algorithms. The initial domain theory comprising of propositional rules is translated into a knowledge based network of threshold logic units (TLU). The domain theory is modified by dynamically adding neurons to the existing network. A constructive neural network learning algorithm is used to add and train these additional neurons using a sequence of labeled examples. We propose a novel hybrid constructive learning algorithm based on the Tiling and Pyramid constructive learning algorithms that allows knowledge based neural network to handle patterns with continuous valued attributes. Results of experiments on two non-trivial tasks (the ribosome binding site prediction and the financial advisor) show that our algorithm compares favo...
ML
crutchfield99evolutionary
The Evolutionary Unfolding of Complexity   We analyze the population dynamics of a broad class of tness functions that exhibit epochal evolution -- a dynamical behavior, commonly observed in both natural and artificial evolutionary processes, in which long periods of stasis in an evolving population are punctuated by sudden bursts of change. Our approach -- statistical dynamics -- combines methods from both statistical mechanics and dynamical systems theory in a way that offers an alternative to current "landscape" models of evolutionary optimization. We describe the population dynamics on the macroscopic level of tness classes or phenotype subbasins, while averaging out the genotypic variation that is consistent with a macroscopic state. Metastability in epochal evolution occurs solely at the macroscopic level of the fitness distribution. While a balance between selection and mutation maintains a quasistationary distribution of fitness, individuals diffuse randomly through selectively neutral subbasins in genotype space. Sudden innovations occur when, through this diffusion, a genotypic portal is discovered that connects to a new subbasin of higher fitness genotypes. In this way, we identify innovations with the unfolding and stabilization of a new dimension in the macroscopic state space. The architectural view of subbasins and portals in genotype space clarifies how frozen accidents and the resulting phenotypic constraints guide the evolution to higher complexity.
ML
lohse97consumer
Consumer Eye Movement Patterns on Yellow Pages Advertising Process tracing data help understand how yellow pages advertisement characteristics influence consumer information processing behavior. A laboratory experiment collected eye movement data while consumers chose businesses from phone directories. Consumers scan listings in alphabetic order. Their scan is not exhaustive. As a result, some ads are never seen. Consumers noticed over 93% of the quarter page display ads but only 26% of the plain listings. Consumers perceived color ads before ads without color, noticed more color ads than non-color ads and viewed color ads 21% longer than equivalent ads without color. Users viewed 42% more bold listings than plain listings. Consumers spent 54% more time viewing ads they end up choosing which demonstrates the importance of attention on subsequent choice behavior.  1 INTRODUCTION  In 1992, yellow pages directories were a $9.4 billion dollar information services business that reached 98% of American households (Mangel 1992). It is the fourth larg...
HCI
myers96easily
Easily Adding Animations to Interfaces Using Constraints Adding animation to interfaces is a very difficult task with  today's toolkits, even though there are many situations in which it would be useful and effective. The Amulet toolkit contains a new form of animation constraint that allows animations to be added to interfaces extremely easily without changing the logic of the application or the graphical objects themselves. An animation constraint detects changes to the value of the slot to which it is attached, and causes the slot to instead take on a series of values interpolated between the original and new values. The advantage over previous approaches is that animation constraints provide significantly better modularity and reuse. The programmer has independent control over the graphics to be animated, the start and end values of the animation, the path through value space, and the timing of the animation. Animations can be attached to any object, even existing widgets from the toolkit, and any type of value can be animated: scalars, ...
HCI
25329
Progress Report on the Disjunctive Deductive Database System . dlv is a deductive database system, based on disjunctive logic programming, which offers front-ends to several advanced KR formalisms. The system has been developed since the end of 1996 at Technische Universitat Wien in an ongoing project funded by the Austrian Science Funds (FWF). Recent comparisons have shown that dlv is nowadays a state-of-the-art implementation of disjunctive logic programming. A major strength of dlv are its advanced knowledge modelling features. Its kernel language extends disjunctive logic programming by strong negation (a la Gelfond and Lifschitz) and integrity constraints; furthermore, front-ends for the database language SQL3 and for diagnostic reasoning are available. Suitable interfaces allow dlv users to utilize base relations which are stored in external commercial database systems. This paper provides an overview of the dlv system and describes recent advances in its implementation. In particular, the recent implementation of incremental techniques fo...
DB
9576
Optimization of Constrained Frequent Set Queries with 2-variable Constraints Currently, there is tremendous interest in providing ad-hoc mining capabilities in database management systems. As a first step towards this goal, in [15] we proposed an architecture for supporting constraint-based, human-centered, exploratory mining of various kinds of rules including associations, introduced the notion of constrained frequent set queries (CFQs), and developed effective pruning optimizations for CFQs with 1-variable (1-var) constraints. While 1-var constraints are useful for constraining the antecedent and consequent separately, many natural examples of CFQs illustrate the need for constraining the antecedent and consequent jointly, for which 2-variable (2-var) constraints are indispensable. Developing pruning optimizations for CFQs with 2-var constraints is the subject of this paper. But this is a difficult problem because: (i) in 2var constraints, both variables keep changing and, unlike 1-var constraints, there is no fixed target for pruning; (ii) as we show, "conv...
DB
lee99unifying
A Unifying Information-theoretic Framework for Independent Component Analysis We show that different theories recently proposed for Independent Component Analysis (ICA) lead to the same iterative learning algorithm for blind separation of mixed independent sources. We review those theories and suggest that information theory can be used to unify several lines of research. Pearlmutter and Parra (1996) and Cardoso (1997) showed that the infomax approach of Bell and Sejnowski (1995) and the maximum likelihood estimation approach are equivalent. We show that negentropy maximization also has equivalent properties and therefore all three approaches yield the same learning rule for a fixed nonlinearity. Girolami and Fyfe (1997a) have shown that the nonlinear Principal Component Analysis (PCA) algorithm of Karhunen and Joutsensalo (1994) and Oja (1997) can also be viewed from information-theoretic principles since it minimizes the sum of squares of the fourth-order marginal cumulants and therefore approximately minimizes the mutual information (Comon, 1994). Lambert (19...
ML
460784
Extending Multi-Agent Cooperation by Overhearing Much cooperation among humans happens following a common pattern:  by chance or deliberately, a person overhears a conversation between  two or more parties and steps in to help, for instance by suggesting answers  to questions, by volunteering to perform actions, by making observations  or adding information. We describe an abstract architecture to support a  similar pattern in societies of articial agents. Our architecture involves  pairs of so-called service agents (or services) engaged in some tasks, and  unlimited number of suggestive agents (or suggesters). The latter have  an understanding of the work behaviors of the former through a publicly  available model, and are able to observe the messages they exchange. Depending  on their own objectives, the understanding they have available,  and the observed communication, the suggesters try to cooperate with  the services, by initiating assisting actions, and by sending suggestions to  the services. These in eect may induce a change in services behavior.  To test our architecture, we developed an experimental, multi-agent Web  site. The system has been implemented by using a BDI toolkit, JACK  Intelligent Agents.  Keywords: autonomous agents, multiagent systems, AI architectures, distributed AI.  1 
Agents
miller01training
Training for Teamwork Teams train by practicing basic interaction skills in order to develop a shared mental model between team members. A computer-based training system must have a model of teamwork to train a new team member to act as part of a team. This model of teamwork can be used to identify weaknesses in team interaction skills in the trainee. Existing multi-agent models for teamwork are limited in their ability to support proactive information exchange among teammates. To address this issue, we have developed and implemented a multi-agent architecture called CAST that simulates teamwork and supports proactive information exchange in a dynamic environment. To this we are adding a tutoring system that monitors, evaluates, and corrects the trainee in the performance of teamwork skills.  Keywords: ITS, teamwork, multi-agent, collaboration, HCI  
Agents
black98probabilistic
A Probabilistic Framework for Matching Temporal Trajectories: Condensation-Based Recognition of Gestures and Expressions . The recognition of human gestures and facial expressions in image sequences is an important and challenging problem that enables a host of human-computer interaction applications. This paper describes a framework for incremental recognition of human motion that extends the "Condensation" algorithm proposed by Isard and Blake (ECCV'96). Human motions are modeled as temporal trajectories of some estimated parameters over time. The Condensation algorithm uses random sampling techniques to incrementally match the trajectory models to the multi-variate input data. The recognition framework is demonstrated with two examples. The first example involves an augmented office whiteboard with which a user can make simple hand gestures to grab regions of the board, print them, save them, etc. The second example illustrates the recognition of human facial expressions using the estimated parameters of a learned model of mouth motion. 1 Introduction Motion is intimately tied with our behavior; we m...
DB
cassell99fully
Fully Embodied Conversational Avatars: Making Communicative Behaviors Autonomous : Although avatars may resemble communicative interface agents, they have for the most part not profited from recent research into autonomous embodied conversational systems. In particular, even though avatars function within conversational environments (for example, chat or games), and even though they often resemble humans (with a head, hands, and a body) they are incapable of representing the kinds of knowledge that humans have about how to use the body during communication. Humans, however, do make extensive use of the visual channel for interaction management where many subtle and even involuntary cues are read from stance, gaze and gesture. We argue that the modeling and animation of such fundamental behavior is crucial for the credibility and effectiveness of the virtual interaction in chat. By treating the avatar as a communicative agent, we propose a method to automate the animation of important communicative behavior, deriving from work in conversation and discourse theory. B...
Agents
202521
Partial Models of Extended Generalized Logic Programs . In recent years there has been an increasing interest in extensions  of the logic programming paradigm beyond the class of normal  logic programs motivated by the need for a satisfactory respresentation  and processing of knowledge. An important problem in this area is to find  an adequate declarative semantics for logic programs. In the present paper  a general preference criterion is proposed that selects the `intended'  partial models of extended generalized logic programs which is a conservative  extension of the stationary semantics for normal logic programs  of [13], [15] and generalizes the WFSX-semantics of [12]. The presented  preference criterion defines a partial model of an extended generalized  logic program as intended if it is generated by a stationary chain. The  GWFSX-semantics is defined by the set-theoretical intersection of all  stationary generated models, and thus generalizes the results from [9]  and [1].  1 Introduction  Declarative semantics provides a mathem...
DB
28315
Spatio-Temporal Data Types: An Approach to Modeling and Querying Moving Objects in Databases Spatio-temporal databases deal with geometries changing over time. In general, geometries cannot only change in discrete steps, but continuously, and we are talking about moving objects. If only the position in space of an object is relevant, then moving point is a basic abstraction; if also the extent is of interest, then the moving region abstraction captures moving as well as growing or shrinking regions. We propose a new line of research where moving points and moving regions are viewed as three-dimensional (2D space + time) or higher-dimensional entities whose structure and behavior is captured by modeling them as abstract data types. Such types can be integrated as base (attribute) data types into relational, object-oriented, or other DBMS data models; they can be implemented as data blades, cartridges, etc. for extensible DBMSs. We expect these spatio-temporal data types to play a similarly fundamental role for spatio-temporal databases as spatial data types have played for spatial databases. The paper explains the approach and discusses several fundamental issues and questions related to it that need to be clarified before delving into specific designs of spatio-temporal algebras.
DB
508732
Active Learning For Automatic Speech Recognition State-of-the-art speech recognition systems are trained using transcribed utterances, preparation of which is labor intensive and time-consuming. In this paper, we describe a new method for reducing the transcription effort for training in automatic speech recognition (ASR). Active learning aims at reducing the number of training examples to be labeled by automatically processing the unlabeled examples, and then selecting the most informative ones with respect to a given cost function for a human to label. We automatically estimate a confidence score for each word of the utterance, exploiting the lattice output of a speech recognizer, which was trained on a small set of transcribed data. We compute utterance confidence scores based on these word confidence scores, then selectively sample the utterances to be transcribed using the utterance confidence scores. In our experiments, we show that we reduce the amount of labeled data needed for a given word accuracy by 27%.
IR
tzouramanis99processing
Processing of Spatiotemporal Queries in Image Databases Overlapping Linear Quadtrees is a structure suitable for storing consecutive raster images according to transaction time (a database of evolving images). This structure saves considerable space without sacrificing time performance in accessing every single image. Moreover, it can be used for answering efficiently window queries for a number of consecutive images (spatio-temporal queries). In this paper, we present three such temporal window queries: strict containment, border intersect and cover. Besides, based on a method of producing synthetic pairs of evolving images (random images with specified aggregation) we present empirical results on the I/O performance of these queries.
DB
levene99probabilistic
A Probabilistic Approach to Navigation in Hypertext One of the main unsolved problems confronting Hypertext is the navigation problem, namely the problem of having to know where you are in the database graph representing the structure of a Hypertext database, and knowing how to get to some other place you are searching for in the database graph. Previously we formalised a Hypertext database in terms of a directed graph whose nodes represent pages of information. The notion of a trail, which is a path in the database graph describing some logical association amongst the pages in the trail, is central to our model. We defined a Hypertext Query Language, HQL, over Hypertext databases and showed that in general the navigation problem, i.e. the problem of finding a trail that satisfies a HQL query (technically known as the model checking problem), is NPcomplete. Herein we present a preliminary investigation of using a probabilistic approach in order to enhance the efficiency of model checking. The flavour of our investigation is that if we h...
DB
du98vulnerability
Vulnerability Testing of Software System Using Fault Injection We describe an approach for testing a software system for possible security flaws. Traditionally, security testing is done using penetration analysis and formal methods. Based on the observation that most security flaws are triggered due to a flawed interaction with the environment, we view the security testing problem as the problem of testing for the fault-tolerance properties of a software system. We consider each environment perturbation as a fault and the resulting security compromise a failure in the toleration of such faults. Our approach is based on the well known technique of fault-injection. Environment faults are injected into the system under test and system behavior observed. The failure to tolerate faults is an indicator of a potential security flaw in the system. An Environment-Application Interaction (EAI) fault model is proposed. EAI allows us to decide what faults to inject. Based on EAI, we present a security-flaw classification scheme. This scheme was used to classify 142 security flaws in a vulnerability database. This classification revealed that 91 % of the security flaws in the database are covered by the EAI model.
Agents
316446
From PETS to Storykit: Creating New Technology with an Intergenerational Design Team Working with children as our design partners, our  intergenerational design team at the University of Maryland  has been developing both new design methodologies and  new storytelling technology for children. In this paper, we  focus on two recent results of our efforts: PETS, a robotic  storyteller, and Storykit, a construction kit of low-tech and  high-tech components to build immersive StoryRooms. We  then describe some lessons we learned.  Introduction  Over the past two years, our intergenerational design team  at the University of Maryland has been developing new design methodologies to create new storytelling technology for children. This team is made of six adult researchers from computer science, education, art, and engineering, and seven children, ages 7 to 11, from local elementary schools. These children stay with us for a long term, at least for one year. The adults are undergraduate students, graduate students, and faculty from art, education, engineering, and computer sc...
HCI
gouveia98technological
A Technological Related Discussion on the Potential of Change in Education, Learning and Training This is a poster prepared based on the presentation material for a paper with the same name presented at the conference. The goal is to discuss the supporting role of Information & Communication Technology (ICT) in education activities and puts in context the impact that CSCW systems can have both in Open and Distance Learning and in general education, learning and training. The NetLab concept is presented and used to support the paper positions and serves as the base to propose a roadmap to a virtual university setting. 1. PRESENTATION CONTEXT At the end of the century education is on change. In particular, the high levels of students that miss presence classes and display a lack of interest to attend most of the subjects in their higher education is already a common
HCI
stockinger01design
Design and Implementation of Bitmap Indices for Scientific Data Bitmap indices are efficient multi-dimensional index data structures for handling complex adhoc queries in read-mostly environments. They have been implemented in several commercial database systems but are only well suited for discrete attribute values which are very common in typical business applications. However, many scientific applications usually operate on floating point numbers and cannot take advantage of the optimisation techniques offered by current database solutions. We thus present a novel algorithm called GenericRangeEval for processing one-sided range queries over floating point values. In addition we present a cost model for predicting the performance of bitmap indices for high-dimensional search spaces. We verify our analytical results by a detailed experimental study and show that the presented bitmap evaluation algorithm scales well also for high-dimensional search spaces requiring only a fairly small index. Because of its simple arithmetic structure, the cost model could easily be integrated into a query optimiser for deciding whether the current multi-dimensional query shall be answered by means of a bitmap index or better by sequentially scanning the data values, without using an index at all.
DB
467024
Declarative Procedural Goals in Intelligent Agent Systems An important concept for intelligent agent systems is goals. Goals have two aspects: declarative (a description of the state sought), and procedural (a set of plans for achieving the goal). A declarative view of goals is necessary in order to reason about important properties of goals, while a procedural view of goals is necessary to ensure that goals can be achieved efficiently in dynamic environments. In this paper we propose a framework for goals which integrates both views. We discuss the requisite properties of goals and the link between the declarative and procedural aspects, then derive a formal semantics which has these properties. We present a high-level plan notation with goals and give its formal semantics. We then show how the use of declarative information permits reasoning (such as the detection and resolution of conflicts) to be performed on goals. 1
Agents
66638
Providing Haptic 'Hints' to Automatic Motion Planners In this paper, we investigate methods for enabling a human operator and an automatic motion planner to cooperatively solve a motion planning query. Our work is motivated by our experience that automatic motion planners sometimes fail due to the difficulty of discovering `critical' configurations of the robot that are often naturally apparent to a human observer.  Our goal is to develop techniques by which the automatic planner can utilize (easily generated) user-input, and determine `natural' ways to inform the user of the progress made by the motion planner. We show that simple randomized techniques inspired by probabilistic roadmap methods are quite useful for transforming approximate, user-generated paths into collision-free paths, and describe an iterative transformation method which enables one to transform a solution for an easier version of the problem into a solution for the original problem. We also show that simple visualization techniques can provide meaningful representatio...
AI
weber01textual
A Textual Case-Based Reasoning Framework for Knowledge Management Applications Knowledge management (KM) systems manipulate organizational  knowledge by storing and redistributing corporate memories that are acquired  from the organization's members. In this paper, we introduce a textual casebased  reasoning (TCBR) framework for KM systems that manipulates  organizational knowledge embedded in artifacts (e.g., best practices, alerts,  lessons learned). The TCBR approach acquires knowledge from human users  (via knowledge elicitation) and from text documents (via knowledge extraction)  using template-based information extraction methods, a subset of natural  language, and a domain ontology. Organizational knowledge is stored in a case  base and is distributed in the context of targeted processes (i.e., within external  distribution systems). The knowledge artifacts in the case base have to be  translated into the format of the external distribution systems. A domain  ontology supports knowledge elicitation and extraction, storage of knowledge  artifacts in a case base, and artifact translation.  
IR
rauterberg97gesture
A Gesture Based Interaction Technique for a Planning Tool for Construction and Design In this article we wish to show a method that goes beyond the established approaches of human-computer interaction. We first bring a serious critique of traditional interface types, showing their major drawbacks and limitations. Promising alternatives are offered by Virtual (or: immersive) Reality (VR) and by Augmented Reality (AR). The AR design strategy enables humans to behave in a nearly natural way. Natural interaction means human actions in the real world with other humans and/or with real world objects. Guided by the basic constraints of natural interaction, we derive a set of recommendations for the next generation of user interfaces: the Natural User Interface (NUI). Our approach to NUIs is discussed in the form of a general framework followed by a prototype. The prototype tool builds on video-based interaction and supports construction and plant layout. A first empirical evaluation is briefly presented.  1 Introduction  The introduction of computers in the work place has had ...
HCI
bihlmeyer98dlv
dlv { An Overview the Intelligent Grounding, the Model Generator and the Model Checker. All of these modules perform a modular evaluation of their input according to various dependency graphs as de  ned in [5, 2] and try to detect and eciently handle special (syntactic) subclasses, which in general yields a tremendous speedup.    Supported by FWF (Austrian Science Funds) under the project P11580-MAT \A Query System for Disjunctive Deductive Databases"    Please address correspondence to this author.  The Intelligent Grounding takes an input program, whose facts can be stored also in the tables of external relational databases, and eciently generates a subset of the program instantiation that has exactly the same stable models as the full program, but is much smaller in general. (For strati  ed programs, for example, the Grounding already computes the single stable model.)  Then the Model Generator is run on the (ground) output of the Intelligent Grounding. It generates one candidate for a stable model a
DB
fraternali98proceedings
Proceedings of the 6th International Workshop on Deductive Databases and Logic . . . The integration of concepts from logic and deduction into databases and knowledge bases has created the field of deductive databases. Logic programming provides a powerful declarative language for accessing and maintaining knowledge in databases. Techniques from relational databases and automated deduction are useful for achieving efficient retrieval and reasoning in large knowledge bases. Thus, deductive databases can be used for building intelligent information systems.  The contributions in this Proceedings of the Sixth International Workshop on Deductive Databases and Logic Programming DDLP'98 are grouped into four sessions: theoretical aspects, applications, Datalog extensions, and semantics and a demo session.  3  4  Contents  Preface 7 Schedule of Presentations 11 Theoretical Aspects 13 Nieves R. Brisaboa, Agustin Gonzales, Hector J. Hernandez, and Jose R. Parama:  Chasing programs in Datalog 13 Francois Bry, Norbert Eisinger, Heribert Schuetz, and Sunna Torge:  SIC: Satisfiabil...
DB
303620
Electronic Books in Digital Libraries 1  Electronic book is an application with a multimedia database of instructional resources, which include hyperlinked text, instructor's audio/video clips, slides, animation, still images, etc. as well as content-based information about these data, and metadata such as annotations, tags, and cross-referencing information. Electronic books in the Internet or on CDs today are not easy to learn from. We propose the use of a multimedia database of instructional resources in constructing and delivering multimedia lessons about topics in an electronic book. We introduce an electronic book data model containing (a) topic objects and (b) instructional resources, called instruction module objects, which are multimedia presentations possibly capturing real-life lectures of instructors. We use the notion of topic prerequisites for topics at different detail levels, to allow electronic book users to request/compose multimedia lessons about topics in the electronic book. We present automated construction of the "best" user-tailored lesson (as a multimedia presentation. 1. 
DB
levene99justification
Justification for Inclusion Dependency Normal Form Functional dependencies (FDs) and inclusion dependencies (INDs) are the most fundamental integrity constraints that arise in practice in relational databases. In this paper we address the issue of normalisation in the presence of FDs and INDs and in particular the semantic justification for Inclusion Dependency Normal Form (IDNF), a normal form which combines Boyce-Codd normal form with the restriction on the INDs that they be noncircular and keybased. We motivate and formalise three goals of database design in the presence of FDs and INDs: non interaction between FDs and INDs, elimination of redundancy and update anomalies, and preservation of entity integrity. We show that, as for FDs, in the presence of INDs being free of redundancy is equivalent to being free of update anomalies. Then, for each of these properties we derive equivalent syntactic conditions on the database design. Individually, each of these syntactic conditions is weaker than IDNF and the restriction that an FD not ...
DB
nwana99perspective
A Perspective on Software Agents Research This paper sets out, ambitiously, to present a brief reappraisal of software agents research. Evidently, software  agent technology has promised much. However some five years after the word `agent' came into vogue in the popular computing press, it is perhaps time the efforts in this fledgling area are thoroughly evaluated with a view to refocusing future efforts. We do not pretend to have done this in this paper -- but we hope we have sown the first seeds towards a thorough first 5-year report of the software agents area. The paper contains some strong views not necessarily widely accepted by the agent community  1  . 1 Introduction  The main goal of this paper is to provide a brief perspective on the progress of software agents research.  Though `agents' research had been going on for more than a fifteen years before, agents really became a buzzword in the popular computing press (and also within the artificial intelligence and computing communities) around 1994. During this year sev...
Agents
smith00neural
Neural Networks in Business: Techniques and Applications for the Operations Researcher This paper presents an overview of the di!erent types of neural network models which are applicable when solving business problems. The history of neural networks in business is outlined, leading to a discussion of the current applications in business including data mining, as well as the current research directions. The role of neural networks as a modern operations research tool is discussed.  Scope and purpose  Neural networks are becoming increasingly popular in business. Many organisations are investing in neural network and data mining solutions to problems which have traditionally fallen under the responsibility of operations research. This article provides an overview for the operations research reader of the basic neural network techniques, as well as their historical and current use in business. The paper is intended as an introductory article for the remainder of this special issue on neural networks in business. # 2000 Elsevier Science Ltd. All rights reserved.  Keywords: N...
DB
bassiliades01interbasekb
InterBase-KB: Integrating a Knowledge Base System with a Multidatabase System for Data Warehousing This paper describes the integration of a multidatabase system and a knowledge-base system to support
DB
340534
Negotiation Protocols and Dialogue Games In a dynamic and open environment negotiation protocols cannot be known  beforehand. We propose a methodology for constructing exible negotiation  protocols based on joint actions and dialogue games. We view negotiation  as a combination of joint actions. Simple dialogue games that consist of  initiatives followed by responses function as `recipes for joint action' from  which larger interactions can be constructed coherently.  1 Introduction  Agent based software engineering is an active research area. One of its main challenges is to bridge theoretical models and practical applications. For example, there are many theoretical results on negotiation [11], but automated negotiation is still rare on the internet. Implementing negotiation protocols raises a number of practical questions. In particular, current negotiation systems are either closed or semi-closed. In closed and semi-closed environments, like in auctions, there is central control over the agents that can participate. In s...
Agents
chalmers98order
The Order of Things: Activity-Centred Information Access This paper focuses on the representation and access of Web-based information, and how to make such a representation adapt to the activities or interests of individuals within a community of users. The heterogeneous mix of information on the Web restricts the coverage of traditional indexing techniques and so limits the power of search engines. In contrast to traditional methods, and in a way that extends collaborative filtering approaches, the path model centres representation on usage histories rather than content analysis. By putting activity at the centre of representation and not the periphery, the path model concentrates on the reader not the author and the browser not the site. We describe metrics of similarity based on the path model, and their application in a URL recommender tool and in visualising sets of URLs. Keywords: heterogeneous data, activity, indexing, collaborative filtering, information retrieval, access and visualization. 1 Introduction As Tim Berners-Lee pointe...
IR
408356
Dynamic CPU Scheduling with Imprecise Knowledge of Computation-Time : The majority of the studies conducted in scheduling real-time transactions mostly concentrate  on concurrency control protocols, while overlooking the CPU as being the primary resource.  Consequently, there are various techniques for scheduling the CPU in conventional time-critical systems;  meanwhile, there does not seem to be any technique that is adequately designed for scheduling  such a resource in Real-Time Database (RTDB) systems. In this paper, we construct an efficient CPU  scheduling scheme that minimizes the preemption rate in order to reduce the frequency by which synchronization  protocols must be invoked, along with their inherited performance degradation. In addition,  we also introduce a new timing model upon which the newly introduced scheduler is incorpo-  rated in order to utilize the system's imprecise knowledge of computation time estimates.  Keywords: CPU Scheduling, lowering-preemption, timeliness-functions, and imprecise computation  estimates.  1. INTRODUCTIO...
DB
stepaniuk98optimizations
Optimizations of Rough Set Model . Rough set methodology is based on concept (set) approximations constructed  from available background knowledge represented in information systems [14]. In many  applications only partial knowledge about approximated concepts is given. Hence quite  often first a parametrized family of concept approximations is built and next by tuning of  the parameters the best, in a sense, approximation is chosen (see e.g. variable precision  rough set model [40]) in approximation spaces. In this paper we follow this approach in  generalized approximation spaces. We discuss rough set model based on approximation  spaces with uncertainty functions and rough inclusions. Both elements of approximation  space are parametrized and for the proper application of such model to a particular data set  it is necessary to make optimization of the parameters. We discuss basic properties of the  mentioned model and also strategies of parameters optimization. We also present different  notions of rough relations....
ML
186241
Optimization of User-Defined Functions in Distributed Object-Relational DBMS Full support of parallelism in object-relational database systems (ORDBMSs) is desired. The parallelization techniques developed for relational database systems are not adequate for ORDBMS because of the introduction of complex abstract data types and operations on ordered domains. In this paper, we consider a data stream paradigm and develop a query parallelization framework that exploits characteristics of user-defined functions in a ORDBMS during query optimization. By introducing the concept of windows and abstract data type orderings, we develop a novel approach for parallelizing user-defined functions in a distributed ORDBMS environment. The implementation issues in providing query services in ordered domains are also discussed. 1 Introduction As time goes by, more and more database vendors agree that Object-Relational DBMSs (ORDBMSs) are the future [23]. Though full support of parallel ORDBMS is expected, the techniques used to parallelize relational database systems are not ad...
DB
rao94meansend
Means-End Plan Recognition - Towards a Theory of Reactive Recognition This paper draws its inspiration from current work in reactive planning to guide plan recognition using "plans as recipes". The plan recognition process guided by such a library of plans is called means-end plan recognition. An extension of dynamic logic, called dynamic agent logic, is introduced to provide a formal semantics for means-end plan recognition and its counterpart, means-end plan execution. The operational semantics, given by algorithms for means-end plan recognition, are then related to the provability of formulas in the dynamic agent logic. This establishes the relative soundness and completeness of the algorithms with respect to a given library of plans. Some of the restrictive assumptions underlying means-end plan recognition are then relaxed to provide a theory of reactive recognition that allows for changes in the external world during the recognition process. Reactive recognition, when embedded with the mental attitudes of belief, desire, and intention, leads to a po...
Agents
41784
Assessing Software Libraries by Browsing Similar Classes, Functions and Relationships Comparing and contrasting a set of software libraries is useful for reuse related activities such as selecting a library from among several candidates or porting an application from one library to another. The current state of the art in assessing libraries relies on qualitative methods. To reduce costs and/or assess a large collection of libraries, automation is necessary. Although there are tools that help a developer examine an individual library in terms of architecture, style, etc., we know of no tools that help the developer directly compare several libraries. With existing tools, the user must manually integrate the knowledge learned about each library. Automation to help developers directly compare and contrast libraries requires matching of similar components (such as classes and functions) across libraries. This is different than the traditional component retrieval problem in which components are returned that best match a user's query. Rather, we need to find those component...
IR
508515
Measuring Knowledge with Workflow Management Systems Expert knowledge is captured in the process design. In organisations knowledge becomes embedded in routines, processes, practices as well as norms and can be evaluated by decisions or actions to which it leads, for example measurable efficiencies, speed or quality gains. Knowledge develops over time, through experience that includes what we absorb from courses, books, and mentors as well as informal learning. In this paper we analyse workflow history and demonstrate that workflow management systems enable knowledge measurement. 1
DB
265990
Using Probabilistic Relational Models for Collaborative Filtering Recent projects in collaborative filtering and information filtering address the task of inferring user preference relationships for products or information. The data on which these inferences are based typically consists of pairs of people and items. The items may be information sources (such as web pages or newspaper articles) or products (such as books, software, movies or CDs). We are interested in making recommendations or predictions. Traditional approaches to the problem derive from classical algorithms in statistical pattern recognition and machine learning. The majority of these approaches assume a ”flat ” data representation for each object, and focus on a single dyadic relationship between the objects. In this paper, we examine a richer model that allows us to reason about many different relations at the same time. We build on the recent work on probabilistic relational models (PRMs), and describe how PRMs can be applied to the task of collaborative filtering. PRMs allow us to represent uncertainty about the existence of relationships in the model and allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. 1
IR
208788
Improving the search on the Internet by using WordNet and lexical operators A vast amount of information is available on the Internet, and naturally,  many information gathering tools have been developed. Search engines with  dijerent characteristics, such as Alta Vista, Lycos, Infoseek, and others are  available. However, there are inherent difficulties associated with the task of  retrieving information on the Internet: (1) the web information is diverse and  highly unstructured, (2) the size of information is large and it grows at an  exponential rate. While these two issues are profound and require long term  solutions, still it is possible to develop software around the search engines to  improve the quality of the information retrieved. In this paper we present a  natural language interface system to a search engine. The search improvement  achieved by our system is based on: (1) a query extension using WordNet and  (2) the use of new lexical operators that replace the classical boolean opera-  tors used by current search engines. Several tests have been performed using  the TIPSTER topics collection, provided at the 6lb Text Retrieval Conference  (TREC-6); the results obtained are presented and discussed.
IR
han99efficient
Efficient Mining of Partial Periodic Patterns in Time Series Database Partial periodicity search, i.e., search for partial periodic patterns in time-series databases, is an interesting data mining problem. Previous studies on periodicity search mainly consider finding full periodic patterns, where every point in time contributes (precisely or approximately) to the periodicity. However, partial periodicity is very common in practice since it is more likely that only some of the time episodes may exhibit periodic patterns. We present several algorithms for efficient mining of partial periodic patterns, by exploring some interesting properties related to partial periodicity, such as the Apriori property and the max-subpattern hit set property, and by shared mining of multiple periods. The max-subpattern hit set property is a vital new property which allows us to derive the counts of all frequent patterns from a relatively small subset of patterns existing in the time series. We show that mining partial periodicity needs only two scans over the time series database, even for mining multiple periods. The performance study shows our proposed methods are very efficient in mining long periodic patterns.
ML
yang00temporal
Temporal View Self-Maintenance . View self-maintenance refers to maintaining materialized views without accessing base data. Self-maintenance is particularly useful in data warehousing settings, where base data comes from sources that may be inaccessible. Selfmaintenance has been studied for nontemporal views, but is even more important when a warehouse stores temporal views over the history of source data, since the source history needed to perform view maintenance may no longer exist. This paper tackles the self-maintenance problem for temporal views. We show how to derive auxiliary data to be stored at the warehouse so that the warehouse views and auxiliary data can be maintained without accessing the sources. The temporal view self-maintenance problem is considerably harder than the nontemporal case because a temporal view may need to be maintained not only when source data is modified but also as time advances, and these two dimensions of change interact in subtle ways. We also seek to minimize the amount of au...
DB
schein01generative
Generative Models for Cold-Start Recommendations Systems for automatically recommending items (e.g., movies, products, or information) to users are becoming increasingly important in e-commerce applications, digital libraries, and other domains where personalization is highly valued. Such recommender systems typically base their suggestions on (1) collaborative data encoding which users like which items, and/or (2) content data describing item features and user demographics. Systems that rely solely on collaborative data fail when operating from a cold start---that is, when recommending items (e.g., first-run movies) that no member of the community has yet seen. We develop several generative probabilistic models that circumvent the cold-start problem by mixing content data with collaborative data in a sound statistical manner. We evaluate the algorithms using MovieLens movie ratings data, augmented with actor and director information from the Internet Movie Database. We find that maximum likelihood learning with the expectation maximization (EM) algorithm and variants tends to overfit complex models that are initialized randomly. However, by seeding parameters of the complex models with parameters learned in simpler models, we obtain greatly improved performance. We explore both methods that exploit a single type of content data (e.g., actors only) and methods that leverage multiple types of content data (e.g., both actors and directors) simultaneously.
IR
brin98what
What can you do with a Web in your Pocket? The amount of information available online has grown enormously over the past decade. Fortunately, computing power, disk capacity, and network bandwidth have also increased dramatically. It is currently possible for a university research project to store and process the entire World Wide Web. Since there is a limit on how much text humans can generate, it is plausible that within a few decades one will be able to store and process all the human-generated text on the Web in a shirt pocket. The Web is a very rich and interesting data source. In this paper, we describe the Stanford WebBase, a local repository of a significant portion of the Web. Furthermore, we describe a number of recent experiments that leverage the size and the diversity of the WebBase. First, we have largely automated the process of extracting a sizable relation of books (title, author pairs) from hundreds of data sources spread across the World Wide Web using a technique we call Dual Iterative Pattern Relation Extraction. Second, we have developed a global ranking of Web pages called PageRank based on the link structure of the Web that has properties that are useful for search and navigation. Third, we have used PageRank to develop a novel search engine called Google, which also makes heavy use of anchor text. All of these experiments rely significantly on the size and diversity of the WebBase. 1
IR
bechhofer01oiled
OilEd: a Reason-able Ontology Editor for the Semantic Web Ontologies will play a pivotal r ole in the "Semantic  Web", where they will provide a source of precisely  defined terms that can be communicated across  people and applications. OilEd, is an ontology editor  that has an easy to use frame interface, yet at the  same time allows users to exploit the full power of  an expressive web ontology language (OIL). OilEd  uses reasoning to support ontology design, facilitating  the development of ontologies that are both  more detailed and more accurate.
IR
degiacomo99incremental
An Incremental Interpreter for High-Level Programs with Sensing Like classical planning, the execution of high-level agent programs requires a reasoner to look all the way to a final goal state before even a single action can be taken in the world. This deferral is a serious problem in practice for large programs. Furthermore, the problem is compounded in the presence of sensing actions which provide necessary information, but only after they are executed in the world. To deal with this, we propose (characterize formally in the situation calculus, and implement in Prolog) a new incremental way of interpreting such high-level programs and a new high-level language construct, which together, and without loss of generality, allow much more control to be exercised over when actions can be executed. We argue that such a scheme is the only practical way to deal with large agent programs containing both nondeterminism and sensing. Introduction  In (De Giacomo, Lesperance, & Levesque 1997) it was argued that when it comes to providing high level control to...
Agents
226296
A Social Semantics for Agent Communication Languages . The ability to communicate is one of the salient properties  of agents. Although a number of agent communication languages (ACLs)  have been developed, obtaining a suitable formal semantics for ACLs remains  one of the greatest challenges of multiagent systems theory. Previous  semantics have largely been mentalistic in their orientation and  are based solely on the beliefs and intentions of the participating agents.  Such semantics are not suitable for most multiagent applications, which  involve autonomous and heterogeneous agents, whose beliefs and intentions  cannot be uniformly determined. Accordingly, we present a social  semantics for ACLs that gives primacy to the interactions among the  agents. Our semantics is based on social commitments and is developed  in temporal logic. This semantics, because of its public orientation, is  essential to providing a rigorous basis for multiagent protocols.  1 Introduction  Interaction among agents is the distinguishing property of multia...
Agents
43223
View-based Interpretation of Real-time Optical Flow for Gesture Recognition We have developed a real-time, view-based gesture recognition system. Optical flow is estimated and segmented into motion blobs. Gestures are recognized using a rule-based technique based on characteristics of the motion blobs such as relative motion and size. Parameters of the gesture (e.g., frequency) are then estimated using context specific techniques. The system has been applied to create an interactive environment for children. 1 Introduction  For many applications, the use of hand and body gestures is an attractive alternative to the cumbersome interface devices for human-computer interaction. This is especially true for interacting in virtual reality environments, where the user is no longer confined to the desktop and should be able to move around freely. While special devices can be worn to achieve these goals, these can be expensive and unwieldy. There has been a recent surge in computer vision research to provide a solution that doesn't use such devices. This paper describe...
HCI
220216
Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy ofsmaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics|as a subroutine hierarchy|and a declarative semantics|as a representation of the value function of a hierarchical policy. MAXQ uni es and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and de ne subtasks that achieve these subgoals. By de ning such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the
Agents
449668
Agents That Talk Back (Sometimes): Filter Programs for Affective Communication This paper introduces a model of interaction between  users and animated agents as well as inter-agent interaction  that supports basic features of affective conversation. As essential requirements for animated agents' capability to engage in and exhibit affective communication  we motivate reasoning about emotion and emotion expression, personality, and social role awareness. The main contribution of our paper is the discussion of  so-called `filter programs' that may qualify an agent's expression of its emotional state by its personality and the social context. All of the mental concepts that determine  emotion expression, such as emotional state, personality, standards, and attitudes, have associated  intensities for fine-tuning the agent's reactions in user-adapted  environments.  
Agents
chakrabarti01integrating
Integrating the Document Object Model with Hyperlinks for Enhanced Topic Distillation and Information Extraction Topic distillation is the process of finding authoritative Web pages a comprehensive "hubs" which reciprocally endorse each other and are relevant to a given query. Hyperlink-based topic distillation has been traditionally applied to a macroscopic Web model where documents are nodes in a directed graph and hyperlinks are edges.Mas.M::[KP models miss va lua44 clues such aba4'::M na viga::M paa els,as templa]M2'0]K inclusions, whicha: embedded in HTML paLM using ma0KP taKP Consequently, results of ma:]6:1M2' distillaKP] atillaKP have been deterioraKP] inqua:1 ya s Webpa0: a becoming more complex. We propose a uniformfine-gra'K] model for the Web in which pa:] a represented by theirta trees (aes caesM their Document Object Models or DOMs)aM these DOM trees ar interconnected by ordinaM hyperlinks. Surprisingly, ma]6:[M2K' distillaKKP atillaKK do not work in the finegra -M: scena:]6 We present a new awM0PK1P suitaK1 for the fine-gra2K0 model. It can dis-aggregate hubs into coherent regions by segmenting their DO trees.utua endorsement between hubs as aM0[1['M2K involve these regions, rans, tha single nodes representing complete hubs. Anecdotesae meatesMP' ts using a 28-query, 366000-document benchmark suite, used in ea0]K4 topic distilla[M2 reseai h, reveal two benefits from the new aM:0KK6M2 distillastion quati y improves, a,a by-product of distillation is the aeM14 y to extra0 relevat snippets from hubs which a: nonly payM40[K relevant to the query.
IR
bartoli00online
Online Reconfiguration in Replicated Databases Based on Group Communication Use of group communication to support replication in database systems has proven to be an attractive alternative to traditional replica control schemes. And various replica control protocols have been developed that use the ordering and reliability semantics of group communication primitives to simplify database system design and to improve performance. Although current solutions are able to mask site failures effectively, they are unable to cope with recovery of failed sites, merging of partitions, or joining of new sites. This paper addresses this important issue and proposes efficient solutions for online system reconfiguration providing new sites with a current state of the database without interrupting transaction processing in the rest of the system. We present various alternatives that can match the needs of different operating environments. We analyze the implications of long and complex reconfigurations on applications such as replicated databases, and argue that their developement may be greatly simplified by extended forms of group communications.
DB
grundy01software
Software Tools Software is growing ever-more complex and new software processes, methods and products put greater demands on software engineers than ever before. The support of appropriate software tools is essential for developers to maximise their ability to effectively and efficiently deliver quality software products. This article surveys current practice in the software tools area, along with recent and expected near-future trends in software tools development. We provide a summary of tool applications during the software lifecycle, but focus on particular aspects of software tools that have changed in recent years and are likely to change in the near future as tools continue to evolve. These include the internal structure of tools, provision of multiple view interfaces, tool integration techniques, collaborative work support and the increasing use of automated assistance within tools. We hope this article will both inform software engineering practitioners of current research trends, and tool researchers of the relevant state-of-the-art in commercial tools and various likely future research trends in tools development.
DB
rosales00hand
3D Hand Pose Reconstruction Using Specialized Mappings A system for recovering 3D hand pose from monocular color sequences is proposed. The system employs a non-linear supervised learning framework, the specialized mappings architecture (SMA), to map image features to likely 3D hand poses. The SMA's fundamental components are a set of specialized forward mapping functions, and a single feedback matching function. The forward functions are estimated directly from training data, which in our case are examples of hand joint configurations and their corresponding visual features. The joint angle data in the training set is obtained via a CyberGlove, a glove with 22 sensors that monitor the angular motions of the palm and fingers. In training, the visual features are generated using a computer graphics module that renders the hand from arbitrary viewpoints given the 22 joint angles. The viewpoint is encoded by two real values, therefore 24 real values represent a hand pose. We test our system both on synthetic sequences and on sequences taken with a color camera. The system automatically detects and tracks both hands of the user, calculates the appropriate features, and estimates the 3D hand joint angles and viewpoint from those features. Results are encouraging given the complexity of the task.
HCI
ehmann01accurate
Accurate and Fast Proximity Queries Between Polyhedra Using Convex Surface Decomposition The need to perform fast and accurate proximity queries arises frequently in physically-based modeling, simulation,  animation, real-time interaction within a virtual environment, and game dynamics. The set of proximity  queries include intersection detection, tolerance verification, exact and approximate minimum distance computation,  and (disjoint) contact determination. Specialized data structures and algorithms have often been designed  to perform each type of query separately. We present a unified approach to perform any of these queries seamlessly  for general, rigid polyhedral objects with boundary representations which are orientable 2-manifolds. The  proposed method involves a hierarchical data structure built upon a surface decomposition of the models. Furthermore,  the incremental query algorithm takes advantage of coherence between successive frames. It has been  applied to complex benchmarks and compares very favorably with earlier algorithms and systems.  1. 
AI
348038
Hierarchical Discriminant Regression The main motivation of this paper is to propose a new classification and regression method for challenging high dimensional data. The proposed new technique casts classification problems (class labels as output) and regression problems (numeric values as output) into a unified regression problem. This unified view enables classification problems to use numeric information in the output space that is available for regression problems but are traditionally not readily available for classification problems -- distance metric among clustered class labels for coarse and fine classifications. A doubly clustered subspace-based hierarchical discriminating regression (HDR) method is proposed in this work. The major characteristics include: (1) Clustering is performed in both output space and input space at each internal node and thus the term "doubly clustered." Clustering in the output space provides virtual labels for computing clusters in the input space. (2) Discriminants in the input spa...
ML
199764
Hunting moving targets: an extension to Bayesian methods in multimedia databases It has been widely recognised that the difference between the level of abstraction of the formulation of a query (by example) and that of the desired result (usually an image with certain semantics) calls for the use of learning methods that try to bridge this gap. Cox et al. have proposed a Bayesian method to learn the user's preferences during each query.  Cox et al.'s system, PicHunter,  1  is designed for optimal performance when the user is searching for a fixed target image. The performance of the system was evaluated using target testing, which ranks systems according to the number of interaction steps required to find the target, leading to simple, easily reproducible experiments.  There are some aspects of image retrieval, however, which are not captured by this measure. In particular, the possibility of query drift (i.e. a moving target) is completely ignored. The algorithm proposed by Cox et al. does not cope well with a change of target at a late query stage, because it is ...
ML
santini99similarity
Similarity Measures With complex multimedia data, we see the emergence of database systems in which the fundamental operation is similarity assessment. Before database issues can be addressed, it is necessary to give a definition of similarity as an operation. In this paper we develop a similarity measure, based on fuzzy logic, that exhibit several features that match experimental findings in humans. The model is dubbed Fuzzy Feature Contrast (FFC) and is an extension to a more general domain of the Feature Contrast model due to Tversky. We show how the FFC model can be used to model similarity assessment from fuzzy judgment of properties, and we address the use of fuzzy measures to deal with dependencies among the properties. 1 Introduction  Comparing two images, or an image and a model, is the fundamental operation for many Visual Information Retrieval systems. In most systems of interest, a simple pixel-by-pixel comparison won't do: the difference that we determine must bear some correlation with the p...
IR
schmidt00novel
A Novel Sensor for Dynamic Tactile Information We present a novel tactile sensor, which is useful for dextrous grasping with a simple robot gripper. The novel part consists of an array of capacitive sensors, which couple to the object by means of little brushes of fibers. These sensor elements are very sensitive (with a threshold of about 5 mN) but robust enough not to be damaged during grasping. They yield two types of dynamical tactile information corresponding roughly to two types of tactile sensors in the human skin. The complete sensor consists of a foil-based static force sensor, which yields the total force and the center of the two-dimensional force distribution and is surrounded by an array of the dynamical sensor elements. One such sensor has been mounted on each of the two gripper jaws of our humanoid robot and equipped with the necessary read-out electronics and a CAN bus interface. As first applications we describe experiments to evaluate the quality of a grip using the sensor measurements and a utility that allows to ...
ML
cummins99language
Language Identification From Prosody Without Explicit Features Most current language identification (LID) systems make little or no use of prosodic information, despite the importance of prosody in LID by humans. The greatest obstacle has been that of finding an appropriate feature set which captures linguistically relevant prosodic information. The only system to attempt LID entirely on the basis of prosodic variables uses a set of over 200 features which are selected and combined in a task-specific manner [12]. We apply a novel recurrent neural network model to the task of pairwise discrimination among languages. Network inputs are limited to delta-F 0 and the first difference of the band limited amplitude envelope. Initial results are based on all pairwise combinations of English, German, Japanese, Mandarin and Spanish, with 90 speakers per language. Keywords: Language identification, Recurrent neural networks, prosody 1. PROSODY AND LANGUAGE IDENTIFICATION Most current approaches to automatic language identification use some form of segment re...
ML
sun01world
World Wide Web Information Retrieval Using Web Connectivity Information Gathering, processing and distributing information from the World Wide Web will be a vital technology for the next century. Web search techniques have played a critical role in the development of information systems. Due to the diverse nature of web documents, traditional search techniques must be improved. Hyperlink structure based methods have proved to be powerful ways of exploring the relationships between web documents. In this project, a prototype web search engine was developed to exploit the link structure of web documents, based on the use of the Companion algorithm. The prototype consists of a web spider, local database, and search software. The system was written using the Java programming language. Our spider crawls and downloads web pages using Lynx, then saves the hyperlinks into an Oracle database. JDBC is used to implement the database processing. Search software makes a vicinity graph for the query URL and returns the most related pages after calculating the hub and authority weights. Finally, HTML web pages provide user interfaces and communicate with CGI using the Perl language.  iii  ACKNOWLEDGMENTS The author would like to express thanks to all of the members of his M.S. committee for their useful comments on the thesis, assistance in scheduling the defense date and kind help during the final defense period. The author would like to express his deepest appreciation to Dr. Wen-Chen Hu, his thesis mentor, for the depth of the training and the appropriate guidance he has provided. The author would also like to acknowledge the Department of Computer Science and Software Engineering of Auburn University for financial support. Finally, thanks especially go to the author's wife Qifang, his son, Alex, and his father and mother for their support and love. ...
IR
witten99greenstone
Greenstone: A Comprehensive Open-Source Digital Library Software System This paper describes the Greenstone digital library software, a comprehensive, open-source system for the construction and presentation of information collections. Collections built with Greenstone offer effective full-text searching and metadata-based browsing facilities that are attractive and easy to use. Moreover, they are easily maintainable and can be augmented and rebuilt entirely automatically. The system is extensible: software “plugins ” accommodate different document and metadata types.
IR
huang01architecture
An Architecture For Web Agents In this paper we propose an extended BDI architecture for web agents. The architecture is general, it covers 2D web agent with text based interfaces for retrieval services as well as 3D web agents like avatar-embodied guides that help visitors to navigate in virtual environments. Furthermore, we define the primitives of sensor/effector of web agents, and show how those different types of web agents can be implemented, based on the general architecture.  
Agents
26484
A Process-Oriented Heuristic for Model Selection Current methods to avoid overfitting are either data-oriented (using separate data for validation) or representation-oriented (penalizing complexity in the model). This paper proposes process-oriented evaluation, where a model's expected generalization error is computed as a function of the search process that led to it. The paper develops the necessary theoretical framework, and applies it to one type of learning: rule induction. A process-oriented version of the CN2 rule learner is empirically compared with the default CN2. The process-oriented version is more accurate in a large majority of the datasets, with high significance, and also produces simpler models. Experiments in artificial domains suggest that processoriented evaluation is particularly useful in high-dimensional domains. 1 INTRODUCTION  Overfitting avoidance is often considered the central problem of machine learning (e.g., (Cheeseman & Oldford, 1994)). If a learner is sufficiently powerful, it must guard against selec...
ML
459887
L-Fuzzy Valued Inclusion Measure, L-Fuzzy Similarity and L-Fuzzy Distance The starting point of this paper is the introduction of a new measure of inclusion of fuzzy set A in fuzzy set B. Previously used inclusion measures take values in the interval [0,1]; the inclusion measure proposed here takes values in a Boolean lattice. In other words, inclusion is viewed as an Lfuzzy valued relation between fuzzy sets. This relation is reflexive, antisymmetric and transitive, i.e. it is a fuzzy order relation; in addition it possesess a number of properties which various authors have postulated as axiomatically appropriate for an inclusion measure. We also define an L-fuzzy valued measure of similarity between fuzzy sets and and an L-fuzzy valued distance function between fuzzy sets; these possess properties analogous to the ones of real-valued similarity and distance functions. Keywords: Fuzzy Relations, inclusion measure, subsethood, L-fuzzy sets, similarity, distance, transitivity. 1
ML
49194
Modelling Rational Inquiry in Non-Ideal Agents The construction of rational agents is one of the goals that has been pursued in Artificial Intelligence (AI). In most of the architectures that have been proposed for this kind of agents, its behaviour is guided by its set of beliefs. In our work, rational agents are those systems that are permanently engaged in the process of rational inquiry; thus, their beliefs keep evolving in time, as a consequence of their internal inference procedures and their interaction with the environment. Both AI researchers and philosophers are interested in having a formal model of this process, and this is the main topic in our work. Beliefs have been formally modelled in the last decades using doxastic logics. The possible worlds model and its associated Kripke semantics provide an intuitive semantics for these logics, but they seem to commit us to model agents that are logically omniscient and perfect reasoners. We avoid these problems by replacing possible worlds by conceivable situations,  which ar...
Agents
mao01adaptivefp
Adaptive-FP: An Efficient And Effective Method For Multi-Level Multi-Dimensional Frequent Pattern Mining Real life transaction databases usually contain both item information and dimension information. Moreover, taxonomies about items likely exist. Knowledge about multilevel and multi-dimensional frequent patterns is interesting and useful. The classic frequent pattern mining algorithms based on a uniform minimum support, such as Apriori and FP-growth, either miss interesting patterns of low support or suffer from the bottleneck of itemset generation. Other frequent pattern mining algorithms, such as Adaptive Apriori, though taking various supports, focus mining at a single abstraction level. Furthermore, as an Apriori-based algorithm, the efficiency of Adaptive Apriori suffers from the multiple database scans. In this thesis, we extend FP-growth to attack the problem of multi-level multidimensional frequent pattern mining. We call our algorithm Ada-FP, which stands for Adaptive FP-growth. The efficiency of our Ada-FP is guaranteed by the high scalability of FP-growth. To increase the effectiveness, our Ada-FP pushes various support constraints into the mining process. First, item taxonomy has been explored. Our Ada-FP can discover both inter-level frequent patterns and intra-level frequent patterns. Second, in our Ada-FP, dimension information has been taken into account. We show that our Ada-FP is more flexible at capturing desired knowledge than previous studies.  
DB
terveen98evaluating
Evaluating Emergent Collaboration on the Web Links between web sites can be seen as evidence of a type of emergent collaboration among web site authors. We report here on an empirical investigation into emergent collaboration. We developed a webcrawling algorithm and tested its performance on topics volunteered by 30 subjects. Our findings include:  . Some topics exhibit emergent collaboration, some do not. The presence of commercial sites reduces collaboration.  . When sites are linked with other sites, they tend to group into one large, tightly connected component.  . Connectivity can serve as the basis for collaborative filtering. Human experts rate connected sites as significantly more relevant and of higher quality.  Keywords  Social filtering, collaborative filtering, computer supported cooperative work, human computer interaction, information access, information retrieval  INTRODUCTION  The field of CSCW sees collaboration as involving people who know they are working together, e.g., to edit a document, to carry out a soft...
HCI
roubos00compact
Compact Fuzzy Models Through Complexity Reduction and Evolutionary Optimization Genetic Algorithms (GAs) and other evolutionary optimization  methods to design fuzzy rules from data for systems modeling and  classification have received much attention in recent literature. We show  that different tools for modeling and complexity reduction can be favorably  combined in a scheme with GA-based parameter optimization. Fuzzy clustering,  rule reduction, rule base simplification and constrained genetic optimization  are integrated in a data-driven modeling scheme with low human  intervention. Attractive models with respect to compactness, transparency  and accuracy, are the result of this symbiosis.  I. INTRODUCTION  We focus on learning fuzzy rules from data with low human intervention. Many tools to initialize, tune and manipulate fuzzy models have been developed. We show that different tools can be favorably combined to obtain compact fuzzy rule-based models of low complexity with still good approximation accuracy. A modeling scheme is presented that combine four pr...
ML
94992
Autonomous Cyber Agents: Rules For Collaboration A cyber agent is any program, machine or person engaged in computer-enabled work. Thus, cyber agents can vary considerably in complexity and intelligence. Can they, despite their variety, be organized to collaborate effectively ? Both empirical evidence and theory suggest that they can. Moreover, there seem to be simple rules for designing problem-solving organizations in which collaboration among cyber agents is automatic and scale-effective (adding agents tends to improve solution-quality; adding computers tends to improve solution-speed). This paper develops some of these rules.  1. INTRODUCTION  Computer networks make it possible to interconnect and therefore, organize, large numbers of distributed cyber agents, varying in type from simple programs to skilled humans. Our goal is to develop a class of organizations in which such agents can collaborate easily and effectively. More specifically, our goal is to develop methods for routinely solving arbitrary instances of the following ...
Agents
roy00integration
Integration Of Speech And Vision Using Mutual Information We are developing a system which learns words from co-occurring spoken and visual input. The goal is to automatically segment continuous speechatword boundaries without a lexicon, and to form visual categories which correspond to spoken words. Mutual information is used to integrate acoustic and visual distance metrics in order to extract an audio-visual lexicon from raw input. Wereport  results of experiments with a corpus of infant-directed speech and images.  1. INTRODUCTION  We are developing systems which learn words from co-occurring  audio and visual input [5, 4]. Input consists of naturally spoken mutliword utterances paired with visual representations of object shapes (Figure 1). Output of the system is an audio-visual lexicon of sound-shape associations which encode acoustic forms of words (or phrases) and their visually grounded referents. We assume that, in general, the audio and visual signals are uncorrelated in time. However, when a wordisspoken, its visual representatio...
ML
7537
Using Reinforcement Learning to Spider the Web Efficiently Consider the task of exploring the Web in order to find pages of a particular kind or on a particular topic. This task arises in the construction of search engines and Web knowledge bases. This paper argues that the creation of efficient web spiders is best framed and solved by reinforcement learning, a branch of machine learning that concerns itself with optimal sequential decision making. One strength of reinforcement learning is that it provides a formalism for measuring the utility of actions that give benefit only in the future. We present an algorithm for learning a value function that maps hyperlinks to future discounted reward by using naive Bayes text classifiers. Experiments on two real-world spidering tasks show a three-fold improvement in spidering efficiency over traditional breadth-first search, and up to a two-fold improvement over reinforcement learning with immediate reward only.  Keywords: reinforcement learning, text classification, World Wide Web, spidering, crawlin...
ML
73094
Using Case-Based Learning to Improve Genetic-Algorithm-Based Design Optimization In this paper we describe a method for improving genetic-algorithm-based optimization using case-based learning. The idea is to utilize the sequence of points explored during a search to guide further exploration. The proposed method is particularly suitable for continuous spaces with expensive evaluation functions, such as arise in engineering design. Empirical results in two engineering design domains and across different representations demonstrate that the proposed method can significantly improve the efficiency and reliability of the GA optimizer. Moreover, the results suggest that the modification makes the genetic algorithm less sensitive to poor choices of tuning parameters such as mutation rate. 1 Introduction  Genetic Algorithms (GAs) [ Goldberg 1989 ] are search algorithms that simulate the process of natural selection. GAs attempt to find a good solution to some problem (e.g., finding the maximum of a function) by randomly generating a collection ("population") of potential...
ML
537064
Enhancing the Sense of Other Learners in Student-Centred Web-Based Education Student-centred learning can be used in Web-courses to increase student activity, motivation and commitment. EDUCO is a system for student-centred learning, both for the learners and the teachers. Students can use EDUCO within a standard web-browser to navigate towards useful information and Web-resources gathered into the system. The key issue is that every participant can see everyone else in the system and their navigational steps, so that the feeling of student companions taking part in the same tasks is increased. The implications of this type of social navigation are discussed along with the description of the system itself. 1. 
ML
boyack01information
Information Visualization, Human-Computer Interaction, and Cognitive Psychology: Domain Visualizations Digital libraries stand to benefit from technology insertions from the fields of information visualization, human-computer interaction, and cognitive psychology, among others. However, the current state of interaction between these fields is not well understood. We use our knowledge visualization tool, VxInsight
HCI
dam01antitonic
Antitonic Logic Programs We propose a framework which extends Antitonic Logic Programs [3] to an arbitrary complete bilattice of truth-values, where belief and doubt are explicitly represented. Based on Fitting's ideas, this framework allows a precise definition of important operators found in logic programming such as explicit negation and the default negation. In particular, it leads to a natural integration of explicit negation with the default negation through the coherence principle [20]. According to this principle, the explicit negation entails the default negation. We then define Coherent Answer Sets, and the Paraconsistent Well-founded Model semantics, generalizing paraconsistent semantics for logic programs (for instance, WFSXp [2]). Our framework is an extension of Antitonic Logic Programs in the most cases, and is general enough to capture Probabilistic Deductive Databases, Possibilistic Logic Programming, Hybrid Probabilistic Logic Programs, and Fuzzy Logic Programming. Thus, we have a powerful mathematical formalism for dealing with default reasoning, paraconsistency, and uncertainty. We illustrate its adumbration of paraconsistency with an embedding of WFSXp into our framework.
ML
ashbrook02learning
Learning Significant Locations and Predicting User Movement with GPS Wearable computers have the potential to act as intelligent agents in everyday life and assist the user in a variety of tasks depending on the context. Location is the most common form of context used by these agents to determine the user's task. However, another potential use is the creation of a predictive model of the user's future movements. We present a system that automatically clusters GPS data taken over an extended period of time into meaningful locations at multiple scales. These locations are then incorporated into a Markov model that can be consulted for use with a variety of applications in both single--user and collaborative scenarios.
HCI
witten00building
Building a Public Digital Library Based on Full-Text Retrieval Digital libraries are expensive to create and maintain, and generally restricted  to a particular corporation or group of paying subscribers. While many indexes  to the World Wide Web are freely available, the quality of what is indexed is  extremely uneven. The digital analog of a public library---a reliable, quality,  community service---has yet to appear. This paper demonstrates the feasibility  of a cost-effective collection of high-quality public-domain information,  available free over the Internet.  One obstacle to the creation of a digital library is the difficulty of providing  formal cataloguing information. Without a title, author and subject database it  seems hard to offer the searching facilities normally available in physical  libraries. Full-text retrieval provides a way of approximating these services  without a concomitant investment of resources. A second is the problem of  finding a suitable corpus of material. Computer science research reports form  the focus of ou...
IR
438351
Awareness and Privacy in Mobile Wearable Computers. IPADS: Interpersonal Awareness Devices An Inter-Personal Awareness Device, or IPAD, is a handheld or wearable device designed to support awareness and collaboration between people who are in physical vicinity of each other. This paper describes three IPAD systems, comparing their characteristics and approaches to the problem of opportunistic meeting. The characteristics are identified according to the Steinfield et al. [SJP99] awareness classification. Some of these mechanisms are discussed and some improvements and ideas are proposed in order to improve the systems.  Key Words: CSCW, Wearable Computers, Awareness, IPAD, Mobility.  1. Introduction  Many studies about collaborative systems are concerned about the use of the desktop workstations and network technologies to provide support for distributed collaborative work and awareness. These systems however, are available on devices that are static and tied to the desk [LH98]. Recently, the development of mobile devices as wearable computers, cellular phones and PDAs (Perso...
HCI
hinze01how
How Does the Observation Strategy Influence the Correctness of Alerting Services? Application elds of alerting services range from digital libraries
IR
sloman98architectures
Architectures and Tools for Human-Like Agents 1  This paper discusses agent architectures which are describable in terms of the "higher level" mental concepts applicable to human beings, e.g. "believes", "desires", "intends" and "feels". We conjecture that such concepts are grounded in a type of information processing architecture, and not simply in observable behaviour nor in Newell's knowledge-level concepts, nor Dennett's "intentional stance." A strategy for conceptual exploration of architectures in design-space and nichespace is outlined, including an analysis of design tradeoffs. The SIM AGENT toolkit, developed to support such exploration, including hybrid architectures, is described briefly.  Keywords:  Architecture, hybrid, mind, emotion, evolution, toolkit.  MENTALISTIC DESCRIPTIONS  The usual motivation for studying architectures is to explain or replicate performance. Another, less common reason, is to account for concepts. This paper discusses "high level" architectures which can provide a systematic non-behavioural c...
DB
73250
Independent Component Analysis: A flexible non-linearity and decorrelating manifold approach Independent Components Analysis finds a linear transformation to variables which are maximally statistically independent. We examine ICA and algorithms for finding the best transformation from the point of view of maximising the likelihood of the data. In particular, we discuss the way in which scaling of the unmixing matrix permits a "static" nonlinearity to adapt to various marginal densities. We demonstrate a new algorithm that uses generalised exponential functions to model the marginal densities and is able to separate densities with light tails. We characterise the manifold of decorrelating matrices and show that it lies along the ridges of high-likelihood unmixing matrices in the space of all unmixing matrices. We show how to find the optimum ICA matrix on the manifold of decorrelating matrices, and as an example use the algorithm to find independent component basis vectors for an ensemble of portraits. 1 Introduction  Finding a natural cooordinate system is an essential first s...
ML
267501
Active User Interfaces The current state of user interfaces for large information spaces imposes an unmanageable cognitive burden upon the user. Determining how to get the right information into the right form with the right tool at the right time has become a monumental task. While advances in graphical user interfaces can partially address this problem, the basic problem of information overload--and under load--can not be solely addressed through development of a better direct manipulation interface to the information space. We survey the state of the art in two research fields, interface agents and user modeling, that address the problem of information overload and under load. First, we take a historical look at how the fields of human-computer interaction and artificial intelligence have viewed interface agent research. Interface agents address the problem of increasing task load by serving as either an assistant or associate, extracting and analyzing relevant information, providing informatio...
Agents
singhal01case
A Case Study in Web Search using TREC Algorithms Web search engines rank potentially relevant pages/sites for a user query. Ranking documents for user queries has also been at the heart of the Text REtrieval Conference (TREC in short) under the label ###### retrieval. The TREC community has developed document ranking algorithms that are known to be the best for searching the document collections used in TREC, which are mainly comprised of newswire text. However, the web search community has developed its own methods to rank web pages/sites, many of which use link structure on the web, and are quite dierentfrom  the algorithms developed at TREC. This study evaluates the performance of a state-of-the-art keyword-based document ranking algorithm (coming out of TREC) on a popular web search task: nding the web page/site of an entity, #### companies, universities, organizations, individuals, etc. This form of querying is quite prevalentonthe web. The results from the TREC algorithms are compared to four commercial web search engines. Results show that for nding the web page/site of an entity, commercial web search engines are notably better than a state-of-the-art TREC algorithm. These results are in sharp contrast to results from several previous studies.  Keywords  Search engines, TREC ad-hoc, keyword-based ranking, linkbased ranking  1. 
IR
carvalho00hybrid
A Hybrid Decision Tree/genetic Algorithm for Coping With the Problem of Small Disjuncts in Data Mining The problem of small disjuncts is a serious  challenge for data mining algorithms. In essence,  small disjuncts are rules covering a small  number of examples. Due to their nature, small  disjuncts tend to be error prone and contribute to  a decrease in predictive accuracy. This paper  proposes a hybrid decision tree/genetic algorithm  method to cope with the problem of small  disjuncts. The basic idea is that examples  belonging to large disjuncts are classified by  rules produced by a decision-tree algorithm,  while examples belonging to small disjuncts  (whose classification is considerably more  difficult) are classified by rules produced by a  genetic algorithm specifically designed for this  task.  1 INTRODUCTION  In the context of the well-known classification task of data mining, the discovered knowledge is often expressed as a set of IF-THEN rules, since this kind of knowledge representation is intuitive for the user. From a logical viewpoint, typically the discovered rules ar...
ML
377458
On-line 3D gesture recognition utilising dissimilarity measures In the field of Human-Computer Interaction (HCI), gesture recognition is becoming increasingly important as a mode of communication, in addition to the more common visual, aural and oral modes, and is of particular interest to designers of Augmentative and Alternative Communication (AAC) systems for people with disabilities. A complete microcomputer system is described, GesRec3D, which facilitates the data acquisition, segmentation, learning, and recognition of 3-Dimensional arm gestures. The gesture data is acquired from a Polhemus electro-magnetic tracker system, where sensors are placed on the finger, wrist and elbow of one arm. Coded gestures are linked to user-defined text, to be typed or spoken by a text-to-speech engine, which is integrated into the system. A segmentation method and an algorithm for classification are both presented, which includes acceptance/rejection thresholds based on intra-class and inter-class dissimilarity measures. Results of recognition hits, confusion ...
HCI
casillas01different
Different approaches to induce cooperation in fuzzy linguistic models under the COR methodology Nowadays, Linguistic Modeling is considered to be one of the most important areas of application for Fuzzy Logic. It is accomplished by linguistic Fuzzy Rule-Based Systems, whose most interesting feature is the interpolative reasoning developed. This characteristic plays a key role in their high performance and is a consequence of the cooperation among the involved fuzzy rules. A new approach that makes good use of this aspect inducing cooperation among rules is introduced in this contribution: the Cooperative Rules methodology. One of its interesting advantages is its flexibility allowing it to be used with dierent combinatorial search techniques. Thus, four specic metaheuristics are considered: simulated annealing, tabu search, genetic algorithms and ant colony optimization. Their good performance is shown when solving a real-world problem.  
ML
529793
S-CREAM -- Semi-automatic CREAtion of Metadata Abstract. Richly interlinked, machine-understandable data constitute the basis for the Semantic Web. We provide a framework, S-CREAM, that allows for creation of metadata and is trainable for a specific domain. Annotating web documents is one of the major techniques for creating metadata on the web. The implementation of S-CREAM, OntoMat supports now the semi-automatic annotation of web pages. This semi-automatic annotation is based on the information extraction component Amilcare. OntoMat extract with the help of Amilcare knowledge structure from web pages through the use of knowledge extraction rules. These rules are the result of a learningcycle based on already annotated pages. 1
IR
464721
Database Schema Matching Using Machine Learning with Feature Selection Schema matching, the problem of finding mappings between the attributes of two  semantically related database schemas, is an important aspect of many database applications  such as schema integration, data warehousing, and electronic commerce.  Unfortunately, schema matching remains largely a manual, labor-intensive process.  Furthermore, the effort required is typically linear in the number of schemas to be  matched; the next pair of schemas to match is not any easier than the previous pair.  In this paper we describe a system, called Automatch, that uses machine learning  techniques to automate schema matching. Based primarily on Bayesian learning, the  system acquires probabilistic knowledge from examples that have been provided by  domain experts. This knowledge is stored in a knowledge base called the attribute  dictionary. When presented with a pair of new schemas that need to be matched (and  their corresponding database instances), Automatch uses the attribute dictionary to  find an optimal matching. We also report initial results from the Automatch project.  1 
DB
baker01heuristic
Heuristic Evaluation of Groupware Based on the Mechanics of Collaboration . Despite the increasing availability of groupware, most systems are  awkward and not widely used. While there are many reasons for this, a  significant problem is that groupware is difficult to evaluate. In particular, there  are no discount usability evaluation methodologies that can discover problems  specific to teamwork. In this paper, we describe how we adapted Nielsen's  heuristic evaluation methodology, designed originally for single user  applications, to help inspectors rapidly, cheaply effectively identify usability  problems within groupware systems. Specifically, we take the `mechanics of  collaboration' framework and restate it as heuristics for the purposes of  discovering problems in shared visual work surfaces for distance-separated  groups.  1. 
HCI
311786
Discovering Comprehensible Classification Rules with a Genetic Algorithm This work presents a classification algorithm based on genetic algorithms (GAs) that discovers comprehensible IF-THEN rules, in the spirit of data mining. The proposed GA has a flexible chromosome encoding where each chromosome corresponds to a classification rule. Although the number of genes (genotype) is fixed, the number of rule conditions (phenotype) is variable. The GA also has specific mutation operators for this chromosome encoding. The algorithm was evaluated on two public domain, realworld data sets (on the medical domains of dermatology and breast cancer).  1 Introduction  This work presents a system based on genetic algorithms (GAs) to perform the task of classification. The system is evaluated in two medical domains: diagnosis of dermatological diseases and prediction of recurrence of breast cancer. The use of GAs in classification is an attempt to effectively exploit the large search space usually associated with classification tasks. The GA presented here was designed ac...
ML
waern98concall
ConCall: An information service for researchers based on EdInfo In this paper, we present new types of web information services, where users and information brokers collaborate in creating a +r...#hqhf#vo/oor information service. Such services impose a novel task on information brokers: they become responsible for maintaining the inference strategies used in user modeling. In return, information brokers obtain more accurate information about user needs, since the adaptivity ensures that user profiles are kept up to date and consistent with what users actually prefer, not only what they say that they prefer. We illustrate the approach by an example application, in which conference calls are collected and distributed to interested readers. Keywords Adaptive Information Services, Intelligent Information Filtering, Agents, WWW, Adaptivity, User Modeling, User Profiling. INTRODUCTION The rapid development of information sources such as the World Wide Web has left readers with an acute problem of information overflow. The problem is not simply one ...
IR
fickas99when
When Cyborgs Meet: Building Communities of Cooperating Wearable Agents This paper ...  Keywords  Wearable computing, personal agents, ...  1 INTRODUCTION  Our modern world/society is characterized by an ever increasing ubiquity/pervasiveness of electronic communication technologies like phone and email. Despite this fact, most human interactions still occur when we physically meet other people. Every day, we encounter a large number of people - friends, colleagues and strangers alike. At places like coffee shops, grocery stores, and offices we interact with people to trade news, tell stories, gossip or exchange goods and services. Often we use these situations  1  to pursue our own goals. For example, we purchase items, coordinate schedules, or make other arrangements when we meet other people.  Wearable computers provide a chance to augment such human every-day interactions and to advance cooperation (Why? Features of wearables: always on, always active, senses environment, proactive -- ability to support user during every-day life, ability to act as use...
HCI
gabbard98usability
Usability Engineering for Virtual Environments through a Framework of Usability Characteristics The goal of much work in virtual environments (VEs) to date has been to produce innovative visual, aural, and haptic technology; until recently, there has been very little user-centered, usability-focused research in VEs. However, there is beginning to be at least some awareness of the need for usability engineering within the VE community, mostly addressing particular parts of the VE usability space. This paper motivates the need for usability engineering methods specifically for VEs and describes a framework of usability characteristics for VEs. It gives a detailed example of use of the framework and supplemental VE usability resources in design and evaluation of a navigation metaphor for a real-world battlefield visualization VE application. Our goal is to increase awareness of the need for VE usability through this framework, which in turn will be used to produce usability engineering methods for development of VEs. KEYWORDS: Virtual environments, virtual reality, usability, usabil...
HCI
185650
An Evolutionary Approach to Case Adaptation . We present a case adaptation method that employs ideas from the field of genetic algorithms. Two types of adaptations, case combination and case mutation, are used to evolve variations on the contents of retrieved cases until a satisfactory solution is found for a new specified problem. A solution is satisfactory if it matches the specified requirements and does not violate any constraints imposed by the domain of applicability. We have implemented our ideas in a computational system called GENCAD, applied to the layout design of residences such that they conform to the principles of feng shui, the Chinese art of placement. This implementation allows us to evaluate the use of GA's for case adaptation in CBR. Experimental results show the role of representation and constraints. 1 Introduction  Many different methods have been proposed for performing the task of case adaptation in CBR. They have been surveyed in several publications, including [1], [2], and [3]. Different approaches ma...
ML
5234
A Case Study on Case-Based and Symbolic Learning (Extended Abstract) )  Stefan Wess and Christoph Globig  University of Kaiserslautern, P.O. Box 3049 D-67653 Kaiserslautern, Germany  fwess, globigg@informatik.uni-kl.de Abstract  Contrary to symbolic learning approaches, which represent a learned concept explicitly, case-based approaches describe concepts implicitly by a pair (CB;sim), i.e. by a measure of similarity sim and a set CB of cases. This poses the question if there are any differences concerning the learning power of the two approaches. In this article we will study the relationship between the case base, the measure of similarity, and the target concept of the learning process. To do so, we transform a simple symbolic learning algorithm (the version space algorithm) into an equivalent case-based variant. The achieved results strengthen the hypothesis of the equivalence of the learning power of symbolic and case-based methods and show the interdependency between the measure used by a case-based algorithm and the target concept. Introduction  I...
ML
hightower01survey
A Survey and Taxonomy of Location Systems for Ubiquitous Computing Emerging mobile computing applications often need to know where  things are physically located. To meet this need, many di#erent location  systems and technologies have been developed. In this paper we present  a the basic techniques used for location-sensing, describe a taxonomy of  location system properties, present a survey of research and commercial  location systems that define the field, show how the taxonomy can be  used to evaluate location-sensing systems, and o#er suggestions for future  research. It is our hope that this paper is a useful reference for researchers  and location-aware application builders alike for understanding and evaluating  the many options in this domain.  1 
HCI
480233
Automatic generation of fuzzy logic rule bases: Examples I Learning fuzzy rule-based systems with genetic algorithms can lead to very useful descriptions of several problems. Many different alternative descriptions can be generated. In many cases, a simple rule base similar to rule bases designed by humans is preferable since it has a higher possibility of being valid in unforeseen cases. Thus, the main idea of this paper is to study the genetic fuzzy rule base learning algorithm FRBL [1] by examples from the machine learning repository [2] and to compare it with some other approaches.
ML
49524
The personal interaction panel - a two-handed interface for augmented reality This paper describes the introduction of a new interaction paradigm to augmented reality applications. The everyday tool handling experience of working with pen and notebooks is extended to create a three dimensional two-handed interface, that supports easy-to-understand manipulation tasks in augmented and virtual environments. In the design step we take advantage from the freedom, given by our very low demands on hardware and augment form and functionality to this device. On the basis of examples from object manipulation, augmented research environments and scientific visualization we show the generality of applicability. Although being in the first stages implementation, we consider the wide spectrum of suitability for different purposes.
HCI
57205
Improved Tracking of Multiple Humans with Trajectory Prediction and Occlusion Modeling A combined 2D, 3D approach is presented that allows for robust tracking of moving bodies in a given environment as observed via a single, uncalibrated video camera. Lowlevel features are often insufficient for detection, segmentation, and tracking of non-rigid moving objects. Therefore, an improved mechanism is proposed that combines lowlevel (image processing) and mid-level (recursive trajectory estimation) information obtained during the tracking process. The resulting system can segment and maintain the tracking of moving objects before, during, and after occlusion. At each frame, the system also extracts a stabilized coordinate frame of the moving objects. This stabilized frame can be used as input to motion recognition modules. The approach enables robust tracking without constraining the system to know the shape of the objects being tracked beforehand; although, some assumptions are made about the characteristics of the shape of the objects, and how they evolve with time. Experim...
AI
ambroszkiewicz98cooperation
Cooperation Mechanisms in a Multi-Agent Distributed Environment In the paper we present our work on design and analysis of agent cooperation in distributed systems. The work is not completed yet, so that some parts of it, especially the formal framework, should be viewed as a preliminary version. Multi-agent systems are represented by BDI-automata, i.e., asynchronous automata composed of non-deterministic agents equipped with mental attitudes like belief, desire, and intentions. These attitudes are acquired by the agents by executing so called mental actions. Behaviours of multi-agent systems are represented by prime event structures. The prime event structure when augmented with utility functions defined on the terminal nodes of the structure may be viewed as games in extensive form defined on local states rather than on global states as in the classical definition. The definition of knowledge, in our framework, captures the change of state due to action executions. A notion of local knowledge-based protocols is defined. A game theory method of backwards induction is applied in order to obtain a very natural definition of rationality in agent's behaviours. All the notions are exemplified using the running example. For this example we construct several cooperation mechanisms for the agents. A team formation mechanism is one of them.
Agents
kohrs99improving
Improving Collaborative Filtering with Multimedia Indexing Techniques to create User-Adapting Web Sites The Internet is evolving from a static collection of hypertext, to a rich assortment of dynamic services and products targeted at millions of Internet users. For most sites it is a crucial matter to keep a close tie between the users and the site.
IR
lienhart02localizing
Localizing and Segmenting Text in Images and Videos Many images---especially those used for page design on web pages---as well as videos contain visible text. If these text occurrences could be detected, segmented, and recognized automatically, they would be a valuable source of high-level semantics for indexing and retrieval. In this paper, we propose a novel method for localizing and segmenting text in complex images and videos. Text lines are identified by using a complex-valued multilayer feed-forward network trained to detect text at a fixed scale and position. The network's output at all scales and positions is integrated into a single text-saliency map, serving as a starting point for candidate text lines. In the case of video, these candidate text lines are refined by exploiting the temporal redundancy of text in video. Localized text lines are then scaled to a fixed height of 100 pixels and segmented into a binary image with black characters on white background. For videos, temporal redundancy is exploited to improve segmentation performance. Input images and videos can be of any size due to a true multiresolution approach. Moreover, the system is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video, so that one text bitmap is created for all instances of that text line. Therefore, our text segmentation results can also be used for object -based video encoding such as that enabled by MPEG-4.
IR
47700
Graph-Theoretic Clustering for Image Grouping and Retrieval Image retrieval algorithms are generally based on the assumption that visually similar images are located close to each other in the feature space. Since the feature vectors usually exist in a very high dimensional space, a parametric characterization of their distribution is impossible, so non-parametric approaches, like the k-nearest neighbor search, are used for retrieval. This paper introduces a graph--theoretic approach for image retrieval by formulating the database search as a graph clustering problem by using a constraint that retrieved images should be consistent with each other (close in the feature space) as well as being individually similar (close) to the query image. The experiments that compare retrieval precision with and without clustering showed an average precision of 0.76 after clustering, which is an improvement by 5.56% over the average precision before clustering.  1. Motivation  Computing feature vectors is an essential step in image database retrieval algorithm...
ML
knight00system
System And Software Visualisation If the entire of what many consider to be the software visualisation field is reviewed, then this article would be much larger in size and also consist mainly of variations on the nodes and arcs theme. Because the use of the third dimension for system and software visualisation is emerging as a viable alternative for the representation of complex artefacts then it was considered much better to focus on this form of software visualisation. The previous techniques had various identified shortcomings and the space and freedom afforded by the extra dimension has the potential to be usefully employed to overcome some of these problems. Software visualisation can be seen as a specialised subset of information visualisation. This is because information visualisation is the process of creating a graphical representation of abstract, generally non-numerical, data. This is exactly what is required when trying to visualise software. The term software visualisation has many ...
HCI
prendinger01social
Social Role Awareness in Animated Agents This paper promotes social role awareness as a desirable capability of animated agents, that are by now strong affective reasoners, but otherwise often lack the social competence observed with humans. In particular, humans may easily adjust their behavior depending on their respective role in a socio-organizational setting, whereas their synthetic pendants tend to be driven mostly by attitudes, emotions, and personality. Our main contribution is the incorporation of `social filter programs' to mental models of animated agents. Those programs may qualify an agent's expression of its emotional state by the social context, thereby enhancing the agent's believability as a conversational partner or virtual teammate. Our implemented system is entirely webbased and demonstrates socially aware animated agents in an environment similar to Hayes-Roth's Cybercaf'e.  Keywords  believability, social agents, human-like qualities of synthetic agents, social dimension in communication, affective reaso...
Agents
244548
Eddies: Continuously Adaptive Query Processing In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...
DB
wang99negation
Negation in Logic and Deductive Databases This thesis studies negation in logic and deductive databases. Among other things, two kinds of negation are discussed in detail: strong negation and nonmonotonic negation.  In the logic part, we have constructed a first-order logic CF  0  of strong negation with bounded quantifiers. The logic is based on constructive logics, in particular, Thomason's logic CF. However, unlike constructive logic, quantifiers in our system as in Thomason's are static rather than dynamic. For the logic CF  0  , the usual Kripke formal semantics is defined but based on situations instead of conventional possible worlds. A sound and complete axiomatic system of CF  0  is established based on the axiomatic systems of constructive logics with strong negation and Thomason's completeness proof techniques. CF  0  is proposed as the underlying logic for situation theory. Thus the connection between CF  0  and infon logic is briefly discussed.  In the database part, based on the study of some main existing semant...
DB
122529
XGobi And XploRe Meet Virgis In this paper we report on a linked environment of the three programs XGobi, XploRe, and ViRGIS. While XGobi and XploRe are statistical packages that focus on dynamic statistical graphics and provide analytical statistical features, respectively, ViRGIS is a 3D Virtual Reality Geographic Information System (GIS) that allows real--time access to, and visualization of, geographic data. The XGobi/XploRe/ViRGIS environment is based on the Remote Procedure Call (RPC) technology previously developed for the ArcView/XGobi/XploRe environment. It allows linked brushing and the exchange of data and commands --- completely transparent to the user.  1. Introduction  In our previous work, we developed an open software system consisting of the Geographic Information System (GIS) ArcView, the dynamic statistical graphics program XGobi, and the statistical computing environment XploRe (Symanzik, Kotter, Schmelzer, Klinke, Cook & Swayne 1998, Symanzik, Cook, Klinke & Lewin 1998). In this current projec...
DB
mark00multiagent
Multiagent Systems Engineering: A Methodology For Analysis And Design Of Multiagent Systems ................................................................................................................................................. IX I. INTRODUCTION ........................................................................................................................................... 1 1.1 Background................................................................................................................................. 2 1.2 Problem....................................................................................................................................... 3 1.3 Goal ............................................................................................................................................ 4 1.4 Assumptions ............................................................................................................................... 4 1.5 Areas of Collaboration.............................................................................
Agents
germans01virpi
VIRPI: A High-Level Toolkit for Interactive Scientific Visualization in Virtual Reality . Research areas that require interactive visualization of simulation data
HCI
bryce99coordination
A Coordination Model for Agents based on Secure Spaces . Shared space coordination models such as Linda are ill-suited  for structuring applications composed of erroneous or insecure components.  This paper presents the Secure Object Space model. In this model,  a data element can be locked with a key and is only visible to a process  that presents a matching key to unlock the element. We give a precise  semantics for Secure Object Space operations and discuss an implementation  in Java for a mobile agent system. An implementation of the  semantics that employs encryption is also outlined for use in untrusted  environments.  1 Introduction  Coordination languages based on shared data spaces have been around for over fifteen years. Researchers have often advocated their use for structuring distributed and concurrent systems because the mode of communication that they provide, sometimes called generative communication, is associative and uncoupled. Communication is associative in that processes do not explicitly name their communication part...
Agents
abowd00charting
Charting Past, Present and Future Research in Ubiquitous Computing The proliferation of computing into the physical world promises more than the ubiquitous availability of computing infrastructure; it suggests new paradigms of interaction inspired by constant access to information and computational capabilities. For the past decade, application-driven research in ubiquitous computing (ubicomp) has pushed three interaction themes: natural interfaces, context-aware applications, and automated capture and access. To chart a course for future research in ubiquitous computing, we review the accomplishments of these efforts and point to remaining research challenges. Research in ubiquitous computing implicitly requires addressing some notion of scale, whether in the number and type of devices, the physical space of distributed computing, or the number of people using a system. We posit a new area of applications research, everyday computing, focussed on scaling interaction with respect to time. Just as pushing the availability of computing away from the traditional desktop fundamentally changes the relationship between humans and computers, providing continuous interaction moves computing from a localized tool to a constant companion. Designing for continuous interaction requires addressing interruption and resumption of interaction, representing passages of time and providing associative storage models. Inherent
HCI
kleinberg98microeconomic
A Microeconomic View of Data Mining We present a rigorous framework, based on optimization, for evaluating data mining operations such as associations and clustering, in terms of their utility in decisionmaking. This framework leads quickly to some interesting computational problems related to sensitivity analysis, segmentation and the theory of games.   Department of Computer Science, Cornell University, Ithaca NY 14853. Email: kleinber@cs.cornell.edu. Supported in part by an Alfred P. Sloan Research Fellowship and by NSF Faculty Early Career Development Award CCR-9701399.  y  Computer Science Division, Soda Hall, UC Berkeley, CA 94720. christos@cs.berkeley.edu  z  IBM Almaden Research Center, 650 Harry Road, San Jose CA 95120. pragh@almaden.ibm.com  1 Introduction  Data mining is about extracting interesting patterns from raw data. There is some agreement in the literature on what qualifies as a "pattern" (association rules and correlations [1, 2, 3, 5, 6, 12, 20, 21] as well as clustering of the data points [9], are ...
ML
aylett00applying
Applying Artificial Intelligence to Virtual Reality: Intelligent Virtual Environments Reearch into virtual environments on the one hand and artificial intelligence and artificial life  on the other has largely been carried out by two different groups of people with different preoccupations  and interests, but some convergence is now apparent between the two fields. Applications  in which activity independent of the user takes place --- involving crowds or other agents --- are  beginning to be tackled, while synthetic agents, virtual humans and computer pets are all areas in  which techniqes from the two fields require strong integration. The two communities have much  to learn from each other if wheels are not to be reinvented on both sides. This paper reviews  the issues arising from combining artificial intelligence and artificial life techniques with those  of virtual environments to produce just such intelligent virtual environments. The discussion is  illustrated with examples that include environments providing knowledge to direct or assist the  user rather than r...
Agents
cruz99user
A User Interface for Distributed Multimedia Database Querying with Mediator Supported Refinement The Delaunay  MM  system supports an interactive, customizable interface for querying multimedia distributed databases, like Digital Libraries. Through this interface, users select virtual document styles that cater the display of query results to their needs, while also offering transparent pre- and post-query refinement and nested querying. Delaunay  MM  's virtual documents preserve context by maintaining a single customizable interface for result viewing. The advanced transparent query features rely on mediation to provide adept access to information. In this paper, we present the framework for Delaunay  MM  , its architecture, the user interface, and results of the first usability study.  1. INTRODUCTION  With an increase in the number of users daily, the World Wide Web has become an indispensable technology. With increasingly diverse user populations and available technologies, the value of reliable searching and information navigation mechanisms is becoming more significant. The...
HCI
526525
ITTALKS: A Case Study in the Semantic Web and DAML . Effective use of the vast quantity of information now available on the web  will require the use of "Semantic Web" markup languages such as the DARPA Agent  Markup Language (DAML). Such languages will enable the automated gathering and  processing of much information that is currently available but insufficiently utilized.  Effectively, such languages will facilitate the integration of multi-agent systems with  the existing information infrastructure. As part of our exploration of Semantic Web  technology, and DAML in particular, we have constructed ITTALKS, a web-based  system for automatic and intelligent notification of information technology talks. In  this paper, we describe the ITTALKS system, and discuss the numerous ways in which  the use of Semantic Web concepts and DAML extend its ability to provide an intelligent  online service to both the human community and the agents assisting them.  1 
IR
mooney00contentbased
Content-Based Book Recommending Using Learning for Text Categorization Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes. Most existing recommender systems use collaborative filtering methods that base recommendations on other users' preferences. By contrast, content-based methods use information about an item itself to make suggestions. This approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization. Initial experimental results demonstrate that this approach can produce accurate recommendations.  KEYWORDS: Recommender systems, information filtering,  machine learning, text categorization  INTRODUCTION  There is a growing interest in recommender systems that suggest music, films, books, and othe...
IR
krasnogor00memetic
A Memetic Algorithm With Self-Adaptive Local Search: TSP as a case study In this paper we introduce a promising hybridization scheme for a Memetic Algorithm (MA). Our MA is composed of two optimization processes, a Genetic Algorithm and a Monte Carlo method (MC). In contrast with other GA-Monte Carlo hybridized memetic algorithms, in our work the MC stage serves two purposes: -- when the population is diverse it acts like a local search procedure and -- when the population converges its goal is to diversify the search. To achieve this, the MC is self-adaptive based on observations from the underlying GA behavior; the GA controls the long-term optimization process. We present preliminary, yet statistically significant, results on the application of this approach to the TSP problem.We also comment it successful application to a molecular conformational problem: Protein Folding.
IR
critchlow98automatic
Automatic Generation of Warehouse Mediators Using an Ontology Engine Data warehouses created for dynamic scientific environments, such as genetics, face significant challenges to their long-term feasibility. One of the most significant of these is the high frequency of schema evolution resulting from both technological advances and scientific insight. Failure to quickly incorporate these modifications will quickly render the warehouse obsolete, yet each evolution requires significant effort to ensure the changes are correctly propagated. DataFoundry utilizes a mediated warehouse architecture with an ontology infrastructure to reduce the maintenance requirements of a warehouse. Among other things, the ontology is used as an information source for automatically generating mediators, the programs that transfer data between the data sources and the warehouse. The identification, definition, and representation of the metadata required to perform this task are the primary contributions of this work.  1 Introduction  The DataFoundry research project at LLNL's ...
DB
vanschooten99report
Report on SIKS course on interactive and multi-agent systems, 30 november - 4 december 1998 Contents  1 Introduction 2 2 Interactive systems 2  2.1 Miscellaneous . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2.1.1 Overview development methods for interactive systems . . . . . . . . . 2 2.1.2 Prototyping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2.1.3 Techniques, tools, development environment, and management . . . . 2 2.2 Techniques per development stage . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2.1 Task analysis and global design specification . . . . . . . . . . . . . . 3 2.2.2 Detailed-design stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2.3 Scenario-based design . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2.4 Evaluation criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2.5 Evaluation techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . 4  3 Multi-agent systems 4  3.1 Agent systems and models . . . . . . . . . . 
Agents
315153
Evolution and Revolutions in LDAP Directory Caches . LDAP directories have recently proliferated with the growth  of the Internet, and are being used in a wide variety of network-based  applications. In this paper, we propose the use of generalized queries,  referred to as query templates, obtained by generalizing individual user  queries, as the semantic basis for low overhead, high benefit LDAP directory  caches for handling declarative queries. We present efficient incremental  algorithms that, given a sequence of user queries, maintain a  set of potentially beneficial candidate query templates, and select a subset  of these candidates for admission into the directory cache. A novel  feature of our algorithms is their ability to deal with overlapping query  templates. Finally, we demonstrate the advantages of template caches  over query caches, with an experimental study based on real data and a  prototype implementation of the LDAP directory cache.  1 Introduction  LDAP (Lightweight Directory Access Protocol) network directories ha...
DB
davison99discoweb
DiscoWeb: Applying Link Analysis to Web Search How often does the search engine of your choice produce results that are less than satisfying, generating endless links to irrelevant pages even though those pages may contain the query keywords? How often are you given pages that tell you things you already know? While the search engines and related tools continue to make improvements in their information retrieval algorithms, for the most part they continue to ignore an essential part of the web – the links. We have found that link analysis can have significant contributions to web page retrieval from search engines, to web community discovery, and to the measurement of web page influence. It can help to rank results and find high-quality index/hub/link pages that contain links to the best sites on the topic of interest. Our work is based on research from IBM’s CLEVER project [7, 4, 6], Stanford’s Google [3], and the Web Archaeology research [2, 1] at Compaq’s Systems Research Center. These research teams have demonstrated some of the contributions that link analysis can make in the web. In our work, we have attempted to generalize and improve upon these approaches. Just as in citation analysis of published works, the most influential documents on the web will have many other documents recommending (pointing to) them. This idea underlies all link analysis efforts, from the straightforward technique of counting the number of incoming edges to a page, to the deeper eigenvector analysis used in our work and in those projects mentioned above. It turns out that the identification of “high-quality ” web pages reduces to a sparse eigenvalue of the adjacency matrix
IR
bergamaschi98intelligent
An Intelligent Approach to Information Integration . Information sharing from multiple heterogeneous sources is a challenging issue which ranges from database to ontology areas. In this paper, we propose an intelligent approach to information integration which takes into account semantic conflicts and contradictions, caused by the lack of a common shared ontology. Our goal is to provide an  integrated access to information sources, allowing a user to pose a single query and to receive a single unified answer. We propose a "semantic" approach for integration where the conceptual schema of each source is provided, adopting a common standard data model and language.  Description Logics plus clustering techniques are exploited. Description Logics is used to obtain a semi-automatic generation of a Common Thesaurus (to solve semantic heterogeneities and to derive a common ontology) while clustering techniques are employed to build the global schema, i.e. the unified view of the data to be used for query processing. keywords: intelligent info...
DB
holden97visual
Visual Recognition of Hand Motion Hand gesture recognition is an active area of research in recent years, being used in various applications from deaf sign recognition systems to humanmachine interaction applications. The gesture recognition process, in general, may be divided into two stages: the motion sensing, which extracts useful data from hand motion; and the classification process, which classifies the motion sensing data as gestures. The existing vision-based gesture recognition systems extract 2-D shape and trajectory descriptors from the visual input, and classify them using various classification techniques from maximum likelihood estimation to neural networks, finite state machines, Fuzzy Associative Memory (FAM) or Hidden Markov Models (HMMs). This thesis presents the framework of the vision-based Hand Motion Understanding (HMU) system that recognises static and dynamic Australian Sign Language (Auslan) signs by extracting and classifying 3-D hand configuration data from the visual input. The HMU system is...
HCI
youll00impulse
Impulse: Location-based Agent Assistance In the physical world, a user experiences products and places, explores physical surroundings, and participates in location-specific activities. Software agents, trapped in their electronic world, offer users valuable assistance online, for example by personalizing searches and queries. The Impulse research project at the MIT Media Lab [1] examines what happens when the rich experience of the physical world is augmented with the low search costs and information resources available through the Internet. This paper presents a subset and implementation of one aspect of the Impulse vision: a scenario demonstrating a mobile device which uses location-aware queries to digitally augment and explore the physical world.  PROJECT OVERVIEW  Related research on learning agents within wireless devices [6] and the combined work of wearable computing and ubiquitous computing [5] explore placing agents into our physical environment. Our work takes these previous explorations and introduces the idea of...
Agents
terrillon98automatic
Automatic Detection of Human Faces in Natural Scene Images by Use of a Skin Color Model and of Invariant Moments We use a skin color model based on the Mahalanobis metric and a shape analysis based on invariant moments to automatically detect and locate human faces in two-dimensional natural scene images. First, color segmentation of an input image is performed by thresholding in a perceptually plausible hue-saturation color space where the effects of the variability of human skin color and the dependency of chrominance on changes in illumination are reduced. We then group regions of the resulting binary image which have been classified as face candidates into clusters of connected pixels. Performing median filtering on the image and discarding the smallest remaining clusters ensures that only a small number of clusters will be used for further analysis. Fully translation-, scale- and in-plane rotation-invariant moments are calculated for each remaining cluster. Finally, in order to distinguish faces from distractors, a multilayer perceptron neural network is used with the invariant moments as the input vector. Supervised learning of the network is implemented with the backpropagation algorithm, at first for frontal views of faces. Preliminary results show the efficiency of the combination of color segmentation and of invariant moments in detecting faces with a large variety of poses and against relatively complex backgrounds. 1.
ML
decker95coordination
Coordination Assistance for Mixed Human and Computational Agent Systems In many application areas (such as concurrent engineering, software development, hospital scheduling, manufacturing scheduling, and military planning), individuals are responsible for an agenda of tasks and face choices about the best way to locally handle each task, in what order to do tasks, and when to do them. Such decisions are often hard to make because of coordination problems: individual tasks are related to the tasks of others in complex ways, and there are many sources of uncertainty (no one has a complete view of the task structure at arbitrary levels of detail, the situation may be changing dynamically, and no one is entirely sure of the outcomes of all of their actions). The focus of this paper is the development of support tools for distributed, cooperative work by groups (collaborative teams) of human and computational agents. We will discuss the design of a set of distributed autonomous computer programs ("agents") that assist people in coordinating their activities by ...
Agents
326373
Grounded Speech Communication Language is grounded in sensory-motor experience. Grounding connects concepts to the physical world enabling humans to acquire and use words and sentences in context. Currently, machines which process text and spoken language are not grounded in human-like ways. Instead, semantic representations in machines are highly abstract and have meaning only when interpreted by humans. We are interested in developing computational systems which represent words, utterances, and underlying concepts in terms of sensory-motor experiences, leading to richer levels of understanding by machines. Inspired by theories of infant cognition, we present a computational model which learns from untranscribed multisensory input. Acquired words are represented in terms associations between acoustic and visual sensory experience. The system has been tested in a robotic embodiment which supports interactive language learning and understanding. Successful learning has also been demonstrated using infant -directed s...
ML
jonker01temporal
Temporal Requirements for Anticipatory Reasoning about Intentional Dynamics in Social Contexts Abstract In this paper a temporal trace language is defined in which formulae can be expressed that provide an external temporal grounding of intentional notions. Justifying conditions are presented that formalise criteria that a (candidate) formula must satisfy in order to qualify as an external representation of a belief, desire or intention. Using these conditions, external represenation formulae for intentional notions can be identified. Using these external representations, anticipatory reasoning about intentional dynamics can be performed. 1
Agents
277897
Data-Driven Theory Refinement Using KBDistAl Knowledge based artificial neural networks offer an attractive approach to extending or modifying incomplete knowledge bases or domain theories through a process of data-driven theory refinement. We present an efficient algorithm for data-driven knowledge discovery and theory refinement using DistAl, a novel (inter-pattern distance based, polynomial time) constructive neural network learning algorithm. The initial domain theory comprising of propositional rules is translated into a knowledge based network. The domain theory is modified using DistAl which adds new neurons to the existing network as needed to reduce classification errors associated with the incomplete domain theory on labeled training examples. The proposed algorithm is capable of handling patterns represented using binary, nominal, as well as numeric (real-valued) attributes. Results of experiments on several datasets for financial advisor and the human genome project indicate that the performance of the proposed algorithm compares quite favorably with other algorithms for connectionist theory refinement (including those that require substantially more computational resources) both in terms of generalization accuracy and network size.  
ML
filliat99incremental
Incremental Evolution of Neural Controllers for Navigation in a 6-legged Robot This paper describes how the SGOCE paradigm has been used within the context of a "minimal simulation " strategy to evolve neural networks controlling locomotion and obstacle-avoidance in a 6-legged robot. Such controllers have been first evolved through simulation and then successfully downloaded on the real robot.
ML
aha97casebased
Case-Based Learning: Beyond Classification of Feature Vectors . The dominant theme of case-based research at recent ML conferences has been on classifying cases represented by feature vectors. However, other useful tasks can be targeted, and other representations are often preferable. We review the recent literature on case-based learning, focusing on alternative performance tasks and more expressive case representations. We also highlight topics in need of additional research. 1 Introduction  The majority of machine learning (ML) research has focussed on supervised learning tasks in which class-labeled cases, each represented as a vector of features, are given to a learning algorithm that induces a concept description. This description can then be used to predict the class labels of unlabeled cases. One approach for solving supervised learning tasks, called case-based,  3  involves storing cases, often as hproblem,solutioni pairs, and retrieving them to solve similar problems. This distinguishes their behavior from approaches that greedily repla...
ML
huang00running
Running the Web Backwards: Appliance Data Services Appliance" digital devices such as handheld cameras, scanners, and microphones generate data that people want to put on Web pages. Unfortunately, numerous complex steps are required. Contrast this with Web output: handheld web browsers enjoy increasing infrastructural support such as user-transparent transformation proxies, allowing unmodified Web pages to be conveniently viewed on devices not originally designed for the task. We hypothesize that the utility of input appliances will be greatly increased if they too were "infrastructure enabled." Appliance Data Services attempts to systematically describe the task domain of providing seamless and graceful interoperability between input appliances and the Web. We offer an application architecture and a validating prototype that we hope will "open up the playing field" and motivate further work. Our initial efforts have identified two main design challenges: dealing with device heterogeneity, and providing a "no-futz" out-of-the-box user experience for novices without sacrificing expressive power for advanced users. We address heterogeneity by isolating device and protocol heterogeneity considerations into a single extensible architectural component, allowing most of the application logic to deal exclusively with Web-friendly protocols and formats. We address the user interface issue in two ways: first, by specifying how to tag input with commands that specify how data is to be manipulated once injected into the infrastructure; second, by describing a late-binding mechanism for these command tags, which allows "natural" extensions of the device's UI for application selection and minimizes the amount of configuration required before end-users benefit from Appliance Data Services. Finally, we describe how to leverage existi...
HCI
376961
Towards an Accommodation of Delay in Temporal Active Databases Business rules can be formulated according to the eventcondition -action structure of triggers in active databases. However, delays in the execution of such rules can cause unexpected and undesired side-effects. While business rules are commonly constructed from an external user's perspective, users often neglect to cater for the cases in which unanticipated sequences of I/O and rule activation events occur. This paper examines this issue from the perspective of temporal databases and discusses a framework for accommodating delay in rule activation. In order to do this the paper also outlines a flexible technique to ensure correct transaction sequencing in transaction-time databases.  1 Introduction  Active database systems allow users to specify business rules, commonly in terms of (sets of) event-condition-action (E-C-A) triplets that specify that certain actions should be invoked when certain events occur and certain conditions hold [17]. Such rules are useful in providing an active...
DB
lin01indexing
Indexing and Retrieving Natural Language Using Ternary Expressions Traditional information retrieval systems based on the "bag-of-words" paradigm cannot completely capture the semantic content of documents. Yet it is impossible with current technology to build a practical information access system that fully analyzes and understands unrestricted natural language. However, if we avoid the most complex and processing-intensive natural language understanding techniques, we can construct a large-scale information access system which is capable of processing unrestricted text, largely understanding it, and answering natural language queries with high precision. We believe that ternary expressions are the most suitable representational structure for such a system; they are expressive enough for information retrieval purposes, yet amenable to rapid large-scale indexing.
IR
jansen00mobile
Mobile Agents In Intrusion Detection And Response Effective intrusion detection capability is an elusive goal, not solved easily or with a single mechanism. However, mobile software agents go a long way toward realizing the ideal behavior desired in an Intrusion Detection System (IDS). This paper is an initial look at the relatively unexplored terrain of using mobile agents for intrusion detection and response. It looks not only at the benefits derived from mobility, but also those associated with software agent technology. We explore these benefits in some detail and propose a number of innovative ways to apply agent mobility to address the shortcomings of current IDS designs and implementations. We also look at new approaches for automating response to an intrusion, once detected. 1  Appeared in the proceedings of the 12th Annual Canadian Information Technology Security Symposium, Ottawa, Canada, June 2000.  MOBILE AGENTS IN INTRUSION DETECTION AND RESPONSE 1 MOBILE AGENTS IN INTRUSION DETECTION AND  RESPONSE  Background  Intrusion...
AI
brafman98knowledge
On the Knowledge Requirements of Tasks In order to successfully perform a task, a situated system requires some information about its domain. If we can understand what information the system requires, we may be able to equip it with more suitable sensors or make better use of the information available to it. These considerations have motivated roboticists to examine the issue of sensor design, and in particular, the minimal information required to perform a task. We show here that reasoning in terms of what the robot knows and needs to know to perform a task is a useful approach for analyzing these issues. We extend the formal framework for reasoning about knowledge, already used in AI and distributed computing, by developing a set of basic concepts and tools for modeling and analyzing the knowledge requirements of tasks. We investigate properties of the resulting framework, and show how it can be applied to robotics tasks. 1 Introduction  The notion of computational complexity has had a profound effect on the development o...
AI
424127
An Integrated Ontology for the WWW . Knowledge-intensive processing of WWW information should  be founded on clear and uniform conceptualisation. An integrated ontology  covering different aspects of the WWW (documents, sites, network  addressing, HTML code) has been laid down, upon which a knowledge  base of the WWW domain is being built. This knowledge base should  support "intelligent" metasearch of the Web, in particular, postprocessing  of hit-lists returned by external search engines.  1 Introduction  During the last few years, the World-Wide Web has become one of the most widespread technologies of information presentation. It is thus not surprising that many Knowledge Engineering (KE) projects focus on it: some use HTML as a cheap, ready-made user-interface, other thrive to mine valuable information hidden inside existing WWW pages. A necessary prerequisite of mutual comprehensibility and knowledge reuse among different KE communities and projects dealing with the Web is a clear and unified conceptualisation, wh...
IR
449621
Signal Detection Using ICA: Application to Chat Room Topic Spotting Signal detection and pattern recognition for online grouping huge amounts of data and retrospective analysis is becoming increasingly important as knowledge based standards, such as XML and advanced MPEG, gain popularity. Independent component analysis (ICA) can be used to both cluster and detect signals with weak a priori assumptions in multimedia contexts. ICA of real world data is typically performed without knowledge of the number of non-trivial independent components, hence, it is of interest to test hypotheses concerning the number of components or simply to test whether a given set of components is significant relative to a "white noise" null hypothesis. It was recently proposed to use the so-called Bayesian information criterion (BIC) approximation, for estimation of such probabilities of competing hypotheses. Here, we apply this approach to the understanding of chat. We show that ICA can detect meaningful context structures in a chat room log file.  1. 
IR
421686
Using Multi-Context Systems to Engineer Executable Agents In the area of agent-based computing there are many proposals  for specific system architectures, and a number of proposals for general  approaches to building agents. As yet, however, there are few  attempts to relate these together, and even fewer attempts to provide  methodologies which relate designs to architectures and then to executable  agents. This paper provides a first attempt to address this  shortcoming; we propose a general method of defining architectures  for logic-based agents which can be directly executed. Our approach is  based upon the use of multi-context systems and we illustrate its use  with an example architecture capable of argumentation-based negotiation.  1 Introduction  Agent-based computing is fast emerging as a new paradigm for engineering complex, distributed systems [13, 27]. An important aspect of this trend is the use of agent architectures as a means of delivering agent-based functionality (cf. work on agent programming languages [14, 23, 25]). In t...
Agents
schwab00learning
Learning to Recommend from Positive Evidence In recent years, many systems and approaches for recommending information, products or other objects have been developed. In these systems, often machine learning methods that need training input to acquire a user interest profile are used. Such methods typically need positive and negative evidence of the user’s interests. To obtain both kinds of evidence, many systems make users rate relevant objects explicitly. Others merely observe the user’s behavior, which fairly obviously yields positive evidence; in order to be able to apply the standard learning methods, these systems mostly use heuristics that attempt to find also negative evidence in observed behavior. In this paper, we present several approaches to learning interest profiles from positive evidence only, as it is contained in observed user behavior. Thus, both the problem of interrupting the user for ratings and the problem of somewhat artificially determining negative evidence are avoided. The learning approaches were developed and tested in the context of the Web-based ELFI information system. It is in real use by more than 1000 people. We give a brief sketch of ELFI and describe the experiments we made based on ELFI usage logs to evaluate the different proposed methods.
IR
505987
Multi-agent Systems as Intelligent Virtual Environments Intelligent agent systems have been the subject of intensive research  over the past few years; they comprise one of the most promising computing  approaches ever, able to address issues that require abstract modelling and  higher level reasoning. Virtual environments, on the other hand, offer the ideal  means to produce simulations of the real world for purposes of entertainment,  education, and others. The merging of these two fields seems to have a lot to  offer to both research and applications, if progress is made on a co-ordinated  manner and towards standardization. This paper is a presentation of VITAL, an  intelligent multi-agent system able to support general-purpose intelligent virtual  environment applications.
Agents
hustadt00normal
Normal Forms and Proofs in Combined Modal and Temporal Logics . In this paper we present a framework for the combination of  modal and temporal logic. This framework allows us to combine different  normal forms, in particular, a separated normal form for temporal  logic and a first-order clausal form for modal logics. The calculus of the  framework consists of temporal resolution rules and standard first-order  resolution rules.  We show that the calculus provides a sound, complete, and terminating  inference systems for arbitrary combinations of subsystems of multimodal   S5 with linear, temporal logic.  1 
DB
baulier00datablitz
DataBlitz Storage Manager: Main-Memory Database Performance for Critical Applications Introduction  General-purpose commercial disk-based database systems, though widely employed in practice, have failed to meet the performance requirements of applications requiring short, predictable response times, and extremely high throughput rates. Main memory is the only technology capable of these characteristics.  DataBlitz  1  is a main-memory storage manager product that supports the development of high-performance and fault-resilient applications requiring concurrent access to shared data. In DataBlitz, core algorithms for concurrency, recovery, index management and space management are optimized for the case that data is memory resident.  2 DataBlitz Architecture and Features  In this section, we give a high-level overview of the architecture and features of the DataBlitz Storage Manager product implemented at Bell Laboratories (for more details, see [1]).  Direct Access to Data. DataBlitz is designed to a
DB
243743
Approximating the non-dominated front using the Pareto Archived Evolution Strategy We introduce a simple evolution scheme for multiobjective optimization problems, called the Pareto Archived Evolution Strategy (PAES). We argue that PAES may represent the simplest possible nontrivial algorithm capable of generating diverse solutions in the Pareto optimal set. The algorithm, in its simplest form, is a (1 + 1) evolution strategy employing local search but using a reference archive of previously found solutions in order to identify the approximate dominance ranking of the current and candidate solution vectors. (1 + 1)-PAES is intended to be a baseline approach against which more involved methods may be compared. It may also serve well in some real-world applications when local search seems superior to or competitive with population-based methods. We introduce (1 + λ) and (μ | λ) variants of PAES as extensions to the basic algorithm. Six variants of PAES are compared to variants of the Niched Pareto Genetic Algorithm and the Nondominated Sorting Genetic Algorithm over a diverse suite of six test functions. Results are analyzed and presented using techniques that reduce the attainment surfaces generated from several optimization runs into a set of univariate distributions. This allows standard statistical analysis to be carried out for comparative purposes. Our results provide strong evidence that PAES performs consistently well on a range of multiobjective optimization tasks.
ML
59359
Introducing a Two-Level Grammar Concept for Design (Extended Abstract) this paper.  A few investigation may first illustrate the differences of our new concepts from known hierarchies. Even if L(G) is regular, L(DG) may be not context-free. There is no need for a proof as it is folklore that pattern languages are usually not context-free. We present a few further trivialities to warm up: In case j V j = 1, L(DG) is regular if and only if L(G) is regular. If T 1 and T 2 are disjoint, it holds: 1. Membership for L(DG) is uniformly decidable. 2. Emptyness for L(DG) is uniformly decidable. 3. Finiteness for L(DG) is uniformly decidable. There is no need for an explicit proof. The results above are immediately inherited from classical results in formal language theory (cf. [HU79]). Further properties seem to require particular assumptions. For the intended application domain, languages which satisfy T 1 n T 2 6= ; are of a particular importance. It may also be of a special interest to consider languages with a certain rate of letters from T 1 n T 2 in terminal words. 3.2 Graph Grammars
ML
schut00intention
Intention Reconsideration in Complex Environments One of the key problems in the design of belief-desire-intention (BDI) agents is that of finding an appropriate policy for intention reconsideration. In previous work, Kinny and Georgeff investigated the effectiveness of several such reconsideration policies, and demonstrated that in general, there is no one best approach -- different environments demand different intention reconsideration strategies. In this paper, we further investigate the relationship between the effectiveness of an agent and its intention reconsideration policy in different environments. We empirically evaluate the performance of different reconsideration strategies in environments that are to varying degrees dynamic, inaccessible, and nondeterministic. In addition to our empirical results, we are able to give preliminary analytical results to explain some of our findings.
Agents
443805
Http://www.playresearch.com/ this report. The following link takes you directly to a page with links to the core publications and other evaluation material in PDF format:
HCI
varela99ant
Ant Colony Optimisation for Virtual-Wavelength-Path Routing and Wavelength Allocation Ant Colony Optimisation (ACO) is applied to the problem of routing and wavelength-allocation in a multi-wavelength all-optical virtual-wavelength-path routed transport network. Three variants of our ACO algorithm are proposed: local update (LU), global update/ distance (GU/D) and global update/occupancy (GU/O). All three extend the usual practice that ants are attracted by the pheromone trail of ants from their own colony: in our work, the artificial ants are also repelled by the pheromone of other colonies. Overall, the best ACO variant, GU/O, provides results that approach those of an earlier problem-specific heuristic on small- and medium-sized networks.  1 Introduction  Multi-wavelength all-optical transport networks have attracted considerable interest in recent years, because of their potential, by using multiple wavelengths in both optical transmission and optical switching, to provide the huge bandwidths necessary if broadband services are to be widely adopted [1]. In addition,...
ML
watson99embodied
Embodied Evolution: Embodying an Evolutionary Algorithm in a Population of Robots We introduce Embodied Evolution (EE) as a methodology for the automatic design of robotic controllers. EE is an evolutionary robotics (ER) technique that avoids the pitfalls of the simulate-and-transfer method, allows the speed-up of evaluation time by utilizing parallelism, and is particularly suited to future work on multi-agent behaviors. In EE, an evolutionary algorithm is distributed amongst and embodied within a population of physical robots that reproduce with one another while situated in the task environment. We have built a population of eight robots and successfully implemented our first experiments. The controllers evolved by EE compare favorably to hand-designed solutions for a simple task. We detail our methodology, report our initial results, and discuss the application of EE to more advanced and distributed robotics tasks. 1. Introduction  Our work is inspired by the following vision. A large number of robots freely interact with each other in a shared environment, atte...
Agents
zhao98empirical
Empirical Performance Analysis of Linear Discriminant Classifiers In face recognition literature, holistic template matching systems and geometrical local feature based systems have been pursued . In the holistic approach, PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis) are popular ones. More recently, the combination of PCA and LDA has been proposed as a superior alternative over pure PCA and LDA. In this paper, we illustrate the rationales behind these methods and the pros and cons of applying them to pattern classification task. A theoretical performance analysis of LDA suggests applying LDA over the principal components from the original signal space or the subspace. The improved performance of this combined approach is demonstrated through experiments conducted on both simulated data and real data.  1 Introduction  Statistical pattern recognition techniques have been successfully applied to many problems, including speech recognition, automatic target recognition and image classification. For a given pattern classificat...
ML
soderland99learning
Learning Information Extraction Rules for Semi-structured and Free Text . A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically. WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences. Such semistructured text has largely been beyond the scope of previous systems. When used in conjunction with a syntactic analyzer and semantic tagging, WHISK can also handle extraction from free text such as news stories.  Keywords: natural language processing, information extraction, rule learning 1. Information extraction  As more and more text becomes available on-line, there is a growing need for systems that extract information automatically from text data. An information extraction (IE) sys...
IR
laviola99wholehand
Whole-Hand and Speech Input in Virtual Environments Recent approaches to providing users with a more natural method of interacting with computer applications have shown that more than one mode of input can be both beneficial and intuitive as a communication medium between humans and computers. Two modalities in particular, whole-hand and speech input, represent a natural form of communication that has been ingrained in our physical and mental makeup since birth. In this thesis, we investigate the use of whole-hand and speech input in virtual environments in the context of two applications domains: scientific visualization and interior design. By examining the two modalities individually and in combination, and through the creation of two application prototypes (Multimodal Scientific Visualization Tool and Room Designer), we present anumber of contributions including a set of interface guidelines and interaction techniques for whole-hand and speech input.
HCI
sutton93online
Online Learning with Random Representations We consider the requirements of online learning---learning which must be done incrementally and in realtime, with the results of learning available soon after each new example is acquired. Despite the abundance of methods for learning from examples, there are few that can be used effectively for online learning, e.g., as components of reinforcement learning systems. Most of these few, including radial basis functions, CMACs, Kohonen 's self-organizing maps, and those developed in this paper, share the same structure. All expand the original input representation into a higher dimensional representation in an unsupervised way, and then map that representation to the final answer using a relatively simple supervised learner, such as a perceptron or LMS rule. Such structures learn very rapidly and reliably, but have been thought either to scale poorly or to require extensive domain knowledge. To the contrary, some researchers (Rosenblatt, 1962; Gallant & Smith, 1987; Kanerva, 1988; Prager ...
ML
polani99fast
Fast Reinforcement Learning through Eugenic Neuro-Evolution In this paper we introduce EuSANE, a novel reinforcement learning algorithm based on the SANE neuroevolution method. It uses a global search algorithm, the Eugenic Algorithm, to optimize the selection of neurons to the hidden layer of SANE networks. The performance of EuSANE is evaluated in the two-pole balancing benchmark task, showing that EuSANE is significantly stronger than other reinforcement learning methods to date in this task.  
ML
tsoumakas01fuzzy
Fuzzy Meta-Learning: Preliminary Results Learning from distributed data is becoming in  our times a necessity, but it is also a complex and  challenging task. Approaches developed so far  have not dealt with the uncertainty, imprecision  and vagueness involved in distributed learning.  Meta-Learning, a successful approach for distributed  data mining, is in this paper extended to  handle the imprecision and uncertainty of the local  models and the vagueness that characterizes  the meta-learning process. The proposed approach,  Fuzzy Meta-Learning uses a fuzzy inductive  algorithm to meta-learn a global model from  the degrees of certainty of the output of local  classifiers. This way more accurate models of  collective knowledge can be acquired from data  with application both to inherently distributed databases  and parts of a very large database. Preliminary  results are promising and encourage further  research towards this direction.  1 
ML
raskar01interacting
Interacting with Spatially Augmented Reality Traditional user interfaces for off-the-desktop applications are designed to display the output on flat 2D surfaces while the input is with 2D or 3D devices. In this paper, we focus on projectorbased augmented reality applications. We describe a framework to easily incorporate the interaction on a continuum of display surfaces and input devices. We first create a 3D understanding of the relationship between the user, the projectors and the display surfaces. Then we use some new calibration and rendering techniques to create a simple procedure to effectively illuminate the surfaces. We describe various underlying techniques and discuss the results in the context of three different applications.
HCI
mann01validating
Validating Access to External Information Sources in a Mediator Environment A mediator integrates existing information sources into a new application. In order to answer complex queries, the mediator splits them up into sub-queries which it sends to the information sources. Afterwards, it combines the replies to answer the original query. Since the information sources are usually external, autonomous systems, the access to them can sometimes be erroneous, most notably when the information source is changed. This results in an incorrect behaviour of the whole system. The question that this paper addresses, is: how to check whether or not the access was correct? The paper introduces a notational framework for the general information access validation problem, describes the typical errors that can occur in a mediator environment, and proposes several validation mechanisms. It is also investigated how the validation functionality can be integrated into the mediator architecture, and what the most important quality measures of a validation method are. Moreover, the practical usability of the presented approaches is demonstrated on a real-world application using Web-based information sources. Several measurements are performed to compare the presented methods with previous work in the field.
IR
bandi98space
Space Discretization for Efficient Human Navigation There is a large body of research on motion control of legs in human models. However, they require specification of global paths in which to move. A method for automatically computing a global motion path for a human in 3D environment of obstacles is presented. Object space is discretized into a 3D grid of uniform cells and an optimal path is generated between two points as a discrete cell path. The grid is treated as graph with orthogonal links of uniform cost. A* search method is applied for path finding. By considering only the cells on the upper surface of objects on which human walks, a large portion of the grid is discarded from the search space, thus boosting efficiency. This is expected to be a higher level mechanism for various local foot placement methods in human animation.  Keywords: global navigation, dynamic programming, A* graph search, articulated body models 1. Introduction  Human walking is a complex and well studied component of articulated body animation research. T...
AI
454312
A Comparison of Usage Evaluation and Inspection Methods for Assessing Groupware Usability Many researchers believe that groupware can only be evaluated by studying real collaborators in their real contexts, a process that tends to be expensive and timeconsuming. Others believe that it is more practical to evaluate groupware through usability inspection methods. Deciding between these two approaches is difficult, because it is unclear how they compare in a real evaluation situation. To address this problem, we carried out a dual evaluation of a groupware system, with one evaluation applying userbased techniques, and the other using inspection methods. We compared the results from the two evaluations and concluded that, while the two methods have their own strengths, weaknesses, and trade-offs, they are complementary. Because the two methods found overlapping problems, we expect that they can be used in tandem to good effect, e.g., applying the discount method prior to a field study, with the expectation that the system deployed in the more expensive field study has a better chance of doing well because some pertinent usability problems will have already been addressed.  Keywords  Evaluation, groupware usability, inspection evaluation techniques, usage evaluation techniques.  
HCI
haarslev99description
A Description Logic with Concrete Domains and a Role-forming Predicate Operator Description Logics (DLs) are a family of logic-based knowledge representation formalisms designed to represent and reason about conceptual knowledge. Due to a nice compromise between expressivity and the complexity of reasoning, DLs have found applications in many areas such as, e.g., modelling database schemas and the semantic web. However, description logics represent knowledge in an abstract way and lack the power to describe more concrete (quantitative) qualities like size, duration, or amounts. The standard solution is to equip DLs with concrete domains, e.g., natural numbers with predicates =, <, + or strings with a string concatenation predicate. Moreover, recently it has been suggested that the expressive power of DLs with concrete domains can be further enhanced by providing them with database-like key constraints. Key constraints can be a source of additional inconsistencies in database schemas, and DLs applied in reasoning about database schemas are thus wanted to be able to capture such constraints. Up to now, only the integration of uniqueness key constraints into DLs with concrete
AI
eiter00difference
On the Difference of Horn Theories In this paper, we consider computing the difference between two Horn theories. This problem may arise, for example, if we take care of a theory change in a knowledge base. In general, the difference of Horn theories is not Horn. Therefore, we consider Horn approximations of the difference in terms of Horn cores (i.e., weakest Horn theories included in the difference) and the Horn envelope (i.e., the strongest Horn theory containing the difference), which have been proposed and analyzed extensively in the literature. We study the problem under the familiar representation of Horn theories by Horn CNFs, as well as under the recently proposed model-based representation in terms of the characteristic models. For all problems and representations, polynomial time algorithms or proofs of intractability for the propositional case are provided; thus, our work gives a complete picture of the tractability/intractability frontier in the propositional Horn theories.
DB
ricci97extending
Extending Local Learners with Error-Correcting Output Codes : Error-correcting output codes (ECOCs) represent classes with a set of output bits, where each bit encodes a binary classification task corresponding to a unique partition of the classes. Algorithms that use ECOCs learn the function corresponding to each bit, and combine them to generate class predictions. ECOCs can reduce both variance and bias errors for multiclass classification tasks when the errors made at the output bits are not correlated. They work well with global (e.g., C4.5) but not with local (e.g., nearest neighbor) classifiers because the latter use the same information to predict each bit's value, which yields correlated errors. This is distressing because local learners are excellent classifiers for some types of applications. We show that the output bit errors of local learners can be decorrelated by selecting different features for each bit. This yields bit-specific distance functions, which causes different information to be used for each bit's prediction. We presen...
ML
315336
A Scalable Algorithm for Answering Queries Using Views The problem of answering queries using views is  to find efficient methods of answering a query using  a set of previously materialized views over the  database, rather than accessing the database relations.  The problem has received significant attention  because of its relevance to a wide variety of  data management problems, such as data integration,  query optimization, and the maintenance of  physical data independence. To date, the performance  of proposed algorithms has received very  little attention, and in particular, their scale up in  the presence of a large number of views is unknown.  We first analyze two previous algorithms, the  bucket algorithm and the inverse-rules algorithm,  and show their deficiencies. We then describe the  MiniCon algorithm, a novel algorithm for finding  the maximally-contained rewriting of a conjunctive  query using a set of conjunctive views.  We present the first experimental study of algorithms  for answering queries using views. The  study shows that the MiniCon algorithm scales up  well and significantly outperforms the previous algorithms.  Finally, we describe an extension of the  MiniCon algorithm to handle comparison predicates,  and show its performance experimentally.    Thanks to Daniela Florescu, Marc Friedman, Zack Ives, Ioana Manolescu, Dan Weld, and Steve Wolfman for their comments on earlier drafts of this paper. This research was funded by a Sloan Fellowship, NSF Grant #IIS-9978567, a NSF Graduate Research Fellowship, and a Lucent Technologies GRPW Grant  Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given tha...
DB
dix98nonmonotonic
Nonmonotonic Reasoning: Towards Efficient Calculi and Implementations 
AI
howell99gesture
Gesture Recognition for Visually Mediated Interaction . This paper reports initial research on supporting Visually Mediated Interaction  (VMI) by developing person-specific and generic gesture models for the  control of active cameras. We describe a time-delay variant of the Radial Basis  Function (TDRBF) network and evaluate its performance on recognising simple  pointing and waving hand gestures in image sequences. Experimental results  are presented that show that high levels of performance can be obtained for this  type of gesture recognition using such techniques, both for particular individuals  and across a set of individuals. Characteristic visual evidence can be automatically  selected, depending on the task demands.  1 Introduction  In general, robust tracking of non-rigid objects such as human bodies is difficult due to rapid motion, occlusion and ambiguities in segmentation and model matching. Ongoing research at the MIT Media Lab has shown progress in the modelling and interpretation of human body activity [24, 30, 31]. Compu...
HCI
liu98relationlog
Relationlog: A Typed Extension to Datalog with Sets and Tuples This paper presents a novel logic programming based language for nested relational and complex value models called Relationlog. It stands in the same relationship to the nested relational and complex value models as Datalog stands to the relational model. The main novelty of the language is the introduction of powerful mechanisms, namely, partial and complete set terms, for representing and manipulating both partial and complete information on nested sets, tuples and relations. They generalize the set grouping and set enumeration mechanisms of LDL and allow the user to directly encode the open and closed world assumptions on nested sets, tuples, and relations. They allow direct inference and access to deeply embedded values in a complex value relation as if the relation is normalized, which greatly increases the ease of use of the language. As a result, the extended relational algebra operations can be represented in Relationlog directly, and more importantly, recursively in a way similar to Datalog. Like Datalog, Relationlog has a well-defined Herbrand model-theoretic semantics, which captures the intended semantics of nested sets, tuples and relations, and also a well-defined proof-theoretic semantics which coincides with its model-theoretic semantics.
DB
benton98compiling
Compiling Standard ML to Java Bytecodes ing with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. Compiling Standard ML to Java Bytecodes Nick Benton Andrew Kennedy George Russell Persimmon IT, Inc. Cambridge, U.K. fnick,andrew,georgeg@persimmon.co.uk Abstract MLJ compiles SML'97 into verifier-compliant Java bytecodes. Its features include type-checked interlanguage working extensions which allow ML and Java code to call each other, automatic recompilation management, compact compiled code and runtime performance which, using a `just in time' compiling Java virtual machine, usually exceeds that of existing specialised bytecode interpreters for ML. Notable features of the compiler itself include whole-program optimisation based on rewriting, compilation of polymorphism by specialisation, a novel monadic intermediate lang...
DB
13229
Distributed Safety Controllers for Web Services We show how to use high-level synchronization constraints, written in a version of monadic second-order logic on finite strings, to synthesize safety controllers for interactive web services. We improve on the naïve runtime model to avoid state-space explosions and to increase the flow capacities of services.
DB
feiner99importance
The Importance of Being Mobile: Some Social Consequences of Wearable Augmented Reality Systems What are the consequences of mobility for augmented reality ? This brief paper explores some of the issues that I believe will be raised by the development and future commonplace adoption of mobile, wearable, augmented reality systems. These include: social influences on tracking accuracy, the importance of appearance and comfort, an increase in collaborative applications, integration with other devices, and implications for personal privacy.  1. Introduction  Over the past decade, the reality of mobile computing has begun to embrace the potential of wearable computing. In the process, several researchers have attempted to clarify what distinguishes wearable computing from mobile computing. Rhodes [10] suggests five criteria for wearable systems: portable while operational, needing minimal manual input, sensitive to the user's surrounding environment, always on, and able to attract the user's attention even when not actively in use. Mann [8] cites three desirable properties for wearabl...
HCI
452506
Event-Learning And Robust Policy Heuristics . In this paper we introduce a novel form of reinforcement learning called event-learning or E-learning. Events are ordered pairs of consecutive states. We define the corresponding event-value function. Learning rules which are guaranteed to converge to the optimal event-value function are derived. Combining our method with a known robust control method, the SDS algorithm, we introduce Robust Policy Heuristics (RPH). It is shown that RPH (a fast-adapting non-Markovian policy) is particularly useful for coarse models of the environment and for partially observed systems. Fast adaptation may allow to separate the time scale of learning to control a Markovian process and the time scale of adaptation of a non-Markovian policy. In our E-learning framework the de nition of modules is straightforward. E-learning is well suited for policy switching and planning, whereas RPH alleviates the `curse of dimensionality' problem. Computer simulations of a two-link pendulum with coarse discretization and noisy controller are shown to demonstrate the underlying principle. Date: First version: May 14, 2001, second version: May 19, 2001. Key words and phrases. reinforcement learning, robust control, event representation, continuous dynamical systems, non-Markovian policy. This work was supported by the Hungarian National Science Foundation (Grant OTKA 32487.) Thanks are due to Tor M. Aamodt for providing his double pendulum software [1] and to Gabor Szirtes for careful reading of the manuscript. THIS WORK IS A SHORTENED VERSION OF THE THESIS WORK OF I. POLIK AND I. SZITA SUPERVISED BY A. LORINCZ (SUBMITTED ON DECEMBER 6, 2000) THAT WON FIRST PRIZE IN THE STUDENT COMPETITION IN HUNGARY IN SECTION `MATHEMATICAL METHODS FOR COMPUTERS' ON APRIL 11, 2001
ML
ahanger99technique
Techniques For Automatic Digital Video Composition Recent developments in digital technology have enabled a class of video-based applications that were not previously viable. However, digital video production systems face the challenge of accessing the inherently linear and time-dependent media of audio and video, and providing effective means of composing them into a cohesive piece for presentation. Moreover, there are no appropriate metrics that allow for assessment of the quality of an automatically-composed video piece. Techniques presently available are limited in scope, and do not account for all the features of a composition. This dissertation presents metrics that evaluate the quality of a video composition. In addition, it proposes techniques for automatic composition of video presentations as well as improvements in access to digital video data. Yet another challenge faced by video production systems is the customization of the presentation to suit user profiles. For instance, certain elements of video compositions, such as v...
IR
laird94evolution
The Evolution of the Soar Cognitive Architecture The origins of the Soar architecture can be traced back to the seminal research of Allen Newell and Herbert Simon on symbol systems, heuristic search, goals, problem spaces, and production systems. Since its official inception in 1982, Soar has evolved through six major releases, as both an AI architecture and as the basis for a unified theory of cognition. This paper traces this evolutionary path, starting with Soar's intellectual roots, and then proceeding through the stages defined by the six major system releases. Each stage is characterized with respect to a hierarchy of four levels of analysis: the knowledge level, the problem space level, the symbolic architecture level, and the implementation level.
ML
23799
Distance Browsing in Spatial Databases Two different techniques of browsing through a collection of spatial objects stored in an R-tree spatial data structure on the basis of their distances from an arbitrary spatial query object are compared. The conventional approach is one that makes use of a k-nearest neighbor algorithm where k is known prior to the invocation of the algorithm. Thus if m#kneighbors are needed, the k-nearest neighbor algorithm needs to be reinvoked for m neighbors, thereby possibly performing some redundant computations. The second approach is incremental in the sense that having obtained the k nearest neighbors, the k +1  st  neighbor can be obtained without having to calculate the k +1nearest neighbors from scratch. The incremental approach finds use when processing complex queries where one of the conditions involves spatial proximity (e.g., the nearest city to Chicago with population greater than a million), in which case a query engine can make use of a pipelined strategy. A general incremental nearest neighbor algorithm is presented that is applicable to a large class of hierarchical spatial data structures. This algorithm is adapted to the R-tree and its performance is compared to an existing k-nearest neighbor algorithm for R-trees [45]. Experiments show that the incremental nearest neighbor algorithm significantly outperforms the k-nearest neighbor algorithm for distance browsing queries in a spatial database that uses the R-tree as a spatial index. Moreover, the incremental nearest neighbor algorithm also usually outperforms the k-nearest neighbor algorithm when applied to the k-nearest neighbor problem for the R-tree, although the improvement is not nearly as large as for distance browsing queries. In fact, we prove informally that, at any step in its execution, the incremental...
DB
santini01query
A Query Paradigm to Discover the Relation between Text and Images This paper studies the relation between images and text in image databases. An analysis of this relation results in the definition of three distinct query modalities: (1) linguistic scenario: images are part of a whole including a self-contained linguistic discourse, and their meaning derives from their interaction with the linguistic discourse. A typical case of this scenario is constituted by images on the World Wide Web; (2) closed world scenario: images are defined in a limited domain, and their meaning is anchored by conventions and norms in that domain. (3) user scenario: the linguistic discourse is provided by the user. This is the case of highly interactive systems with relevance feedback.  This paper deals with image databases of the first type. It shows how the relation between images (or parts of images) and text can be inferred, and exploited for search. The paper develops a similarity model in which the similarity between two images is given by both their visual similarity...
IR
kim99compensatory
Compensatory Negotiation for Agent-Based Project Schedule Coordination Practitioners have tried to solve a project schedule coordination problem involving many subcontractors with a centralized approach, but failed to provide a cohesive solution. They have overlooked a principle that general contractors cannot coordinate subcontractors like they do their own forces. The project schedule coordination problem could be solved better by subcontractors in a distributed manner. This paper presents a formalized negotiation methodology for distributed project schedule coordination --- a framework wherein a project can be rescheduled dynamically by all of the concerned project participants. The compensatory negotiation methodology is developed to allow agents to transfer utility to other agents for compensation of disadvantageous agreements through a multi-linked negotiation process. By employing software agents that are capable of compensatory negotiation, practitioners now can solve the problem and explore and exploit new opportunities an agent-based framework offers.  1. 
Agents
huang00dialogue
Dialogue Management for Multimodal User Registration User registration refers to associating certain personal information with a user. It is widely used in hospitals, hotels and conferences. In this paper, we propose an approach to interactive user registration by combining face recognition, speech recognition and speech synthesis technologies together through an efficient dialogue manager. In order to minimize a user’s effort, we employ a new dialogue management model based on a finite state automaton (FSA), which uses a Baysian network to fuse the user’s information from multiple channels (e.g., face image, speech, records stored in a pre-constructed database) to reliably estimate the confidence about user identity. Instead of fixing weights, the FSA adjusts its weights dynamically by integrating partial information from multiple information sources. This is achieved by maximizing an objective function to determine an optimal action at each succeeding state according to current confidence and information cues. Thus the transition between states can be done along the shortest path from the initial state to the goal state. We have developed a multimodal user registration system to demonstrate the feasibility of the proposed approach.
HCI
295196
Task-Oriented Collaboration with Embodied Agents in Virtual Worlds   We are working toward animated agents that can collaborate with human students in virtual worlds. The agent's objective is to help students learn to perform physical, procedural tasks, such as operating and maintaining equipment. Like most of the previous research on task-oriented dialogues, the agent (computer) serves as an expert that can provide guidance to a human novice. Research on such dialogues dates back more than twenty years (Deutsch 1974), and the subject remains an active research area (Allen et al. 1996; Lochbaum 1994; Walker 1996). However, most of that research has focused solely on verbal dialogues, even though the earliest studies clearly showed the ubiquity of nonverbal communication in human task-oriented dialogues (Deutsch 1974). To allow a wider variety of interactions among agents and human students, we use virtual reality (Durlach and Mavor 1995); agents and students cohabit a threedimensional, interactive, simulated mock-up of the student'
Agents
453663
Improving Min/Max Aggregation over Spatial Objects We examine the problem of computing MIN/MAX aggregate queries over a collection of spatial objects. Each spatial object is associated with a weight (value), for example, the average temperature or rainfall over the area covered by the object. Given a query rectangle, the MIN/MAX problem computes the minimum/maximum weight among all objects intersecting the query rectangle. Traditionally such queries have been performed as range search queries. Assuming that the objects are indexed by a spatial access method, the MIN/MAX is computed as objects are retrieved. This requires effort proportional to the number of objects intersecting the query interval, which may be large. A better approach is to maintain aggregate information among the index nodes of the spatial access method; then various index paths can be eliminated during the range search. In this paper we propose four optimizations that further improve the performance of MIN/MAX queries. Our experiments show that the proposed optimizations offer drastic performance improvement over previous approaches. Moreover, as a by-product of this work we present an optimized version of the MSB-tree, an index that has been proposed for the MIN/MAX computation over 1-dimensional interval objects.
DB
14648
Content-Based Book Recommending Using Learning for Text Categorization Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes. Most existing recommender systems use collaborative filtering methods that base recommendations on other users' preferences. By contrast, content-based methods use information about an item itself to make suggestions. This approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization. Initial experimental results demonstrate that this approach can produce accurate recommendations.  KEYWORDS: Recommender systems, information filtering,  machine learning, text categorization  INTRODUCTION  There is a growing interest in recommender systems that suggest music, films, books, and othe...
ML
christel98information
Information Visualization within a Digital Video Library . The Informedia Digital Video Library contains over a thousand hours of video, consuming over a terabyte of disk space. This paper summarizes the multimedia abstractions used to represent this video in prior systems and introduces the visualization techniques employed to browse and navigate multiple video documents at once.  Keywords: digital video library, information visualization, multimedia abstraction  1. Introduction  The Informedia Project at Carnegie Mellon University deals primarily with video. The goal of the project is to enable full content search and retrieval from digital video libraries (Christel et al., 1996; Wactlar et al., 1997). Consider the task of trying to find a five-minute video clip of interest from a library of a thousand hour-long videotapes. In the analog domain, this task would be interminable and the frustrated user would probably walk away without completing the task. Simply digitizing the video will not make the job easier. Through the use of speech rec...
IR
jaimes00integrating
Integrating Multiple Classifiers In Visual Object Detectors Learned From User Input There have been many recent efforts in contentbased retrieval to perform automatic classification of images/visual objects. Most approaches, however, have focused on using individual classifiers. In this paper, we study the way in which, in a dynamic framework, multiple classifiers can be combined when applying Visual Object Detectors. We propose a hybrid classifier combination approach, in which decisions of individual classifiers are combined in the following three ways: (1) classifier fusion, (2) classifier cooperation, and (3) hierarchical combination. In earlier work, we presented the Visual Apprentice framework, in which a user defines visual object models via a multiple-level object-definition hierarchy (region, perceptual-area, object part, and object). As the user provides examples from images or videos, visual features are extracted and multiple classifiers are learned for each node of the hierarchy. In this paper, we discuss the benefits of hybrid classifier combination in the Visual Apprentice framework, and show some experimental results in classifier fusion. These results suggest possible improvements in classification accuracy, particularly of detectors reported earlier for Baseball video, images with skies, and images with handshakes.
ML
509693
The Security Architecture of the M&M Mobile Agent Framework In the Mobile Agent programming model, small threads of execution migrate from machine to machine, performing their operations locally. For being able to deploy such a model into real world applications, security is a vital concern. In the M&M project we have developed a system that departures from the traditional platform-based execution model for mobile agents. In M&M there are no agent platforms. Instead there is a component framework that allows the applications to become able of sending and receiving agents by themselves in a straightforward manner. In this paper we examine the security mechanisms available in M&M, and how integration with existing applications is done. One difficult aspect of this work is that all the features must work with the security mechanisms that already exist on the applications. This is so because the components are integrated from within into the applications, which already have security mechanisms in place. Currently, M&M provides features like fine-grain security permissions, encryption of agents and data, certificate distribution using LDAP and cryptographic primitives for agents. For validating the approach and solutions found, we have integrated the framework into several off-the-shelf web servers, having the security mechanisms running, with no problems.
Agents
21158
Learning to Classify Text from Labeled and Unlabeled Documents In many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap. This paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text based on the combination of Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents; it then trains a new classifier using the labels for all the documents, and iterates to convergence. Experimental results, obtained using text from three different realworld tasks, show that the use of unlabeled data reduces classification error by up to 33%.
ML
362539
Comparing Structures using a Hopfield-style Neural Network Labeled graphs are an appropriate and popular representation of structured objects in many domains. If the labels describe the properties of real world objects and their relations, finding the best match between two graphs turns out to be the weakly defined, NP-complete task of establishing a mapping between them that maps similar parts onto each other preserving as much as possible of their overall structural correspondence. In this paper, former approaches of structural matching and constraint relaxation by spreading activation in neural networks and the method of solving optimization tasks using Hopfield-style nets are combined. The approximate matching task is reformulated as the minimization of a quadratic energy function. The design of the approach enables the user to change the parameters and the dynamics of the net so that knowledge about matching preferences is included easily and transparently. In the last section, some examples demonstrate the successful application of ...
ML
rocha00biologically
Biologically Motivated Distributed Design for Adaptive Knowledge Management We discuss how distributed designs that draw from biological network metaphors can largely improve the current state of information retrieval and knowledge management of distributed information systems. In particular, two adaptive recommendation systems named TalkMine and @ApWeb are discussed in more detail. TalkMine operates at the semantic level of keywords. It leads different databases to learn new and adapt existing keywords to the categories recognized by its communities of users using distributed algorithms.
HCI
128910
Non-Standard Crossover for a Standard Representation -- Commonality-Based Feature Subset Selection The Commonality-Based Crossover Framework has been presented as a general model for designing problem specific operators. Following this model, the Common Features/Random Sample Climbing operator has been developed for feature subset selection--a binary string optimization problem. Although this problem should be an ideal application for genetic algorithms with standard crossover operators, experiments show that the new operator can find better feature subsets for classifier training. 1 INTRODUCTION  A classification system is used to predict the decision class of an object based on its features. When training a classifier, it is beneficial to use only the features relevant to prediction accuracy, and to ignore the irrelevant features [Koh95]. The benefit arises from an increase in the "signalto -noise ratio" of the data, and a reduction in the time required to train the classifier. Thus, the objective of feature subset selection is to identify the (most) relevant features. Feature sub...
ML
ray01representing
Representing Sentence Structure in Hidden Markov Models for Information Extraction We study the application of Hidden Markov Models (HMMs) to learning information extractors for ¤-ary relations from free text. We propose an approach to representing the grammatical structure of sentences in the states of the model. We also investigate using an objective function during HMM training which maximizes the ability of the learned models to identify the phrases of interest. We evaluate our methods by deriving extractors for two binary relations in biomedical domains. Our experiments indicate that our approach learns more accurate models than several baseline approaches. 1
IR
312028
The 3W Model and Algebra for Unified Data Mining Real data mining/analysis applications call for a  framework which adequately supports knowledge  discovery as a multi-step process, where the input  of one mining operation can be the output of another.  Previous studies, primarily focusing on fast  computation of one specific mining task at a time,  ignore this vital issue.  Motivated by this observation, we develop a unified  model supporting all major mining and analysis  tasks. Our model consists of three distinct  worlds, corresponding to intensional and extensional  dimensions, and to data sets. The notion of  dimension is a centerpiece of the model. Equipped  with hierarchies, dimensions integrate the output  of seemingly dissimilar mining and analysis operations  in a clean manner.  We propose an algebra, called the dimension algebra,  for manipulating (intensional) dimensions,  as well as operators that serve as "bridges" between  the worlds. We demonstrate by examples  that several real data mining processes can be captured ...
DB
levene01web
Web Interaction and the Navigation Problem in Hypertext written for Encyclopedia of Microcomputers The web has become a ubiquitous tool, used in day-to-day work, to find information  and conduct business, and it is revolutionising the role and availability of information. One  of the problems encountered in web interaction, which is still unsolved, is the navigation  problem, whereby users can "get lost in hyperspace", meaning that when following a  sequence of links, i.e. a trail of information, users tend to become disoriented in terms of  the goal of their original query and the relevance to the query of the information they are  currently browsing.  Herein we build statistical foundations for tackling the navigation problem based on a  formal model of the web in terms of a probabilistic automaton, which can also be viewed  as a finite ergodic Markov chain. In our model of the web the probabilities attached  to state transitions have two interpretations, namely, they can denote the proportion of  times a user followed a link, and alternatively they can denote the expected utility of  following a link. Using this approach we have developed two techniques for constructing a  web view based on the two interpretations of the probabilities of links, where a web view  is a collection of relevant trails. The first method we describe is concerned with finding  frequent user behaviour patterns. A collection of trails is taken as input and an ergodic  Markov chain is produced as output with the probabilities of transitions corresponding  to the frequency the user traversed the associated links. The second method we describe  is a reinforcement learning algorithm that attaches higher probabilities to links whose  expected trail relevance is higher. The user's home page and a query are taken as input  and an ergodic Markov chain is produced as output with the probabilities of...
IR
oliver99bayesian
A Bayesian Computer Vision System for Modeling Human Interactions AbstractÐWe describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task [1]. The system is particularly concerned with detecting when interactions between people occur and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach [2]. We propose and compare two different state-based learning architectures, namely, HMMs and CHMMs for modeling behaviors and interactions. The CHMM model is shown to work much more efficiently and accurately. Finally, to deal with the problem of limited training data, a synthetic ªAlife-styleº training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training. Index TermsÐVisual surveillance, people detection, tracking, human behavior recognition, Hidden Markov Models. 1
ML
319653
User Modeling for Information Access Based on Implicit Feedback User modeling can be used in information filtering and retrieval systems to improve the representation of a user's information needs. User models can be constructed by hand, or learned automatically based on feedback provided by the user about the relevance of documents that they have examined. By observing user behavior, it is possible to infer implicit feedback without requiring explicit relevance judgments. Previous studies based on Internet discussion groups (USENET news) have shown reading time to be a useful source of implicit feedback for predicting a user's preferences. The study reported in this paper extends that work by providing framework for considering alternative sources of implicit feedback, examining whether reading time is useful for predicting a user's preferences for academic and professional journal articles, and exploring whether retention behavior can usefully augment the information that reading time provides. Two user studies were conducted in which undergradua...
IR
373966
Language Models for Financial News Recommendation We present a unique approach to identifying news stories that influence the behavior of financial markets. Specifically, we describe the design and implementation of &AElig;nalyst, a system that can recommend interesting news stories -- stories that are likely to affect market behavior. &AElig;nalyst operates by correlating the content of news stories with trends in financial time series. We identify trends in time series using piecewise linear fitting and then assign labels to the trends according to an automated binning procedure. We use language models to represent patterns of language that are highly associated with particular labeled trends. &Aelig;nalyst can then identify and recommend news stories that are highly indicative of future trends. We evaluate the system in terms of its ability to recommend the stories that will a ect the behavior of the stock market. We demonstrate that stories recommended by &Aelig;nalyst could be used to profitably predict forthcoming trends in stock prices.
IR
104045
Guaranteeing No Interaction between Functional Dependencies and Tree-Like Inclusion Dependencies Functional dependencies (FDs) and inclusion dependencies (INDs) are the most fundamental integrity constraints that arise in practice in relational databases. A given set of FDs does not interact with a given set of INDs if logical implication of any FD can be determined solely by the given set of FDs, and logical implication of any IND can be determined solely by the given set of INDs. The set of tree-like INDs constitutes a useful subclass of INDs whose implication problem is polynomial time decidable. We exhibit a necessary and sufficient condition for a set of FDs and tree-like INDs not to interact; this condition can be tested in polynomial time. 1 Introduction  The implication problem for FDs and INDs is the problem of deciding for a given set \Sigma of FDs and INDs whether \Sigma logically implies oe, where oe is an FD or an IND. The implication problem is central in data dependency theory and is also utilised in the process of database design, since it can be used to test wheth...
DB
7980
1BC: a First-Order Bayesian Classifier . In this paper we present 1BC, a first-order Bayesian Classifier. Our approach is to view individuals as structured terms, and to distinguish between structural predicates referring to subterms (e.g. atoms from molecules), and properties applying to one or several of these subterms (e.g. a bond between two atoms). We describe an individual in terms of elementary features consisting of zero or more structural predicates and one property; these features are considered conditionally independent following the usual naive Bayes assumption. 1BC has been implemented in the context of the first-order descriptive learner Tertius,  and we describe several experiments demonstrating the viability of our approach. 1 Introduction  In this paper we present 1BC, a first-order Bayesian Classifier. While the propositional Bayesian Classifier makes the naive Bayes assumption of statistical independence of elementary features (one attribute taking on a particular value) given the class value, it is not i...
ML
441105
Improving Cross Language Retrieval with Triangulated Translation Most approaches to cross language information retrieval assume that resources providing a direct translation between the query and document languages exist. This paper presents research examining the situation where such an assumption is false. Here, an intermediate (or pivot) language provides a means of transitive translation of the query language to that of the document via the pivot, at the cost, however, of introducing much error. The paper reports the novel approach of translating in parallel across multiple intermediate languages and fusing the results. Such a technique removes the error, raising the effectiveness of the tested retrieval system, up to and possibly above the level expected, had a direct translation route existed. Across a number of retrieval situations and combinations of languages, the approach proves to be highly effective.
DB
feiner99wearing
Wearing It Out: First Steps Toward Mobile Augmented Reality Systems Introduction  Over the past decade, there has been a ground swell of activityintwo #elds of user interface research: augmented reality and wearable computing. Augmentedreality #1# refers to the creation of virtual environments that supplement, rather than replace, the real world with additional information. This is accomplished through the use of #see-through" displays that enrich the user's view of the world byoverlaying visual, auditory, and even haptic, material on what she experiences. Visual augmented reality systems typically, but not exclusively, employ head-tracked, head-worn displays. These either use half-silvered mirror beam splitters to re#ect small computer displays, optically combining them with a view of the real world, or use opaque displays fed by electronics that merge imagery captured by head-worn cameras with synthesized graphics. Wearable computing moves computers o# the desktop and onto the user's body, made possible through the 
HCI
pavlovic96gestural
Gestural Interface to a Visual Computing Environment for Molecular Biologists In recent years there has been tremendous progress in 3D, immersive display and virtual reality (VR) technologies. Scientific visualization of data is one of many applications that has benefited from this progress. To fully exploit the potential of these applications in the new environment there is a need for "natural" interfaces that allow the manipulation of such displays without burdensome attachments. This paper describes the use of visual hand gesture analysis enhanced with speech recognition for developing a bimodal gesture/speech interface for controlling a 3-D display. The interface augments an existing application, VMD, which is a VR visual computing environment for molecular biologists. The free hand gestures are used for manipulating the 3-D graphical display together with a set of speech commands. We concentrate on the visual gesture analysis techniques used in developing this interface. The dual modality of gesture/speech is found to greatly aid the interaction capability....
HCI
467998
Secure Mobile Agent using Strong Non-designated Proxy Signature It is expected that mobile agent will be widely used for electronic  commerce as an important key technology. If a mobile agent can  sign a message in a remote server on behalf of a customer without exposing  his/her private key, it can be used not only to search for special  products or services, but also to make a contract with a remote server. To  construct mobile agents, [KBC00] used an RSA-based undetachable signature  scheme, but it does not provide server's non-repudiation because  the undetachable signature does not contain server's signature.  Mobile agent is a very good application example of proxy signature,  and the undetachable signature can be considered as an example of  proxy signature. In this paper we show that secure mobile agent can be  constructed using strong non-designated proxy signature [LKK01] which  represents both the original signer's (customer) and the proxy signer's  (remote server) signatures. We provide RSA-based and Schnorr-based  constructions of secure mobile agent, and moreover we show that the  Schnorr-based scheme can be used very eciently in multi-proxy mobile  agent situation.  Keywords. Secure mobile agent, strong non-designated proxy signature,  multi-proxy signature.  1 
Agents
kokku02halfpipe
Half-pipe Anchoring: An Efficient Technique for Multiple Connection Handoff This paper presents Half--pipe anchoring, a novel technique to build a multiple connection handoff  mechanism that enables efficient use of resources in a server cluster, improves the scalability of  the cluster and supports construction of heterogeneous cluster architectures where nodes are specialized  to perform specific tasks of client requests efficiently. The key idea behind our approach is  to decouple the two unidirectional half-pipes that make up a TCP connection between a client and  a server and anchor the unidirectional half--pipe from the client to the cluster at a designated server  while allowing the half--pipe from the cluster to the client to migrate on a per--request basis to an  optimal server where the request is best serviced. We describe the design and implementation of a  prototype multiple connection handoff mechanism in the Linux kernel and demonstrate the benefits  of our technique.
IR
aha98feature
Feature Weighting for Lazy Learning Algorithms :  Learning algorithms differ in the degree to which they process their inputs prior to their use in performance tasks. Many algorithms eagerly compile input samples and use only the compilations to make decisions. Others are lazy: they perform less precompilation and use the input samples to guide decision making. The performance of many lazy learners significantly degrades when samples are defined by features containing little or misleading information. Distinguishing feature relevance is a critical issue for these algorithms, and many solutions have been developed that assign weights to features. This chapter introduces a categorization framework for feature weighting approaches used in lazy similarity learners and briefly surveys some examples in each category. 1.1 INTRODUCTION  Lazy learning algorithms are machine learning algorithms (Mitchell, 1997) that are welcome members of procrastinators anonymous. Purely lazy learners typically display the following characteristics (Aha, 19...
ML
holmquist99supporting
Supporting Group Collaboration with Inter-Personal Awareness Devices . An Inter-Personal Awareness Device, or IPAD, is a hand-held or  wearable device designed to support awareness and collaboration between  people who are in the physical vicinity of each other. An IPAD is designed to  supply constant awareness information to users in any location without relying  on an underlying infrastructure. We have constructed one such device, the  Hummingbird, which gives members of a group continuous aural and visual  indication when other group members are close. We have used the  Hummingbirds in several different situations to explore how they affect group  awareness. These experiences indicated that the Hummingbird increased  awareness between group members, and that it could complement other forms of  communication such as phone and e-mail. In particular, we found the  Hummingbird to be useful when a group of people were in an unfamiliar  location, for instance during a trip, where no other communication support was  available. We argue that IPADs such as th...
HCI
schiele99situation
Situation Aware Computing with Wearable Computers 1 Motivation for contextual aware computing: For most computer systems, even virtual reality systems, sensing techniques are a means of getting input directly from the user. However, wearable sensors and computers offer a unique opportunity to re-direct sensing technology towards recovering more general user context. Wearable computers have the potential to "see" as the user sees, "hear" as the user hears, and experience the life of the user in a "first-person" sense. This increase in contextual and user information may lead to more intelligent and fluid interfaces that use the physical world as part of the interface. Wearable computers are excellent platforms for contextually aware applications, but these applications are also necessary to use wearables to their fullest. Wearables are more than just highly portable computers, they perform useful work even while the wearer isn't directly interacting with the system. In such environments the user needs to concentrate on his environment, not on the computer interface, so the wearable needs to use information from the wearer's context to be the least distracting. For example, imagine an interface which is aware of the user's location: while being in the subway, the system might alert him with a
HCI
tova99active
Active Views for Electronic Commerce Electronic commerce is emerging as a major Web-supported application. In this paper we argue that database technology can, and should, provide the backbone for a wide range of such applications. More precisely, we present here the ActiveViews system, which, relaying on an extensive use of database features including views, active rules (triggers), and enhanced mechanisms for notification, access control and logging/tracing of users activities, provides the needed basis for electronic commerce. Based on the emerging XML standards (DOM, query languages for XML, etc.), the system offers a novel declarative view specification language, describing the relevant data and activities of all actors (e.g. vendors and clients) participating in electronic commerce activities . Then, acting as an application generator, the system generates an actual, possibly customized, Web application that allows users to perform the given set of controlled activities and to work interactively on the specified dat...
DB
hermansky99temporal
Temporal Patterns (TRAPs) in ASR of Noisy Speech In this paper we study a new approach to processing temporal information for automatic speech recognition (ASR). Specifically, we study the use of rather longtime TempoRAl Patterns (TRAPs) of spectral energies in place of the conventional spectral patterns for ASR. The proposed Neural TRAPs are found to yield significant amount of complementary information to that of the conventional spectral feature based ASR system. A combination of these two ASR systems is shown to result in improved robustness to several types of additive and convolutive environmental degradations.  1. INTRODUCTION 1.1. Spectral features  Spectrum-based techniques form the basis of most feature extraction methods in current ASR. A drawback of the spectral features is that they are quite sensitive to changes in the communication environment e.g. characteristics of different communication channels or environmental noise. Subsequently, recognizers based on spectral features exhibit rapid degradation in performance in ...
DB
81142
Remote Agent: To Boldly Go Where No AI System Has Gone Before Renewed motives for space exploration have inspired NASA to work toward the goal of establishing a virtual presence in space, through heterogeneous effets of robotic explorers. Information technology, and Artificial Intelligence in particular, will play a central role in this endeavor by endowing these explorers with a form of computational intelligence that we call remote agents. In this paper we describe the Remote Agent, a specific autonomous agent architecture based on the principles of model-based programming, on-board deduction and search, and goal-directed closed-loop commanding, that takes a significant step toward enabling this future. This architecture addresses the unique characteristics of the spacecraft domain that require highly reliable autonomous operations over long periods of time with tight deadlines, resource constraints, and concurrent activity among tightly coupled subsystems. The Remote Agent integrates constraint-based temporal planning and scheduling, robust multi-threaded execution, and model-based mode identification and reconfiguration. The demonstration of the integrated system as an on-board controller for Deep Space One, NASA's rst New Millennium mission, is scheduled for a period of a week in late 1998. The development of the Remote Agent also provided the opportunity to reassess some of AI's conventional wisdom about the challenges of implementing embedded systems, tractable reasoning, and knowledge representation. We discuss these issues, and our often contrary experiences, throughout the paper.
AI
gaizauskas98information
Information Extraction: Beyond Document Retrieval In this paper we give a synoptic view of the growth text processing technology of information  extraction (IE) whose function is to extract information about a pre-specified  set of entities, relations or events from natural language texts and to record this information  in structured representations called templates. Here we describe the nature of the  IE task, review the history of the area from its origins in AI work in the 1960's and  70's till the present, discuss the techniques being used to carry out the task, describe  application areas where IE systems are or are about to be at work, and conclude with  a discussion of the challenges facing the area. What emerges is a picture of an exciting  new text processing technology with a host of new applications, both on its own and in  conjunction with other technologies, such as information retrieval, machine translation  and data mining.   
IR
tammer96learning
Learning Case Classification for Improving Case-Based Reasoning this paper, we consider unconstrained graphs which may be either directed or undirected and, possibly, labelled or unlabelled. Computing structural similarity is essentially based on graph matching which actually means computing subgraph isomorphism. This subproblem is known to be NP-complete. It becomes more tractable when using labelled and directed graphs. The particular approach developed and investigated throughout the present workshop presentation can be easily generalized to other structural similarity concepts. For readability, we confine ourselves to graph representations. Because of NP-completeness of the subgraph isomorphism problem, classical case retrieval is extremely expensive, if every member of a usually huge case base is potentially queried. The performance of this search can be increased enormously by prestructuring the case base in case classes, i.e. clustering. Preferably, a given case base CB is clustered with respect to structural similarity, i.e. graphs of a close structural relationship are grouped together and represented by the graph(s) describing their common structural similarity. Let us assume that any given  case base CB is separated in n partitions or classes. Each class CB i (i = 1; :::; n) consists of a set of graph-represented cases and is determined by a graph \Theta
ML
schwabacher98multilevel
Multilevel Simulation and Numerical Optimization of Complex Engineering Designs Multilevel representations have been studied extensively by artificial intelligence researchers. We present a general method that utilizes the multilevel paradigm to attack the problem of performing multidiscipline engineering design optimization in the presence of many local optima. The method uses a multidisciplinary simulator at multiple levels of abstraction, paired with a multilevel search space. We tested the method in the domain of conceptual design of supersonic transport aircraft, focusing on the airframe and the exhaust nozzle, and using sequential quadratic programming as the optimizer at each level. We found that using multilevel simulation and optimization can decrease the cost of design space search by an order of magnitude. 1 Introduction  A major barrier to the use of gradient-based search methods for engineering design is that complex, multidisciplinary design spaces tend    An earlier version of this article was presented at 6  th  AIAA/NASA/USAF Multidisciplinary Ana...
ML
parekh00constructive
Constructive Neural Network Learning Algorithms for Multi-Category Pattern Classification Constructive learning algorithms offer an attractive approach for the incremental construction of near-minimal neural-network architectures for pattern classification. They help overcome the need for ad hoc and often inappropriate choices of network topology in algorithms that search for suitable weights in a priori fixed network architectures. Several such algorithms are proposed in the literature and shown to converge to zero classification errors (under certain assumptions) on tasks that involve learning a binary to binary mapping (i.e., classification problems involving binary-valued input attributes and two output categories). We present two constructive learning algorithms MPyramid-real and MTiling-real that extend the pyramid and tiling algorithms, respectively, for learning real to M-ary mappings (i.e., classification problems involving real-valued input attributes and multiple output classes). We prove the convergence of these algorithms and empirically demonstrate their applicability to practical pattern classification problems. Additionally, we show how the incorporation of a local pruning step can eliminate several redundant neurons from MTiling-real networks.
ML
bardram97plans
Plans as Situated Action: An Activity Theory Approach to Workflow Systems : Within the community of CSCW the notion and nature of workflow systems as prescriptions of human work has been debated and criticised. Based on the work of Suchman (1987) the notion of situated action has often been viewed as opposed to planning work. Plans, however, do play an essential role in realising work. Based on experiences from designing a computer system that supports the collaboration within a hospital, this paper discusses how plans themselves are made out of situated action, and in return are realised in situ. Thus, work can be characterised as situated planning. This understanding is backed up by Activity Theory, which emphasises the connection between plans and the contextual conditions for realising these plans in actual work.  Introduction  The issue of workflow systems has been addressed by several authors as ways of routing information objects among users, and to specify automatic actions to be taken in that routing typically according to certain process models (Me...
HCI
marques01providing
Providing Applications with Mobile Agent Technology Over the last couple of years we have been working on the development of mobile agents systems and its application to the areas of telecommunications and network management. This work path produced positive results: a competitive mobile agent platform was built, the run-time benefits of mobile agents were proved, and our industrial partners have developed practical applications that are being integrated into commercial products. However,
Agents
huget02language
A Language for Exchanging Agent UML Protocol Diagrams For several years, interaction protocol designers have a new formalism which takes into account multiagent system features: autonomy, cooperation, etc. This formalism is called Agent UML [6]. For the moment, designers can describe protocols with the Agent UML protocol diagrams but they do not have a textual language in order to exchange protocols or to check properties on them. The aim of this paper is to provide such a language. This language is called AXF (Agent UML eXchange Format) and is structured as an XML file. This paper presents the syntax of this language and applies AXF to the example of the English Auction Protocol. This paper is published as the technical report ULCS-02-009 from the department of computer science, University of Liverpool.
Agents
hermansky98traps
Traps - Classifiers Of Temporal Patterns The work proposes a radically different set of features for ASR where TempoRAl Patterns of spectral energies are used in place of the conventional spectral patterns. The approach has several inherent advantages, among them robustness to stationary or slowly varying disturbances.  1. INTRODUCTION 1.1. Spectral features  In 1665 Isaac Newton made the following observation:  'The filling of a very deepe flaggon with a constant streame of beere or water sounds yer vowells in this order w, u, !,  o, a, e, i, y' [8]. What young Newton observed was the spectral resonance peak which enhanced the spectrum of the beer pouring sound and moved up in frequency as the "deepe flaggon" was filling up. Since then, attempts to find acoustic correlates of phonetic categories mostly followed Newton's lead and studied the spectrum of speech. Spectrum-based techniques form the basis of most feature extraction methods in current ASR. A problem with the spectrum of sound is that it can easily be modified by v...
DB
wijsen99temporal
Temporal FDs on Complex Objects ing with credit is permitted. To copy otherwise, to republish, to Post on servers, or to redistribute to lists, requires prior speci#c permission and#or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 #212# 869-0481, or permissions@acm.org.  2  #  J. Wijsen  in reality, regardless of the past and future.  In many applications, information about the past and future is just as important as information about the present. Incorporating the time dimension into existing database theory and practice is interesting and important. Temporal databases extend classical #snapshot" databases by supporting the storage and the access of time-related information. When history is taken into account, integrity constraints can place restrictions on the evolution of data in time. This paper deals with extending dependency theory for temporal databases with complex objects.  In general, classical dependencies can be applied to temporal databases in a straightforward manner. From an abstr...
DB
bui99invariant
Invariant Fourier-Wavelet Descriptor For Pattern Recognition We present a novel set of descriptors for recognizing complex patterns such as roadsigns, keys, aircrafts, characters, etc. Given a pattern, we first transform it to polar coordinate (r; `) using the centre of mass of the pattern as origin. We then apply the Fourier transform along the axis of polar angle ` and the wavelet transform along the axis of radius r. The features thus obtained are invariant to translation, rotation, and scaling. As an example, we apply the method to a database of 85 printed Chinese characters. The result shows that the Fourier-Wavelet descriptor is an efficient representation which can provide for reliable recognition. Feature Extraction, Fourier Transform, Invariant Descriptor, Multiresolution Analysis, Pattern Recognition, Wavelet Transform. 1 Introduction  Feature extraction is a crucial processing step for pattern recognition  (15)  . Some authors  (5\Gamma7;13) extract 1-D features from 2-D patterns. The advantage of this approach is that we can save spa...
ML
475379
Clipping and Analyzing News Using Machine Learning Techniques Generating press clippings for companies manually requires a considerable amount of resources. We describe a system that monitors online newspapers and discussion boards automatically. The system extracts, classifies and analyzes messages and generates press clippings automatically, taking the specific needs of client companies into account. Key components of the system are a spider, an information extraction engine, a text classifier based on the Support Vector Machine that categorizes messages by subject, and a second classifier that analyzes which emotional state the author of a newsgroup posting was likely to be in. By analyzing large amount of messages, the system can summarize the main issues that are being reported on for given business sectors, and can summarize the emotional attitude of customers and shareholders towards companies.
IR
arkin98cooperative
Cooperative Multiagent Robotic Systems Introduction  Teams of robotic systems at first glance might appear to be more trouble than they are worth. Why not simply build one robot that is capable of doing everything we need? There are several reasons why two robots (or more) can be better than one:  ffl Distributed Action: Many robots can be in many places at the same time  ffl Inherent Parallelism: Many robots can do many, perhaps different things at the same time  ffl Divide and Conquer: Certain problems are well suited for decomposition and allocation among many robots  ffl Simpler is better: Often each agent in a team of robots can be simpler than a more comprehensive single robot solution No doubt there are more reasons as well. Unfortunately there are also drawbacks, in particular regarding coordination and elimination of interference. The degree of difficulty imposed depends heavily upon the task and the communication and control strategies chosen.
AI
310437
Distance Metrics and Indexing Strategies for a Digital Library of Popular Music Introduction  Digital libraries until now could hardly be described as popular: they tend to be based on esoteric, scholarly sources close to the interests of digital library researchers themselves. We are developing a digital library containing the quintessence of popular culture: music. The principal mode of searching this library will be by sung query: the system should be able to answer the kinds of queries that shop assistants in music stores deal with every day, where a customer can sing a tune, but can't remember the title or artist.  The operation of our musical digital library is sketched in Figure 1. At the time of collection creation, MIDI files are gathered from the internet, and indexed based on their note sequences. At retrieval time, a user's sung query is transformed from a waveform to a sequence of pitch-duration events by a pitch tracker. We are using an off-the-shelf pitch tracker: pitch tracking is beyond the scope of the current project. Pot
IR
omer00javacorba
A Java/CORBA based Visual Program Composition Environment for PSEs A Problem Solving Environment (PSE) is a complete, integrated computing environment for composing, compiling and running applications in a specific problem area or domain. Parts of the PSE are domain independent, such as the Visual Programming Composition Environment (VPCE), which may be used for constructing application in a number of different domains, however, other parts are domain specific, such as rules to support particular types of components. A domain independent VPCE is first described, which serves as a user interface for a PSE, and uses Java and CORBA to provide a framework of tools to enable the construction of scientific applications from components. The VPCE consists of a component repository, from which the user can select off-the-shelf or in-house components, a graphical composition area on which components can be combined, various tools that facilitate the configuration of components, the integration of legacy codes into components and the design and bui...
AI
langheinrich00first
First Steps Towards an Event-Based Infrastructure for Smart Things In this paper, we examine requirements for an infrastructure that supports implementation and deployment of smart things in the real world. We describe a case study (RFID Chef) where kitchen items and ingredients, equipped with remotely accessible electronic tags, drive an interactive context-aware recipe finder through the use of an event-based infrastructure.
HCI
vandertorre99violation
Violation Contexts and Deontic Independence . In this paper we discuss the role of context and independence in normative reasoning. First, deontic operators -- obligations, prohibitions, permissions -- referring to the ideal context may conflict with operators referring to a violation (or contrary-to-duty) context. Second, deontic independence is a powerful concept to derive deontic operators from such operators of other violation contexts. These two concepts are used to determine how to proceed once a norm has been violated, a key issue of deontic logic applications in computer science. We also show how violation contexts and deontic independence can be used to give a new analysis of several notorious paradoxes of deontic logic. 1 Introduction  Deontic logic is a modal logic in which flp is read as `p ought to be (done),'  Fp as `p is forbidden to be (done)' and Pp as `p is permitted to be (done).' Deontic logic has traditionally been used by philosophers to analyze the structure of the normative use of language. In the eightie...
Agents
florescu99query
Query Optimization in the Presence of Limited Access Patterns 1 Introduction The goal of a query optimizer of a database system is to translate a declarative query expressed on a logical schema into an imperative query execution plan that accesses the physical storage of the data, and applies a sequence of relational operators. In building query execution plans, traditional relational query optimizers try to find the most efficient method for accessing the necessary data. When possible, a query optimizer will use auxiliary data structures such as an index on a file in order to efficiently retrieve a certain set of tuples in a relation. However, when such structures do not exist or are not useful for the given query, the alternative of scanning the entire relation always exists. The existence of the fall back option to perform a complete scan is an important assumption in traditional query optimization. Several recent query processing applications have the common characteristic that it is not always possible to perform complete scans on the data. Instead, the query optimization problem is complicated by the fact that there are only limited access patterns to the data. One such
DB
flach98strongly
Strongly Typed Inductive Concept Learning . In this paper we argue that the use of a language with a type system, together with higher-order facilities and functions, provides a suitable basis for knowledge representation in inductive concept learning and, in particular, illuminates the relationship between attribute-value learning and inductive logic programming (ILP). Individuals are represented by closed terms: tuples of constants in the case of attribute-value learning; arbitrarily complex terms in the case of ILP. To illustrate the point, we take some learning tasks from the machine learning and ILP literature and represent them in Escher, a typed, higher-order, functional logic programming language being developed at the University of Bristol. We argue that the use of a type system provides better ways to discard meaningless hypotheses on syntactic grounds and encompasses many ad hoc approaches to declarative bias. 1. Motivation and scope  Inductive concept learning consists of finding mappings of individuals (or objects...
AI
cavedon95revisiting
Revisiting Rationality for Agents With Intentions Formal frameworks for the specification of autonomous agents are commonly based on logics of intention and belief. Desirable properties for logics of intention are particularly non-standard, even more so than for logics of belief. In this paper, we address problems with existing logics of intention and belief by shifting to a non-classical semantics, making use of Rantala's impossible, or non-normal, worlds. Our framework invalidates the problematic properties of intention and, by imposing certain constraints on the algebraic structure of the models, we show that that many desirable properties can be obtained. The non-normal worlds framework provides a fine-grained semantics and proves to be an extremely powerful and flexible tool for the logical specification of rational agent behaviour. 1. Introduction  Logics of Belief, Desire and Intention (BDI ) have recently received much attention in the AI literature on the design of autonomous intelligent agents (e.g. [2, 3, 6]). The importanc...
Agents
31199
Projective Translations and Affine Stereo Calibration This paper investigates the structure of projective translations - rigid translations expressed as homographies in projective space. A seven parameter representation is proposed, which explicitly represents the geometric entities constraining and defining the translation. A practical algebraic method for estimating these parameters is developed. It provides affine calibration of a stereo rig, determines the translation axis, and allows projective translations to be composed. The practical effectiveness of the calibration is evaluated on synthetic and real image data.  1 Introduction  The recovery of structure and motion is a basic problem in machine vision. The difficulties increase when uncalibrated cameras are considered, since in this case metric information is missing. Nevertheless, the rigidity of 3D-motions imposes strong consistency constraints. This allows metric information to be extracted. Mathematically, given a set of points reconstructed in projective space, the effect of ...
AI
schmalstieg99using
Using Transparent Props For Interaction With The Virtual Table The Virtual Table presents stereoscopic graphics to a user in a workbench-like setting. This paper reports on a user interface and new interaction techniques for the Virtual Table based on transparent props---a tracked hand-held pen and a pad. These props, but in particular the pad, are augmented with 3D graphics from the Virtual Table's display. This configuration creates a very powerful and flexible interface for two-handed interaction that can be applied to other back-projected stereographic displays as well: the pad can serve as a palette for tools and controls as well as a window-like see-through interface, a plane-shaped and throughthe -plane tool, supporting a variety of new interaction techniques. 1. INTRODUCTION While the desktop metaphor is well-understood and represents an effective approach to human-computer interaction for documentoriented 2D tasks, transplanting it to 3D reveals inherent limitations (e.g. [8]). In contrast, interfaces that incorporate true 3D input and ...
HCI
baral00reasoning
Reasoning Agents In Dynamic Domains The paper discusses an architecture for intelligent agents based on the use of A-Prolog - a language of logic programs under the answer set semantics. A-Prolog is used to represent the agent's knowledge  about the domain and to formulate the agent's reasoning tasks. We outline how these tasks can be reduced to answering questions about  properties of simple logic programs and demonstrate the methodology  of constructing these programs.  Keywords: Intelligent agents, logic programming and nonmonotonic reasoning.  1 INTRODUCTION  This paper is a report on the attempt by the authors to better understand  the design of software components of intelligent agents capable of reasoning, planning and acting in a changing environment. The class of such agents includes, but is not limited to, intelligent mobile robots, softbots, immobots, intelligent information systems, expert systems, and decision-making systems. The ability to design intelligent agents (IA) is crucial for such diverse tasks as ...
Agents
meyer99automatic
Automatic Construction of Intelligent Diagrammatic Environments Introduction  Graphical user interfaces have become an integral part of almost every modern application type and it can be claimed that they are among the driving forces that have made the computer accessible to non-expert users. However, comparing the use of graphics in existent user interfaces with that in non-computer-based work, the inadequacy of standard GUIs for complex visual communication is revealed: Most GUIs are still WIMP interfaces centered around such simple interaction devices like icons, buttons, menus or image maps. On the contrary, in non-computerbased work rich and highly structured graphical notations prevail. There are diagrammatic languages in almost every technical discipline, for example circuit diagrams, architectural floor plans or chemical formulas, and modern software engineering is embracing all kinds of diagrammatic specification methods. Likewise, non-technical fields use their own well-established diagrammatic systems, for example choreography no
HCI
11708
Conflicts in a Simple Autonomy-Based Multi-Agent System This paper shows that, in some situations, conflict can deliberately be left in an autonomy-based multi-agent system. This study, supported by experimental results, has two major outcomes. First, it proves that conflict does not necessarily alter the global outcome of the system in a qualitative way. Second, it shows that it is possible to effect the way the global task is achieved by appropriately modifying the environment of the agents. Introduction  Our work fits in the framework of Bottom-Up Artificial Intelligence (Brooks 1986), (Brooks 1991) and more particularly, in that of Autonomous Agents (Pfeifer 1995). We are concerned with collective phenomena and their issues and more precisely, the way to carry out solutions that allow an autonomy-based multi-agent system to achieve a global task by virtue of emergence and self-organization. Emergence offers indeed a bridge between the necessity of complex and adaptive behavior at a macro level (the one of the system) and situationbased ...
Agents
6729
Reinventing the Familiar: Exploring an Augmented Reality Design Space for Air Traffic Control This paper describes our exploration of a design space for an augmented reality prototype. We began by observing air traffic controllers and their interactions with paper flight strips. We then worked with a multi-disciplinary team of researchers and controllers over a period of a year to brainstorm and prototype ideas for enhancing paper flight strips. We argue that augmented reality is more promising (and simpler to implement) than the current strategies that seek to replace flight strips with keyboard/monitor interfaces. We also argue that an exploration of the design space, with active participation from the controllers, is essential not only for designing particular artifacts, but also for understanding the strengths and limitations of augmented reality in general.  Keywords: Augmented Reality, Design Space, Interactive Paper, Participatory Design, Video Prototyping  INTRODUCTION  Air traffic control is a complex, collaborative activity, with well-established and successful work p...
HCI
300048
Towards Detection of Human Motion Detecting humans in images is a useful application of computer vision. Loose and textured clothing, occlusion and scene clutter make it a difficult problem because bottom-up segmentation and grouping do not always work. We address the problem of detecting humans from their motion pattern in monocular image sequences; extraneous motions and occlusion may be present. We assume that we may not rely on segmentation, nor grouping and that the vision front-end is limited to observing the motion of key points and textured patches in between pairs of frames. We do not assume that we are able to track features for more than two frames. Our method is based on learning an approximate probabilistic model of the joint position and velocity of different body features. Detection is performed by hypothesis testing on the maximum a posteriori estimate of the pose and motion of the body. Our experiments on a dozen of walking sequences indicate that our algorithm is accurate and efficient.
ML
manolescu01efficient
Efficient Data and Program Integration Using Binding Patterns : In this work, we investigate data and program integration in a fully distributed peer-to-peer mediation architecture. The challenge in making such a system succeed at a large scale is twofold. First, sharing a resource should be easy; therefore, we need a simple concept for modeling resources. Second, we need an ecient architecture for distributed query execution, capable of handling well costly computations and large data transfers. To model heterogeneous resources, we propose using the unied abstraction of table with binding patterns, simple yet powerful enough to capture data and programs. To exploit a resource with restricted binding patterns, we propose an ecient BindJoin operator, following the classical iterator model, in which we build optimization techniques for minimizing large data transfers and costly computations, and maximizing parallelism. Furthermore, our BindJoin operator can be tuned to deliver most of its output in the early stages of the execution, which is an important asset in a system meant for human interaction. Our preliminary experimental evaluation validates the proposed BindJoin algorithms, and shows they can provide good performance in queries involving distributed data and expensive programs.  Key-words: data and program integration, distributed query processing, binding patterns   INRIA, Caravel project. Contact: Ioana.Manolescu@inria.fr  y  PRISM laboratory, University of Versailles and INRIA, Caravel project. Contact: Luc.Bouganim@prism.uvsq.fr  z  INRIA, Caravel project. Contact: Francoise.Fabret@inria.fr  x  INRIA, Caravel project. Contact: Eric.Simon@inria.fr  Intgration ecace de donnes et de programmes  utilisants des patterns d'accs  Rsum : Dans ce rapport, nous tudions l'intgration de donnes et de programmes dans une archite...
DB
531861
A Framework For Designing, Modeling and Analyzing Agent Based Software Systems The agent paradigm is gaining popularity because it brings intelligence, reasoning and autonomy to software systems. Agents are being used in an increasingly wide variety of applications from simple email filter programs to complex mission control and safety systems. However there appears to be very little work in defining practical software architecture, modeling and analysis tools that can be used by software engineers. This should be contrasted with object-oriented paradigm that is supported by models such as UML and CASE tools that aid during the analysis, design and implementation phases of object-oriented software systems. In our research we are developing a framework and extensions to UML to address this need. Our approach is rooted in the BDI formalism, but stresses the practical software design methods instead of reasoning about agents. In this paper we describe our preliminary ideas  Index Terms: Agent-Oriented programming, ObjectOriented programming, BDI, UML  1. 
Agents
soundalgekar01internet
Internet search for Indian languages With the Internet growing at an exponential rate, no single search engine can index all of the web. It is therefore necessary to build specialized search engines that fulfill particular needs of a community of people. An example is Citeseer, which indexes research papers on the web. Also, as the web is increasingly hosting web pages in different languages, it is essential to be able to search for information stored in a specific language. For a search engine aimed at information in a particular language, an easy to use user interface is as essential as good response time and relevance of results. We introduce Shodh, a search engine for an Indian language. A prototype has been developed for selected set of pages and results are satisfactory. The user interface of the search engine includes both querying facilities as well as display of query results in the same language in which the information is stored. Contents 1
IR
heinzmann98visual
A Visual Interface for Human-Robot Interaction This paper describes the architecture of a human-robot interaction system and recent work on the vision based human-robot interface. The aim of the project is to develop a robotic system that is safely able to work with a human operator. This means in particular that the operator should be allowed into the work space of the robot and even to interact with the robot by direct contact with the manipulator. Safety issues are of major importance here, so open loop force control and backdrivability of the manipulator are mandatory features of the system. The human-robot interface is visionbased to achieve a natural interaction between the operator and the robot. Important aspects of this interaction are the ability of the vision system to find and track the operators face, to recognise facial gestures and to determine the users gaze point. These functions are implemented in a robust way so tracking failures can be compensated for and temporary occlusions of the face are tolerated. 1 Introdu...
AI
bailey95active
Active Databases and Agent Systems - A Comparison This paper examines Active Databases and Agent Systems, comparing their purpose, structure, functionality, and implementation. Our presentation is aimed primarily at an audience familiar with active database technology. We show that they draw upon very similar paradigms in their quest to supply reactivity. This presents opportunities for migration of techniques and formalisms between the two fields. 1  fjbailey,kemp,dnk,raog@cs.mu.oz.au  2  georgeff@aaii.oz.au  3  Appears in T.Sellis, editor, Proceedings of the Second International Workshop on Rules in Database Systems, Lecture Notes in Computer Science 985, pages 342-356, Athens, Greece, 1995.  1 Introduction  In recent times, two technologies have become prominent in the database and artificial intelligence research communities. An Active Database (ADB) is a system which supplements traditional database functionality by reacting automatically to state changes, both internal and external, without user intervention. An Agent System (A...
Agents
496736
Flexible Interoperability in a Federated Digital Library of Theses and Dissertations Federated digital libraries are composed of autonomous, possibly heterogeneous information services distributed across the Internet. Federation provides users with a seamless, integrated view of the collected information. We are creating a  federated system for the Networked Digital Library of Theses and Dissertations (NDLTD), an international consortium of  universities, libraries, and other supporting institutions focused on electronic theses and dissertations (ETDs). The NDLTD  allows its members minimal restrictions and maximal autonomy, so federating requires dealing flexibly with differences among  ontologies, data formats, and finding aids involving several thousand ETDs in four formats and two languages.
IR
myers01interacting
Interacting At a Distance Using Semantic Snarfing . It is difficult to interact with computer displays that are across the  room, which can be important in meetings and when controlling computerized  devices. A popular approach is to use laser pointers tracked by a camera, but interaction  techniques using laser pointers tend to be imprecise, error-prone, and  slow. Therefore, we have developed a new interaction style, where the laser  pointer (or other pointing technique such pointing with a finger or even eye  tracking) indicates the region of interest, and then the item there is copied  ("snarfed") to the user's handheld device, such as a Palm or PocketPC handheld.  If the content changes on the PC, the handheld's copy will be updated as well.  Interactions can be performed on the handheld using familiar direct manipulation  techniques, and then the modified version is sent back to the PC. The content  often must be reformatted to fit the properties of the handheld to facilitate  natural interaction.  1 
HCI
sistla98querying
Querying the Uncertain Position of Moving Objects In this paper we propose a data model for representing moving objects with uncertain positions in database systems. It is called the Moving Objects Spatio-Temporal (MOST) data model. We also propose Future Temporal Logic (FTL) as the query language for the MOST model, and devise an algorithm for processing FTL queries in MOST. 1 Introduction  Existing database management systems (DBMS's) are not well equipped to handle continuously changing data, such as the position of moving objects. The reason for this is that in databases, data is assumed to be constant unless it is explicitly modified. For example, if the salary field is 30K, then this salary is assumed to hold (i.e. 30K is returned in response to queries) until explicitly updated. Thus, in order to represent moving objects (e.g. cars) in a database, and answer queries about their position (e.g., How far is the car with license plate RWW860 from the nearest hospital?) the car's position has to be continuously updated. This is unsa...
DB
deutschmann98compressive
Compressive Computation in Analog VLSI Motion Sensors . We introduce several different focal plane analog VLSI motion sensors developed in the past. We show how their pixel-parallel architecture can be used to extract low-dimensional information from a higher dimensional data set. As an example we present an algorithm and corresponding experiments to compute the focus of expansion, focus of contraction and the axis of rotation from natural visual input. A fully integrated system for real-time computation of these quantities is proposed as well. In computer simulations it is shown that the direction of motion vector field is best suited to perform the algorithm even at high noise levels. 1 Analog VLSI Motion Sensors  In the past the computer vision communityhas invested much effort into developing motion detection algorithms; for a critical review see [BFB94]. Implementing these algorithms in real-time systems proved challenging for computational reasons. Additionally it has been realized that a motion vector field is useful mainly as star...
ML
suthers01collaborative
Collaborative Representations: Supporting Face to Face and Online Knowledge-building Discourse The present widespread interest in the use of electronic media for presents an unprecedented opportunity for leveraging the computational medium's strengths for learning. However, existing software tools provide only primitive support for online knowledge-building discourse. Further work is needed in supporting coordinated use of disciplinary representations, discourse representations, and knowledge representations. This paper introduces the concept of representational guidance for discourse along with results of an initial study of this phenomenon in face to face situations. The paper then considers the requirements for supporting asynchronous online knowledge-building discourse, finding existing computer mediated communication tools to be particularly deficient in supporting artifact-centered discourse. A solution is proposed that coordinates discourse representations with disciplinary and knowledge representations.  1. Introduction  There is a great deal of interest in the use of el...
HCI
freitas98pkdd
PKDD'98 Tutorial on Scalable, High-Performance Data Mining with Parallel Processing Contents 1 Introduction 2 Overview of 7 different approaches for speeding up data mining in large databases 3 An overview of parallel processing for data mining 4 Parallel rule induction 5 Parallel Instance-Based Learning 6 Parallel Genetic Algorithms 7 Parallel Neural Networks 8 Conclusions  Introduction.  Problem: How to perform efficient data mining in very large databases. Natural solution: parallelism Performance issues:  any sequential data mining algorithm: O(N) parallelism reduces this lower bound to O(N/p) (N = No. of tuples, p = No. of processors) Cost-benefit issues:  many data warehouses are already implemented on cost-effective parallel database servers  2 Overview of 7 different approaches for speeding up data mining in large databases. Data-Oriented Approaches: (1) Sampling (reduces number of tuples) (2) Attribute selection (reduces number of attributes) (3) Discretization (reduces number of values of attributes, which in
ML
marini00hemasl
HEMASL: A Flexible Language to Specify Heterogeneous Agents In the realization of agent-based applications the developer generally needs to use heterogeneous agent architectures, so that each application component can optimally perform its task. Languages that easily model the heterogeneity of agents' architectures are very useful in the early stages of the application development. This paper presents HEMASL, a simple meta-language used to specify heterogeneous agent architectures, and sketches how HEMASL should be implemented in an object-oriented commercial programming language as Java. Moreover, the paper briefly discusses the benefits of adding HEMASL to CaseLP, a LP-based specification and prototyping environment for multi-agent systems, in order to enhance its flexibility and usability.  1. Introduction  Intelligent agents and multi-agent systems (MASs) are more and more recognized as the "new" modeling techniques to be used to engineer complex and distributed software applications [12]. Agent-based software engineering is concerned with ...
Agents
tzitzikas01democratic
Democratic Data Fusion for Information Retrieval Mediators Our research presented in this paper concerns the problem of fusing the results returned by the underlying systems to a mediating retrieval system, also called meta-retrieval system, meta-search engine, or mediator. We propose a fusion technique which is based solely on the actual results returned by each system for each query. The final (fused) ordering of documents is derived by aggregating the orderings of each system in a democratic manner. In addition, the fused ordering is accompanied by a level of democracy (alternatively construed as the level of confidence).
IR
coelho00developing
Developing Haptic and Visual Perceptual Categories for Reaching and Grasping with a Humanoid Robot Properties of the human embodiment -- sensorimotor apparatus and neurological structure -- participate directly in the growth and development of cognitive processes against enormous worst case complexity. It is our position that relationships between morphology and perception over time lead to increasingly comprehensive models that describe the agent's relationship to the world. We are applying insight derived from neuroscience, neurology, and developmental psychology to the design of advanced robot architectures. To investigate developmental processes, we have begun to approximate the human sensorimotor configuration and to engage sensory and motor subsystems in developmental sequences. Many such sequences have been documented in studies of infant development, so we intend to bootstrap cognitive structures in robots by emulating some of these growth processes that bear an essential resemblance to the human morphology. In this paper, we will show two related examples in which a humanoid robot determines the models and representations that govern its behavior. The first is a model that captures the dynamics of a haptic exploration of an object with a dextrous robot hand that supports skillful grasping. The second example constructs constellations of visual features to predict relative hand/object postures that lead reliably to haptic utility. The result is a rst step in a trajectory toward associative visual-haptic categories that bounds the incremental complexity of each stage of development.
ML
295186
Assessment Methods for Information Quality Criteria Information quality (IQ) is one of the most important aspects of information integration on the Internet. Many projects realize and address this fact by gathering and classifying IQ criteria. Hardly ever do the projects address the immense difficulty of assessing scores for the criteria. This task must precede any usage of criteria for qualifying and integrating information.  After reviewing previous attempts to classify IQ criteria, in this paper we also classify criteria, but in a new, assessment-oriented way. We identify three sources for IQ scores and thus, three IQ criterion classes, each with different general assessment possibilities. Additionally, for each criterion we give detailed assessment methods. Finally, we consider confidence measures for these methods. Confidence expresses the accuracy, lastingness, and credibility of the individual assessment methods.  1 Introduction  Low information quality is one of the most pressing problems for consume rs of information that is di...
IR
531754
Prometheus: A Methodology for Developing Intelligent Agents Abstract. As agents gain acceptance as a technology there is a growing need for practical methods for developing agent applications. This paper presents the Prometheus methodology, which has been developed over several years in collaboration with Agent Oriented Software. The methodology has been taught at industry workshops and university courses. It has proven effective in assisting developers to design, document, and build agent systems. Prometheus differs from existing methodologies in that it is a detailed and complete (start to end) methodology for developing intelligent agents which has evolved out of industrial and pedagogical experience. This paper describes the process and the products of the methodology illustrated by a running example. 1
Agents
278114
Extending Classical Logic with Inductive Definitions The goal of this paper is to extend classical logic with  a generalized notion of inductive definition supporting  positive and negative induction, to investigate the  properties of this logic, its relationships to other logics  in the area of non-monotonic reasoning, logic programming  and deductive databases, and to show its application  for knowledge representation by giving a typology  of definitional knowledge.  
DB
hollerer99situated
Situated Documentaries: Embedding Multimedia Presentations in the Real World We describe an experimental wearable augmented reality system that enables users to experience hypermedia presentations that are integrated with the actual outdoor locations to which they are are relevant. Our mobile prototype uses a tracked see-through head-worn display to overlay 3D graphics, imagery, and sound on top of the real world, and presents additional, coordinated material on a hand-held pen computer. We have used these facilities to create several  situated documentaries that tell the stories of events that took place on our campus. We describe the software and hardware that underly our prototype system and explain the user interface that we have developed for it.
HCI
wills01open
An Open Platform for Reconfigurable Control bility.  . Openness: Reconfigurability and component interchangeability require software architectures that are flexible and that support tools and algorithms from a variety of sources and domains. This requires a shift away from traditional control system implementation, which tends to be practiced with a particular apJune 2001 IEEE Control Systems Magazine 49  By Linda Wills, Suresh Kannan, Sam Sander, Murat Guler,  Bonnie Heck, J.V.R. Prasad, Daniel Schrage, and George Vachtsevanos Wills (linda.wills@ece.gatech.edu), Sander, Guler, Heck, and Vachtsevanos are with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332-0250, U.S.A. Kannan, Prasad, and Schrage are with the School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA 30332-0250, U.S.A.   1998 Corbis Corp. 0272-1708/01/$10.002001IEEE  plication in mind and which makes rigid,
ML
harel99workload
Workload of a Media-Enhanced Classroom Server We charaterize a workload of media-enhanced classrooms. Such classrooms include equipment for presenting multimedia streams and for capturing streams of information (audio, video and notes) during a lecture. We present detailed quantitative performance measurements of one media-enhanced classroom system, Classroom 2000. We characterize the workload from the point of view of a server that supports multiple classrooms. The workload includes server bandwidth, network bandwidth and server storage requirements. We identify patterns in user behavior, and demonstrate how the number of simultaneous study sessions varies with time of day and with the proximity of a specific date to exams. 1 Workload of a Media-Enhanced Classroom Server 1 Introduction The ways we teach and learn will be dramatically affected by current, unprecedented rates of improvement in computational power and network bandwidth, as well as the development of innovative user interfaces and virtual environments. Alre...
HCI
dempski99augmented
Augmented Workspace: The World as Your Desktop . We live in a three dimensional world, and much of what we do and  how we interact in the physical world has a strong spatial component. Unfortunately,  most of our interaction with the virtual world is two dimensional. We  are exploring the extension of the 2D desktop workspace into the 3D physical  world, using a stereoscopic see-through head-mounted display. We have built a  prototype that enables us to overlay virtual windows on the physical world.  This paper describes the Augmented Workspace, which allows a user to pos ition  windows in a 3D work area.  Keywords. Ubiquitous computing, cooperative buildings, human-computer interaction,  physical space, context awareness, visualization.  1. Introduction  In our daily lives, much of what we do and how we interact has a strong spatial component. Your calendar is on a wall, or on a certain part of your desk, and sticky notes are placed on walls and whiteboards. Yet, as an increasing portion of our work is done on computers, a large m...
HCI
brandt00exploring
Exploring auction-based leveled-commitment contracting - Part I: English-type auctioning A key problem addressed in the area of multiagent systems is the automated  assignment of multiple tasks to executing agents. The automation  of multiagent task assignment requires that the individual agents (i) use  a common protocol that prescribes how they have to interact in order to  come to an agreement and (ii) fix their final agreement in a contract that  specifies the commitments resulting from the assignment on which they  agreed. This report describes a novel approach to automated task assignment  in multiagent systems that is based on an auction-based protocol  and on leveled commitment contracting. This approach is applicable in a  broad range of realistic scenarios in which knowledge-intensive negotiation  among agents is not feasible and in which future environmental changes  may require agents to breach their contracts.  1 Introduction  The area of multiagent systems (e.g., [5, 8, 16]), which is concerned with systems composed of technical entities called agents that in...
Agents
jantke97theoretical
Theoretical Investigations And Experimental Explorations Of The Necessity Of User Guidance In Case-Based Knowledge Acquisition The intention of the present paper is to justify both theoretically and experimentally that user guidance is inevitable in case-based knowledge acquisition. The methodology of our approach is quite simple: We choose a well-understood area which is tailored to case-based knowledge acquisition. Furthermore, we choose a prototypical case-based learning algorithm which is obviously suitable for the problem domain under consideration. Then, we perform a number of knowledge acquisition experiments. They clearly exhibit essential limitations of knowledge acquisition from randomly chosen cases. As a consequence, we develop scenarios of user guidance. Based on these theoretical concepts, we prove a few theoretical results characterizing the power of our approach. Next, we perform a new series of more constrained results which support our theoretical investigations. The present report aims at presenting a large amount of experimental data exceeding the space available in conference proceedings, ...
ML
chien00version
Version Management of XML Documents The problem of ensuring efficient storage and fast retrieval  for multi-version structured documents is important because of the recent  popularity of XML documents and semistructured information on  the web. Traditional document version control systems, e.g. RCS, which  model documents as a sequence of lines of text and use the shortest edit  script to represent version differences, can be inefficient and they do not  preserve the logical structure of the original document. Therefore, we propose  a new approach where the structure of the documents is preserved  intact, and their sub-objects are timestamped hierarchically for efficient  reconstruction of current and past versions. Our technique, called the  Usefulness Based Copy Control (UBCC), is geared towards efficient version  reconstruction while using small storage overhead. Our analysis and  experiments illustrate the effectiveness of the overall approach to version  control for structured documents. Moreover UBCC can easily support  multiple concurrent versions as well as partial document retrieval.  1 
DB
suryadi99learning
Learning Models of Other Agents Using Influence Diagrams We adopt decision theory as a descriptive paradigm to model rational agents. We use influence diagrams as a modeling representation of agents, which is used to interact with them and to predict their behavior. In this paper, we provide a framework that an agent can use to learn the models of other agents in a multi-agent system (MAS) based on their observed behavior. Since the correct model is usually not known with certainty our agents maintain a number of possible models and assign a probability to each of them being correct. When none of the available models is likely to be correct, we modify one of them to better account for the observed behaviors. The modification refines the parameters of the influence diagram used to model the other agent's capabilities, preferences, or beliefs. The modified model is then allowed to compete with the other models and the probability assigned to it being correct can be arrived at based on how well it predicts the behaviors of the other agent alrea...
Agents
zhao99discriminant
Discriminant Analysis based Feature Extraction We propose a new feature extraction scheme called  Discriminant Component Analysis. The new scheme decomposes a signal into orthonormal bases such that for each base there is an eigenvalue representing the discriminatory power of projection in that direction. The bases and eigenvalues are obtained based on certain classification criterion. For simplicity, a criterion used in Fisher's Discriminant Analysis (DA) is chosen and is applied iteratively to implement the scheme. We illustrate the motivation of this new scheme and show how it can be used to construct new distance metrics. We then argue that these new distance metrics are more robust than DA based metrics. Finally, very good classification performance on simulation data and real face images are demonstrated using these new distance metrics.  1 Introduction  It is important that for different applications, we use different representations for the same signal [1]. For example, PCA (Principal Component Analysis) or wavelet decompos...
ML
147445
Process-Oriented Estimation of Generalization Error Methods to avoid overfitting fall into two broad  categories: data-oriented (using separate data  for validation) and representation-oriented (penalizing  complexity in the model). Both have  limitations that are hard to overcome. We  argue that fully adequate model evaluation is  only possible if the search process by which  models are obtained is also taken into account.  To this end, we recently proposed a method  for process-oriented evaluation (POE), and successfully  applied it to rule induction [ Domingos,  1998b ] . However, for the sake of simplicity this  treatment made a number of rather artificial assumptions.  In this paper the assumptions are  removed, and a simple formula for error estimation  is obtained. Empirical trials show the  new, better-founded form of POE to be as accurate  as the previous one, while further reducing  theory sizes.  1 Introduction  Overfitting avoidance is a central problem in machine learning. If a learner is su#ciently powerful, whatever repre...
ML
sarkar01applying
Applying Co-Training methods to Statistical Parsing We propose a novel Co-Training method for statistical parsing. The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text. The algorithm iteratively labels the entire data set with parse trees. Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data. 1
IR
23780
Advances in Large Margin Classifiers Introduction to Large Margin Classiers Alexander J. Smola  GMD FIRST Rudower Chaussee 5 12489 Berlin, Germany smola@rst.gmd.de http://www.rst.gmd.de/smola  Peter Bartlett  Australian National University, RSISE Canberra, ACT 0200, Australia Peter.Bartlett@anu.edu.au http://keating.anu.edu.au/people/bartlett  Bernhard Scholkopf  GMD FIRST Rudower Chaussee 5 12489 Berlin, Germany bs@rst.gmd.de http://www.rst.gmd.de/bs  Dale Schuurmans  Department of Computer Science Unversity of Waterloo Waterloo, Ontario N2L 3G1, Canada dale@cs.uwaterloo.ca http://www.cs.uwaterloo.ca/dale The aim of this chapter is to provide a brief introduction to the basic concepts of large margin classiers for readers unfamiliar with the topic. Moreover it is aimed at establishing a common basis in terms of notation and equations, upon which the subsequent chapters will bu
ML
glover02using
Using Web Structure for Classifying and Describing Web Pages The structure of the web is increasingly being used to improve organization, search, and analysis  of information on the web. For example, Google uses the text in citing documents (documents that  link to the target document) for search. We analyze the relative utility of document text, and the text  in citing documents near the citation, for classification and description. Results show that the text in  citing documents, when available, often has greater discriminative and descriptive power than the text  in the target document itself. The combination of evidence from a document and citing documents  can improve on either information source alone. Moreover, by ranking words and phrases in the citing  documents according to expected entropy loss, we are able to accurately name clusters of web pages,  even with very few positive examples. Our results confirm, quantify, and extend previous research using  web structure in these areas, introducing new methods for classification and description of pages.
IR
dhillon01coclustering
Co-clustering documents and words using Bipartite Spectral Graph Partitioning Both document clustering and word clustering are important and well-studied problems. By using the vector space model, a document collection may be represented as a word-document matrix. In this paper, we present the novel idea of modeling the document collection as a bipartite graph between documents and words. Using this model, we pose the clustering probliem as a graph partitioning problem and give a new spectral algorithm that simultaneously yields a clustering of documents and words. This co-clustrering algorithm uses the second left and right singular vectors of an appropriately scaled word-document matrix to yield good bipartitionings. In fact, it can be shown that these singular vectors give a real relaxation to the optimal solution of the graph bipartitioning problem. We present several experimental results to verify that the resulting co-clustering algoirhm works well in practice and is robust in the presence of noise.
IR
burke01salticus
Salticus: Guided Crawling for Personal Digital Libraries In this paper, we describe Salticus, a web crawler that learns from users' web browsing activity. Salticus enables users to build a personal digital library by collecting documents and generalizing over the user's choices.  Keywords  personal digital library, business intelligence, web crawling, document acquisition  1. 
IR
collier99genia
The GENIA project: corpus-based knowledge acquisition and information extraction from genome research papers We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts. GENIA will be available over the Internet and is designed to aid in information extraction, retrieval and visualisation and to help reduce information overload on researchers. The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet. 1 Introduction  In the context of the global research effort to map the human genome, the Genome Informatics Extraction project, GENIA (GENIA, 1999), aims to support such research by automatically extracting information from biochemical papers and their abstracts such as those available from MEDLINE (MEDLINE, 1999) written by domain specialists. The vast repository of research papers which are the results...
IR
stolzenburg01from
From the Specification of Multiagent Systems by Statecharts to Their Formal Analysis by Model Checking: Towards Safety-Critical Applications A formalism for the specification of multiagent systems should be expressive  and illustrative enough to model not only the behavior of one single agent, but  also the collaboration among several agents and the influences caused by external  events from the environment. For this, state machines [25] seem to provide an  adequate means. Furthermore, it should be easily possible to obtain an implementation  for each agent automatically from this specification. Last but not least, it is  desirable to be able to check whether the multiagent system satisfies some interesting  properties. Therefore, the formalism should also allow for the verification  or formal analysis of multiagent systems, e.g. by model checking [6].  In this paper, a framework is introduced, which allows us to express declarative  aspects of multiagent systems by means of (classical) propositional logic  and procedural aspects of these systems by means of state machines (statecharts).  Nowadays statecharts are a well accepted means to specify dynamic behavior of  software systems. They are a part of the Unified Modeling Language (UML). We  describe in a rigorously formal manner, how the specification of spatial knowledge  and robot interaction and its verification by model checking can be done,  integrating different methods from the field of artificial intelligence such as qualitative  (spatial) reasoning and the situation calculus. As example application domain,  we will consider robotic soccer, see also [24, 31], which present predecessor  work towards a formal logic-based approach for agents engineering.  
Agents
holliday00exploiting
Exploiting Planned Disconnections in Mobile Environments We present the notion of a distributed database made up entirely of mobile components. Since disconnections will be frequent in such an environment, we develop a disconnection and reconnection procedure to allow normal processing on the connected components. We briefly discuss a protocol based on epidemic communication to support such a system while ensuring one-copy serializability.  1 Introduction  Mobile computers and wireless networks are now being integrated into a variety of enterprises for different applications. The prevailing mode of operation with occasional disconnection by a single user will rapidly evolve into a situation where many if not all users are disconnecting and reconnecting in networks that are created in an ad hoc manner, e.g., a wireless network in a meeting room. This will result in mobile computers being integrated as first class entities in distributed information systems. Such mobile computers will inevitably contain data and information that will need to b...
DB
304865
Database Replication Using Epidemic Communication . There is a growing interest in asynchronous replica management  protocols in which database transactions are executed locally, and  their effects are incorporated asynchronously on remote database copies.  In this paper we investigate an epidemic update protocol that guarantees  consistency and serializability in spite of a write-anywhere capability  and conduct simulation experiments to evaluate this protocol. Our results  indicate that this epidemic approach is indeed a viable alternative  to eager update protocols for a distributed database environment where  serializability is needed.  1 Introduction  Data replication in distributed databases is an important problem that has been investigated extensively. In spite of numerous proposals, the solution to efficient access of replicated data remains elusive. Data replication has long been touted as a technique for improved performance and high reliability in distributed databases. Unfortunately, data replication has not delivered on ...
DB
bowman99interaction
Interaction Techniques For Common Tasks In Immersive Virtual Environments - Design, Evaluation, And Application 13.44> . Drew Kessler for help with the SVE toolkit . The Virtual Environments group at Georgia Tech . The numerous experimental subjects who volunteered their time . Dawn Bowman iv TABLE OF CONTENTS Introduction ..................................................................... ................. 1 1.1 Motivation ..................................................................... ...............1 1.2 Definitions.......................................................... ..........................4 1.3 Problem Statement............................................................ ...............6 1.4 Scope of the Research............................................................. ..........7 1.5 Hypotheses........................................................... ........................8 1.6 Contributions........................................................ .....
HCI
82362
Twenty-One at TREC-7: Ad-hoc and Cross-language track This paper describes the official runs of the Twenty-One group for TREC-7. The Twenty-One group participated in the ad-hoc and the cross-language track and made the following accomplishments: We developed a new weighting algorithm, which outperforms the popular Cornell version of BM25 on the ad-hoc collection. For the CLIR task we developed a fuzzy matching algorithm to recover from missing translations and spelling variants of proper names. Also for CLIR we investigated translation strategies that make extensive use of information from our dictionaries by identifying preferred translations, main translations and synonym translations, by defining weights of possible translations and by experimenting with probabilistic boolean matching strategies. 1 Introduction  Twenty-One is a 2 MECU project with 12 partners funded by the EU Telematics programme, sector Information Engineering. The project subtitle is "Development of a Multimedia Information Transaction and Dissemination Tool". Twenty...
IR
thomas00creating
Creating a Customized Access Method for Blobworld We present the design and analysis of a customized access method for the content-based image retrieval system, Blobworld. Using the amdb access method analysis tool, we analyze three existing multidimensional access methods that support nearest neighbor search in the context of the Blobworld application. Based on this analysis, we propose several variants of the R-tree, tailored to address the problems the analysis revealed. We implemented the access methods we propose in the Generalized Search Trees (GiST) framework and analyzed them using amdb, a tool that enables visualization and performance analysis of access methods. We found that two of our access methods have better performance characteristics for the Blobworld application than any of the traditional multi-dimensional access methods we examined. Based on this experience, we draw conclusions for nearest neighbor access method design, and for the task of constructing custom access methods tailored to particular applications. In particular, we found that our \Top X Jagged Bites &quot; bounding predicate performed better than all the other access methods we tested. 1
DB
howe97savvysearch
SavvySearch: A Meta-Search Engine that Learns which Search Engines to Query Search engines are among the most successful applications on the Web today. So many search engines have been created that it is difficult for users to know where they are, how to use them and what topics they best address. Meta-search engines reduce the user burden by dispatching queries to multiple search engines in parallel. The SavvySearch meta-search engine is designed to efficiently query other search engines by carefully selecting those search engines likely to return useful results and by responding to fluctuating load demands on the Web. SavvySearch learns to identify which search engines are most appropriate for particular queries, reasons about resource demands and represents an iterative parallel search strategy as a simple plan. 1 The Application: Meta-Search on the Web  Companies, institutions and individuals must have a presence on the Web; each are vying for the attention of millions of people. Not too surprisingly then, the most successful applications on the Web to dat...
IR
faruquie00translingual
Translingual Visual Speech Synthesis Audio-driven facial animation is an interesting and evolving technique for human-computer interaction. Based on an incoming audio stream, a face image is animated with full lip synchronization. This requires a speech recognition system in the language in which audio is provided to get the time alignment for the phonetic sequence of the audio signal. However, building a speech recognition system is data intensive and is a very tedious and time consuming task. We present a novel scheme to implement a language independent system for audio-driven facial animation given a speech recognition system for just one language, in our case, English. The method presented here can also be used for text to audio-visual speech synthesis.  1. 
HCI
36645
Monte Carlo Localization: Efficient Position Estimation for Mobile Robots This paper presents a new algorithm for mobile robot localization, called Monte Carlo Localization (MCL). MCL is a version of Markov localization, a family of probabilistic approaches that have recently been applied with great practical success. However, previous approaches were either computationally cumbersome (such as grid-based approaches that represent the state space by high-resolution 3D grids), or had to resort to extremely coarse-grained resolutions. Our approach is computationally efficient while retaining the ability to represent (almost) arbitrary distributions. MCL applies sampling-based methods for approximating probability distributions, in a way that places computation " where needed." The number of samples is adapted on-line, thereby invoking large sample sets only when necessary. Empirical results illustrate that MCL yields improved accuracy while requiring an order of magnitude less computation when compared to previous approaches. It is also much easier to implement...
AI
altamura00transforming
Transforming Paper Documents into XML Format with WISDOM++ The transformation of scanned paper documents to a form suitable for an Internet browser is a complex process that requires solutions to several problems. The application of an OCR to some parts of the document image is only one of the problems. In fact, the generation of documents in HTML format is easier when the layout structure of a page has been extracted by means of a document analysis process. The adoption of an XML format is even better, since it can facilitate the retrieval of documents in the Web. Nevertheless, an effective transformation of paper documents into this format requires further processing steps, namely document image classification and understanding. WISDOM++ is a document processing system that operates in five steps: document analysis, document classification, document understanding, text recognition with an OCR, and text transformation into HTML/XML format. The innovative aspects described in the paper are: the preprocessing algorithm, the adaptive page segmen...
IR
norman95goal
Goal Creation in Motivated Agents . Goal creation is an important consideration for an agent that is required to behave autonomously in a real-world domain. This paper describes an agent that is directed, not by a conjunction of top level goals, but by a set of motives. The agent is motivated to create and prioritise different goals at different times as a part of an on-going activity under changing circumstances. Goals can be created both in reaction to, and in anticipation of a situation. While there has been much work on the creation of reactive goals, i.e. goals created in reaction to a situation, the issues involved in the creation of anticipatory, or proactive goals have not been considered in depth. The solution to the goal creation problem outlined here provides an agent with an effective method of creating goals both reactively and proactively, giving the agent a greater degree of autonomy. 1 Introduction  The focus of planning research has principally been concerned with the creation of good plans to satisfy ...
Agents
xu00maintaining
Maintaining Horizontally Partitioned Warehouse Views Data warehouses usually store large amounts of information, representing  an integration of base data from different data sources over a long  time period. Aggregate views can be stored as a set of its horizontal fragments  for the purposes of reducing warehouse query response time and  maintenance cost.  This paper proposes a scheme that efficiently maintains horizontally  partitioned data warehouse views. Using the proposed scheme, only one  view fragment holding the relevant subset of tuples of the view is accessed  for each update. The scheme also includes an approach to reduce the  refresh time for maintaining views that compute aggregate functions MIN  and MAX.  Keywords: Data Warehouse Applications, View Maintenance, Horizontal Partitioning, Performance Improvement.  1 
DB
kiciman00using
Using Dynamic Mediation to Integrate COTS Entities in a Ubiquitous Computing Environment . The original vision of ubiquitous computing [14] is about enabling  people to more easily accomplish tasks through the seamless interworking  of the physical environment and a computing infrastructure.  A major challenge to the practical realization of this vision involves the  integration of commercial-o-the-shelf (COTS) hardware and software  components: consider the awkwardness of such a mundane task as exporting  a textual memo written on a Palm Pilot to a Microsoft Word  document. It is not enough to overcome the protocol and data format  mismatches that currently impede the interoperation of these entities: for  the user experience to be truly seamless, we must provide a framework  for the dynamic connection of such endpoints on demand, to support the  ad-hoc interactions that are an integral part of ubiquitous computing. To  this end, we oer a dynamic mediation framework called Paths. A Path  consists of dynamically instantiated, automatically composable operators  that brid...
HCI
525119
A Secure Infrastructure for Service Discovery Access in Pervasive Computing Security is paramount to the success of pervasive computing environments. The system presented in this paper provides a communications and security infrastructure that goes far in advancing the goal of anywhere - anytime computing. Our work securely enables clients to access and utilize services in heterogeneous networks. We provide a service registration and discovery mechanism implemented through a hierarchy of service management. The system is built upon a simplified Public Key Infrastructure that provides for authentication, non-repudiation, anti-playback, and access control. Smartcards are used as secure containers for digital certificates. The system is implemented in Java and we use Extensible Markup Language as the sole medium for communications and data exchange. Currently, we are solely dependent on a base set of access rights for our distributed trust model however, we are expanding the model to include the delegation of rights based upon a predefined policy. In our proposed expansion, instead of exclusively relying on predefined access rights, we have developed a flexible representation of trust information, in Prolog, that can model permissions, obligations, entitlements, and prohibitions. In this paper, we present the implementation of our system and describe the modifications to the design that are required to further enhance distributed trust. Our implementation is applicable to any distributed service infrastructure, whether the infrastructure is wired, mobile, or ad-hoc.
HCI
342407
Integration of Machine Learning and Knowledge Acquisition Introduction  "Integration of Machine Learning and Knowledge Acquisition" may be a surprising title for an ECAI-94 workshop since most Machine Learning (ML) systems are dedicated to Knowledge Acquisition (KA). What could thus mean integrating ML and KA ? The answer lies in the difference between the approaches developed by what is referred to as ML and KA research. Apart from some major exceptions, such as learning apprentice tools [ Mitchell et al., 1989 ] , or libraries like Machine Learning Toolbox [ MLT, 1993 ] , most ML algorithms were described without any characterization in terms of real application needs, in term of what they could be effectively useful for. However, ML methods were applied to "real world" problems, but few general and reusable conclusions were drawn from these knowledge acquisition experiments. As ML techniques become more and more sophisticated and able to produce various forms of knowledge, the number of possible applications grows. 
ML
dautenhahn99studying
Studying Robot Social Cognition Within A Developmental Psychology Framework This paper discusses two prominent theories of cognitive development and relates them to experiments in social robotics. The main difference between these theories lies in the different views on the relationship between a child and its social environment: a) the child as a solitary thinker (Piaget) and b) the child in society (Vygotsky). We discuss the implications this has on the design of socially intelligent agents, focusing on robotic agents. We argue that the framework proposed by Vygotsky provides a promising research direction in autonomous agents. We give examples of implementations in the area of social robotics which support our theoretical considerations. More specifically, we demonstrate how a teacher-learner setup can be used to teach a robot a proto-language. The same control architecture is also used for a humanoid doll robot which can interact with a human by imitation. Another experiment addresses dynamic coupling of movements between a human and a mobile robot. Here, ...
Agents
300584
Data Mining on an OLTP System (Nearly) for Free This paper proposes a scheme for scheduling disk requests that takes advantage of the ability of high-level functions to operate directly at individual disk drives. We show that such a scheme makes it possible to support a Data Mining workload on an OLTP system almost for free: there is only a small impact on the throughput and response time of the existing workload. Specifically, we show that an OLTP system has the disk resources to consistently provide one third of its sequential bandwidth to a background Data Mining task with close to zero impact on OLTP throughput and response time at high transaction loads. At low transaction loads, we show much lower impact than observed in previous work. This means that a production OLTP system can be used for Data Mining tasks without the expense of a second dedicated system. Our scheme takes advantage of close interaction with the on-disk scheduler by reading blocks for the Data Mining workload as the disk head “passes over ” them while satisfying demand blocks from the OLTP request stream. We show that this scheme provides a consistent level of throughput for the background workload even at very high foreground loads. Such a scheme is of most benefit in combination with an Active Disk environment that allows the background Data Mining application to also take advantage of the processing power and memory available directly on the disk drives. This research was sponsored by DARPA/ITO through ARPA Order D306, and issued
DB
finn02genre
Genre Classification and Domain Transfer for Information Filtering The World Wide Web is a vast repository of information, but the sheer volume makes it difficult to identify useful documents. We identify document genre is an important factor in retrieving useful documents and focus on the novel document genre dimension of subjectivity.
IR
xia99comprehensive
Comprehensive Hardware and Software Support for Operating Systems to Exploit MP Memory Hierarchies AbstractÐHigh-performance multiprocessor workstations are becoming increasingly popular. Since many of the workloads running on these machines are operating-system intensive, we are interested in exploring the types of support for the operating system that the memory hierarchy of these machines should provide. In this paper, we evaluate a comprehensive set of hardware and software supports that minimize the performance losses for the operating system in a sophisticated cache hierarchy. These supports, selected from recent papers, are code layout optimization, guarded sequential instruction prefetching, instruction stream buffers, support for block operations, support for coherence activity, and software data prefetching. We evaluate these supports under a simulated environment. We show that they have a largely complementary impact and that, when combined, speed up the operating system by an average of 40 percent. Finally, a cost-performance comparison of these schemes suggests that the most cost-effective ones are code layout optimization and block operation support, while the least cost-effective one is software data prefetching. Index TermsÐCache hierarchies, shared-memory multiprocessors, architectural support for operating system, prefetching, tracedriven simulations, performance, block operations. 1
DB
thevenin99adaptation
Adaptation and Plasticity of User Interfaces . This paper introduces the notion of plasticity, a new property of interactive systems that denotes a particular type of user interface adaptation. It also presents a generic framework inspired from the model-based approach, for supporting the development of plastic user interfaces. This framework is illustrated with simple case studies.  KEYSWORDS. User interface adaptation, plasticity.  1. Introduction: A Design Space for Adaptation  In HCI, adaptation is modeled as two complementary system properties: adaptability and adaptivity. Adaptability is the capacity of the system to allow users to customize their system from a predefined set of parameters. Adaptivity is the capacity of the system to perform adaptation automatically without deliberate action from the user's part. Whether adaptation is performed on human requests or automatically, the design space for adaptation includes three additional orthogonal axes (see Figure 1):  . The target for adaptation. This axis denotes the enti...
HCI
bilgic97risk
Risk Management in Concurrent Engineering in Presence of Intelligent Agents Contents 1 Introduction 2 1.1 Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.3 Proposal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.4 Working principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2 Current Status 5 2.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 SDMA as an agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3 Assumptions about the environment . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.4 SDMA-Risk Version 0.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3 Summary 8 A KQML Specification for SD
Agents
mayol02designing
Designing a Miniature Wearable Visual Robot In this paper we report on two methods we have developed to aid in the design of a Wearable Visual Robot --- a body mounted robot for which the main sensor is a camera. Specifically, we have first refined the analysis of sensor placement through the computation of the field of view and body motion using a 3D model of the human form. Second we have improved the design of the robot's morphology with the help of an optimization algorithm based on the Pareto front, within constraints set by the overall choice of robot kinematic chain and the need to specify obtainable actuators and sensors. The methods could be of use for the design and performance evaluation of rather different kinds of wearable robots and devices.
HCI
116696
A Framework for Describing Visual Interfaces to Databases In the field of HCI there exist many formalisms for analysing, describing and evaluating interactive systems. However, in developing and evaluating user interfaces to databases, we found it necessary to be able to describe presentation and interaction aspects that are catered for poorly or not at all in current formalisms. This paper presents a framework for the systematic description of data model, presentation and interaction components that together form a graphical user interface. The utility of the framework is then demonstrated by showing how it can be used to describe two existing visual query interfaces. These examples show that the framework provides a systematic method for the concise description of graphical interfaces to databases that can be used either during interface design or as a communication aid. 1 Introduction  Research in user interfaces for databases is gaining momentum with many recent conferences and workshops [10, 21, 22, 23, 36]. However, many papers on datab...
DB
ambite00flexible
Flexible and Scalable Cost-Based Query Planning in Mediators: A Transformational Approach The Internet provides access to a wealth of information. For any given topic or application domain there are a variety of available information sources. However, current systems, such as search engines or topic directories in the World Wide Web, offer only very limited capabilities for locating, combining, and organizing information. Mediators, systems that provide integrated access and database-like query capabilities to information distributed over heterogeneous sources, are critical to realize the full potential of meaningful access to networked information.  Query planning, the task of generating a cost-efficient plan that computes a user query from the relevant information sources, is central to mediator systems. However, query planning is a computationally hard problem due to the large number of possible sources and possible orderings on the operations to process the data. Moreover, the choice of sources, data processing operations, and their ordering, strongly affects the plan c...
DB
brass99equivalence
On the Equivalence of the Static and Disjunctive Well-Founded Semantics and its Computation In recent years, much work was devoted to the study of theoretical foundations of Disjunctive Logic Programming and Disjunctive Deductive Databases. While the semantics of non-disjunctive programs is fairly well understood, the declarative and computational foundations of disjunctive logic programming proved to be much more elusive and difficult. Recently, two new and promising semantics have been proposed for the class of disjunctive logic programs. The first one is the static semantics STATIC, proposed by Przymusinski, and, the other is the disjunctive well-founded semantics D-WFS, proposed by Brass and Dix.  Although the two semantics are based on very different ideas, both of them have been shown to share a number of natural and intuitive properties. In particular, both Preprint submitted to Elsevier Preprint 4 October 1999 of them extend the well-founded semantics of normal logic programs. Nevertheless, since the static semantics employs a much richer underlying language than the ...
DB
bastert01landscapes
Landscapes on Spaces of Trees Combinatorial optimization problems defined on sets of phylogenetic trees are an important issue in computational biology, for instance the problem of reconstruction a phylogeny using maximum likelihood or parsimony approaches. The collection of possible phylogenetic trees is arranged as a so-called Robinson graph by means of the nearest neighborhood interchange move. The coherent algebra and spectra of Robinson graphs are discussed in some detail as their knowledge is important for an understanding of the landscape structure. We consider simple model landscapes as well as landscapes arising from the maximum parsimony problem, focusing on two complementary measures of ruggedness: the amplitude spectrum arising from projecting the cost functions onto the eigenspaces of the underlying graph and the topology of local minima and their connecting saddle points. 
AI
263968
On Constructing the Right Sort of CBR Implementation Case based reasoning implementations as currently  constructed tend to #t three general  models, characterized by implementation constraints:   task-based #task alone#, enterprise #integrating  databases#, and web-based #integrating  web representations#. These implementations  represent the targets for automatic system  construction, and it is important to understand  the strengths of each, how they are built,  and how one may be constructed by transforming  another. This paper describes a framework  that relates the three types of CBR implementation,  discusses their typical strengths and  weaknesses, and describes practical methods  for automating the construction of new CBR  systems by transforming and synthesizing existing  resources.  1 Introduction  CBR systems as currently constructed tend to #t three general implementation models, de#ned by broad implementation constraints on representation and process.  Traditionally, task-based implementations have addressed system goals bas...
DB
broersen00leveled
Leveled Commitment and Trust in Negotiation As agents become more autonomous, agent negotiation and motivational attitudes such as commitment  and trust become more important. In this paper we consider the important choice in advanced  negotiation applications whether negotiation parameters -- such as cardinality of interaction, agent attitude,  and agent architectures -- are incorporated in the negotiation protocol or in the negotiation strategy.  Only in the first case parameters are fixed and agents do not have to reason about them when they choose  their strategy. We define a dynamic deontic logic which can also be used for the second case, because it  models concepts like leveled commitment and trust. For example, it formalizes that violating commitments  leads to a decrease in trustworthiness.  1 Introduction  In advanced applications of multi-agent systems agents interact more frequently, deliberate more extensively, and in general act more autonomously. For example, in electronic commerce agents are allowed to negotiate ...
Agents
90601
Classifying Unseen Cases with Many Missing Values Handling missing attribute values is an important issue for classifier learning, since missing attribute values in either training data or test (unseen) data affect the prediction accuracy of learned classifiers. In many real KDD applications, attributes with missing values are very common. This paper studies the robustness of four recently developed committee learning techniques, including Boosting, Bagging, Sasc, and SascMB, relative to C4.5 for tolerating missing values in test data. Boosting is found to have a similar level of robustness to C4.5 for tolerating missing values in test data in terms of average error in a representative collection of natural domains under investigation. Bagging performs slightly better than Boosting, while Sasc and SascMB perform better than them in this regard, with  SascMB performing best. Furthermore, we propose a novel voting weight scheme for the committee learning techniques. Although it is very simple, it can improve the robustness of all these ...
ML
dey99conference
The Conference Assistant: Combining Context-Awareness with Wearable Computing We describe the Conference Assistant, a prototype mobile, context-aware application that assists conference attendees. We discuss the strong relationship between context-awareness and wearable computing and apply this relationship in the Conference Assistant. The application uses a wide variety of context and enhances user interactions with both the environment and other users. We describe how the application is used and the context-aware architecture on which it is based. 1. Introduction  In human-human interaction, a great deal of information is conveyed without explicit communication, but rather by using cues. These shared cues, or context,  help to facilitate grounding between participants in an interaction [3]. We define context to be any information that can be used to characterize the situation of an entity, where an entity can be a person, place, or physical or computational object. In human--computer interaction, there is very little shared context between the human and the co...
HCI
sampaio98deductive
Deductive Queries in ODMG Databases: the DOQL Approach The Deductive Object Query Language (DOQL) is a rule-based query  language designed to provide recursion, aggregates, grouping and virtual  collections in the context of an ODMG compliant object database system.  This paper provides a description of the constructs supported by DOQL  and the algebraic operational semantics induced by DOQL's query translation  approach to implementation. The translation consists of a logical  rewriting step used to normalise DOQL expressions into molecular forms,  and a mapping step that transforms the canonical molecular form into  algebraic expressions. The paper thus not only describes a deductive  language for use with ODMG databases, but indicates how this language  can be implemented using conventional query processing techniques.  1 Introduction  The ODMG standard is an important step forward due to the provision of a reference architecture for object databases. This architecture encompasses an object model and type system, a set of imperative lan...
DB
485557
AT Humboldt in RoboCup-99 The paper describes the architecture and the scientific goals of the virtual soccer team "AT Humboldt 99", which is the successor of vice champion "AT Humboldt 98" from RoboCup-98 in Paris and world champion "AT Humboldt 97" from RoboCup-97 in Nagoya. Scientific goals are the development of agent-oriented techniques and learning methods.
Agents
griffiths99cooperative
Cooperative Plan Selection Through Trust Cooperation plays a fundamental role in multi-agent systems in which  individual agents must interact for the overall system to function effectively.
Agents
201850
Conjunctive-Query Containment and Constraint Satisfaction Conjunctive-query containment is recognized as a fundamental problem in database query evaluation and optimization. At the same time, constraint satisfaction is recognized as a fundamental problem in artificial intelligence. What do conjunctive-query containment and constraint satisfaction have in common? Our main conceptual contribution in this paper is to point out that, despite their very different formulation, conjunctive-query containment and constraint satisfaction are essentially the same problem. The reason is that they can be recast as the following fundamental algebraic problem: given two finite relational structures A and B, is there a homomorphism h : A ! B? As formulated above, the homomorphism problem is uniform in the sense that both relational structures A and B are part of the input. By fixing the structure B, one obtains the following non-uniform problem: given a finite relational structure A, is there a homomorphism h : A ! B? In general, non-uniform tractability results do not uniformize. Thus, it is natural to ask: which tractable cases of non-uniform tractability results for constraint satisfaction and conjunctive-query containment do uniformize? Our main technical contribution in this paper is to show that several cases of tractable non-uniform constraint satisfaction problems do indeed uniformize. We exhibit three non-uniform tractability results that uniformize and, thus, give rise to polynomial-time solvable cases of constraint satisfaction and conjunctive-query containment.
DB
461740
How to Interpret Neural Networks In Terms of Fuzzy Logic? Neural networks are a very efficient learning tool, e.g., for transforming an experience of an expert human controller into the design of an automatic controller. It is desirable to reformulate the neural network expression for the input-output function in terms most understandable to an expert controller, i.e., by using words from natural language. There are several methodologies for transforming such natural-language knowledge into a precise form; since these methodologies have to take into consideration the uncertainty (fuzziness) of natural language, they are usually called fuzzy logics.  1 
ML
bomze99maximum
The Maximum Clique Problem Contents  1 Introduction 2 1.1 Notations and Definitions . . . . . . . . . . . . . . . . . . . . . . . . 3 2 Problem Formulations 4 2.1 Integer Programming Formulations . . . . . . . . . . . . . . . . . . . 5 2.2 Continuous Formulations . . . . . . . . . . . . . . . . . . . . . . . . 8 3 Computational Complexity 12 4 Bounds and Estimates 15 5 Exact Algorithms 19 5.1 Enumerative Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 19 5.2 Exact Algorithms for the Unweighted Case . . . . . . . . . . . . . . 21 5.3 Exact Algorithms for the Weighted Case . . . . . . . . . . . . . . . . 25 6 Heuristics 27 6.1 Sequential Greedy Heuristics . . . . . . . . . . . . . . . . . . . . . . 28 6.2 Local Search Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . 29 6.3 Advanced Search Heuristics . . . . . . . . . . . . . . . . . . . . . . . 30 6.3.1 Simulated annealing . . . . . . . . . . . . . . . . . . . . . . . 30 6.3.2 Neural networks . . . . . . . . . . . . . . . . . . . . . . . .
ML
242836
Single Display Groupware: Exploring Computer Support for Co-located Collaboration This panel will explore an interaction paradigm for colocated computer-based collaboration we term Single Display Groupware (SDG). SDG is a class of applications that support multiple simultaneous users interacting in the same room on a single shared display with multiple inputdevices. SDG are being used in various applications in the educational, entertainment and research communities, but many issues remain to be explored.  Keywords  Computer-supported cooperative work (CSCW); computersupported collaborative learning (CSCL); computersupported collaborative entertainment (CSCE), multiple input devices.  INTRODUCTION  Single Display Groupware (SDG) is a class of applications that support multiple simultaneous users interacting in a colocated environment on a single shared display with multiple input-devices [4]. SDG allows users to interact more naturally and comfortably around the computer. Such applications take advantage of our human ability to interact and communicate in a face-to-...
HCI
ives00adaptive
Adaptive Query Processing for Internet Applications As the area of data management for the Internet has gained in popularity, recent work has focused on effectively dealing with unpredictable, dynamic data volumes and transfer rates using adaptive query processing techniques. Important requirements of the Internet domain include: (1) the ability to process XML data as it streams in from the network, in addition to working on locally stored data; (2) dynamic scheduling of operators to adjust to I/O delays and flow rates; (3) sharing and re-use of data across multiple queries, where possible; (4) the ability to output results and later update them. An equally important consideration is the high degree of variability in performance needs for different query processing domains: perhaps an ad-hoc query application should optimize for display of incomplete and partial incremental results, whereas a corporate data integration application may need the best time-to-completion and may have very strict data "freshness" guarantees. The goal of...
DB
chen01txnwrap
TxnWrap: A Transactional Approach to Data Warehouse Maintenance A Data Warehouse Management System (DWMS) maintains materialized views derived from one or more information sources (ISs) under source changes. Much recent research has developed maintenance algorithms to achieve data warehouse consistency under source data updates typically by sending additional compensation-based messages to information source space. Given the highly dynamic nature of modern distributed environments such as the WWW, both source data or schema changes are likely to occur autonomously and even concurrently. Most previous solutions become invalid under this new requirement, causing both wrong query results returned from IS space or even complete rejection messages. This paper now proposes to tackle this problem from a different angle by rephasing it as a global distributed transaction problem, and develops a novel solution strategy that is not only handling this as of now unsolved problem of concurrent source shcmea changes but also added benefit is more efficient than previous solutions from the literature. As foundation of our solution, we encapsulate the complete data warehouse maintenance process as a DWMS Transaction. We design a multiversion timestamp source wrapper materialization and associated concurrency control algorithm that guarantees a consistent view of the information source space data inside each DWMS Transaction, thus removing the maintenance anomaly problem. This integrated solution called TxnWrap is proven to be correct and achieve complete consistency of data warehouse maintenance even under a mixture of concurrent data updates or schema changes. TxnWrap is complementary to previous maintenance algorithms of removing their concurrency considerations. We have implemented TxnWrap and plugged it into an existing DWMS testbed...
DB
bowling01convergence
Convergence of Gradient Dynamics with a Variable Learning Rate As multiagent environments become more prevalent  we need to understand how this changes the  agent-based paradigm. One aspect that is heavily  affected by the presence of multiple agents  is learning. Traditional learning algorithms have  core assumptions, such as Markovian transitions,  which are violated in these environments.
Agents
294630
Experiences Using Case-Based Reasoning to Predict Software Project Effort This paper explores some of the practical issues associated with the use of case-based reasoning (CBR) or estimation by analogy. We note that different research teams have reported widely differing results with this technology. Whilst we accept that underlying characteristics of the datasets being used play a major role we also argue that configuring a CBR system can also have an impact. We examine the impact of the choice of number of analogies when making predictions; we also look at different adaptation strategies. Our analysis is based on a dataset of software projects collected by a Canadian software house. Our results show that choosing analogies is important but adaptation strategy appears to be less so. These findings must be tempered, however, with the finding that it was difficult to show statistical significance for smaller datasets even when the accuracy indicators differed quite substantially. For this reason we urge some degree of caution when comparing competing predicti...
ML
cavedon95logical
A Logical Framework for Multi-Agent Systems and Joint Attitudes We present a logical framework for reasoning about multi-agent systems. This framework uses Giunchiglia et al.'s notion of a logical context to define a methodology for the modular specification of agents and systems of agents. In particular, the suggested methodology possesses important features from the paradigm of object-oriented (OO) design. We are particularly interested in the specification of agent behaviours via BDI theories---i.e., theories of belief, desire and intention. We explore various issues arising from the BDI specification of systems of agents and illustrate how our framework can be used to specify bottom-level agent behaviour via the specification of top-level intentions, or to reason about complex "emergent behaviour" by specifying the relationship between simple interacting agents.  1 Introduction  The formal specification of autonomous reasoning agents has recently received much attention in the AI community, particular under the paradigm of agent-oriented progr...
Agents
marques00going
Going beyond Mobile Agent Platforms: Component-Based Development of Mobile Agent Systems Although mobile agents are a promising programming paradigm, the actual deployment of this technology in real applications has been far away from what the researchers were expecting. One important reason for this is the fact that in the current mobile agent frameworks it is quite difficult to develop applications without having to center them on the mobile agents and on the agent platforms. In this paper, we present a component-based framework that enables ordinary applications to use mobile agents in an easy and flexible way. By using this approach, applications can be developed using current objectoriented approaches and become able of sending and receiving agents by the simple drag-and-drop of mobility components. The framework was implemented using the JavaBeans component model and provides integration with ActiveX, which allows applications to be written in a wide variety of programming languages. By using this framework, the development of applications that can make use of mobile agents is greatly simplified, which can contribute to a wider spreading of the mobile agent technology.
IR
vanlaerhoven01realtime
Real-time Analysis of Data from Many Sensors with Neural Networks Much research has been conducted that uses sensorbased modules with dedicated software to automatically distinguish the user's situation or context. The best results were obtained when powerful sensors (such as cameras or GPS systems) and/or sensor-specific algorithms (like sound analysis) were applied. A  somewhat new approach is to replace the one smart sensor by many simple sensors. We argue that neural networks are ideal algorithms to analyze the data coming from these sensors and describe how we came to one specific algorithm that gives good results, by giving an overview of several requirements. Finally, wearable implementations are given to show the feasibility and benefits of this approach and its implications.  1.
HCI
gray99finding
Finding and Moving Constraints in Cyberspace Agent-based architectures are an effective method for constructing open, dynamic, distributed information systems. The KRAFT system exploits such an architecture, focusing on the exchange of information --- in the form of constraints and data --- among participating agents. The KRAFT approach is particularly wellsuited to solving design and configuration problems, in which constraints and data are retrieved from agents representing customers and vendors on an extranet network, transformed to a common ontology, and processed by mediator agents. This paper describes the KRAFT system, discusses the issues involved in joining a KRAFT network from the point-of-view of information providers in Cyberspace, and examines the role of autonomous and mobile agents in KRAFT. Introduction  Traditional distributed database systems provide uniform and transparent access to data objects across the network. These systems, however, are focused on the utilisation of data instead of other semantic knowledg...
Agents
falk99amplifying
Amplifying Reality . Many novel applications take on the task of moving the personal  computer away from the desktop with the approach to merge digital  information with physical space and objects. These new applications have given  rise to a plethora of notions and terms used to classify them. We introduce  amplified reality as a concept complementary to that of augmented reality. To  amplify reality is to enhance the publicly available properties of persons and  physical objects, by means of using wearable or embedded computational  resources. The differences between the two concepts are discussed and  examples of implementations are given. The reason for introducing this term is  to contribute to the terminology available to discuss already existing  applications, but also to open up for a discussion of interesting design  implications.  Keywords: amplified reality, augmented reality, ubiquitous computing,  wearable computing, embedded vs. superimposed properties, private vs. public  1 Breaking away f...
HCI
iglezakis00towards
Towards the Use of Case Properties for Maintaining Case-Based Reasoning Systems . Because of the importance of maintenance in the realm of case--based reasoning systems, methods of maintaining case bases using case properties will be presented. The necessary notation is given, along with definitions of the properties themselves, which are correctness, consistency, incoherence, minimality, and uniqueness. Use of these properties in five experiments is explained, and the results of these experiments on three real world case bases is given. While the prediction accuracy remains constant the case base size is reduced up to 69.1%. 1 Introduction The maintaining of case--based reasoning systems has become increasingly an important research topic during the last few years. For example, a workshop entitled Flexible Strategies for Maintaining Knowledge Containers [7] held at the 14th European Conference on Artificial Intelligence gave researchers the opportunity to present and discuss new progress in this field. The development of useful and accurate quality measur...
ML
guessoum99from
From Active Objects to Autonomous Agents This paper studies how to extend the concept of active objects  into a structure of agents. It first discusses the requirements for autonomous  agents that are not covered by simple active objects. We  propose then the extension of the single behavior of an active object  into a set of behaviors with a meta-behavior scheduling their activities.  To make a concrete proposal based on these ideas we describe how we  extended a framework of active objects, named Actalk, into a generic  multi-agent platform, named DIMA. We discuss how this extension  has been implemented. We finally report on one application of DIMA  to simulate economic models.  Keywords: active object, agent, implementation, meta-behavior, modularity, re-usability, simulation.  1 Introduction  Object-oriented concurrent programming (OOCP) is the most appropriate and promising technology to implement agents. The concept of active object may be considered as the basic structure for building agents. Furthermore, the combinat...
Agents
bohte00current
On Current Technology for Information Filtering and User Profiling in Agent-Based Systems, Part I: A Perspective Several current techniques and methods in information filtering and  profiling are surveyed, including state-of-the art technology, various  techniques currently used by large businesses, and the academic stateof  -the-art projects. Given the simplicity of the techniques currently  applied in the field, the further development and application of technology  currently available in AI and algorithmics will yield significant  improvements in both filtering and profiling results.  1 Introduction  The terms "Information Filtering" and "profiling" are widely used. Here "Information Filtering" will refer to computer software systems which  ffl split (usually large) data streams into useful and not useful components and direct the useful to interested users. Of particular interest are systems which recognize their users are different and split the data stream into separate (possible overlapping) streams which are directed at distinct users or groups of users. Typically data in the streams is c...
IR
brandt01vicious
Vicious Strategies for Vickrey Auctions AB$C/?-	 ?D:*.>/ 	22*/	<(=.->?--5$%	/@"AB$C/?-	 +;/	2LM+;	- <(=.->?--5$%	/@"AB$C/?-	 /  *(25	/N8 28860-45710  -5$%	/@"AB$C/? -37LU/-5VTDW.> UXU*%T2OK!8(;O'*(; 8(0$%'	-$%+C('"Y66TZ(C34!*((;*5X8( )U[6		M3&C($%/Z!*((;*5%6-.	5\( 5X8( 28810   )U[6		M3&C($% M%DW.>/ 0-40490  $%/Z!*((;* 	U*	2'!#	U1	`*%++;+;I ?]3BbA/-/	*2cd5	/ +-	+8 N=VU*?+;/		-	@Te?N-+#		]122]/`8B, -/	*2M-]8(;?37?	 Te?N-+#		]122 f$C>	Bg-h-ijk 8 o9p hq8g-r6s o r6tWkru:qh o l rg.vZ	]&$C%528!#	-*( N-+#		]1 	]VO'\+8	/x<:*/- (;	(y@ZzW8(; ]122] 	2{ O'\+8	/x< 5	/|	(VZ(	 *39$}(;$%8/a-;,~	22*5'6-.-,  5	/|	(VZ(	 *39$} 	5?37W!*((; . 17590-33150  $} D:U>/ W!*((; . 17590-33150  $}(;$%8/a-;,~	22*5'6-. 5	/:*.( . 17590-33150  $}(;$%8/a...
Agents
chien01efficient
Efficient Management of Multiversion Documents by Object Referencing Traditional approaches to versioning semistructured information are edit-based, i.e., subsequent document versions are represented by using edit scripts. This paper proposes a reference-based version management scheme that preserves the logical structure of the evolving document through the use of object references. By preserving the document structure among versions the new scheme facilitates more efficient query support. In particular, we examine queries involving projections and selections on the document versions, as well as queries on the document evolution history. Moreover, we show that the proposed scheme provides an effective representation of multiversioned XML documents, both at the transport and exchange levels. In fact, with the reference-based scheme, a document's history can also be viewed and processed as yet another XML document. Furthermore, we demonstrate the effectiveness of the new scheme at the storage level. In particular, the scheme is enhanced with a usefulness-based page management policy that extends and adapts techniques used in transaction-time databases to ensure efficient clustering of information among versions. An extensive comparison of the reference-based versioning against representations used in temporal databases and persistent object managers depicts the performance advantages of the new approach. Finally it should be noted that reference-based versioning is applicable to other kinds of semistructured information (besides XML documents), and can be used to replace traditional version control schemes, such as the edit-based RCS and the timestamp-based SCCS. 
DB
weiss98answer
ANSWER: Network Monitoring Using Object-Oriented Rules This paper describes ANSWER, the expert system  responsible for monitoring AT&T's 4ESS switches. These switches are extremely important, since they handle virtually all of AT&T's long distance traffic. ANSWER is implemented in R++, a rule-based extension to the C++ object-oriented programming language, and is innovative  because it employs both rule-based and object-oriented programming paradigms. The use of object technology in ANSWER has provided a principled way of modeling the 4ESS and of reasoning about failures within the 4ESS. This has resulted in an expert system that is more clearly organized, easily understood and maintainable than its predecessor, which was implemented using the rule-based paradigm alone. ANSWER has been deployed for more than a year and handles all 140 of AT&T's 4ESS switches and processes over 100,000 4ESS alarms per week. Introduction  Network reliability is of critical concern to AT&T, since its reputation for network reliability has taken many years to ...
ML
bourdeau99three
Three Dimensional Optimization of Supersonic Inlets This paper presents the implementation of these new design techniques and their application to a Mach 3 inlet case. The significant improvements obtained using two different optimizers are presented and compared. The results of these optimizations have been verified using a full Reynolds Averaged Navier-Stokes solver. All the following results are thoroughly analysed and placed into an industrial context. 1 Introduction
ML
cal01accessing
Accessing Data Integration Systems through Conceptual Schemas Data integration systems provide access to a set of heterogeneous,  autonomous data sources through a so-called global, or mediated  view. There is a general consensus that the best way to describe the global  view is through a conceptual data model, and that there are basically two  approaches for designing a data integration system. In the global-as-view  approach, one defines the concepts in the global schema as views over  the sources, whereas in the local-as-view approach, one characterizes the  sources as views over the global schema. It is well known that processing  queries in the latter approach is similar to query answering with incomplete  information, and, therefore, is a complex task. On the other hand,  it is a common opinion that query processing is much easier in the former  approach. In this paper we show the surprising result that, when the  global schema is expressed in terms of a conceptual data model, even a  very simple one, query processing becomes di#cult in the global-as-view  approach also. We demonstrate that the problem of incomplete information  arises in this case too, and we illustrate some basic techniques for  e#ectively answering queries posed to the global schema of the data integration  system.  1 
DB
496354
Knowledge Management in Heterogeneous Data Warehouse Environments This paper addresses issues related to Knowledge Management in the  context of heterogeneous data warehouse environments. The traditional notion  of data warehouse is evolving into a federated warehouse augmented by a  knowledge repository, together with a set of processes and services to support  enterprise knowledge creation, refinement, indexing, dissemination and  evolution.
DB
bernstein00panel
Panel: Is Generic Metadata Management Feasible? dels,   such as invert and compose?   ## What is the role of an expression language that captures the semantics of models and mappings, not only   for design but also for run-time execution?   ## Does a generic approach offer any advantages for   model manipulation areas of current interest, such as   data integration and XML?   If the skeptics are right that a generic approach to model   management is unachievable pie-in-the-sky, are writers of   metadata-driven applications doomed forever to writing   special-purpose object-at-a-time code for navigating their   information structures? If so, what is the leverage that the   database field can offer for these problems?   2. Panelists   ## Dr. Laura Haas, IBM Research is working on a tool   that can (semi-)automatically produce mappings   between two data representations. She has been   working on various aspects of data integration since   starting the Garlic project in 1994.   ##<
DB
12247
Soft Margins for AdaBoost Recently ensemble methods like AdaBoost have been applied successfully in many problems, while seemingly defying the problems of overfitting. AdaBoost rarely overfits in the low noise regime, however, we show that it clearly does so for higher noise levels. Central to the understanding of this fact is the margin distribution. AdaBoost can be viewed as a constraint gradient descent in an error function with respect to the margin. We find that AdaBoost asymptotically achieves a hard margin distribution, i.e. the algorithm concentrates its resources on a few hard-to-learn patterns that are interestingly very similar to Support Vectors. A hard margin is clearly a sub-optimal strategy in the noisy case, and regularization, in our case a ``mistrust'' in the data, must be introduced in the algorithm to alleviate the distortions that single difficult patterns (e.g. outliers) can cause to the margin distribution. We propose several regularization methods and generalizations of the original AdaBoost algorithm to achieve a soft margin. In particular we suggest (1) regularized AdaBoost-Reg where the gradient decent is done directly with respect to the soft margin and (2) regularized linear and quadratic programming (LP/QP-) AdaBoost, where the soft margin is attained by introducing slack variables. Extensive simulations demonstrate that the proposed regularized AdaBoost-type algorithms are useful and yield competitive results for noisy data.
ML
christiansen99integrity
Integrity Constraints and Constraint Logic Programming It is shown that constraint logic is useful for evaluation of integrity constraints in deductive databases. Integrity constraints are represented as calls to a metainterpreter for negation-as-failure implemented as a constraint solver. This procedure, called lazy negationas -failure, yields an incremental evaluation: It starts checking the existing database and each time an update request occurs, simplified constraints are produced for checking the particular update and new constraints corresponding to specialized integrity constraints are generated for the updated database. 1 Introduction  There is a relationship between integrity constraints in databases and the constraints of constraint logic programming going beyond the partial overlap of the names applied for these phenomena. Both concern conditions that should be ensured for systems of interdependent entities: the different tuples in a database, and the set of variables in a program execution state. Both relate to problems that e...
DB
lomuscio96qlb
QLB: A Quantified Logic for Belief . This paper describes QLB, a quantified logic of belief that is a possible extension of the modal system KD45n to predicate level. The main features of QLB are that: (i) it is allowed to quantify over the agents of belief; (ii) the belief operator can be indexed by any term of the formal language; (iii) terms are not rigid designators, but are interpreted contextually; (iv) automatic theorem proving is possible in QLB (but it is not presented in this paper). QLB is constructed as a partial logic with a monotonic semantics on ordered sets, and its semantic theorems are defined as the formulae that are sometimes true and never false. 1 Introduction  Agents are complex objects: they can be modelled in terms of mental states, like knowledge, beliefs, intentions, goals, plans, etc. and they perform actions (sometimes cooperatively) in a society of other agents. In this book the reader can find contributions from different schools of thought related to agents: agent theories for specificati...
Agents
erni98generic
Generic Agent Framework for Internet Information Systems For effective Internet database services, it is essential that the information requirements of regular users can be met without the typical delays currently experienced using Internet browsers and the World Wide Web. We use cooperating agents to manage both client and server caches, thereby bringing significant performance improvements. The caching and prefetching of information is based on both user and application profiles and agents communicate to ensure the currency of client caches. According to specific application requirements, various forms of agents can be installed on the server and client sides to provide value-added services to both casual and regular users. All component agents are instantiations and/or specialisations of a generic agent. We describe how a specific Internet brokering system for engineering product data has been constructed using our general framework for the development of Internet information systems. 1 Introduction With the development of World-Wide Web ...
DB
chan98toward
Toward Scalable Learning with Non-uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection Many factors influence the performance of a learned classifier. In this paper we study different methods of measuring performance based on a unified set of cost models and the effects of training class distribution with respect to these models. Observations from these effects help us devise a distributed multi-classifier meta-learning approach to learn in domains with skewed class distributions, non-uniform cost per error, and large amounts of data. One such domain is credit card fraud detection and our empirical results indicate that, up to a certain degree of skewed distribution, our approach can significantly reduce loss due to illegitimate transactions. Introduction  Inductive learning research has been focusing on devising algorithms that generate highly accurate classifiers. Many factors contribute to the quality of the learned classifier. One factor is the class distribution in the training set. Using the same algorithm, different training class distributions can generate classi...
ML
sabater99engineering
Engineering Executable Agents Using Multi-Context Systems In the area of agent-based computing there are many proposals for specific system architectures, and a number of proposals for general approaches to building agents. As yet, however, there are comparatively few attempts to relate these together, and even fewer attempts to provide methodologies which relate designs to architectures and then to executable agents. This paper provides a first attempt to address this shortcoming. We propose a general method of specifying logic-based agents, which is based on the use of multi-context systems, and give examples of its use. The resulting specifications can be directly executed, and we discuss an implementation which makes this direct execution possible.
Agents
abello99maximum
On Maximum Clique Problems In Very Large Graphs . We present an approach for clique and quasi-clique computations in very large multi-digraphs. We discuss graph decomposition schemes used to break up the problem into several pieces of manageable dimensions. A semiexternal greedy randomized adaptive search procedure (GRASP) for finding approximate solutions to the maximum clique problem and maximum quasiclique problem in very large sparse graphs is presented. We experiment with this heuristic on real data sets collected in the telecommunications industry. These graphs contain on the order of millions of vertices and edges. 1. Introduction  The proliferation of massive data sets brings with it a series of special computational challenges. Many of these data sets can be modeled as very large multidigraphs  M with a special set of edge attributes that represent special characteristics of the application at hand [1]. Understanding the structure of the underlying digraph D(M) is essential for storage organization and information retrieval...
IR
294031
A Foundation for Representing and Querying Moving Objects Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously which describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, are of interest, respectively. We propose to represent such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point and moving region, a relatively large number of auxiliary data types is needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real ” to represent the time-dependent distance of two moving points. It then becomes crucial to achieve (i) orthogonality in the design of the type system, i.e., type constructors can be applied uniformly, (ii) genericity
DB
meyer99constraint
Constraint Diagram Reasoning Diagrammatic human-computer interfaces are now becoming standard. In the near future, diagrammatic front-ends, such as those of UML-based CASE tools, will be required to offer a much more intelligent behavior than just editing. Yet there is very little formal support and there are almost no tools available for the construction of such environments. The present paper introduces a constraint-based formalism for the specification and implementation of complex diagrammatic environments. We start from grammar-based definitions of diagrammatic languages and show how a constraint solver for diagram recognition and interpretation can automatically be constructed from such grammars. In a second step, the capabilities of these solvers are extended by allowing to axiomatise formal diagrammatic systems, such as Venn Diagrams, so that they can be regarded as a new constraint domain. The ultimate aim of this schema is to establish a language of type CLP(Diagram) for diagrammatic reasoni...
HCI
rousseau96personality
Personality in Synthetic Agents ID: A043 Personality in Synthetic Agents Rousseau, Daniel KSL, Stanford University Hayes-Roth, Barbara KSL, Stanford University Abstract Personality characterizes an individual through a set of psychological traits that influence his or her behavior. Combining visions from psychology, artificial intelligence and theater, we are studying the use of personality by intelligent, automated actors able to improvise their behavior in order to portray characters, and to interact with users in a multimedia environment We show how psychological personality traits can be exploited to produce a performance that is theatrically interesting and believable without being completely predictable. We explain how personality can influence moods and interpersonal relationships. We describe the model of a synthetic actor that takes into account those concepts to choose its behavior in a given context. In order to test our approach, we observe the performance of autonomous actors portraying waiters with di...
Agents
reijers01requirements
Requirements for a Group Communication Service for FLARE This document explores what the requirements for a group communication service for the Framework for Location-aware Augmented Reality Environments (FLARE) are. This chapter provides an introduction to FLARE. The next chapter will explain the game rules for the rst application called Quazoom that we will build using FLARE. Since network partitions are important, we rst describe the game rules in the case when there are no partitions, and treat the partitioned case in a seperate section
HCI
455997
The RETSINA MAS Infrastructure RETSINA is an implemented Multi-Agent System infrastructure that has been developed for several years and applied in many domains ranging from financial portfolio management to logistic planning. In this paper, we distill from our experience in developing MASs to clearly define a generic MAS infrastructure as the domain independent and reusable substratum that supports the agents' social interactions. In addition, we show that the MAS infrastructure imposes requirements on an individual agent if the agent is to be a member of a MAS and take advantage of various components of the MAS infrastructure. Although agents are expected to enter a MAS and seamlessly and e ortlessly interact with the agents in the MAS infrastructure, the current state of the art demands agents to be programmed with the knowledge of what infrastructure they will utilize, and what are various fallback and recovery mechanisms that the infrastructure provides. By providing an abstract MAS infrastructure model and a concrete implemented instance of the model, RETSINA, we contribute towards the development of principles and practice to make the MAS infrastructure "invisible" and ubiquitous to the interacting agents.
Agents
herlocker00explaining
Explaining Collaborative Filtering Recommendations $XWRPDWHG FROODERUDWLYH ILOWHULQJ #$&)# V\VWHPV SUHGLFW D SHUVRQV DIILQLW\ IRU LWHPV RU LQIRUPDWLRQ E\ FRQQHFWLQJ WKDW SHUVRQV UHFRUGHG LQWHUHVWV ZLWK WKH UHFRUGHG LQWHUHVWV RI D FRPPXQLW\ RI SHRSOH DQG VKDULQJ UDWLQJV EHWZHHQ OLNH# PLQGHG SHUVRQV# +RZHYHU# FXUUHQW UHFRPPHQGHU V\VWHPV DUH EODFN ER[HV# SURYLGLQJ QR WUDQVSDUHQF\ LQWR WKH ZRUNLQJ RI WKH UHFRPPHQGDWLRQ# ([SODQDWLRQV SURYLGH WKDW WUDQVSDUHQF\# H[SRVLQJ WKH UHDVRQLQJ DQG GDWD EHKLQG D UHFRPPHQGDWLRQ# ,Q WKLV SDSHU# ZH DGGUHVV H[SODQDWLRQ LQWHUIDFHV IRU $&) V\VWHPV  KRZ#WKH\#VKRXOG#EH#LPSOHPHQWHG#DQG#ZK\#WKH\#  VKRXOG#EH#LPSOHPHQWHG##7R#H[SORUH#KRZ##ZH#SUHVHQW#D#PRGHO#  IRU#H[SODQDWLRQV#EDVHG#RQ#WKH#XVHUV#FRQFHSWXDO#PRGHO#RI#WKH#  UHFRPPHQGDWLRQ#SURFHVV##:H#WKHQ#SUHVHQW#H[SHULPHQWDO#  UHVXOWV#GHPRQVWUDWLQJ#ZKDW#FRPSRQHQWV#RI#DQ#H[SODQDWLRQ#DUH#  WKH#PRVW#FRPSHOOLQJ##7R#DGGUHVV#ZK\##ZH#SUHVHQW#  H[SHULPHQWDO#HYLGHQFH#WKDW#VKRZV#WKDW#SURYLGLQJ#H[SODQDWLRQV#  FDQ#LPSURYH#WKH#DFFHSWDQFH#RI#$&)#V\VWHPV##:H#DOVR#  GHVFULEH#VRPH#LQL...
IR
kopp00knowledgebased
A Knowledge-based Approach for Lifelike Gesture Animation . The inclusion of additional modalities into the communicative behavior of virtual agents besides speech has moved into focus of human-computer interface researchers, as humans are more likely to consider computer-generated figures lifelike when appropriate nonverbal behaviors are displayed in addition to speech. In this paper, we propose a knowledge-based approach for the automatic generation of gesture animations for an articulated figure. It combines a formalism for the representation of spatiotemporal gesture features, methods for planning individual gestural animations w.r.t. to form and timing, and formation of arm trajectories. Finally, enhanced methods for rendering animations from motor programs are incorporated in the execution of planned gestures. The approach is targetted to achieve a great variety of gestures as well as a higher degree of lifelikeness in synthetic agents.  1 Introduction  The communicative behaviors of virtual anthropomorphic agents, widely used in human-...
Agents
konrad98model
Model Generation without Normal Forms and Applications in Natural-Language Semantics . I present a new tableaux-based model generation method for first-order formulas without function symbols. Unlike comparable approaches, the Relational Models (RM) tableaux calculus does not require clausal input theories. I propose some applications of the RM calculus in natural-language semantics and discuss its usefulness as an inference procedure in natural-language processing. 1 Introduction  Refutational methods in automated deduction prove the unsatisfiability of logical theories. For many applications, the interpretations of a theory that show its satisfiability are at least as interesting as proofs. Model generation refers to the automatic construction of such interpretations from first-order theories. In the recent years, there has been a growing interest in the automated deduction community in developing model generation methods for various application areas such as finite mathematics [25, 22], deductive databases [7], diagnosis [13, 1], and planning [19]. As a result, mode...
DB
olson00probabilistic
Probabilistic Self-Localization for Mobile Robots Localization is a critical issue in mobile robotics. If the robot does not know where it is, it, cannot effectively plan movements, locate objects, or reach goals. In this paper, we describe probabilistic self-localization techniques for mobile robots that are based on the principal of maximum-likelihood estimation. The basic method is to compare a map generated at the current robot position to a previously generated map of the environment to prohabilistically maximize the agreement between the maps. This method is able to operate in both indoor and outdoor environments using either discrete features or an occupancy grid to represent the world map. The map may be generated using any method to detect features in the robot's surroundings, including vision, sonar, a d laser range-finder. A global search of the pose space is performed that guarantees that the best position in a discretized pose space is found according to the probabilistic: map agreement measure. In addition, fitting the likelihood function with a parameterized smface allows both subpixel localization and uncertainty estimation to be performed. The application of these techniques in several experiments is described, including experimental localization results for the Sojourner Mars rover. 1
AI
manolescu01microworkflow
Micro-Workflow: A Workflow Architecture Supporting Compositional Object-Oriented Software Development This dissertation proposes micro-workflow, a new workflow architecture that bridges the gap between the type of functionality provided by current workflow systems and the type of workflow functionality required in object-oriented applications. Micro-workflow provides a better solution when the focus is on customizing the workflow features and integrating with other systems. In this thesis I discuss how micro-workflow leverages object technology to provide workflow functionality. As an example, I present the design of an object-oriented framework which provides a reusable micro-workflow architecture and enables developers to customize it through framework-specific reuse techniques. I show how through composition, developers extend micro-workflow to support history, persistence, monitoring, manual intervention, worklists, and federated workflow. I evaluate this approach with three case studies that implement processes with different requirements
IR
453969
Information Triage using Prospective Criteria : In many applications, large volumes of time-sensitive textual information require triage: rapid, approximate prioritization for subsequent action. In this paper, we explore the use of prospective   indications of the importance of a time-sensitive document, for the purpose of producing better document filtering or ranking. By prospective, we mean importance that could be assessed by actions that occur in the future. For example, a news story may be assessed (retrospectively) as being important, based on events that occurred after the story appeared, such as a stock-price plummeting or the issuance of many follow-up stories. If a system could anticipate (prospectively) such occurrences, it could provide a timely indication of importance. Clearly, perfect prescience is impossible. However, sometimes there is sufficient correlation between the content of an information item and the events that occur subsequently. We describe a process for creating and evaluating approximate information-triage procedures that are based on prospective indications. Unlike many information-retrieval applications for which document labeling is a laborious, manual process, for many prospective criteria it is possible to build very large, labeled, training corpora automatically. Such corpora can be used to train text classification procedures that will predict the (prospective) importance of each document. This paper illustrates the process with two case studies, demonstrating the ability to predict whether the stock price of one or more companies mentioned in a news story will move significantly following the appearance of that story. We conclude by discussing that the comprehensibility of the learned classifiers can be critical to success.  1 
IR
360736
Proposal Id: P480 Proposal for MPEG-7 Image Description Scheme Name: this document, although any specific DDL selected by MPEG-7 can be used to serve the same purpose as well. We will demonstrate that new features can be easily accommodated using the hierarchical structures and entity relation structures.
IR
452308
Creating a Semantic web Interface with Virtual Reality Novel initiatives amongst the Internet community such as Internet2 [1] and Qbone [2] are based on the use of high bandwidth and powerful computers. However the experience amongst the majority of Internet users is light-years from these emerging technologies. We describe the construction of a distributed high performance search engine, utilizing advanced threading techniques on a diskless Linux cluster. The resulting Virtual Reality scene is passed to a standard client machine for viewing. This search engine bridges the gap between the Internet of today, and the Internet of the future.  Keywords: Internet Searching, High Performance VRML, Visualization.  1. 
IR
83728
Using Guidelines to Constrain Interactive Case-Based HTN Planning This paper describes HICAP, a general purpose and interactive case-based planning architecture. HICAP is a decision support tool for planning a hierarchical course of action. It integrates a hierarchical task editor, HTE, with a conversational case-based planner, NaCoDAE/HTN. HTE maintains a task hierarchy representing guidelines that constrain the final plan. HTE also encodes the hierarchical organization responsible for these tasks. This supports bookkeeping, which is crucial for real-world large-scale planning tasks. HTE can be used to activate NaCoDAE/HTN to interactively refine user-selected guideline tasks into a concrete plan. Our application of HICAP to the task of noncombatant evacuation operations inspired its architecture. In this application, our empirical evaluation with ModSAF simulations confirms that the plans output by HICAP outperform those generated using alternative approaches on three dimensions.
AI
ghani01using
Using Error-Correcting Codes for Efficient Text Classification with a Large Number of Categories We investigate the use of Error-Correcting Output Codes (ECOC) for efficient text classification with a large number of categories and propose several extensions which improve the performance of ECOC. ECOC has been shown to perform well for classification tasks, including text classification, but it still remains an under-explored area in ensemble learning algorithms. We explore the use of error-correcting codes that are short (minimizing computational cost) but result in highly accurate classifiers for several real-world text classification problems. Our results also show that ECOC is particularly effective for highprecision classification. In addition, we develop modifications and improvements to make ECOC more accurate, such as intelligently assigning codewords to categories according to their confusability, and learning the decoding (combining the decisions of the individual classifiers) in order to adapt to different datasets. To reduce the need for labeled training data, we develop a framework for ECOC where unlabeled data can be used to improve classification accuracy. This research will impact any area where efficient classification of documents is useful such as web portals, information filtering and routing, especially in open-domain applications where the number of categories is usually very large, and new documents and categories are being constantly added, and the system needs to be very efficient.
ML
vagina03cryptographic
Cryptographic Traces for Mobile Agents . Mobile code systems are technologies that allow applications  to move their code, and possibly the corresponding state, among the  nodes of a wide-area network. Code mobility is a flexible and powerful  mechanism that can be exploited to build distributed applications  in an Internet scale. At the same time, the ability to move code to and  from remote hosts introduces serious security issues. These issues include  authentication of the parties involved and protection of the hosts from  malicious code. However, the most difficult task is to protect mobile  code against attacks coming from hosts. This paper presents a mechanism  based on execution tracing and cryptography that allows one to  detect attacks against code, state, and execution flow of mobile software  components.  1 Introduction  Mobile code technologies are languages and systems that exploit some form of code mobility in an Internet-scale setting. In this framework, the network is populated by several loosely coupled co...
Agents
457229
Mixed-Initiative Interaction = Mixed Computation We show that partial evaluation can be usefully viewed as a programming model for realizing mixed-initiative  functionality in interactive applications. Mixed-initiative interaction between two participants is one where the  parties can take turns at any time to change and steer the flow of interaction. We concentrate on the facet of  mixed-initiative referred to as `unsolicited reporting' and demonstrate how out-of-turn interactions by users can  be modeled by `jumping ahead' to nested dialogs (via partial evaluation). Our approach permits the view of  dialog management systems in terms of their native support for staging and simplifying interactions; we characterize  three different voice-based interaction technologies using this viewpoint. In particular, we show that the  built-in form interpretation algorithm (FIA) in the VoiceXML dialog management architecture is actually a (well  disguised) combination of an interpreter and a partial evaluator.    This work is supported in part by US National Science Foundation grants DGE-9553458 and IIS-9876167.  1  1 
HCI
bonner98state
The State of Change: A Survey . Updates are a crucial component of any database programming language. Even the simplest database transactions, such as withdrawal from a bank account, require updates. Unfortunately, updates are not accounted for by the classical Horn semantics of logic programs and deductive databases, which limits their usefulness in real-world applications. As a short-term practical solution, logic programming languages have resorted to handling updates using ad hoc operators without a logical semantics. A great many works have been dedicated to developing logical theories in which the state of the underlying database can evolve with time. Many of these theories were developed with specific applications in mind, such as reasoning about actions, database transactions, program verification, etc. As a result, the different approaches have different strengths and weaknesses. In this survey, we review a number of these works, discuss their application domains, and highlight their strong and weak points...
DB
wilke96framework
A Framework for Learning Adaptation Knowledge Based on Knowledge Light Approaches In this paper, we present a framework for learning adaptation knowledge with knowledge light approaches for case-based reasoning (CBR) systems. "Knowledge light" means that these approaches use knowledge already acquired and represented inside the CBR system. Therefore, we describe the sources of knowledge inside a CBR system along with the different knowledge containers. Next we present our framework in terms of these knowledge containers. Further, we apply our framework to two very different knowledge light approaches for learning adaptation knowledge. After that we point out some issues which should be addressed during the design or the use of such algorithms for learning adaptation knowledge. From our point of view, many of these issues should be the topic of further research. Finally we close with a short discussion. 1 Introduction  One of the major challenges during designing a case-based reasoning (CBR) system is the modeling of appropriate adaptation knowledge. Usually adaptati...
ML
asada98robocup
The RoboCup Physical Agent Challenge: Phase I Traditional AI research has not given due attention to the important role that physical bodies play for agents as their interactions produce complex emergent behaviors to achieve goals in the dynamic real world. The RoboCup Physical Agent Challenge provides a good testbed for studying how physical bodies play a signi cant role in realizing intelligent behaviors using the RoboCup framework [Kitano, et al., 95]. In order for the robots to play a soccer game reasonably well, a wide range of technologies needs to be integrated and a number of technical breakthroughs must be made. In this paper, we present three challenging tasks as the RoboCup Physical Agent Challenge Phase I: (1) moving the ball to the speci ed area (shooting, passing, and dribbling) with no, stationary, or moving obstacles, (2) catching the ball from an opponent or a teammate (receiving, goal-keeping, and intercepting), and (3) passing the ball between two players. The rst two are concerned with single agent skills while the third one is related to a simple cooperative behavior. Motivation for these challenges and evaluation methodology are given. 1.
AI
409610
Return from the Ant - Synthetic Ecosystems for Manufacturing Control The synthetic ecosystems approach attempts to adopt basic principles of natural ecosystems in the design of multiagent systems. Natural agent systems like insect colonies are fascinating in that they are robust, flexible, and adaptive. Made up of millions of very simple entities, these systems express a highly complex and coordinated global behavior. There are several branches in different sciences, for instance in biology, physics, economics, or in computer science, that focus on distributed systems of locally interacting entities. Their research yields a number of commonly observed characteristics. To supply engineered systems with similar characteristics this thesis proposes a set of principles that should be observed when designing synthetic ecosystems. Each principle is systematically stated and motivated, and its consequences for the manufacturing control domain are discussed. Stigmergy has shown its usefulness in the coordination of large crowds of agents in a synthetic ecosystem...
Agents
lavrac00intelligent
Intelligent Data Analysis in Medicine Extensive amounts of knowledge and data stored in medical databases require the  development of specialized tools for storing and accessing of data, data analysis, and effective  use of stored knowledge and data. This paper focuses on methods and tools for  intelligent data analysis, aimed at narrowing the increasing gap between data gathering  and data comprehension. The paper sketches the history of research that led to the  development of current intelligent data analysis techniques, discusses the need for intelligent  data analysis in medicine, and proposes a classification of intelligent data analysis  methods. The scope of the paper covers temporal data abstraction methods and data  mining methods. A selection of methods is presented and illustrated in medical problem  domains. Presently data abstraction and data mining are attracting considerable research  interest. However the two technologies, in spite of the fact that they share their central  objective, namely the intelligen...
AI
shah99amdb
Amdb: A Visual Access Method Development Tool The development process for access methods (AMs) in database systems is complex and tedious. Amdb is a graphical tool that facilitates the design and tuning process for height-balanced tree-structured AMs. Central to amdb's  user interface is a suite of graphical views that visualize the entire search tree, paths and subtrees within the tree, and data contained in the tree. These views animate search tree operations in order to visualize the behavior of an access method. Amdb provides metrics that characterize the performance of queries, the tree structure, and the structureshaping aspects of an AM implementation. The visualizations can be used to browse the performance metrics in the context of the tree structure. The combination of these features allows a designer to locate the sources of performance loss reported by the metrics and investigate causes for those deficiencies. 1. Introduction  The recent explosion in the volume and diversity of electronically available information has ...
DB
308936
Evaluation challenges for a federation of heterogeneous information providers: The case of NASA's Earth Science Information Partnerships NASA's Earth Science Information Partnership Federation is an experiment funded to assess the ability of a group of widely heterogeneous earth science data or service providers to self organize and provide improved and cheaper access to an expanding earth science user community. As it is organizing itself, the federation is mandated to set in place an evaluation methodology and collect metrics reflecting the health and benefits of the Federation. This paper describes the challenges of organizing such a federated partnership self-evaluation and discusses the issues encountered during the metrics definition phase of the early data collection.  Keyword: metrics, quantitative evaluation, qualitative evaluation, earth science  1 Introduction  Beside the obvious need to evaluate any experiment to measure its positive and negative impact the impact of the Government Performance and Results Act (GPRA) is slowly changing the way federal projects are being conducted. Quantitative and qualitative...
HCI
weng99face
Face Recognition Identifying a human individual from his or her face is one of the most nonintrusive modalities in biometrics. However, it is also one of the most challenging ones. This chapter discusses why it is challenging and the factors that a practitioner can take advantage of in developing a practical face recognition system. Some major existing approaches are discussed along with some algorithmic considerations. A face recognition algorithm is presented as an example along with some experimental data. Some possible future research directions are outlined at the end of the chapter.  1.1 INTRODUCTION  Face recognition from images is a sub-area of the general object recognition problem. It is of particular interest in a wide variety of applications. Applications in law enforcement for mugshot identification, verification for personal identification such as driver's licenses and credit cards, gateways to limited access areas, surveillance of crowd behavior are all potential applications of a succes...
ML
9619
Bimodal System for Interactive Indexing and Retrieval of Pathology Images The prototype of a system to assist the physicians in differential diagnosis of lymphoproliferative disorders of blood cells from digitized specimens is presented. The user selects the region of interest (ROI) in the image which is then analyzed with a fast, robust color segmenter. Queries in a database of validated cases can be formulated in terms of shape (similarity invariant Fourier descriptors), texture (multiresolution simultaneous autoregressive model), color (L    u    v    space), and area, derived from the delineated ROI. The uncertainty of the segmentation process (obtained through a numerical method) determines the accuracy of shape description (number of Fourier harmonics). Tenfold cross-validated classification over a database of 261 color 640\Theta480 images was implemented to assess the system performance. The ground truth was obtained through immunophenotyping by flow cytometry. To provide a natural man-machine interface, most input commands are bimodal: either using t...
DB
chomicki99animating
Animating Spatiotemporal Constraint Databases . Constraint databases provide a very expressive framework  for spatiotemporal database applications. However, animating such databases  is difficult because of the cost of constructing a graphical representation  of a single snapshot of a constraint database. We present a novel approach  that makes the efficient animation of constraint databases possible. The  approach is based on a new construct: parametric polygon. We present  an algorithm to construct the set of parametric polygons that represent  a given linear constraint database. We also show how to animate objects  defined by parametric polygons, analyze the computational complexity  of animation, and present empirical data to demonstrate the efficiency  of our approach.  1 Introduction  Spatiotemporal databases have recently begun to attract broader interest [10, 12, 27]. While the temporal [4, 23, 24] and spatial [14, 28] database technologies are relatively mature, their combination is far from straightforward. In this conte...
DB
454640
A Case for Dynamic View Management this paper, we present DynaMat, a system that manages dynamic collections of materialized aggregate views in a data warehouse. At query time DynaMat utilizes a dedicated disk space for storing computed aggregates that are further engaged for answering new queries. Queries are executed independently, or can be bundled within a multi query expression. In the latter case we present an execution mechanism that exploits dependencies among the queries and the materialized set to further optimize their execution. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window. We show how to derive an efficient update plan with respect to the available maintenance window, the different update policies for the views and the dependencies that exist among them.  Categories and Subject Descriptors: H.2.7 [DATABASE MANAGEMENT]: Database Administration ---Data warehouse and re
DB
bertino99approach
An Approach to Classify Semi-Structured Objects . Several advanced applications, such as those dealing with the Web, need to handle data whose structure is not known a-priori. Such requirement severely limits the applicability of traditional database techniques, that are based on the fact that the structure of data (e.g. the database schema) is known before data are entered into the database. Moreover, in traditional database systems, whenever a data item (e.g. a tuple, an object, and so on) is entered, the application species the collection (e.g. relation, class, and so on) the data item belongs to. Collections are the basis for handling queries and indexing and therefore a proper classication of data items in collections is crucial. In this paper, we address this issue in the context of an extended object-oriented data model. We propose an approach to classify objects, created without specifying the class they belong to, in the most appropriate class of the schema, that is, the class closest to the object state. In particular, w...
DB
535578
A Parameterized Algebra for Event Notification Services Event notification services are used in various applications such as digital libraries, stock tickers, traffic control, or facility management. However, to our knowledge, a common semantics of events in event notification services has not been defined so far. In this paper, we propose a parameterized event algebra which describes the semantics of composite events for event notification systems. The parameters serve as a basis for flexible handling of duplicates in both primitive and composite events. 1.
IR
dreilinger97experiences
Experiences with Selecting Search Engines Using Metasearch Search engines are among the most useful and high profile resources on the Internet. The problem of finding information on the Internet has been replaced with the problem of knowing where search engines are, what they are designed to retrieve and how to use them. This paper describes and evaluates SavvySearch, a meta-search engine designed to intelligently select and interface with multiple remote search engines. The primary meta-search issue examined is the importance of carefully selecting and ranking remote search engines for user queries. We studied the efficacy of SavvySearch's incrementally acquired meta-index approach to selecting search engines by analyzing the effect of time and experience on performance. We also compared the meta-index approach to the simpler categorical approach and showed how much experience is required to surpass the simple scheme.  1 Introduction  Search engines are powerful tools for assisting the otherwise unmanageable task of navigating the rapidly ex...
IR
367226
Using the Resources Model in Virtual Environment Design this paper we take a step back from the formal specification of VEs to investigate where requirements and design information are located within these environments and how it can be structured and analysed. More specifically, we are interested in considering VEs in terms of distributed cognition (DC) [5, 7, 14, 20].
HCI
liu01rulebased
A Rule-based Query Language for HTML With the recent popularity of the web, enormous amount of information is now available on line. Most web documents available over the web are in HTML format and are hierarchically structured in nature. How to query such web documents based on their internal hierarchical structure becomes more and more important. In this paper, we present a rule-based language called WebQL to support effective and flexible web queries. Unlike other web query languages, WebQL is a high level declarative query language with a logical semantics. It allows us to query web documents based on their internal hierarchical structures. It supports not only negation and recursion, but also query result restructuring in a natural way. We also describe the implementation of the system that supports the WebQL query language.
DB
cakmakci02context
Context Awareness in Systems with Limited Resources Mobile embedded systems often have strong limitations regarding available resources. In this paper we propose a statistical approach which could scale down to microcontrollers with scarce resources, to model simple contexts based on raw sensor data. As a case study, two experiments are provided where statistical modeling techniques were applied to learn and recognize different contexts, based on accelerometer data. We furthermore point out applications that utilize contextual information for power savings in mobile embedded systems.
HCI
457130
Enlightened Agents in TuCSoN In the network-centric computing era, applications often involve sets of autonomous, unpredictable, and possibly mobile entities interacting within open, dynamic, and possibly unreliable environments: Intelligent Environments are a typical case. The complexity of such scenarios requires novel engineering tools, providing effective support from the analysis to the deployment stage. In this paper we illustrate the impact of a general-purpose coordination infrastructure for multiagent systems -- providing a model, a run-time, and suitable deployment tools -- on the engineering of such applications. As a case study, we consider the intelligent management of lights inside a building: despite its simplicity, this problem endorses the typical challenges of this class of applications. The case study is built upon the TuCSoN coordination infrastructure, which provides engineers  with both the abstractions and the run-time support for effectively managing the application complexity.  I. INFRASTR...
Agents
mk00implementing
Implementing Clinical Practice Guidelines While Taking Account of Changing Evidence: ATHENA DSS, an Easily Modifiable Decision-Support System for Managing Hypertension in Primary Care This paper describes the ATHENA Decision Support System (DSS), which operationalizes guidelines for hypertension using the EON architecture. ATHENA DSS encourages blood pressure control and recommends guideline-concordant choice of drug therapy in relation to comorbid diseases. ATHENA DSS has an easily modifiable knowledge base that specifies eligibility criteria, risk stratification, blood pressure targets, relevant comorbid diseases, guideline-recommended drug classes for patients with comorbid disease, preferred drugs within each drug class, and clinical messages. Because evidence for best management of hypertension evolves continually, ATHENA DSS is designed to allow clinical experts to customize the knowledge base to incorporate new evidence or to reflect local interpretations of guideline ambiguities. Together with its database mediator Athenaeum, ATHENA DSS has physical and logical data independence from the legacy Computerized Patient Record System(CPRS) supplying the patient data, so it can be integrated into a variety of electronic medical record systems.
DB
20587
Inductive Bias in Case-Based Reasoning Systems In order to learn more about the behaviour of case-based reasoners as learning systems, we formalise a simple case-based learner as a PAC learning algorithm, using the case-based representation  hCB; oei. We first consider a `naive' case-based learning algorithm CB1(oeH ) which learns by collecting all available cases into the case-base and which calculates similarity by counting the number of features on which two problem descriptions agree. We present results concerning the consistency of this learning algorithm and give some partial results regarding its sample complexity. We are able to characterise CB1(oeH ) as a `weak but general' learning algorithm. We then consider how the sample complexity of case-based learning can be reduced for specific classes of target concept by the application of inductive bias, or prior knowledge of the class of target concepts. Following recent work demonstrating how case-based learning can be improved by choosing a similarity measure appropriate to t...
ML
abowd00classroom
Classroom 2000: An Experiment with the Instrumentation of a Living Educational Environment One potentially useful feature of future computing environments is the ability to capture the live experiences of the occupants and to provide that record to users for later access and review. Over the last 3 years, we have designed and extensively used a particular instrumented environment, the classroom, designed to facilitate the easy capture of the traditional lecture experience. We will describe the history of the Classroom 2000 project at Georgia Tech, and provide results of extended evaluations of the impact of automated capture on the teaching and learning experience. In addition to understanding the impact of automated capture in this educational domain, there are many important lessons to take away from this long-term, largescale experiment with a living ubiquitous computing environment. The environment needs to address issues of scale and extensibility, and there needs to be a way to continuously evaluate the effectiveness of the environment and understand and react to the w...
HCI
dom99mining
Mining the Link Structure of the World Wide Web The World Wide Web contains an enormous amount of information, but it can be exceedingly difficult for users to locate resources that are both high in quality and relevant to their information needs. We develop algorithms that exploit the hyperlink structure of the WWW for information discovery and categorization, the construction of high-quality resource lists, and the analysis of on-line hyperlinked communities. 1 Introduction  The World Wide Web contains an enormous amount of information, but it can be exceedingly difficult for users to locate resources that are both high in quality and relevant to their information needs. There are a number of fundamental reasons for this. The Web is a hypertext corpus of enormous size --- approximately three hundred million Web pages as of this writing --- and it continues to grow at a phenomenal rate. But the variation in pages is even worse than the raw scale of the data: the set of Web pages taken as a whole has almost no unifying structure, wi...
IR
martin99embedding
Embedding Knowledge in Web Documents The paper argues for the use of general and intuitive knowledge representation languages (and simpler notational variants, e.g. subsets of natural languages) for indexing the content of Web documents and representing knowledge within them. We believe that these languages have advantages over metadata languages based on the Extensible Mark-up Language (XML). Indeed, the retrieval of precise information is better supported by languages designed to represent semantic content and support logical inference, and the readability of such a language eases its exploitation, presentation and direct insertion within a document (thus also avoiding information duplication). We advocate the use of Conceptual Graphs and simpler notational variants that enhance knowledge readability. To further ease the representation process, we propose techniques allowing users to leave some knowledge terms undeclared. We also show how lexical, structural and knowledge-based techniques may be combined to retrieve or ...
AI
22187
Solving Stabilization Problems in Case-Based Knowledge Acquisition Case-based reasoning is widely deemed an important methodology towards alleviating the bottleneck of knowledge acquisition. The key idea is to collect cases representing a human's or a system's experience directly rather than trying to construct generalizations. Episodic knowledge accumulated this way may be used flexibly for different purposes by determining similarities between formerly solved problems and current situations under investigation. But the flexibility of case-based reasoning brings with it a number of disadvantages. One crucial difficulty is that every new experience might seem worth to be memorized. As a result, a case-based reasoning system may substantially suffer from collecting a huge amount of garbage without being able to separate the chaff from the wheat. This paper presents a case study in case-based learning. Some target concept has to be learned by collecting cases and tuning similarity concepts. It is extremely difficult to avoid collecting a huge amount of ...
ML
johnson00affectively
Affectively tunable environments for the virtual stage (Abstract) Affective computing [8, 9] is an emerging area of human-computer interaction which studies the ways in which computer systems can recognize and work with human emotions. The word affective is an adjective used in psychology to mean "relating to emotions". Examples of affective computing are the creation of systems which respond in different ways according to the system's perception of the user's emotional state [1, 2, 3], and systems which facilitate the communication of emotion through a virtual environment [7].  One aspect of aective computing which is particularly apposite to the theatre is the ability to create environments in which the capacity for communication of emotion (what we might call the affective bandwidth) of the environment is variable [6]. Consider the follo...
HCI
tomsich01optimizing
Optimizing the parSOM Neural Network Implementation for Data Mining with Distributed Memory Systems and Cluster Computing The self-organizing map is a prominent unsupervised neural network model which lends itself to the analysis of high-dimensional input data and data mining applications. However, the high execution times required to train the map put a limit to its application in many high-performance data analysis application domains.  In this paper we discuss the parSOM implementation, a software-based parallel implementation of the selforganizing map, and its optimization for the analysis of high-dimensional input data using distributed memory systems and clusters. The original parSOM algorithm scales very well in a parallel execution environment with low communication latencies and exploits parallelism to cope with memory latencies. However it suffers from poor scalability on distributed memory computers. We present optimizations to further decouple the subprocesses, simplify the communication model and improve the portability of the system.  1 Introduction  The self-organizing map (SOM) [5] is a pr...
IR
manmatha01modeling
Modeling Score Distributions for Combining the Outputs of Search Engines In this paper the score distributions of a number of text search engines are modeled. It is shown empirically that the score distributions on a per query basis may be fitted using an exponential distribution for the set of non-relevant documents and a normal distribution for the set of relevant documents. Experiments show that this model fits TREC-3 and TREC-4 data for not only probabilistic search engines like INQUERY but also vector space search engines like SMART for English. We have also used this model to fit the output of other search engines like LSI search engines and search engines indexing other languages like Chinese. It is then shown that given a query for which relevance information is not available, a mixture model consisting of an exponential and a normal distribution can be fitted to the score distribution. These distributions can be used to map the scores of a search engine to probabilities. We also discuss how the shape of the score distributions arise given certain assumptions about word distributions in documents. We hypothesize that all 'good' text search engines operating on any language have similar characteristics. This model has many possible applications. For example, the outputs of different search engines can be combined by averaging the probabilities (optimal if the search engines are independent) or by using the probabilities to select the best engine for each query. Results show that the technique performs as well as the best current combination techniques. This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC-9209623, in part by the National Science Foundation under grant numbers IRI-9619117 and IIS-9909073, in part by N...
IR
jarke00concept
Concept Based Design of Data Warehouses: The DWQ Demonstrators The ESPRIT Project DWQ (Foundations of Data Warehouse Quality) aimed at improving the quality of DW design and operation through systematic enrichment of the semantic foundations of data warehousing. Logic-based knowledge representation and reasoning techniques were developed to control accuracy, consistency, and completeness via advanced conceptual modeling techniques for source integration, data reconciliation, and multi-dimensional aggregation. This is complemented by quantitative optimization techniques for view materialization, optimizing timeliness and responsiveness without losing the semantic advantages from the conceptual approach. At the operational level, query rewriting and materialization refreshment algorithms exploit the knowledge developed at design time. The demonstration shows the interplay of these tools under a shared metadata repository, based on an example extracted from an application at Telecom Italia.  1 Overview of the Demonstration  The demonstration follows ...
DB
fjeld99designing
Designing Graspable Groupware for Co-Located Planning and Configuration Tasks This paper shows some of the vital steps in the design process of a graspable groupware system. Activity theory is the theoretical foundation for our research. Our design philosophy is based on the tradition of Augmented Reality (AR), which enriches natural communication with virtual features. Another important part of our design philosophy is the use of coinciding action and perception spaces. We developed groupware for layout planning and configuration tasks called the BUILD-IT system. This system enables users, grouped around a table, to cooperate in the design manipulation of a virtual setting, thus supporting colocated, instead of distributed, interaction (Rauterberg et al., 1997a, 1997b, 1998; Fjeld et al., 1998a). The multi-user nature of BUILD-IT overcomes a serious drawback often seen with CSCW tools, namely that they are based on single-user applications (Grudin, 1988). We believe that co-location is an indispensable factor for the early stage of a complex planning process. Input and output, however, can be prepared and further developed off-line (Fjeld et al., 1998b), using any conventional CAD system.
HCI
60236
Distributed Query Scheduling Service: An Architecture and Its Implementation We present the systematic design and development of a distributed query scheduling service (DQS) in the context of DIOM, a distributed and interoperable query mediation system [26]. DQS consists of an extensible architecture for distributed query processing, a three-phase optimization algorithm for generating efficient query execution schedules, and a prototype implementation. Functionally, two important execution models of distributed queries, namely moving query to data or moving data to query, are supported and combined into a unified framework, allowing the data sources with limited search and filtering capabilities to be incorporated through wrappers into the distributed query scheduling process. Algorithmically, conventional optimization factors (such as join order) are considered separately from and refined by distributed system factors (such as data distribution, execution location, heterogeneous host capabilities), allowing for stepwise refinement through three optimization phases: compilation, parallelization, site selection and execution. A subset of DQS algorithms has been implemented in Java to demonstrate the practicality of the architecture and the usefulness of the distributed query scheduling algorithm in optimizing execution schedules for inter-site queries.
IR
galanis01following
Following the paths of XML Data: An algebraic framework for XML query evaluation This paper introduces an algebraic framework for expressing and evaluating queries over XML data.  It presents the underlying assumptions of the framework, describes the input and output of the algebraic  operators, and defines these operators and their semantics. It evaluates the framework with regard to  other proposed XML query algebras. Examples show that this framework is flexible enough to capture  queries expressed in Quilt, one of the dominant XML query languages. We have used this algebra in the  context of an Internet query engine, in which it is used to formulate logical plans for XML-QL queries.  We define equivalence rules that provide opportunities for optimization, and give example cases that  point out the usefulness of these rules.  1 
DB
vavouras00modeling
Modeling and Executing the Data Warehouse Refreshment Process Data warehouse refreshment is often viewed as a problem of maintaining
DB
robinson99requirement
Requirements Interaction Management ion. Requirements may be distinguished based on the abstraction level of their description. A requirement may be further defined by add new details defined in more specialized subrequirements. Through specialization of abstract requirements, or generalization of detailed requirement, a requirement abstraction hierarchy can be defined.  . Development p roperties . Requirements may be distinguished based on their development properties. For example, a requirement may have just been proposed. Late r, it may be accepted or  rejected.  . Representational properties. Requirements may be distinguished based on their representation. A requirement may begin as an informal sketch, then become a natural language sentence (e.g., "The system shall ..."). Finall y, more formal representations, such as UML, Z, or predicate cal-  Requirements Interaction Management - Definition and scope 6  1999 William N. Robinson Requirements Interaction Management GSU CIS 99-7 culus, may be used to express a requir...
HCI
356540
OMS Java: Lessons Learned from Building a Multi-Tier Object Management Framework We present the object-oriented multi-tier application framework OMS Java which is  independent of the underlying database management system (DBMS). We detail the storage  management component and sketch which part of the framework has to be extended when  introducing a new DBMS. We compare versions of OMS Java using the persistent storage  engine ObjectStore PSE Pro for Java, the object-oriented DBMS Objectivity/DB, the objectrelational  DBMS Oracle and the proprietary DBMS Berkley DB.  1 Introduction  Most applications create data that extends the life of an application process making it necessary that application objects can be stored in and retrieved from non-volatile storage. Furthermore, looking at pure object-oriented applications, i.e. applications developed entirely using an objectoriented language environment such as Java [KA96], application objects typically refer to many other application objects resulting in complex object hierarchies. It is therefore crucial to find mechan...
DB
70863
Essential Principles for Workflow Modelling Effectiveness While the specification languages of workflow management systems focus on process execution  semantics, the successful development of workflows relies on a fuller conceptualisation of  business processing, including process semantics. Traditionally, the success of conceptual modelling   techniques has depended largely on the adequacy of certain requirements: conceptualisation  (following the Conceptualisation Principle), expressive power (following the One Hundred Principle)  , comprehensibility and formal foundation. An equally important requirement, particularly  with the increased conceptualisation of business aspects, is business suitability. In this paper, the  focus is on the suitability of workflow modelling for a commonly encountered class of (operational)  business processing, e.g. those of insurance claims, bank loans and land conveyancing.  Based on a previously conducted assessment of a number of integrated techniques, the results of  which are summarised in this paper, fiv...
HCI
artale02temporal
A Temporal Description Logic for Reasoning over Conceptual Schemas and Queries This paper introduces a new logical formalism, intended for temporal conceptual modelling,  as a natural combination of the well-known description logic DLR and point-based linear  temporal logic with Since and Until. The expressive power of the resulting DLRUS logic is illustrated  by providing a characterisation of the most important temporal conceptual modelling  constructs appeared in the literature. We define a query language (where queries are non-recursive  Datalog programs and atoms are complex DLRUS expressions) and investigate the problem of  checking query containment under the constraints defined by DLRUS conceptual schemas---i.e.,  DLRUS knowledge bases---as well as the problems of schema satisfiability and logical implication.
DB
globig95learning
Learning in Case-Based Classification Algorithms While symbolic learning approaches encode the knowledge provided by the presentation of the cases explicitly into a symbolic representation of the concept , e.g. formulas, rules, or decision trees, case-based approaches describe learned concepts implicitly by a pair (CB;d), i.e. by a set CB  of cases and a distance measure d. Given the same information, symbolic as well as the case-based approach compute a classification when a new case is presented. This poses the question if there are any differences concerning the learning power of the two approaches. In this work we will study the relationship between the case base, the measure of distance, and the target concept of the learning process. To do so, we transform a simple symbolic learning algorithm (the version space algorithm) into an equivalent case-based variant. The achieved results strengthen the conjecture of the equivalence of the learning power of symbolic and casebased methods and show the interdependency between the measure...
ML
bonifati01warehousing
Warehousing Workflow Data: Challenges and Opportunities Workflow management systems (WfMSs) are  software platforms that allow the definition,  execution, monitoring, and management of  business processes. WfMSs log every event that  occurs during process execution. Therefore,  workflow logs include a significant amount of  information that can be used to analyze process  executions, understand the causes of high- and  low-quality process executions, and rate the  performance of internal resources and business  partners. In this paper we present a packaged  data warehousing solution, coupled with HP  Process Manager, for collecting and analyzing  workflow execution data. We first present the  main challenges involved in this effort, and then  detail the proposed approach.  1. 
DB
518092
Methods and Metrics for Cold-Start Recommendations Ve have developed a method for recommending items that combines content and collaborative data under a single probabifistic framework. We benchmark our algorithm against a naYve Bayes classifier on the cold-start problem, where we wish to recommend items that no one in the commu- nity has yet rated. Ve systematically explore three testing methodologies using a publicly available data set, and explain how these methods apply to specific real-world appli- cations. Ve advocate heuristic recommeuders when bench- marking to give competent baseline performance. Ve introduce a nev perfbrmance metric, the CROC curve, and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems. Though the emphasis of onr testing is on cold-start recommending, our methods fbr recommending and evaluation are general.
IR
508859
A Visual Modality for the Augmentation of Paper In this paper we describe how we have enhanced our multimodal paper-based system, Rasa, with visual perceptual input. We briefly explain how Rasa improves upon current decisionsupport tools by augmenting, rather than replacing, the paperbased tools that people in command and control centers have come to rely upon. We note shortcomings in our initial approach, discuss how we have added computer-vision as another input modality in our multimodal fusion system, and characterize the advantages that it has to offer. We conclude by discussing our current limitations and the work we intend to pursue to overcome them in the future.
HCI
289676
Erratic Fudgets: A Semantic Theory for an Embedded Coordination Language The powerful abstraction mechanisms of functional programming languages provide the means to develop domain-specific programming languages within the language itself. Typically, this is realised by designing a set of combinators (higher-order reusable programs) for an application area, and by constructing individual applications by combining and coordinating individual combinators. This paper is concerned with a successful example of such an embedded programming language, namely Fudgets, a library of combinators for building graphical user interfaces in the lazy functional language Haskell. The Fudget library has been used to build a number of substantial applications, including a web browser and a proof editor interface to a proof checker for constructive type theory. This paper develops a semantic theory for the non-deterministic stream processors that are at the heart of the Fudget concept. The interaction of two features of stream processors makes the development of such a semantic theory problematic:  the sharing of computation provided by the lazy evaluation mechanism of the underlying host language, and  the addition of non-deterministic choice needed to handle the natural concurrency that reactive applications entail.  We demonstrate that this combination of features in a higher-order functional language can be tamed to provide a tractable semantic theory and induction principles suitable for reasoning about contextual equivalence of Fudgets.
ML
knight01visualisation
Visualisation Effectiveness Providing evaluations of visualisations is one way to demonstrate that they support a purpose and are adequate for the role claimed for them. The problem in doing so is that there is no central source of evaluation issues that one can use a subset of for this purpose. There is also very little in the way of agreement over what constitutes a good visualisation hence the evaluation criteria differ. There are the human-computer interaction ideals, the slightly differing ones from usability engineering, those from the visualisation community, and also the need to be able to support the variable abilities of the users. Graphics, as the medium behind visualisation, may support greater bandwidth, but is also prone to more likes and dislikes than other forms of interface. The concept of visualisation effectiveness and therefore ways of evaluating visualisations provide the focus for this paper.  Keywords: Visualisation, Evaluation, Usability,  Understanding  1 
HCI
412057
Discovering Internet Resources to Enrich a Structured Personal Information Space The Internet is a tremendous resource where one can find documents to enrich a personal information space. The question is: how can one find relevant documents and how can these be organized into an information space? In this paper, we describe a prototype which aims to provide the user with assistance in these two tasks. Our approach assumes the existence of an initial concept structure set up by the user. This structure may contain only rudimentary descriptions for each concept. The system's task is to find relevant documents from the Internet and to insert them in the appropriate places in the concept structure.  1. Information Management for Internet Users  The amount of information available through the Internet is overwhelming; as a result, most of this information goes unnoticed or gets lost again soon after having been noticed. The problem is not new, it is just being exacerbated by two factors: a sudden growth in the number of information consumers accompanied by acceleration ...
IR
boros98how
How Statistics and Prosody can guide a Chunky Parser Introduction  Following the most common architecture of spoken dialog systems as shown in Figure 1, the main task of linguistic processing is to yield a semantic representation of what the user said. utterance User System answer Word recognizer Generator Linguistic processor Database  base Knowl. control Dialog Figure 1. Typical dialog system architecture.  These semantic representations are interpreted by the dialog module according to the dialog context and the system answer will be generated accordingly. The system utterance depends on whether the system still needs certain information or if all necessary information has been given to accomplish its task. In order to know, when all required information has been provided, the dialog  This work was partly funded by the European Community in the framework of the SQEL--Project (Spoken Queries in European Languages), Copernicus Project No. 1634. The responsibility for the contents lies with
ML
munroe00bbq
BBQ: A Visual Interface for Integrated Browsing and Querying of XML In this paper we present BBQ (Blended Browsing and Querying), a graphic user interface for seamlessly browsing and querying XML data sources. BBQ displays the structure of multiple data sources using a paradigm that resembles drilling-down in Windows' directory structures. BBQ allows queries incorporating one or more of the sources. Queries are constructed in a query-by-example (QBE) manner, where DTDs play the role of schema. The queries are arbitrary conjunctive queries with GROUPBY, and their results can be subsequently used and refined. To support query refinement, BBQ introduces virtual result views: standalone virtual data sources that (i) are constructed by user queries, from elements in other data sources, and (ii) can be used in subsequent queries as first-class data sources themselves. Furthermore, BBQ allows users to query data sources with loose or incomplete schema, and can augment such schema with a DTD inference mechanism.
DB
164953
Computational Logic and Machine Learning: A roadmap for Inductive Logic Programming Computational logic has already significantly influenced (symbolic) machine learning through the field of inductive logic programming (ILP) which is concerned with the induction of logic programs from examples and background knowledge. In ILP, the shift of attention from program synthesis to knowledge discovery resulted in advanced techniques that are practically applicable for discovering knowledge in relational databases. Machine learning, and ILP in particular, has the potential to influence computational logic by providing an application area full of industrially significant problems, thus providing a challenge for other techniques in computational logic. This paper gives a brief introduction to ILP, presents state-of-the-art ILP techniques for relational knowledge discovery as well as some research and organizational directions for further developments in this area. 1 Introduction  Inductive logic programming (ILP) [35, 39, 29] is a research area that has its backgrounds in induct...
DB
wiesmann99systematic
A systematic classification of replicated database protocols based on atomic broadcast Database replication protocols based on group communication primitives have recently emerged as a promising technology to improve database faulttolerance and performance. Roughly speaking, this approach consists in exploiting the order and atomicity properties provided by group communication primitives or, more specifically Atomic Broadcast, to guarantee transaction properties. This paper proposes a systematic classification of non voting database replication algorithms based on Atomic Broadcast. 1.
DB
wooldridge99methodology
A Methodology for Agent-Oriented Analysis and Design . This article presents Gaia: a methodology for agent-oriented analysis and design. The Gaia methodology is both general, in that it is applicable to a wide range of multi-agent systems, and comprehensive, in that it deals with both the macro-level (societal) and the micro-level (agent) aspects of systems. Gaia is founded on the view of a multi-agent system as a computational organisation consisting of various interacting roles. We illustrate Gaia through a case study (an agent-based business process management system).  1. Introduction  Progress in software engineering over the past two decades has been made through the development of increasingly powerful and natural high-level abstractions with which to model and develop complex systems. Procedural abstraction, abstract data types, and, most recently, objects and components are all examples of such abstractions. It is our belief that agents represent a similar advance in abstraction: they may be used by software developers to more n...
Agents
493991
The Abels Framework For Linking Distributed Simulations Using Software Agents Many simulations need access to dynamicallychanging data from other sources such as sensors or even other simulations. For example, a forest fire simulation may need data from sensors in the forest and a weather simulation at a remote site. We have developed an agentbased software framework to facilitate the dynamic exchange of data between distributed simulations and other remote data resources. The framework, called ABELS (Agent-Based Environment for Linking Simulations), allows independently designed simulations to communicate seamlessly with no a priori knowledge of the details of other simulations and data resources. This paper discusses our architecture and current implementation using Sun Microsystems' Jini technology and the D'Agents mobile agent system. This paper extends earlier work by describing the implementation of the brokering system for matching data producers and consumers and providing additional details of other components.
Agents
girod00development
Development and Characterization of an Acoustic Rangefinder Localization is important to wearable and embedded applications at many levels. For example, it might be used to implement \physical" user interfaces which detect body language or the user's positioning of objects in the environment. This paper presents the initial results of a prototype implementation of an acoustic rangender. Unlike other published work in this area, this design uses a sliding correlator to detect the acoustic signal. This technique signicantly improves performance in obstructed and noisy environments. Ongoing work towards a more compact implementation and towards new protocols for establishing coordinate systems is also described.  1 Introduction  When the members of our research group are brainstorming applications for our soon-to-be-indispensible ad-hoc networked sensor technology, we often run into the same brick wall: how do we do anything that is physically motivated and context aware without localization ? Virtually every application we propose degenerates t...
HCI
defago00totally
Totally Ordered Broadcast and Multicast Algorithms: A Comprehensive Survey Total order multicast algorithms constitute an important class of problems in distributed systems, especially  in the context of fault-tolerance. In short, the problem of total order multicast consists in sending  messages to a set of processes, in such a way that all messages are delivered by all correct destinations in the  same order. However, the huge amount of literature on the subject and the plethora of solutions proposed so  far make it difficult for practitioners to select a solution adapted to their specific problem. As a result, naive  solutions are often used while better solutions are ignored.  This paper proposes a classification of total order multicast algorithms based on the ordering mechanism  of the algorithms, and describes a set of common characteristics (e.g., assumptions, properties) with which  to evaluate them. In this classification, more than fifty total order broadcast and multicast algorithms are surveyed.  The presentation includes asynchronous algorithms ...
DB
501178
Supporting Internet-Scale Multi-Agent Systems ts a model of AgentScape from the agent perspective, that is, the location comprising the middleware and the resources are represented by a location manager agent and resource objects. Calls from an agent to the middleware are modeled by requests to the location manager agent to, for example, create an agent or move an agent. Information about resources residing at the location can be retrieved by binding to the resource objects, which are local distributed objects. These objects can be accessed only within the location they reside, not from outside the location.  For development of agent applications, an application programming interface (API) and a runtime system (RTS) are provided, see Fig. 1. The default API and RTS can be extended to provide a higher-level application programming interface with, for example, a model that offers more structure and semantics to the agent application developer.  Within AgentScape, management of large-scale agent systems is an important issue, includi
Agents
115971
Exception Handling in Agent Systems A critical challenge to creating effective agent-based systems is allowing them to operate effectively when the operating environment is complex, dynamic, and error-prone. In this paper we will review the limitations of current "agent-local" approaches to exception handling in agent systems, and propose an alternative approach based on a shared exception handling service that is "plugged", with little or no customization, into existing agent systems. This service can be viewed as a kind of "coordination doctor"; it knows about the different ways multi-agent systems can get "sick", actively looks system-wide for symptoms of such "illnesses", and prescribes specific interventions instantiated for this particular context from a body of general treatment procedures. Agents need only implement their normative behavior plus a minimal set of interfaces. We claim that this approach offers simplified agent development as well as more effective and easier to modify exception handling behavior. T...
Agents
528220
Efficient Web Search on Mobile Devices with Multi-Modal Input and Intelligent Text Summarization Ease of browsing and searching for information on mobile devices has been an area of increasing interest in the World Wide Web research community [1, 2, 3, 6, 7]. While some work has been done to enhance the usability of handwriting recognition to input queries through techniques such as automatic word suggestion [2], the use of speech as an input mechanism has not been extensively studied. This paper presents a system which combines spoken query in addition to automatic title summarization to ease searching for information on a mobile device. Preliminary usability study with 10 subjects indicates that spoken queries is preferred over other input methods.
IR
martinez99face
Face Image Retrieval Using HMMs This paper introduces a new face recognition system that can be used to index (and thus retrieve) images and videos of a database of faces. New face recognition approaches are needed because, although much progress has been made to identify face taken from different viewpoints, we still cannot robustly identify faces under different illumination conditions, or when the facial expression changes, or when a part of the face is occluded on account of glasses or parts of clothing. When face recognition methods have worked in the past, it was only when all possible "image variations" were learned. Principal Components Analysis (PCA) and Fisher Discriminant Analysis (FDA) are well-known cases of such methods. In this paper we present a different approach to the indexing of face images. Our approach is based on identifying frontal faces and it allows reasonable variability in facial expressions, illumination conditions, and occlusions caused by eye-wear or items of clothing such as scarves. W...
ML
garcke02classification
Classification With Sparse Grids Using Simplicial Basis Functions Recently we presented a new approach [20] to the classification problem arising in data mining. It is based on the regularization network approach but in contrast to other methods, which employ ansatz functions associated to data points, we use a grid in the usually high-dimensional feature space for the minimization process. To cope with the curse of dimensionality, we employ sparse grids [52]. Thus, only O(h −1 n n d−1) instead of O(h −d n) grid points and unknowns are involved. Here d denotes the dimension of the feature space and hn = 2 −n gives the mesh size. We use the sparse grid combination technique [30] where the classification problem is discretized and solved on a sequence of conventional grids with uniform mesh sizes in each dimension. The sparse grid solution is then obtained by linear combination. The method computes a nonlinear classifier but scales only linearly with the number of data points and is well suited for data mining applications where the amount of data is very large, but where the dimension of the feature space is moderately high. In contrast to our former work, where d-linear functions were used, we now apply linear basis functions based on a simplicial discretization. This allows to handle more dimensions and the algorithm needs less operations per data point. We further extend the method to so-called anisotropic sparse grids, where now different a-priori chosen mesh sizes can be used for the discretization of each attribute. This can improve the run time of the method and the approximation results in the case of data sets with different importance of the attributes. We describe the sparse grid combination technique for the classification problem, give implementational details and discuss the complexity of the algorithm. It turns out that the method scales linearly with the number of given data points. Finally we report on the quality of the classifier built by our new method on data sets with up to 14 dimensions. We show that our new method achieves correctness rates which are competitive to those of the best existing methods.
ML
4588
Semantics-Based Information Retrieval : In this paper we investigate the use of conceptual descriptions based on description logics for contentbased information retrieval and present several innovative contributions. We provide a query-byexamples retrieval framework which avoids the drawback of a sophisticated query language. We extend an existing DL to deal with spatial concepts. We provide a content-based similarity measure based on the least common subsumer which extracts conceptual similarities of examples. 1 Introduction As more and more information of various kinds becomes available for an increasing number of users, one major challenge for Computer Science is to provide e#cient access and retrieval mechanisms. This is not only true for Web-based information which by its nature tends to be highly unorganized and heterogeneous, but also for dedicated databases which are designed to provide a particular service. The guiding example of this paper is a "TV-Assistant" with a database containing TV-program information. Its...
IR
huang01appliance
Appliance Data Services: Making Steps Towards an Appliance Computing World Although digital appliances are designed to be easy to use, their users often cannot even perform simple tasks because the devices lack infrastructural support. The Appliance Data Services project seeks to explore the attributes of an appliance computing world and develop the infrastructure required to support users with digital appliances.  1 
HCI
108580
Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem The problem of merging multiple databases of information about common entities is frequently encountered in KDD and decision support applications in large commercial and government organizations. The problem we study is often called the Merge/Purge problem and is difficult to solve both in scale and accuracy. Large repositories of data typically have numerous duplicate information entries about the same entities that are difficult to cull together without an intelligent "equational theory" that identifies equivalent items by a complex, domain-dependent matching process. We have developed a system for accomplishing this Data Cleansing task and demonstrate its use for cleansing lists of names of potential customers in a direct marketing-type application. Our results for statistically generated data are shown to be accurate and effective when processing the data multiple times using different keys for sorting on each successive pass. Combing results of individual passes using transitive c...
ML
horling99diagnosis
Diagnosis as an Integral Part of Multi-Agent Adaptability Agents working under real world conditions may face an environment capable of changing rapidly from one moment to the next, either through perceived faults, unexpected interactions or adversarial intrusions. To gracefully and efficiently handle such situations, the members of a multi-agent system must be able to adapt, either by evolving internal structures and behavior or repairing or isolating those external influenced believed to be malfunctioning. The first step in achieving adaptability is diagnosis - being able to accurately detect and determine the cause of a fault based on its symptoms. In this paper we examine how domain independent diagnosis plays a role in multi-agent systems, including the information required to support and produce diagnoses. Particular attention is paid to coordination based diagnosis directed by a causal model. Several examples are described in the context of an Intelligent Home environment, and the issue of diagnostic sensitivity versus efficiency is ad...
Agents
druin99role
The Role of Children in the Design of New Technology This paper suggests a framework for understanding the roles that children can play in the technology design process, particularly in regards to designing technologies that support learning. Each role, user, tester, informant, and design partner has been defined based upon a review of the literature and my lab’s own research experiences. This discussion does not suggest that any one role is appropriate for all research or development needs. Instead, by understanding this framework the reader may be able to make more informed decisions about the design processes they choose to use with children in creating new technologies. This paper will present for each role a historical overview, research and development methods, as well as the strengths, challenges, and unique contributions associated with children in the design process.
HCI
366926
Combining Statistical Measures to Find Image Text Regions We present a method based on statistical properties of local image pixels for focussing attention on regions of text in arbitrary scenes where the text plane is not necessarily fronto-parallel to the camera. This is particularly useful for Desktop or Wearable Computing applications. The statistical measures are chosen to reveal charactersitic properties of text. We combine a number of localised measures using a neural network to classify each pixel as text or non-text. We demonstrate our results on typical images. 1. Introduction  To automatically enter the contents of a text document into a computer, one can place it on a flatbed scanner and use state of the art Optical Character Recognition (OCR) software to retrieve the characters. However, automatic segmentation and recognition of text in arbitrary scenes, where the text may or may not be fronto-parallel to the viewing plane, is an area of computer vision which has not been extensively researched previously. The problems involved a...
HCI
dhillon01concept
Concept Decompositions for Large Sparse Text Data using Clustering Abstract. Unlabeled document collections are becoming increasingly common and available; mining such data sets represents a major contemporary challenge. Using words as features, text documents are often represented as high-dimensional and sparse vectors–a few thousand dimensions and a sparsity of 95 to 99 % is typical. In this paper, we study a certain spherical k-means algorithm for clustering such document vectors. The algorithm outputs k disjoint clusters each with a concept vector that is the centroid of the cluster normalized to have unit Euclidean norm. As our first contribution, we empirically demonstrate that, owing to the high-dimensionality and sparsity of the text data, the clusters produced by the algorithm have a certain “fractal-like ” and “self-similar ” behavior. As our second contribution, we introduce concept decompositions to approximate the matrix of document vectors; these decompositions are obtained by taking the least-squares approximation onto the linear subspace spanned by all the concept vectors. We empirically establish that the approximation errors of the concept decompositions are close to the best possible, namely, to truncated singular value decompositions. As our third contribution, we show that the concept vectors are localized in the word space, are sparse, and tend towards orthonormality. In contrast, the singular vectors are global in the word space and are dense. Nonetheless, we observe the surprising fact that the linear subspaces spanned by the concept vectors and the leading singular vectors are quite close in the sense of small principal angles between them. In conclusion, the concept vectors produced by the spherical k-means
IR
brass99computation
Computation of the Semantics of Autoepistemic Belief Theories Recently, one of the authors introduced a simple and yet powerful non-monotonic knowledge representation framework, called the Autoepistemic Logic of Beliefs, AEB. Theories in AEB are called autoepistemic belief theories. Every belief theory  T has been shown to have the least static expansion T which is computed by iterating a natural monotonic belief closure operator \Psi T starting from T . This way, the least static expansion T of any belief theory provides its natural non-monotonic semantics which is called the static semantics.  It is easy to see that if a belief theory T is finite then the construction of its least static expansion T stops after countably many iterations. However, a somewhat surprising result obtained in this paper shows that the least static expansion of any finite belief theory T is in fact obtained by means of a single iteration of the belief closure operator \Psi T (although this requires T to be of a special form, we also show that T can be always put in th...
DB
jamil99belief
Belief Reasoning in MLS Deductive Databases It is envisaged that the application of the multilevel security (MLS) scheme will enhance exibility and e ectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view de nitions on a per user basis. However, as advances in this area are being made and ideas crystallized, the concomitantweaknesses of the MLS databases are also surfacing. We insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported. Critics also argue that it is imperative for MLS database users to theorize about the belief of others, perhaps at di erent security levels, an apparatus that is currently missing and the absence of which is seriously felt. The impetus for our current research is this need to provide an adequate framework for belief reasoning in MLS databases. We demonstrate that a prudent application of the concept of inheritance in a deductive database setting will help capture the notion of declarative belief and belief reasoning in MLS databases in an elegantway. To this end, we develop a function to compute belief in multiple modes which can be used to reason about the beliefs of other users. We strive to develop a poised and practical logical characterization of MLS databases for the rst time based on the inherently di cult concept of non-monotonic inheritance. We present an extension of the acclaimed Datalog language, called the MultiLog, and show that Datalog is a special case of our language. We also suggest an implementation scheme for MultiLog as a front-end for CORAL. Key Words: MLS databases, belief assertion, reasoning,
DB
130387
Distributed Scheduling to Support a Call Centre: a Co-operative Multi-Agent Approach This paper introduces a multi-agent system architecture to increase the value of 24 hour a day call centre service. This system supports call centres in making appointments with clients on the basis of knowledge of employees and their schedules. Relevant activities of employees are scheduled for employees in preparation of such appointments. The multi-agent system architecture is based on principled design, using the compositional development method for multi-agent systems DESIRE. To schedule procedures in which more than one employee is involved, each employee is represented by its own personal assistant agent, and a work manager agent co-ordinates the schedules of the personal assistant agents, and clients through the call centre. The multi-agent system architecture has been applied to the banking domain, in co-operation with and partially funded by the Rabobank. 1 Introduction  Over the past few years, more and more companies and organisations have become aware of the potential of a...
Agents
liu98face
Face Recognition Using Evolutionary Pursuit . This paper describes a novel and adaptive dictionary method for face recognition using genetic algorithms (GAs) in determining the optimal basis for encoding human faces. In analogy to pursuit methods, our novel method is called Evolutionary Pursuit (EP), and it allows for different types of (non-orthogonal) bases. EP processes face images in a lower dimensional whitened PCA subspace. Directed but random rotations of the basis vectors in this subspace are searched by GAs where evolution is driven by a fitness function defined in terms of performance accuracy and class separation (scatter index). Accuracy indicates the extent to which learning has been successful so far, while the scatter index gives an indication of the expected fitness on future trials. As a result, our approach improves the face recognition performance compared to PCA, and shows better generalization abilities than the Fisher Linear Discriminant (FLD) based methods. 1 Introduction  A successful face recognition met...
ML
abiteboul98logical
A Logical View of Structured Files .<F3.733e+05> Structured data stored in files can benefit from standard database technology. In particular, we show here how such data can be queried and updated using declarative database languages. We introduce the notion of<F3.967e+05> structuring <F3.733e+05> schema, which consists of a grammar annotated with database programs. Based on a structuring schema, a file can be viewed as a database structure, queried and updated as such. For<F3.967e+05><F3.733e+05> queries, we show that almost standard database optimization techniques can be used to answer queries without having to construct the entire database. For<F3.967e+05><F3.733e+05> updates, we study in depth the propagation to the file of an update specified on the database view of this file. The problem is not feasible in general and we present a number of negative results. The positive results consist of techniques that allow to propagate updates efficiently under some reasonable<F3.967e+05> locality<F3.733e+05>  conditions on ...
IR
jantke95communications
Developing and Investigating Two-level Grammar Concepts For Design  
ML
6439
A Media-Independent Content Language for Integrated Text and Graphics Generation This paper describes a media-independent knowledge representation scheme, or content language, for describing the content of communicative goals and actions. The language is used within an intelligent system for automatically generating integrated text and information graphics presentations about complex, quantitative information. The language is designed to satisfy four requirements: to represent information about complex quantitative relations and aggregate properties; compositionality; to represent certain pragmatic distinctions needed for satisfying communicative goals; and to be usable as input by the media-specific generators in our system. 1 Introduction  This paper describes a media-independent knowledge representation scheme, or content language, for describing the content of communicative goals and actions. The language is used within an intelligent system for automatically generating integrated text and information graphics  1  presentations about complex, quantitative infor...
AI
reddy01coordinating
Coordinating Heterogeneous Work: Information and Representation in Medical Care Introduction  The concept of a common information space, or CIS, has become an influential way to think about the use of shared information in collaboration. Originating in the work of Schmidt and Bannon (1992), and further explored by Bannon and Bdker (1997), it was designed to extend then-current notions about the role of technology and shared information.  At the time this was originally proposed, a great deal of technical attention was being paid to the development of "shared workspace" systems (e.g. Lu and Mantei 1991; Ishii et al. 1992). These systems attempted to extend the workspaces of conventional single-user applications such as word processors and drawing tools, allowing synchronous or asynchronous collaboration across digital networks. Designing effective shared workspace systems presented a range of technical challenges concerning appropriate network protocols, synchronisation, concurrency control mechanisms, and user interface design. Still, over time con
HCI
alberto00pattern
Pattern Search Methods for Molecular Geometry Problems This paper deals with the application of pattern search methods to the numerical  solution of a class of molecular geometry problems with important applications in  molecular physics and chemistry. The goal is to nd a conguration of a cluster or a  molecule with minimum total energy.  The minimization problems in this class of geometry molecular problems have no  constraints and the objective function is smooth. The diculties arise from the existence  of several local minima, and especially, from the expensive function evaluation  (total energy) and the possible non-availability of rst-order derivatives.  We introduce a pattern search approach that attempts to exploit the physical nature  of the problem by using energy lowering geometrical transformations. Numerical  results with a particular instance of this new class of pattern search methods and with  the parallel direct search method of Dennis and Torczon are presented showing the  promise of our approach.  Key words. molecular ...
AI
jamil01case
A Case for Parameterized Views and Relational Unification In this paper, we address the issue of remedying the scepter of impedance mismatch in object-relational SQL. Our approach makes it possible to remain within the current connes of relational models, yet oers the capability of dening methods by SQL's declarative means, thereby preserving all opportunities of query optimization to the fullest extent. We propose the idea of parameterized views and an extension of SQL's create view construct with an optional with parameter clause. Parameterizing enables traditional SQL views to accept input values and delay the computation of the view until invoked with a call statement. This extension empowers users with the capability of modifying the behavior of predened procedures (views) by sending arguments and evaluating the procedure on demand.  Keywords: parameterized views, declarative methods, objectrelational databases, inheritance and overriding, reasoning, unication.  1 Introduction  An outstanding issue in object-relational databases dem...
DB
478775
Rank Aggregation Revisited The rank aggregation problem is to combine many different rank orderings on the same set of candidates,  or alternatives, in order to obtain a "better" ordering. Rank aggregation has been studied extensively  in the context of social choice theory, where several "voting paradoxes" have been discovered. The problem
IR
346630
Evolving User Profiles to Reduce Internet Information Overload . This paper discusses the use of Evolving Personal Agent Environments as a potential solution to the problem of information overload as experienced in habitual Web surfing. Some first experimental results on evolving user profiles using speciating hybrid GAs, the reasoning behind them and support for their potential application in mobile, wireless and location aware information devices are also presented.  1 Information Overload  In everyday life, the Internet user is faced with the ever increasing problem of information overload, whether this occurs at home, at the workplace, or as will soon be happening, everywhere [1] [2]. The overwhelming information feed that computer users face leads to anxiety, strain, inefficiency and finally results in uninformed (or misinformed) and frustrated users [3] [4]. Continuously and increasingly Internet users are confronted with laborious and difficult tasks of information filtering and/or gathering, which are inherently computer-oriented processes...
IR
joseph01why
Why Autonomy Makes the Agent This paper works on the premise that the position stated by Jennings et al. [17] is correct. Specifically that, amongst other things, the agent metaphor is a useful extension of the object-oriented metaphor. Object-oriented (OO) programming [29] is programming where data-abstraction is achieved by users defining their own data-structures (see figure 1), or "objects". These objects encapsulate data and methods for operating on that data; and the OO framework allows new objects to be created that inherit the properties (both data and methods) of existing objects. This allows archetypeal objects to be defined and then extended by different programmers, who needn't have complete understanding of exactly how the underlying objects are implemented
Agents
rao95formal
Formal Models and Decision Procedures for Multi-Agent Systems The study of computational agents capable of rational behaviour has received a great deal of attention in recent years. A number of theoretical formalizations for such multiagent systems have been proposed. However, most of these formalizations do not have a strong semantic basis nor a sound and complete axiomatization. Hence, it has not been clear as to how these formalizations could be used in building agents in practice. This paper explores a particular type of multi-agent system, in which each agent is viewed as having the three mental attitudes of belief (B), desire (D), and intention (I). It provides a family of multi-modal branching-time BDI logics with a semantics that is grounded in traditional decision theory and a possible-worlds framework, categorizes them, provides sound and complete axiomatizations, and gives constructive tableaubased decision procedures for testing the satisfiability and validity of formulas. The computational complexity of these decision procedures is n...
Agents
becker99gripsee
GripSee: A Gesture-controlled Robot for Object Perception and Manipulation We have designed a research platform for a perceptually guided robot, which also serves as a demonstrator for a coming generation of service robots. In order to operate semi-autonomously, these require a capacity for learning about their environment and tasks, and will have to interact directly with their human operators. Thus, they must be supplied with skills in the fields of human-computer interaction, vision, and manipulation. (GripSee is able to autonomously grasp and manipulate objects on a table in front of it. The choice of object, the grip to be used, and the desired final position are indicated by an operator using hand gestures. Grasping is performed similar to human behavior: the object is first fixated, then its form, size, orientation, and position are determined, a grip is planned, and finally the object is grasped, moved to a new position, and released. As a final example for useful autonomous behavior we show how the calibration of the robot's image-to-world coordinate transform can be learned from experience, thus making detailed and unstable calibration of this important subsystem superfluous. The integration concepts developed at our institute have led to a flexible library of robot skills that can be easily recombined for a variety of useful behaviors.
Agents
livingston01closing
Closing the Loop: Heuristics for Autonomous Discovery Autonomous discovery systems will be able to peruse very large databases more thoroughly than people can. In a companion paper [1], we describe a general framework for autonomous systems. We present and evaluate heuristics for use in this framework. Although these heuristics were designed for a prototype system, we believe they provide good initial solutions to problems encountered when implementing fully autonomous discovery systems. As such, these heuristics may be used as the starting point for future research into fully autonomous discovery systems. 1.
ML
443686
Steps towards C+C: a Language for Interactions . We present in this paper our reflections about the requirements of
AI
sche01active
Active Hidden Markov Models for Information Extraction Information extraction from HTML documents requires a classifier capable of assigning semantic labels to the words or word sequences to be extracted. If completely labeled documents are available for training, well-known Markov model techniques can be used to learn such classifiers. In this paper, we consider the more challenging task of learning hidden Markov models (HMMs) when only partially (sparsely) labeled documents are available for training. We first give detailed account of the task and its appropriate loss function, and show how it can be minimized given an HMM. We describe an EM style algorithm for learning HMMs from partially labeled data. We then present an active learning algorithm that selects "difficult" unlabeled tokens and asks the user to label them. We study empirically by how much active learning reduces the required data labeling effort, or increases the quality of the learned model achievable with a given amount of user effort.
IR
kinny96methodology
A Methodology and Modelling Technique for Systems of BDI Agents The construction of large-scale embedded software systems demands the use of design methodologies and modelling techniques that support abstraction, inheritance, modularity, and other mechanisms for reducing complexity and preventing error. If multi-agent systems are to become widely accepted as a basis for large-scale applications, adequate agentoriented methodologies and modelling techniques will be essential. This is not just to ensure that systems are reliable, maintainable, and conformant, but to allow their design, implementation, and maintenance to be carried out by software analysts and engineers rather than researchers. In this paper we describe an agent-oriented methodology and modelling technique for systems of agents based upon the Belief-Desire-Intention (BDI) paradigm. Our models extend existing Object-Oriented (OO) models. By building upon and adapting existing, well-understood techniques, we take advantage of their maturity to produce an approach that can be easily lear...
Agents
455961
Towards A Semantic Framework For Service Description The rapid development of the Internet and of distributed computing has led to a proliferation of online service providers such as digital libraries, web information sources, electronically requestable traditional services, and even software-to-software services such as those provided by persistence and event managers. This has created a need for catalogs of services, based on description languages covering both traditional and electronic services. This paper presents a classification and a domainindependent characterisation of services, which provide a foundation for their description to potential consumers. For each of the service characteristics that we consider, we identify the range of its possible values in di#erent settings, and when applicable, we point to alternative approaches for representing these values. The idea is that by merging # This work was funded by an Australian Research Council SPIRT Grant entitled "Selfdescribing transactions operating in a open, heterogeneous and distributed environment" involving QUT and GBST Holdings Pty Ltd.  1  2  these individual approaches, and by mapping them into a unified notation, it is possible to design service description languages suitable for advertisement and matchmaking within specific application settings.  1. 
IR
415731
Mixed Initiative Interfaces for Learning Tasks: SMARTedit Talks Back Applications of machine learning can be viewed as teacherstudent interactions in which the teacher provides training examples and the student learns a generalization of the training examples. One such application of great interest to the IUI community is adaptive user interfaces. In the traditional learning interface, the scope of teacher-student interactions consists solely of the teacher/user providing some number of training examples to the student/learner and testing the learned model on new examples. Active learning approaches go one step beyond the traditional interaction model and allow the student to propose new training examples that are then solved by the teacher. In this paper, we propose that interfaces for machine learning should even more closely resemble human teacher-student relationships. A teacher's time and attention are precious resources. An intelligent student must proactively contribute to the learning process, by reasoning about the quality of its knowledge, collaborating with the teacher, and suggesting new examples for her to solve. The paper describes a variety of rich interaction modes that enhance the learning process and presents a decision-theoretic framework, called DIAManD, for choosing the best interaction. We apply the framework to the SMARTedit programming by demonstration system and describe experimental validation and preliminary user feedback.
IR
morley96semantics
Semantics of BDI Agents and their Environment This paper describes an approach for reasoning about the interactions of multiple agents in moderately complex environments. The semantics of Belief Desire Intention (BDI) agents has been investigated by many researchers and the gap between theoretical specification and practical design is starting to be bridged. However, the research has concentrated on single-agent semantics rather than multiagent semantics and has not emphasised the semantics of the environment and its interaction with the agent. This paper describes a class of simple BDI agents and uses a recently introduced logic of actions to provide semantics for these agents independent of the environment in which they may be embedded. The same logic is used to describe the semantics of the environment itself and the interactions between the agent and the environment. As there is no restriction on the number of agents the environment may interact with, the approach can be used to address the semantics of multiple interacting ag...
Agents
bonner98logic
Logics for Databases and Information Systems Temporal Databases 34 3.2.2 Relational Database Histories 36 3.3 Temporal Queries 36 3.3.1 Abstract Temporal Query Languages 37 3.3.2 Expressive Power 41 3.3.3 Space-efficient Encoding of Temporal Databases 44 3.3.4 Concrete Temporal Query Languages 46 3.3.5 Evaluation of Abstract Query Languages using Compilation 47 3.3.6 SQL and Derived Temporal Query Languages 48 3.4 Temporal Integrity Constraints 53 3.4.1 Notions of constraint satisfaction 53 3.4.2 Temporal Integrity Maintenance 54 3.4.3 Temporal Constraint Checking 56 3.5 Multidimensional Time 58 3.5.1 Why Multiple Temporal Dimensions? 59 3.5.2 Abstract Query Languages for Multi-dimensional Time 59 3.5.3 Encoding of Multi-dimensional Temporal Databases 61 3.6 Beyond First-order Temporal Logic 62 3.7 Conclusion 65 References 65 4 The Role of Deontic Logic in the Specification of Information Systems 71  J.-J. Ch. Meyer, R.J. Wieringa, and F.P.M. Dignum  4.1 Introduction: Soft Constraints and Deontic Logic 72 4.1.1 Integrity Constrai...
DB
hero99estimation
Estimation of Rényi Information Divergence via Pruned Minimal Spanning Trees In this paper we develop robust estimators of the R enyi information divergence (I-divergence) given a reference distribution and a random sample from an unknown distribution. Estimation is performed by constructing a minimal spanning tree (MST) passing through the random sample points and applying a change of measure which flattens the reference distribution. In a mixture model where the reference distribution is contaminated by an unknown noise distribution one can use these results to reject noise samples by implementing a greedy algorithm for pruning the k-  longest branches of the MST, resulting in a tree called the k-MST. We illustrate this procedure in the context of density discrimination and robust clustering for a planar mixture model. 1. Introduction Let Xn = fx 1 ; x 2 ; : : : ; xn g denote a sample of i.i.d. data points in R  d  having unknown Lebesgue multivariate density f(x i ) supported on [0; 1]  d  . Define the order   Renyi entropy of f [7] H  (f) = 1 1   ln  Z  ...
ML
lueg98supporting
Supporting Situated Actions in High Volume Conversational Data Situations The global conferencing system Usenet news offers an amount of articles per day that exceeds human cognitive capabilities by far although the articles are already organized in hierarchically structured discussion groups covering distinct topics. We report here on a situated information filtering system that significantly reduces the burden by supporting the user in acting situated. Interpreting the user's actions as situated actions, the approach complements current filtering and recommender approaches by completely avoiding the modeling of user interests; the user is the only instance for assigning (un-)interestingness to Usenet discussions.  Keywords  Situated cognition, situated actions, Usenet news, information filtering  INTRODUCTION  The huge and increasing amount of information available in the information age suggests to investigate new ways to support humans in gathering information that might be interesting, helpful, or necessary for them. Since the overall amount of informat...
IR
hekanaho98dogma
DOGMA: A GA-Based Relational Learner We describe a GA-based concept learning/theory revision system DOGMA and discuss how it can be applied to relational learning. The search for better theories in DOGMA is guided by a novel fitness function that combines the minimal description length and information gain measures. To show the efficacy of the system we compare it to other learners in three relational domains. Keywords: Relational Learning, Genetic Algorithms, Minimal Description Length  1 Introduction  Genetic Algorithms (GAs) are stochastic general purpose search algorithms, that have been applied to a wide range of Machine Learning problems. They work by evolving a population of chromosomes, each of which encodes a potential solution to the problem at hand. The task of a GA is to find a highly fit chromosome through the application of different selection and perturbation operators. In this paper we consider the use of GAs in relational concept learning, i.e. in the process of learning and extracting relational classif...
AI
hourcade99architecture
Architecture and Implementation of a Java Package for Multiple Input Devices (MID) A major difficulty in writing Single Display Groupware (co-present collaborative) applications is getting input from multiple devices. We introduce MID, a Java package that addresses this problem and offers an architecture to access advanced events through Java. In this paper, we describe the features, architecture and limitations of MID. We also briefly describe an application that uses MID to get input from multiple mice: KidPad. Keywords Single Display Groupware (SDG), Computer-Supported Cooperative Work (CSCW), Multiple Input Devices (MID), Multi-Modal Input, Java, DirectInput, Windows 98, Universal Serial Bus (USB), KidPad, Jazz, Pad++. INTRODUCTION Communication, collaboration, and coordination are brought to many people's desktops thanks to groupware applications such as Lotus Notes and Microsoft Exchange, some of the leading commercial products in the field of Computer-Supported Cooperative Work (CSCW). They help people collaborate when they are not in the same place at th...
HCI
vasconcelos99probabilistic
Probabilistic Retrieval: New Insights and Experimental Results We present new insights on the relations between a recently introduced probabilistic formulation of the content-based retrieval problem and standard solutions. New experimental results are presented, providing evidence that probabilistic retrieval has superior performance. Finally, a unified representation for texture and color is introduced. 1 Introduction  The problem of retrieving images or video from a database is naturally formulated as a problem of pattern recognition. Given a representation (or feature) space F for the entries in the database, the design of a retrieval system consists of finding a map  g : F ! M = f1; : : : ; Kg  x ! y  from F to the set M of classes identified as useful for the retrieval operation. K, the cardinality of M , can be as large as the number of items in the database (in which case each item is a class by itself), or smaller. If the goal of the retrieval system is to minimize the probability of error, i.e. P (g(x) 6= y), it is well known that the opt...
ML
bradshaw97introduction
An Introduction to Software Agents ion and delegation: Agents can be made extensible and composable in ways that common iconic interface objects cannot. Because we can "communicate" with them, they can share our goals, rather than simply process our commands. They can show us how to do things and tell us what went wrong (Miller and Neches 1987). . Flexibility and opportunism: Because they can be instructed at the level of 16 BRADSHAW goals and strategies, agents can find ways to "work around" unforeseen problems and exploit new opportunities as they help solve problems. . Task orientation: Agents can be designed to take the context of the person's tasks and situation into account as they present information and take action. . Adaptivity: Agents can use learning algorithms to continually improve their behavior by noticing recurrent patterns of actions and events. Toward Agent-Enabled System Architectures In the future, assistant agents at the user interface and resource-managing agents behind the scenes will increas...
HCI
parthasarathy01tackling
Tackling Multimodal Problems in Hybrid Genetic Algorithms A method is proposed to address the issue of multimodality while using hybrid genetic algorithms (GAs). The hybrid GA framework that is used is one in which a local searcher is employed during...
ML
486097
Rewriting Logic: Roadmap and Bibliography Machine [218]; (7) CCS and LOTOS [230,208,314,45,89,311,309,201]; (8) the  calculus [316,292]; (9) concurrent objects and actors [218,220,300,302,304]; (10) the UNITY language [218]; (11) concurrent graph rewriting [223]; (12) dataflow [223]; (13) neural networks [223]; (14) real-time systems, including timed automata, timed transition systems, hybrid automata, and timed Petri nets [268,262]; and (15) the tile logic [146,147,135] model of synchronized concurrent computation [232,39,34,148].
Agents
383856
Declarative Semantics Of Belief Queries In MLS Deductive Databases A logic based language, called MultiLog, for multi level secure relational databases has recently been proposed. It has been shown that MultiLog is capable of capturing the notion of user belief, of ltering unwanted and \useless" information in its proof theory. Additionally, it can guard against a previously unknown security breach { the so called surprise stories. In this paper, we outline a possible approach to a declarative characterization of belief queries in MultiLog in a very informal manner. We show that for \simple programs" with belief queries, the semantics is rather straight forward. Semantics for the general Horn programs may be developed based on the understanding of the model theoretic characterization of belief queries developed in this paper.  Keywords: Multi level security, belief queries, declarative semantics, completeness.  Introduction  In a recent research, Jukic and Vrbsky [8] demonstrate that users in the relational MLS model potentially have a cluttered view...
DB
uhrmacher00plug
"Plug And Test" - Software Agents In Virtual Environments James - A Java Based agent modeling environment for simulation has been developed to support the compositional construction of test beds for multi-agent systems and their execution in distributed environments. The modeling formalism of James imposes only few constraints on the modeling of agents and facilitates a \plug and test" with pieces of agent code which has been demonstrated in earlier work. However, even entire agents can be run in James as they are run in their run-time environment. The integration of agents as a whole is based on model templates which serve as the agents' interface and representative during the simulation run. The eort which is put into dening model templates for selected agent systems obviates the need for the single agent programmer to get acquainted with the underlying modeling and simulation formalism. Instead the agent programmer can compose the experimental frame and test the programmed agents as they are. The approach is illustrated with agents of the mobile agent system Mole.  1 
Agents
horling01using
Using Self-Diagnosis to Adapt Organizational Structures The specific organization used by a multi-agent system is crucial for its effectiveness and efficiency. In dynamic environments, or when the objectives of the system shift, the organization must therefore be able to change as well. In this abstract we propose using a general diagnosis engine to drive this process of adaptation, using the TMS modeling language as the primary representation of organizational information. A complete version of this paper is at [1].  As the sizes of multi-agent systems grow in the number of their participants, the organization of those agents will be increasingly important. In such an environment, an organization is used to limit the range of control decisions agents must make, which is a necessary component of scalable systems. Are agent agents arranged in clusters, a hierarchy, a graph, or some other type of organization? Are the agents` activities or behaviors driven solely by local concerns, or do external peers or managers have direct influence as wel...
Agents
kumar00adaptive
The Adaptive Agent Architecture: Achieving Fault-Tolerance Using Persistent Broker Teams Brokers are used in many multi-agent systems for locating agents, for routing and sharing  information, for managing the system, and for legal purposes, as independent third parties.  However, these multi-agent systems can be incapacitated and rendered non-functional when the  brokers become inaccessible due to failures such as machine crashes, network breakdowns, and  process failures that can occur in any distributed software system.  We propose that the theory of teamwork can be used to create robust brokered architectures that  can recover from broker failures, and we present the Adaptive Agent Architecture (AAA) to show  the feasibility of this approach. The AAA brokers form a team with a joint commitment to serve  any agent that registers with the broker team as long as the agent remains registered with the  team. This commitment enables the brokers to substitute for each other when needed. A multiagent  system based on the AAA can continue to work despite broker failures as long...
Agents
55643
Path Constraints on Deterministic Graphs We study path constraints for deterministic graph model [9], a variation of semistructured data model in which data is represented as a rooted edge-labeled directed graph with deterministic edge relations. The path constraint languages considered include the class of word constraints introduced in [4], the language P c investigated in [8], and an extension of P c defined in terms of regular expressions. Complexity results on the implication and finite implication problems for these constraint languages are established. 1 Introduction  Semistructured data is characterized as having no type constraints, irregular structure and rapidly evolving or missing schema [1, 6]. Examples of such data can be found on the WorldWide -Web, in biological databases and after data integration. In particular, documents of XML (eXtensible Markup Language [5]) can also be viewed as semistructured data [10]. The unifying idea in modeling semistructured data is the representation of data as an edge-labeled, r...
DB
112493
Foundations of Spatioterminological Reasoning with Description Logics This paper presents a method for reasoning about spatial objects and their qualitative spatial relationships. In contrast to existing work, which mainly focusses on reasoning about qualitative spatial relations alone, we integrate quantitative and qualitative information with terminological reasoning. For spatioterminological reasoning we present the description logic  ALCRP(D)  and define an appropriate concrete domain  D  for polygons. The theory is motivated as a basis for knowledge representation and query processing in the domain of deductive geographic information systems. 1 Introduction  Qualitative relations play an important role in formal reasoning systems that can be part of, for instance, geographic information systems (GIS). In this context, inferences about spatial relations should not be considered in isolation but should be integrated with formal inferences about structural descriptions of domain objects (e.g. automatic consistency checking and classification) and infer...
AI
casillas00improving
Improving the Wang and Mendel's Fuzzy Rule Learning Method by Inducing Cooperation Among Rules Nowadays, Linguistic Modeling (LM) is considered  to be one of the most important areas  of application for Fuzzy Logic. It is accomplished  by descriptive Fuzzy Rule-Based  Systems (FRBSs), whose most interesting  feature is the interpolative reasoning they  develop. This characteristic plays a key role  in the high performance of FRBSs and is a  consequence of the cooperation among the  fuzzy rules involved in the FRBS.  A large quantity of automatic techniques has  been proposed to generate these fuzzy rules  from numerical data. One of the most interesting  families of techniques, due to its  simplicity and quickness, is the ad hoc datadriven  methods. However, its main drawback  is the cooperation among the rules  which is not suitably considered.  With the aim of facing up this drawback,  which makes the obtained models not to  be as accurate as desired, a new approach  to improve the performance obtaining more  cooperative rules is introduced in this paper.  Following this appro...
ML
492133
Using Augmented Reality to Visualise Architecture Designs in an Outdoor Environment This paper presents the use of a wearable computer system to visualise outdoor architectural features using augmented reality. The paper examines the question -  How does one visualise a design for a building, modification to a building, or extension to an existing building relative to its physical surroundings?  The solution presented to this problem is to use a mobile augmented reality platform to visualise the design in spatial context of its final physical surroundings. The paper describes the mobile augmented reality platform TINMITH2 used in the investigation. The operation of the system is described through a detailed example of the system in operation. The system was used to visualise a simple extension to a building on one of the University of South Australia campuses.
HCI
rao95bdi
BDI Agents: from Theory to Practice The study of computational agents capable of rational behaviour has received a great deal of attention in recent years. Theoretical formalizations of such agents and their implementations have proceeded in parallel with little or no connection between them. This paper explores a particular type of rational agent, a BeliefDesire -Intention (BDI) agent. The primary aim of this paper is to integrate (a) the theoretical foundations of BDI agents from both a quantitative decision-theoretic perspective and a symbolic reasoning perspective; (b) the implementations of BDI agents from an ideal theoretical perspective and a more practical perspective; and (c) the building of large-scale applications based on BDI agents. In particular, an air-traffic management application will be described from both a theoretical and an implementation perspective.  Introduction  The design of systems that are required to perform high-level management and control tasks in complex dynamic environments is becoming ...
Agents
schmalstieg00bridging
Bridging Multiple User Interface Dimensions with Augmented Reality Studierstube is an experimental user interface system, which uses collaborative augmented reality to incorporate true 3D interaction into a productivity environment. This concept is extended to bridge multiple user interface dimensions by including multiple users, multiple host platforms, multiple display types, multiple concurrent applications, and a multi-context (i. e., 3D document) interface into a heterogeneous distributed environment. With this architecture, we can explore the user interface design space between pure augmented reality and the popular ubiquitous computing paradigm. We report on our design philosophy centered around the notion of contexts and locales, as well as the underlying software and hardware architecture. Contexts encapsulate a live application together with 3D (visual) and other data, while locales are used to organize geometric reference systems. By separating geometric relationships (locales) from semantic relationships (contexts), we achieve a great amou...
HCI
kang00visualization
Visualization Methods for Personal Photo Collections: Browsing and Searching in the PhotoFinder Software tools for personal photo collection management are proliferating, but they usually have limited searching and browsing functions. We implemented the PhotoFinder prototype to enable non-technical users of personal photo collections to search and browse easily. PhotoFinder provides a set of visual Boolean query interfaces, coupled with dynamic query and query preview features. It gives users powerful search capabilities. Using a scatter plot thumbnail display and dragand -drop interface, PhotoFinder is designed to be easy to use for searching and browsing photos.  Keywords : PhotoFinder, user interface, dynamic query, query preview, search, browsing, Boolean query, digital photo library.  1. INTRODUCTION  Digital cameras, scanners and personal computers are now common. But as collections grow in size, the need to organize, search, and browse digital photos increases [1]. There are many personal photo collection management tools available either commercially or non-commercially. ...
HCI
meuss98dag
DAG Matching Techniques for Information Retrieval on Structured Documents With the establishment of international standards for document representation like SGML, ODA, or XML, attention in Information Retrieval has shifted to representation models and query languages that make active use both of the logical structure and the contents of the documents in a document database. At the same time, representation of structure has become more and more important in other types of databases as well. Among several related approaches, Kilpelainen's Tree Matching is one of the most expressive and intuitive formalisms for querying databases with treestructured entities. However, in its original formulation it leaves aside most of the problems that arise in real-life applications of Information Retrieval. In this paper we extend Tree Matching to DAG Matching and suggest various techniques that should be useful when using the formalism in a practical IR system. In particular we suggest a representation of answers that can cope with the potentially huge number of entities in...
IR
280639
TIP: A Temporal Extension to Informix Commercial relational database systems today provide only limited temporal support. To address the  needs of applications requiring rich temporal data and queries, we have built TIP (Temporal Information  Processor), a temporal extension to the Informix database system based on its DataBlade technology.  Our TIP DataBlade extends Informix with a rich set of datatypes and routines that facilitate temporal  modeling and querying. TIP provides both C and Java libraries for client applications to access a TIPenabled  database, and provides end-users with a GUI interface for querying and browsing temporal data.  1 Introduction  Our research in temporal data warehouses [9, 10] has led us to require a relational database system with full SQL as well as rich temporal support, in order to experiment with our temporal view-maintenance techniques. Most commercial relational database systems support only a DATE type (or its variants). An attribute of type DATE can be used to timestamp a tuple with...
DB
527640
Cross-Entropy Guided Ant-like Agents Finding Cyclic Paths in Scarcely Meshed Networks Telecommunication network owners and operators have for half a century been well aware of the potential loss of revenue if a major trunk is damaged, thus dependability at high cost has been implemented. A simple, effective and common dependability scheme is 1:1 protection with 100% capacity redundancy in the network. A growing number of applications in need of dependable connections with specific requirements to bandwidth and delay have started using the internet (which only provides best effort transport) as their base communication service. In this paper we adopt the 1:1 protection scheme and incorporate it as part of a routing system applicable for internet infrastructures. 100% capacity redundancy is no longer required. A distributed stochastic path finding (routing) algorithm based on Rubinstein's Cross Entropy method for combinatorial optimisation is presented. Early results from Monte Carlo simulations indeed indicate that the algorithm is capable of finding pairs of independent primary and backup paths satisfying specific bandwidth a constraints.
Agents
78138
A Formal Approach to Detecting Security Flaws in Object-Oriented Databases this paper is to show an efficient decision algorithm for detecting a security flaw under a given authorization. This problem is solvable in polynomial time in practical cases by reducing it to the congruence closure problem. This paper also mentions the problem of finding a maximal subset of a given authorization under which no security flaw exists.
DB
6411
Building Domain-Specific Search Engines with Machine Learning Techniques Domain-specific search engines are becoming increasingly popular because they offer increased accuracy and extra features not possible with the general, Web-wide search engines. For example, www.campsearch.com allows complex queries by agegroup, size, location and cost over summer camps. Unfortunately, these domain-specific search engines are difficult and time consuming to maintain. This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. We describe new research in reinforcement learning, text classification and information extraction that automates efficient spidering, populating topic hierarchies, and identifying informative text segments. Using these techniques, we have built a demonstration system: a search engine for computer science research papers. It already contains over 33,000 papers and is publicly available at  www.cora.jprc.com. 1 Introduction  As the amount of information on the World ...
ML
najork01breadthfirst
Breadth-First Search Crawling Yields High-Quality Pages This paper examines the average page quality over time of pages downloaded during a web crawl of 328 million unique pages. We use the connectivity-based metric PageRank to measure the quality of a page. We show that traversing the web graph in breadth-first search order is a good crawling strategy, as it tends to discover high-quality pages early on in the crawl.
IR
471293
BUILD-IT: a computer vision-based interaction technique of a planning tool for construction and design It is time to go beyond the established approaches in human-computer interaction. With the  Augmented Reality (AR) design strategy humans are able to behave as much as possible in a natural way: behavior  of humans in the real world with other humans and/or real world objects. Following the fundamental  constraints of natural way of interacting we derive a set of recommendations for the next generation of user  interfaces: the Natural User Interface (NUI). The concept of NUI is presented in form of a runnable demonstrator:  a computer vision-based interaction technique for a planning tool for construction and design tasks.  KEYWORDS augmented reality, natural user interface, computer vision-based interaction  1. 
HCI
amin99agentoriented
Agent-Oriented Programming in Linear Logic This thesis investigates how a linear logic programming language, such as Lygon, can be  used in the implementation of agent-oriented programs. Agent-oriented programming is a recent  computational framework of interest to both academic and industrial researchers. Agent  methodology is being successfully utilised in designing complex (distributed) applications  that require concurrency, reasoning, communication, sharing and integration of knowledge,  and, of course, intelligence. On the other hand, linear logic, a logic of resource-consumption,  provides the possibility to construct efficient tools for modelling updates, reasoning about the  environment and implementing concurrency. Linear logic has been used as a basis for creating  a number of programming languages. One of these is the logic programming language  Lygon. The aim of this thesis is to investigate the possibility of implementing agents with  Lygon. A number of experiments have been carried out and results analysed, which...
Agents
527452
Generating Code for Agent UML Sequence Diagrams For several years, a new category of description techniques  exists: Agent UML [10] which is based on UML. Agent UML is an extension  of UML to tackle dierences between agents and objects. Since  this description technique is rather new, it does not supply tools or algorithms  for protocol synthesis. Protocol synthesis corresponds to generate  code for a formal description of a protocol. The derived program behaves  like the formal description. This work presents rst elements to help designers  generating code for Agent UML sequence diagrams. The protocol  synthesis is applied to the example of English Auction protocol.
Agents
464421
Real-Time Input of 3D Pose and Gestures of a User's Hand and Its Applications for HCI In this paper, we introduce a method for tracking a user's hand in 3D and recognizing the hand's gesture in real-time without the use of any invasive devices attached to the hand. Our method uses multiple cameras for determining the position and orientation of a user's hand moving freely in a 3D space. In addition, the method identifies predetermined gestures in a fast and robust manner by using a neural network which has been properly trained beforehand. This paper also describes results of user study of our proposed method and its application for several types of applications, including 3D object handling for a desktop system and 3D walk-through for a large immersive display system.  1. 
HCI
dekhtyar99probabilistic
Probabilistic Temporal Databases, I: Algebra Dyreson and Snodgrass have drawn attention to the fact that in many temporal database applications,  there is often uncertainty present about the start time of events, the end time of events, the duration of  events, etc. When the granularity of time is small (e.g. milliseconds), a statement such as "Packet p  was shipped sometime during the first 5 days of January, 1998" leads to a massive amount of uncertainty  (5 \Theta 24 \Theta 60 \Theta 60 \Theta 1000)possibilities. As noted in [53], past attempts to deal with uncertainty in databases  have been restricted to relatively small amounts of uncertainty in attributes. Dyreson and Snodgrass have  taken an important first step towards solving this problem.  In this paper, we first introduce the syntax of Temporal-Probabilistic (TP) relations and then show  how they can be converted to an explicit, significantly more space-consuming form called Annotated  Relations. We then present a Theoretical Annotated Temporal Algebra (TATA). Being e...
DB
haddaway99overview
An Overview of Some Recent Developments in Bayesian Problem Solving Techniques The last five years have seen a surge in interest in the use of techniques from Bayesian decision theory to address problems in AI. Decision theory provides a normative framework for representing and reasoning about decision problems under uncertainty. Within the context of this framework, researchers in uncertainty in the AI community have been developing computational techniques for building rational agents and representations suited to engineering their knowledge bases. This special issue reviews recent research in Bayesian problem-solving techniques. The articles cover the topics of inference in Bayesian networks, decision-theoretic planning, and qualitative decision theory. Here, I provide a brief introduction to Bayesian networks and then cover applications of Bayesian problem-solving techniques, knowledge-based model construction and structured representations, and the learning of graphical probability models. The past five years or so have seen increased interest and tremendous...
ML
ando03mostlyunsupervised
Mostly-Unsupervised Statistical Segmentation of Japanese Kanji Sequences Given the lack of word delimiters in written Japanese, word segmentation is generally considered a crucial first step in processing Japanese texts. Typical Japanese segmentation algorithms rely either on a lexicon and syntactic analysis or on pre-segmented data; but these are labor-intensive, and the lexico-syntactic techniques are vulnerable to the unknown word problem. In contrast, we introduce a novel, more robust statistical method utilizing unsegmented training data. Despite its simplicity, the algorithm yields performance on long kanji sequences comparable to and sometimes surpassing that of state-of-the-art morphological analyzers over a variety of error metrics. The algorithm also outperforms another mostly-unsupervised statistical algorithm previously proposed for Chinese. Additionally, we present a two-level annotation scheme for Japanese to incorporate multiple segmentation granularities, and introduce two novel evaluation metrics, both based on the notion of a compatible bracket, that can account for multiple granularities simultaneously.
IR
greenwald01autonomous
Autonomous Bidding Agents in the Trading Agent Competition Designing agents that can bid in online simultaneous auctions is a complex task.The authors describe task-specific details and strategies of agents in a trading agent competition. Anatural offshoot of the growing prevalence of online auctions is the creation of autonomous bidding agents that monitor and participate in these auctions. It is straightforward to write a bidding agent to participate in an online auction for a single good, particularly when the value of that good is fixed ahead of time: the agent can bid slightly over the ask price until the auction closes or the price exceeds the value. In simultaneous auctions offering complementary and substitutable goods, however, agent deployment is a much more complex endeavor. The first trading agent competition (TAC), held in Boston, Massachusetts, on 8 July 2000, challenged participants to design a trading agent capable of bidding in online simultaneous auctions for complimentary and substitutable goods. TAC was organized by a group of researchers and developers led by Michael Wellman of
Agents
naumann01from
From Databases to Information Systems - Information Quality Makes the Difference Research and business is currently moving from centralized databases towards in-formation systems integrating distributed and autonomous data sources. Simultane-ously, it is a well acknowledged fact that consideration of information quality - IQ-reasoning - is an important issue for large-scale integrated information systems. We show that IQ-reasoning can be the driving force of the current shift from databases to integrated information systems. In this paper, we explore the implications and consequences of this shift. All areas of answering user queries are affected - from user input, to query planning and query optimization, and finally to building the query result. The application of IQ-reasoning brings both challenges, such as new cost models for optimization, and opportunities, such as improved query planning. We highlight several emerging aspects and suggest solutions toward a pervasion of information quality in information systems.
DB
sampaio00design
Design and Implementation of a Deductive Query Language for ODMG Compliant Object Databases Introduction  Deductive object-oriented databases (DOODs) seek to provide the combined support for the expressive modelling features available in the object-oriented data model and the powerful query language features available in deductive databases. When successfully engineered, this combination can broaden the spectrum of declarative queries that can be supported by the DBMS, and ease their implementation due to the increased functionality in the query capabilities of the resulting system. The extra leverage obtained from support for deductive functionality is relevant to building database middleware for distributed information systems [19], managing semistructured data [11], and for building decision support and knowledge discovery systems [4].  Unlike deductive relational database systems (DRDBs), which were designed and implemented based on the formal denition of the relational data model by Codd, and on the widely researched deductive query language model (language cons
DB
wang99ssuptree
The S&sup2;-Tree: An Index Structure for Subsequence Matching of Spatial Objects We present the S²-Tree, an indexing method for subsequence matching of spatial objects. The S²-Tree locates subsequences within a collection of spatial sequences, i.e., sequences made up of spatial objects, such that the subsequences match a given query pattern within a specified tolerance. Our method is based on (i) the string-searching techniques that locate substrings within a string of symbols drawn from a discrete alphabet (e.g., ASCII characters) and (ii) the spatial access methods that index (unsequenced) spatial objects. Particularly, the S²-Tree can be applied to solve problems such as subsequence matching of time-series data, where features of subsequences are often extracted and mapped into spatial objects. Moreover, it supports queries such as "what is the longest common pattern of the two time series?", which previous subsequence matching algorithms find difficult to solve efficiently.
DB
crabbe01goal
Goal Directed Adaptive Behavior in Second-Order Neural Networks: The MAXSON family of architectures The paper presents a neural network architecture (MAXSON) based on second-order connections that can learn a multiple goal approach/avoid task using reinforcement from the environment. It also enables an agent to learn vicariously, from the successes and failures of other agents. The paper shows that MAXSON can learn certain spatial navigation tasks much faster than traditional Q-learning, as well as learn goal directed behavior, increasing the agent's chances of long-term survival. The paper shows that an extension of MAXSON (V-MAXSON) enables agents to learn vicariously, and this improves the overall survivability of the agent population.
ML
20706
View Disassembly . We explore a new form of view rewrite called view disassembly. The objective is to rewrite views in order to "remove" certain sub-views (or unfoldings) of the view. This becomes pertinent for complex views which may defined over other views and which may involve union. Such complex views arise necessarily in environments as data warehousing and mediation over heterogeneous databases. View disassembly can be used for view and query optimization, preserving data security, making use of cached queries and materialized views, and view maintenance. We provide computational complexity results of view disassembly. We show that the optimal rewrites for disassembled views is at least NP-  hard. However, we provide good news too. We provide an approximation algorithm that has much better run-time behavior. We show a pertinent class of unfoldings for which their removal always results in a simpler disassembled view than the view itself. We also show the complexity to determine when a collection...
DB
488525
CS 395T Large-Scale Data Mining Fall 2001 ojects the vectors x 1 : : : x n onto the principal components. Note that the eigenvectors of ^   are the left singular vectors of the matrix 1  p  n 1 [x 1 ; x 2 ; : : : ; x n ]: Thus PCA can be obtained from the SVD of mean centered data. The mean centering is the important dierence between PCA and SVD and can yield qualitatively dierent results for data sets where the mean is not equal to 0, as shown in gure 1. SVD PCA SVD,PCA Figure 1: The leading singular vector may not be in the same direction as the principal component 1  2 CS 395T: Large-Scale Data Mining 2 Clustering Considerations  Clustering is the grouping together of similar objects. Usually, the clustering problem is posed as a
IR
sarawagi98discoverydriven
Discovery-driven Exploration of OLAP Data Cubes .  Analysts predominantly use OLAP data cubes to identify regions of anomalies that may represent problem areas or new opportunities. The current OLAP systems support hypothesis-driven exploration of data cubes through operations such as drill-down, roll-up, and selection. Using these operations, an analyst navigates unaided through a huge search space looking at large number of values to spot exceptions. We propose a new discovery-driven exploration paradigm that mines the data for such exceptions and summarizes the exceptions at appropriate levels in advance. It then uses these exceptions to lead the analyst to interesting regions of the cube during navigation. We present the statistical foundation underlying our approach. We then discuss the computational issue of finding exceptions in data and making the process efficient on large multidimensional data bases.  1 Introduction  On-Line Analytical Processing (OLAP) characterizes the operations of summarizing, consolidating, viewing, a...
DB
440714
Layered Learning in Genetic Programming for a Cooperative Robot Soccer Problem We present an alternative to standard genetic programming (GP) that applies layered learning techniques to decompose a problem. GP is applied to subproblems sequentially, where the population in the last generation of a subproblem is used as the initial generation of the next subproblem. This method is used to evolve agents to play keep-away soccer, a subproblem of robotic soccer that requires cooperation among multiple agents in a dynamic environment. The layered learning paradigm allows GP to evolve better solutions faster than standard GP. Results show that the layered learning GP outperforms standard GP by evolving a lower fitness faster and an overall better fitness. Results indicate a wide area of future research with layered learning in GP.
AI
1639
Support Vector Machine - Reference Manual this document will describe these programs. To find out more about SVMs, see the bibliography. We will not describe how SVMs work here. The first program we will describe is the paragen program, as it specifies all parameters needed for the SVM. 3 paragen When using the support vector machine for any given task, it is always necessary to specify a set of parameters. These parameters include information such as whether you are interested in pattern recognition or regression estimation, what kernel you are using, what scaling is to be done on the data, etc... paragen generates parameter files used by the SVM program, if no file was generated the user will be asked interactively.
ML
pitoura01locating
Locating Objects in Mobile Computing In current distributed systems, the notion of mobility is emerging in many forms and applications.  Mobility arises naturally in wireless computing, since the location of users changes as  they move. Besides mobility in wireless computing, software mobile agents are another popular  form of moving objects. Locating objects, i.e., identifying their current location, is central to  mobile computing. In this paper, we present a comprehensive survey of the various approaches  to the problem of storing, querying, and updating the location of objects in mobile computing.  The fundamental techniques underlying the proposed approaches are identified, analyzed and  classified along various dimensions.  Keywords: mobile computing, location management, location databases, caching, replication,  moving objects, spatio-temporal databases  1 Introduction  In current distributed systems, the notion of mobility is emerging in many forms and applications. Increasingly many users are not tied to a fixed...
Agents
taniguchi01mining
Mining Semi-Structured Data by Path Expressions A new data model for ltering semi-structured texts is presented.
IR
1894
Results and Challenges in Web Search Evaluation A frozen 18.5 million page snapshot of part of the Web has been created to enable and encourage meaningful and reproducible evaluation of Web search systems and techniques. This collection is being used in an evaluation framework within the Text Retrieval Conference (TREC) and will hopefully provide convincing answers to questions such as, "Can link information result in better rankings?", "Do longer queries result in better answers?", and, "Do TREC systems work well on Web data?" The snapshot and associated evaluation methods are described and an invitation is extended to participate. Preliminary results are presented for an effectivess comparison of six TREC systems working on the snapshot collection against five well-known Web search systems working over the current Web. These suggest that the standard of document rankings produced by public Web search engines is by no means state-of-the-art.  1999 Published by Elsevier Science B.V. All rights reserved.  Keywords: Evaluation; Search...
IR
210603
Machine Learning based User Modeling for WWW Search The World Wide Web (Www) offers a huge number of documents which deal with information concerning nearly any topic. Thus, search engines and meta search engines currently are the key to finding information. Search engines with crawler based indexes vary in recall and offer a very bad precision. Meta search engines try to overcome these lacks by simple methods for information extraction, information filtering and integration of heterogenous information resources. Only few search engines employ intelligent techniques in order to increase precision. Recently, user modeling techniques have become more and more popular since they proved to be a useful means for user centered information filtering and presentation. Many personal agent based system for web browsing are currently developed. It is a straightforward idea to incorporate the idea of machine learning based user modeling (see [17]) methods into web search services. We propose an abstract prototype which is being developed at the Uni...
IR
510172
Selecting a Fuzzy Logic Operation from the DNF-CNF Interval: How Practical Are the Resulting Operations? In classical (two-valued) logic, CNF and DNF  forms of each propositional formula are equivalent  to each other. In fuzzy logic, CNF and  DNF forms are not equivalent, they form an interval  that contains the fuzzy values of all classically  equivalent propositional formulas. If we  want to select a single value from this interval,  then it is natural to select a linear combination  of the interval's endpoints. In particular, we  can do that for CNF and DNF forms of "and"  and "or", thus designing natural fuzzy analogues  of classical "and" and "or" operations.
ML
520593
Extended Experimental Explorations Of The Necessity Of User Guidance In Case-Based Learning This is an extended report focussing on experimental results to explore the necessity of user guidance in case-based knowledge acquisition. It is covering a collection of theoretical investigations as well. The methodology of our approach is quite simple: We choose a well-understood area which is tailored to case-based knowledge acquisition. Furthermore, we choose a prototypical case-based learning algorithm which is obviously suitable for the problem domain under consideration. Then, we perform a number of knowledge acquisition experiments. They clearly exhibit essential limitations of knowledge acquisition from randomly chosen cases. As a consequence, we develop scenarios of user guidance. Based on these theoretical concepts, we prove a few theoretical results characterizing the power of our approach. Next, we perform a new series of more constrained results which support our theoretical investigations. The main experiments deal with the difficulties of learning from randomly arrange...
ML
simula99selforganizing
The Self-Organizing Map in Industry Analysis The Self-Organizing Map (SOM) is a powerful neural network method for the analysis and visualization of high-dimensional data. It maps nonlinear statistical relationships between high-dimensional measurement data into simple geometric relationships, usually on a two-dimensional grid. The mapping roughly preserves the most important topological and metric relationships of the original data elements and, thus, inherently clusters the data. The need for visualization and clustering occurs, for instance, in the data analysis of complex processes or systems. In various engineering applications, entire fields of industry can be investigated using SOM based methods. The data exploration tool presented in this chapter allows visualization and analysis of large data bases of industrial systems. Forest industry is the ørst chosen application for the tool. To illustrate the global nature of forest industry, the example case is used to cluster the pulp and paper mills of the world.
ML
satoh01network
Network Processing of Mobile Agents, by Mobile Agents, for Mobile Agents . This paper presents a framework for building network protocols for  migrating mobile agents over a network. The framework allows network protocols  for agent migration to be naturally implemented within mobile agents and  to be constructed in a hierarchy as most data transmission protocols are. These  protocols are given as mobile agents and they can transmit other mobile agents  to remote hosts as first-class objects. Since they can be dynamically deployed at  remote hosts by migrating the agents that carry them, these protocols can dynamically  and flexibly customize network processing for agent migration according  to the requirements of respective visiting agents and changes in the environments.  A prototype implementation was built on a Java-based mobile agent system, and  several practical protocols for agent migration were designed and implemented.  The framework can make major contributions to mobile agent technology for  telecommunication systems.  1 
Agents
decker98coordinating
Coordinating Human and Computer Agents . In many application areas individuals are responsible for an agenda of tasks and face choices about the best way to locally handle each task, in what order to do tasks, and when to do them. Such decisions are often hard to make because of coordination problems: individual tasks are related to the tasks of others in complex ways, and there are many sources of uncertainty (no one has a complete view of the task structure at arbitrary levels of detail, the situation may be changing dynamically, and no one is entirely sure of the outcomes of all of their actions). The focus of this paper is the development of support tools for distributed, cooperative work by groups (collaborative teams) of human and computational agents. We will discuss the design of a set of distributed autonomous computer programs ("agents") that assist people in coordinating their activities by helping them to manage their agendas. We describe several ongoing implementations of these ideas including 1) simulated agen...
Agents
more00tertiary
Tertiary Storage Organization for Large Multidimensional Datasets  
DB
499517
Reasoning About Intentions in Uncertain Domains The design of autonomous agents that are situated in real  world domains involves dealing with uncertainty in terms of dynamism,  observability and non-determinism. These three types of uncertainty,  when combined with the real-time requirements of many application domains,  imply that an agent must be capable of eectively coordinating its  reasoning. As such, situated belief-desire-intention (bdi) agents need an  ecient intention reconsideration policy, which denes when computational  resources are spent on reasoning, i.e., deliberating over intentions,  and when resources are better spent on either object-level reasoning or  action. This paper presents an implementation of such a policy by modelling  intention reconsideration as a partially observable Markov decision  process (pomdp). The motivation for a pomdp implementation of intention  reconsideration is that the two processes have similar properties  and functions, as we demonstrate in this paper. Our approach achieves  better results than existing intention reconsideration frameworks, as is  demonstrated empirically in this paper.
Agents
satoh01flying
Flying Emulator: Rapid Building and Testing of Networked Applications for Mobile Computers This paper presents a mobile-agent framework for building and testing  mobile computing applications. When a portable computing device is moved into  and attached to a new network, the proper functioning of an application running  on the device often depends on the resources and services provided locally in the  current network. To solve this problem, this framework provides an applicationlevel  emulator of portable computing devices. Since the emulator is constructed  as a mobile agent, it can carry target applications across networks on behalf of  a device, and it allows the applications to connect to local servers in its current  network in the same way as if they were moved with and executed on the device  itself. This paper also demonstrates the utility of this framework by describing  the development of typical location-dependent applications in mobile computing  settings.
Agents
rintanen98planning
A Planning Algorithm not based on Directional Search The initiative in STRIPS planning has recently been taken by work on propositional satisfiability. Best current planners, like Graphplan, and earlier planners originating in the partial-order or refinement planning community have proved in many cases to be inferior to general-purpose satisfiability algorithms in solving planning problems. However, no explanation of the success of programs like Walksat or relsat in planning has been offered. In this paper we discuss a simple planning algorithm that reconstructs the planner in the background of the SAT/CSP approach. 1 INTRODUCTION  Many of the recent interesting results in AI planning did not originate in traditional planning research, but in work on algorithms for checking the satisfiability of propositional formulae. STRIPS planning problems have been used as benchmarks to test SAT algorithms based on greedy local search [Kautz and Selman, 1992; Kautz and Selman, 1996] , and new developments [Bayardo, Jr. and Schrag, 1997] of the well-...
AI
brusilovsky98adaptive
Adaptive Navigation Support in Educational Hypermedia: the Role of Student Knowledge Level and the Case for Meta-Adaptation This paper provides a brief overview of main adaptive navigation support techniques and analyzes the results of most representative empirical studies of these techniques. It demonstrates an evidence that different known techniques work most efficiently in different context. In particular, the studies summarized in the paper have provided evidence that users with different knowledge level of the subject may appreciate different adaptive navigation support technologies. The paper argues that more empirical studies are required to help the developers of adaptive hypermedia systems in selecting most relevant adaptation technologies. It also attempts to build a case for meta-adaptive hypermedia systems, ie, systems that are able to adapt the very adaptation technology to the given user and context
HCI
paradiso99interactive
Interactive Music for Instrumented Dancing Shoes We have designed and built a pair of sneakers that each sense 16 different tactile and free-gesture parameters.  These include continuous pressure at 3 points in the forward sole, dynamic pressure at the heel, bidirectional  bend of the sole, height above instrumented portions of the floor, 3-axis orientation about the Earth's magnetic  field, 2-axis gravitational tilt and low-G acceleration, 3-axis shock, angular rate about the vertical, and  translational position via a sonar transponder. Both shoes transfer these parameters to a base station across an  RF link at 50 Hz State updates. As they are powered by a local battery, there are no tethers or wires running off  the shoe. A PC monitors the data streaming off both shoes and translates it into real-time interactive music. The  shoe design is introduced, and the interactive music mappings that we have developed for dance performances  are discussed.  1) Introduction  A trained dancer is capable of expressing highly dexterous control...
HCI
502642
Signer-independent Continuous Sign Language Recognition Based on SRN/HMM A divide-and-conquer approach is presented for signer-independent  continuous Chinese Sign Language(CSL) recognition in this paper. The problem  of continuous CSL recognition is divided into the subproblems of isolated  CSL recognition. We combine the simple recurrent network(SRN) with the  hidden Markov models(HMM) in this approach. The improved SRN is introduced  for segmentation of continuous CSL. Outputs of SRN are regarded as the  states of HMM, and the Lattice Viterbi algorithm is employed to search the best  word sequence in the HMM framework. Experimental results show SRN/HMM  approach has better performance than the standard HMM one.
HCI
83444
Incremental Recompilation of Knowledge Approximating a general formula from above and below by Horn formulas (its Horn  envelope and Horn core, respectively) was proposed in [22] as a form of "knowledge compilation,  " supporting rapid approximate reasoning; on the negative side, this scheme is  static in that it supports no updates, and has certain complexity drawbacks pointed out  in [17]. On the other hand, the many frameworks and schemes proposed in the literature  for theory update and revision are plagued by serious complexity-theoretic impediments,  even in the Horn case, as was pointed out in [6], and is further demonstrated in the present  paper. More fundamentally, these schemes are not inductive, in that they may lose in a  single update any positive properties of the represented sets of formulas (small size, Horn  structure, etc.). In this paper  1  we propose a new scheme, incremental recompilation, which  combines Horn approximation and model-based updates; this scheme is inductive and very  efficient, free of...
AI
kan01domainspecific
Domain-Specific Informative and Indicative Summarization for Information Retrieval In this paper, we propose the use of multidocument summarization as a post-processing step in document retrieval. We examine the use of the summary as a replacement to the standard ranked list. The form of the summary is novel because it has both informative and indicate elements, designed to help di#erent users perform their tasks better. Our summary uses the documents' topical structure as a backbone for its own structure, as it was deemed the most useful document feature in our study of a corpus of summaries.  1. 
HCI
187087
Supporting Conflict Management in Cooperative Design Teams The design of complex artifacts has increasingly become a cooperative process, with the detection and resolution of conflicts between design agents playing a central role. Effective tools for supporting the conflict management process, however, are still lacking. This paper describes a system called DCSS (the Design Collaboration Support System) developed to meet this challenge in design teams with both human and machine-based agents. Every design agent is provided with an "assistant" that provides domain-independent conflict detection, classification and resolution expertise. The design agents provide the domainspecific expertise needed to instantiate this general expertise, including the rationale for their actions, as a part of their design activities. DCSS has been used successfully to support the cooperative design of Local Area Networks by human and machine-based designers. This paper includes a description of DCSS's underlying model and implementation, examples of  its operation...
Agents
traum96reactivedeliberative
A Reactive-Deliberative Model of Dialogue Agency . For an agent to engage in substantive dialogues with other agents, there are several complexities which go beyond the scope of standard models of rational agency. In particular, an agent must reason about social attitudes that span more than one agent, as well as the dynamic and fallible process of plan execution. In this paper we sketch a theory of plan execution which allows the representation of failure and repair, extend the underlying agency model with social attitudes of mutual belief, obligation, and multi-agent plan execution, and describe an implemented dialogue agent which uses these notions, reacting to its environment and mental state, and deliberating and planning action only when more pressing concerns are absent. 1 Overview  For autonomous agents that operate in a realm of heterogeneous agents (including human agents), an agent theory should allow many of the features of natural language dialogue. The agent communication protocols should allow flexible turn-taking and ...
Agents
303146
Multi-modal Identity Verification using Support Vector Machines (SVM) The contribution of this paper is twofold: (1) to formulate a decision fusion problem encountered in the design of a multi-modal identity verification system as a particular classification problem, (2) to propose to solve this problem by a Support Vector Machine (SVM). The multi-modal identity verification system under consideration is built of d modalities in parallel, each one delivering as output a scalar number, called score, stating how well the claimed identity is verified. A fusion module receiving as input the d scores has to take a binary decision: accept or reject identity. This fusion problem has been solved using Support Vector Machines. The performances of this fusion module have been evaluated and compared with other proposed methods on a multimodal database, containing both vocal and visual modalities. Keywords: Decision Fusion, Support Vector Machine, Multi-Modal Identity Verification.  1 Introduction  Automatic identification/verification is rapidly becoming an importa...
ML
30494
A Neural Network Diagnosis Model without Disorder Independence Assumption . Generally, the disorders in a neural network diagnosis model are assumed independent each other. In this paper, we propose a neural network model for diagnostic problem solving where the disorder independence assumption is no longer necessary. Firstly, we characterize the diagnostic tasks and the causal network which is used to represent the diagnostic problem, then we describe the neural network diagnosis model, finally, some experiment results will be given.  1 Introduction  Finding explanations for a given set of events is an important aspect of general intelligent behaviour. The process of finding the best explanation was defined as  Abduction by the philosopher C. S. Peirce [8]. Diagnosis is a typical abductive problem. For a set of manifestations(observations), the diagnostic inference is to find the most plausible faults or disorders which can explain the manifestations observed. In general, an individual fault or disorder can explain only a portion of the manifestations. Ther...
ML
sterritt01soft
Soft Computing and Fault Management Soft computing is a partnership between A.I. techniques that are tolerant of imprecision, uncertainty and partial truth, with the aim of obtaining a robust solution for complex systems. Telecommunication systems are built with extensive redundancy and complexity to ensure robustness and quality of service. To facilitate this requires complex fault identification and management systems. Fault identification and management is generally handled by reducing the amount of alarm events (symptoms) presented to the operating engineer through monitoring, filtering and masking. The ultimate goal is to determine and present the actual underlying fault. Fault Management is a complex task subject to uncertainty in the 'symptoms' presented and as such is ideal for treatment by soft computing techniques. The aim of this paper is to present a soft computing approach to fault management in telecommunication systems. Two key approaches are considered; AI & soft computing rule discovery and techniques to attempt to present less symptoms with greater diagnostic assistance for the more traditional rule based system approach and a hybrid soft computing approach which utilises a genetic algorithm to learn Bayesian belief networks (BBNs). It is also highlighted that research and development of the two target Fault Management Systems are complementary. Keywords: Network management, fault management, knowledge discovery, Bayesian belief networks, genetic algorithms, soft computing. 1.
ML
codish98efficient
Efficient Goal Directed Bottom-up Evaluation of Logic Programs This paper introduces a new strategy for the efficient goal directed bottomup evaluation of logic programs. Instead of combining a standard bottomup evaluation strategy with a Magic-set transformation, the evaluation strategy is specialized for the application to Magic-set programs which are characterized by clause bodies with a high degree of overlapping. The approach is similar to other techniques which avoid re-computation by maintaining and reusing partial solutions to clause bodies. However, the overhead is considerably reduced as these are maintained implicitly by the underlying Prolog implementation. The technique is presented as a simple meta-interpreter for goal directed bottom-up evaluation. No Magic-set transformation is involved as the dependencies between calls and answers are expressed directly within the interpreter. The proposed technique has been implemented and shown to provide substantial speed-ups in applications of semantic based program analysis based on bottom-up...
DB
fernandes00combining
Combining Inductive and Deductive Inference in Knowledge Management Tasks This paper indicates how different logic programming technologies can underpin an architecture for distributed knowledge management in which higher throughput in information supply is achieved by a (semi-)automated solution to the more challenging problem of knowledge creation. The paper first proposes working definitions of the notions of data, knowledge and information in purely logical terms, and then shows how existing technologies can be combined into an inference engine, referred to as a knowledge, information and data engine (KIDE), integrating inductive and deductive capabilities. The paper then briefly introduces the notion of virtual organizations and uses the set-up stage of virtual organizations to exemplify the value-adding potential of KIDEs in knowledge management contexts.
DB
43410
Towards Text Knowledge Engineering We introduce a methodology for automating the maintenance of domain-specific taxonomies based on natural language text understanding. A given ontology is incrementally updated as new concepts are acquired from real-world texts. The acquisition process is centered around the linguistic and conceptual "quality" of various forms of evidence underlying the generation and refinement of concept hypotheses. On the basis of the quality of evidence, concept hypotheses are ranked according to credibility and the most credible ones are selected for assimilation into the domain knowledge base. Appeared in:  AAAI'98 - Proceedings of the 15th National Conference on Artificial Intelligence, July 26-30, 1998, Madison, Wisconsin (forthcoming)  Towards Text Knowledge Engineering  Udo Hahn & Klemens Schnattinger  L F Computational Linguistics Group Text Knowledge Engineering Lab Freiburg University Werthmannplatz 1, D-79085 Freiburg, Germany  http://www.coling.uni-freiburg.de Abstract  We introduce a me...
AI
313926
Modeling Spatial Dependencies for Mining Geospatial Data: An Introduction Spatial data mining is a process to discover interesting, potentially useful and high utility  patterns embedded in spatial databases. Efficient tools for extracting information from spatial  data sets can be of importance to organizations which own, generate and manage large spatial  data sets. The current approach towards solving spatial data mining problems is to use classical  data mining tools after "materializing" spatial relationships. However, the key property of  spatial data is that of spatial autocorrelation. Like temporal data, spatial data values are  influenced by values in their immediate vicinity. Ignoring spatial autocorrelation in the modeling  process leads to results which are a poor-fit and unreliable. In this chapter we will first review  spatial statistical techniques which explictly model spatial autocorrelation. Second, we will  propose PLUMS(Predicting Locations Using Map Similarity), a new approach for supervised  spatial data mining problems. PLUMS searches the space of solutions using a map-similarity  measure which is more appropriate in the context of spatial data. We will show that compared  to state-of-the-art spatial statistics approaches, PLUMS achives comparable accuracy but at  a fraction of the computational cost. Furthermore, PLUMS provides a general framework for  specializing other data mining techniques for mining spatial data.
ML
232607
Heterogeneous Active Agents, III: Polynomially Implementable Agents . In (Eiter, Subrahmanian, and Pick 1999), the authors have introduced techniques to build agents on top of arbitrary data structures, and to "agentize" new/existing programs. They provided a series of successively more sophisticated semantics for such agent systems, and showed that as these semantics become epistemically more desirable, a computational price may need to be paid. In this paper, we identify a class of agents that are called weak regular---this is done by first identifying a fragment of agent programs (Eiter, Subrahmanian, and Pick 1999) called weak regular agent programs  (WRAPs for short). It is shown that WRAPs are definable via three parameters---checking for a property called "safety", checking for a property called "conflict freedom" and checking for a "deontic stratifiability" property. Algorithms for each of these are developed. A weak regular agent is then defined in terms of these concepts , and a regular agent is one that satisfies an additional "boundedness" ...
Agents
21655
Wrapper Induction: Efficiency and Expressiveness (Extended Abstract) Recently, many systems have been built that automatically interact with Internet information resources. However, these resources are usually formatted for use by people; e.g., the relevant content is embedded in HTML pages. Wrappers are often used to extract a resource's content, but hand-coding wrappers is tedious and error-prone. We advocate wrapper induction,  a technique for automatically constructing wrappers. We have identified several wrapper classes that can be learned quickly (most sites require only a handful of examples, consuming a few CPU seconds of processing), yet which are useful for handling numerous Internet resources (70% of surveyed sites can be handled by our techniques). Introduction  The Internet presents a stunning variety of on-line information resources: telephone directories, retail product catalogs, weather forecasts, and many more. Recently, there has been much interest in systems (such as software agents (Etzioni & Weld 1994; Kwok & Weld 1996) or informati...
ML
520300
SpiderServer: the MetaSearch Engine of WebNaut Search engines on the Web are valuable tools for searching information according to a user's interests whether an individual or a software agent. In the present article we describe the design and the operation mode of SpiderServer, a metasearch engine used for the submission of a query followed by the retrieving of results from five popular search engines. SpiderServer is the metasearch engine of the WebNaut system but it can be easily used by any other metasearch platform. There are two files for every search engine describing the phases of query formation and filtering respectively. These files contain directions on the way a query must be modified for a specific search engine and on the methodology SpiderServer must follow in order to parse the results from the specific search engine. The ultimate goal is to construct platform independent meta-search engines, which can be easily programmed to adapt to any search engine available on the WEB.
IR
oliver98statistical
Statistical Modeling of Human Interactions In this paper we describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task. The system is particularly concerned with detecting when interactions between people occur, and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth.  Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach. We propose and compare two different state-based learning architectures, namely HMMs and CHMMs, for modeling behaviors and interactions. The CHMM model is shown to work much more efficiently and accurately.  Finally, a synthetic agent training system is used to develop a priori models for recognizing human behaviors and interactions. We demonstrate the ability to use these a priori models to accurately classify real human beha...
Agents
baralis00algebraic
An Algebraic Approach to Static Analysis of Active Database Rules ing with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works, requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept, ACM Inc., 1515 Broadway, New York, NY 10036 USA, fax +1 (212) 869-0481, or permissions@acm.org. This is a preliminary release of an article accepted by ACM Transactions on Database Systems. The definitive version is currently in production at ACM and, when released, will supersede this version.  2 \Delta E. Baralis and J. Widom  1. INTRODUCTION  An active database system is a conventional database system extended with a facility for managing active rules (or triggers). Incorporating active rules into a conventional database system has raised considerable interest both in the scientific community and in the commercial world: A number of prototypes that incorporate active rules into relational and object-oriented database system...
DB
borrajo93bounded
Bounded Explanation and Inductive Refinement For Acquiring Control Knowledge One approach to learning control knowledge  from a problem solving trace consists  of generating explanations for the local  decisions made during the search process.
ML
beeferman99statistical
Statistical Models for Text Segmentation Abstract. This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms. 1.
ML
itskevitch01automatic
Automatic Hierarchical E-Mail Classification Using Association Rules The explosive growth of on-line communication, in particular e-mail communication, makes it necessary to organize the information for faster and easier processing and searching. Storing e-mail messages into hierarchically organized folders, where each folder corresponds to a separate topic, has proven to be very useful.  Previous approaches to this problem use Nave Bayes- or TF-IDF-style classifiers that are based on the unrealistic term independence assumption. These methods are also context-insensitive in that the meaning of words is independent of presence/absence of other words in the same message. It was shown that text classification methods that deviate from the independence assumption and capture context achieve higher accuracy. In this thesis, we address the problem of term dependence by building an associative classifier called Classification using Cohesion and Multiple Association Rules, or COMAR in short. The problem of context capturing is addressed by looking for phrases in message corpora. Both rules and phrases are generated using an efficient FP-growth-like approach. Since the amount of rules and phrases produced can be very large, we propose two new measures, rule cohesion and phrase cohesion, that possess the anti-monotone property which allows the push of rule and phrase pruning deeply into the process of their generation. This approach to pattern pruning proves to be much more efficient than "generate-and-prune" methods.  Both unstructured text attributes and semi-structured non-text attributes, such as senders and recipients, are used for the classification. COMAR classification algorithm uses multiple rules to predict several highest probability topics for each message. Different feature selection and rule ranking methods are compared. Our studies show ...
IR
piekarski99architecture
An Architecture for Outdoor Wearable Computers to Support Augmented Reality and Multimedia Applications This paper describes an architecture to support a hardware and software platform for research into the use of wearable computers and augmented reality in an outdoor environment. The architecture supports such devices as a GPS, compass, and head-mounted display. A prototype system was built to support novel applications by drawing graphics (such as maps, building plans, and compass bearings) on the head-mounted display, projecting information over that normally seen by the user and hence augmenting a user's perception of reality. This paper presents a set of novel augmented reality and multimedia applications operated in an outdoor environment.  1 
HCI
ponnekanti01icrafter
ICrafter: A Service Framework for Ubiquitous Computing Environments In this paper, we propose ICrafter, a framework for services  and their user interfaces in a class of ubiquitous computing environments.
HCI
arampatzis00term
Term Selection for Filtering based on Distribution of Terms over Time In this article we investigate the use of time distributions in retrieval tasks. Specifically, we introduce a novel term selection method, namely Term Occurrence Uniformity (TOU), based on the hypothesis that terms which occur uniformly in time are more valuable than others. Our empirical evaluation so far has neither proved nor disproved this hypothesis. However, results are promising and suggest the need for a deeper theoretical and empirical investigation. Our current concern is filtering, but this line of research may easily be extended to other retrieval tasks which involve temporally-dependent data.  1 Introduction  Information Filtering is the process of searching in large amounts of data for information which matches a user information need. The filtering task is usually described as the inverse of the traditional retrieval task. In retrieval, a one-time user request (called query) is matched to a static collection of information objects. In filtering, users issue a long-term r...
IR
magoulas01hybrid
Hybrid Methods Using Evolutionary Algorithms for On-line Training A novel hybrid evolutionary approach is presented in this paper for improving the performance of neural network classifiers in slowly varying environments. For this purpose, we investigate a coupling of Differential Evolution Strategy and Stochastic Gradient Descent, using both the global search capabilities of Evolutionary Strategies and the effectiveness of on-line gradient descent. The use of Differential Evolution Strategy is related to the concept of evolution of a number of individuals from generation to generation and that of on-line gradient descent to the concept of adaptation to the environment by learning. The hybrid algorithm is tested in two real-life image processing applications. Experimental results suggest that the hybrid strategy is capable to train on-line effectively leading to networks with increased generalization capability.
AI
joachims02unbiased
Unbiased Evaluation of Retrieval Quality using Clickthrough Data This paper proposes a new method for evaluating the quality of retrieval functions. Unlike traditional methods that require relevance judgements by experts or explicit user feedback, it is based entirely on clickthrough data. This is a key advantage, since clickthrough data can be collected at very low cost and without overhead for the user. Taking an approach from experiment design, the paper proposes an experiment setup that generates unbiased feedback about the relative quality of two search results without explicit user feedback. A theoretical analysis shows that the method gives the same results as evaluation with traditional relevance judgements under mild statistical assumptions. An empirical analysis verifies that the assumptions are indeed justified and that the new method leads to conclusive results in a WWW retrieval study.
IR
316064
Towards Automatic Discovery of Object Categories We propose a method to learn heterogeneous models of object classes for visual recognition. The training images contain a preponderance of clutter and learning is unsupervised. Our models represent objects as probabilistic constellations of rigid parts (features). The variability within a class is represented by a joint probability density function on the shape of the constellation and the appearance of the parts. Our method automatically identifies distinctive features in the training set. The set of model parameters is then learned using expectation maximization (see the companion paper [11] for details). When trained on different, unlabeled and unsegmented views of a class of objects, each component of the mixture model can adapt to represent a subset of the views. Similarly, different component models can also "specialize" on sub-classes of an object class. Experiments on images of human heads, leaves from different species of trees, and motor-cars demonstrate that the method works...
ML
536579
A Scalable Integrated Region-Based Image Retrieval System Statistical clustering is critical in designing scalable image retrieval systems. In this paper, we presentascalable algorithm for indexing and retrieving images based on region segmentation. The method uses statistical clustering on region features and IRM (Integrated Region Matching), a measure developed to evaluate overall similaritybetween images that incorporates properties of all the regions in the images by a region-matching scheme. Compared with retrieval based on individual regions, our overall similarity approach (a) reduces the inuence of inaccurate segmentation, (b) helps to clarify the semantics of a particular region, and (c) enables a simple querying interface for region-based image retrieval systems. The algorithm has been implemented as a part of our experimental SIMPLIcity image retrieval system and tested on large-scale image databases of both general-purpose images and pathology slides. Experiments have demonstrated that this technique maintains the accuracy and robustness of the original system while reducing the matching time significantly.
IR
martelli99logic
A Logic Programming Framework for Component-Based Software Prototyping The paper presents CaseLP, a logic-based prototyping environment for specifying and verifying  complex distributed applications. CaseLP provides a set of languages for modeling intelligent  and interacting components (agents) at different levels of abstraction. It also furnishes  tools for integrating legacy software into a prototype.  The possibility of integrating, into the same executable prototype, agents which are only  specified as well as already developed components can prove extremely useful in the engineering  process of complex applications. In fact, the reusability of existing components can be verified  before the application has been implemented and the developer can be more confident on the  correctness of the new components specification, if it has been executed and tested by means  of an interaction with the existing components.  Besides the aspects of integration and reuse, CaseLP also faces another fundamental issue of  nowadays applications, namely distribution. The...
Agents
haveliwala02evaluating
Evaluating Strategies for Similarity Search on the Web Finding pages on the Web that are similar to a query page (Related Pages) is an important component of modern search engines. A variety of strategies have been proposed for answering Related Pages queries, but comparative evaluation by user studies is expensive, especially when large strategy spaces must be searched (e.g., when tuning parameters). We present a technique for automatically evaluating strategies using Web hierarchies, such as Open Directory, in place of user feedback. We apply this evaluation methodology to a mix of document representation strategies, including the use of text, anchor-text, and links. We discuss the relative advantages and disadvantages of the various approaches examined. Finally, we describe how to efficiently construct a similarity index out of our chosen strategies, and provide sample results from our index.
IR
bass01glimpse
A glimpse into the future of ID Cyberspace is a complex dimension of both enabling and inhibiting data flows in electronic data networks. Current generation intrusion detection (ID) systems are not technologically advanced enough to create the situational knowledge required to manage these networks. Next generation ID system will fuse data, combining both short-term sensor data with long-term knowledge databases, to create cyberspace situational awareness. This article offers a glimpse into the foggy crystal ball of future ID systems. Before diving into the technical discussion we ask the reader to keep in mind the generic model of a datagram traversing the Internet. Figure 1 illustrates an IP datagram moving in a store-and-forward environment from source to destination; routed based on a destination address with a uncertain source address decrementing the datagram time-to-live (TTL) at every router hop [1]. The datagram is routed through major Internets and IP transit providers. There is striking similarity between the transit of a datagram in the Internet and an airplane through airspace; future network management and air traffic control. At a very high abstract level, the concepts used to monitor objects in airspace apply to monitoring objects in networks. The Federal Aviation Administration (FAA) divides airspace management into two distinct entities. On the one hand, local controllers guide aircraft into and out of the air space surrounding an airport. Their job is to maintain awareness of the location of all aircraft in their vicinity, ensure proper separation, identify threats to aircraft, and manage the overall safety of passengers. Functionally, this is similar to the role of network controllers who must control the environment within their administrative domains. The network administrator must ensure the proper ports are open and the information is not delayed, the collisions are kept to a minimum, and the integrity of the delivery systems are not compromised. This is naturally similar to the situational awareness required in current generation air traffic control (ATC).
AI
12369
How to Avoid Building DataBlades That Know the Value of Everything and the Cost of Nothing The object-relational database management system (ORDBMS) offers many potential benefits for scientific, multimedia and financial applications. However, work remains in the integration of domain-specific class libraries (data cartridges, extenders, DataBlades  ®  ) into ORDBMS query processing. A major problem is that the standard mechanisms for query selectivity estimation, taken from relational database systems, rely on properties specific to the standard data types; creation of new mechanisms remains extremely difficult because the software interfaces provided by vendors are relatively low-level. In this paper, we discuss extensions of the generalized search tree, or GiST, to support a higher-level but less type-specific approach. Specifically, we discuss the computation of selectivity estimates with confidence intervals using a variety of index-based approaches and present results from an experimental comparison of these methods with several estimators from the literature. 1. Intro...
DB
455426
HyperQueries: Dynamic Distributed Query Processing on the Internet In this paper we propose a new framework for  dynamic distributed query processing based  on so-called HyperQueries which are essentially  query evaluation sub-plans "sitting behind  " hyperlinks. We illustrate the flexibility  of this distributed query processing architecture  in the context of B2B electronic market  places. Architecting an electronic market  place as a data warehouse by integrating all  thedatafromall participating enterprises in  one centralized repository incurs severe problems.  Using HyperQueries, application integration  is achieved via dynamic distributed  query evaluation plans. The electronic market  place serves as an intermediary between clients  and providers executing their sub-queries referenced  via hyperlinks. The hyperlinks are embedded  within data objects of the intermediary  's database. Retrieving such a virtual object  will automatically initiate the execution of the  referenced HyperQuery in order to materialize  the entire object. Thus, sensitive data remains  under the full control of the data providers.  1 
DB
efe00shape
The Shape of the Web and Its Implications for Searching the Web With the rapid growth of the number of web pages, designing a search engine that can retrieve high quality information in response to a user query is a challenging task. Automated search engines that rely on keyword matching usually return too many low quality matches and they take a long time to run. It is argued in the literature that link-following search methods can substantially increase the search quality, provided that these methods use an accurate assumption about useful patterns in the hyperlink topology of the web. Recent work in the field has focused on detecting identi able patterns in the web graph and exploiting this information to improve the performance of search algorithms. We survey relevant work in this area and comment on the implications of these patterns for other areas such as advertisement and marketing.
IR
chen01detection
Detection and Correction of Conflicting Concurrent Data Warehouse Updates Data integration over multiple heterogeneous data sources has become increasingly important for modern applications. The integrated data is usually stored in materialized views to allow better access, performance and high availability. Materialized view must be maintained after the data sources change. In a loosely-coupled environment, such as the Data Grid, the data sources are autonomous. Hence the source updates can be concurrent and cause erroneous maintenance results. State-of-the-art maintenance strategies apply compensating queries to correct such errors, making the restricting assumption that all source schemata remain static over time. However, in such dynamic environments, the data sources may change not only their data but also their schema, query capabilities or semantics. Consequently, either the maintenance queries or compensating queries would fail. We now propose a novel solution that handles both concurrent data and schema changes. First, we analyze the concurrency between source updates and classify them into different classes of dependencies. We then propose Dyno, a two-pronged strategy composed of dependency detection and correction algorithms to handle these new classes of concurrency. Our techniques are not tied to specific maintenance algorithms nor to a particular data model. To our knowledge, this is the first comprehensive solution to the view maintenance concurrency problems in loosely-coupled environments. Our experimental results illustrate that Dyno imposes an almost negligible overhead on existing maintenance algorithms for data updates while now allowing for this extended functionality.
DB
cohen01equix
EquiX-A Search and Query Language for XML EquiX is a search language for XML that combines the power of querying with the  simplicity of searching. Requirements for such languages are discussed and it is shown  that EquiX meets the necessary criteria. Both a graph-based abstract syntax and a formal  concrete syntax are presented for EquiX queries. In addition, the semantics is defined and an  evaluation algorithm is presented. The evaluation algorithm is polynomial under combined  complexity.  EquiX combines pattern matching, quantification and logical expressions to query both  the data and meta-data of XML documents. The result of a query in EquiX is a set of XML  documents. A DTD describing the result documents is derived automatically from the query.  1 
IR
vazov01system
A System for Extraction of Temporal Expressions from French Texts We present a system for extraction of temporal expressions from French texts. The  identication of the temporal expressions is based on a context-scanning strategy (CSS)  which is carried out by two complementary techniques: search for regular expressions and  left-to-right and right-to-left local chart-parsing.  A System for Extraction of Temporal Expressions  from French Texts  Paper-ID: ACL-2001-XXXX  1 Introduction  The identication and the interpretation of temporal and aspectual information plays an important role in text understanding. This information is encoded in the natural languages by a wide array of linguistic means ranging from grammatical (morpho-syntactic) to lexical (verbs and adverbials) or strictly syntactic phenomena (temporal anaphora (Webber, 1988) or argument structure of the verb (Verkuyl, 1972; Verkuyl, 1993)).  In this paper we present a system for identi- cation of lexical non-verbal means of expressing temporal information in French texts. The system det...
IR
455220
Closing the Loop: an Agenda- and Justification-Based Framework for Selecting the Next Discovery Task to Perform We propose and evaluate an agenda- and justificationbased  architecture for discovery systems that contains  a mechanism for selecting the next task to perform.  This framework has many desirable properties: (1) its  use of heuristics to perform and propose tasks  facilitates the use of general discovery strategies that  are able to use a variety of background knowledge, (2)  through the use of justifications its mechanism for  selecting the next task to perform is able to reason  about the appropriateness of the tasks being  considered, and (3) its mechanism for selecting the  next task to perform also considers the users interests,  allowing a discovery program to tailor its behavior  toward them.  We evaluate the extent to which both reasons and  estimates of interestingness contribute to performance  in the domain of protein crystallization. With both  aspects contributing to task selection, a high fraction  of discoveries by the HAMB prototype were judged  interesting by an expert (21% interesting and novel;  45% interesting but rediscoveries).  1. 
ML
alferes98dynamic
Dynamic Logic Programming In this paper we investigate updates of knowledge bases represented by logic programs. In order to represent negative information, we use generalized logic programs which allow default negation not only in rule bodies but also in their heads.We start by introducing the notion of an update $P\oplus U$ of a logic program $P$ by another logic program $U$. Subsequently, we provide a precise semantic characterization of $P\oplus U$, and study some basic properties of program updates. In particular, we show that our update programs generalize the notion of interpretation update. We then extend this notion to compositional sequences of logic programs updates $P_{1}\oplus P_{2}\oplus \dots $, defining a dynamic program update, and thereby introducing the paradigm of \emph{dynamic logic programming}. This paradigm significantly facilitates modularization of logic programming, and thus modularization of non-monotonic reasoning as a whole. Specifically, suppose that we are given a set of logic program modules, each describing a different state of our knowledge of the world. Different states may represent different time points or different sets of priorities or perhaps even different viewpoints. Consequently, program modules may contain mutually contradictory as well as overlapping information. The role of the dynamic program update is to employ the mutual relationships existing between different modules to precisely determine, at any given module composition stage, the declarative as well as the procedural semantics of the combined program resulting from the modules.
DB
freitag99information
Information Extraction with HMMs and Shrinkage Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling time series data, and have been applied with success to many language-related tasks such as part of speech tagging, speech recognition, text segmentation and topic detection. This paper describes the application of HMMs to another language related task|information extraction|the problem of locating textual sub-segments that answer a particular information need. In our work, the HMM state transition probabilities and word emission probabilities are learned from labeled training data. As in many machine learning problems, however, the lack of suÆcient labeled training data hinders the reliability of the model. The key contribution of this paper is the use of a statistical technique called \shrinkage" that signi cantly improves parameter estimation of the HMM emission probabilities in the face of sparse training data. In experiments on seminar announcements and Reuters acquisitions articles, shrinkage is shown to r...
ML
marchiori98simple
A Simple Heuristic Based Genetic Algorithm for the Maximum Clique Problem This paper proposes a novel heuristic based genetic algorithm  (HGA) for the maximum clique problem, which consists of the combination of a simple genetic algorithm and a naive heuristic algorithm. The heuristic based genetic algorithm is tested on the so-called DIMACS benchmark graphs, with up to 4000 nodes and up to 5506380 edges, consisting of randomly generated graphs with known maximum clique and of graphs derived from various practical applications. The performance of HGA on these graphs is very satisfactory both in terms of solution quality and running time. Despite its simplicity, HGA dramatically improves on all previous approaches based on genetic algorithms we are aware of, and yields results comparable to those of more involved heuristic algorithms based on local search. This provides empirical evidence of the effectiveness of heuristic based genetic algorithms as a search technique for solving the maximum clique problem, which is competitive with respect to other (variants...
ML
huffman95flexibly
Flexibly Instructable Agents This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a exible (and thus powerful) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise. To support this exibility, however, the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions. Our approach, called situated explanation, achieves such learning through a combination of analytic and inductive techniques. It combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations. The approach is implemented in an agent called Instructo-Soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. Instructo-Soar meets three key requirements of exible instructability that distinguish it from previous systems: (1) it can take known or unknown commands at any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation speci ed in language (as in, for instance, conditional instructions); and (3) it can learn, from instructions, each class of knowledge it uses to perform tasks. 1.
ML
punyakanok01use
The Use of Classifiers in Sequential Inference We study the problem of combining the outcomes of several different  classifiers in a way that provides a coherent inference that satisfies some  constraints. In particular, we develop two general approaches for an important  subproblem - identifying phrase structure. The first is a Markovian  approach that extends standard HMMs to allow the use of a rich observation  structure and of general classifiers to model state-observation  dependencies. The second is an extension of constraint satisfaction formalisms.  We develop efficient combination algorithms under both models  and study them experimentally in the context of shallow parsing.  1 Introduction  In many situations it is necessary to make decisions that depend on the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints - the sequential nature of the data or other domain specific constraints. Consider, for example, the problem of chunking natural language sentences ...
IR
phan01scalable
A Scalable, Distributed Middleware Service Architecture to Support Mobile Internet Applications Middleware layers placed between user clients and application servers have been used to perform a variety of functions. In previous work we have used middleware to perform a new capability, application session handoff, using a single Middleware Server to provide all functionality. However, to improve the scalability of our architecture, we have designed an efficient distributed Middleware Service layer that properly maintains application session handoff semantics while being able to service a large number of clients. We show that this service layer improves the scalability of general clientto -application server interaction as well as the specific case of application session handoff. We detail protocols involved in performing handoff and analyse an implementation of the architecture that supports the use of a real medical teaching tool. From experimental results it can be seen that our Middleware Service effectively provides scalability as a response to increased workload.  1. 
Agents
madhavan01generic
Generic Schema Matching with Cupid Schema matching is a critical step in many applications,  such as XML message mapping, data warehouse  loading, and schema integration. In this paper, we  investigate algorithms for generic schema matching,  outside of any particular data model or application. We  first present a taxonomy for past solutions, showing  that a rich range of techniques is available. We then  propose a new algorithm, Cupid, that discovers mappings  between schema elements based on their names,  data types, constraints, and schema structure, using a  broader set of techniques than past approaches. Some  of our innovations are the integrated use of linguistic  and structural matching, context-dependent matching  of shared types, and a bias toward leaf structure where  much of the schema content resides. After describing  our algorithm, we present experimental results that  compare Cupid to two other schema matching systems.
DB
bridge98defining
Defining and Combining Symmetric and Asymmetric Similarity Measures . In this paper, we present a framework for the definition of similarity measures using lattice-valued functions. We show their strengths (particularly for combining similarity measures). Then we investigate a particular instantiation of the framework, in which sets are used both to represent objects and to denote degrees of similarity. The paper concludes by suggesting some generalisations of the findings. 1 Introduction  There are many different ways of computing the similarity of object representations. These include:  -- the feature-based approach, in which objects are represented by sets of features, and similarity is based on feature commonality and difference (e.g. [13]);  -- the geometric approach, in which objects are represented by points in an n-  dimensional space (usually specified by sets of pairs of attributes and atomic values), and similarity is based on the inverse of the distance between objects in the space (e.g. [12]); and  -- the structural approach, which uses gr...
ML
abraham00optimal
Optimal Design of Neural Nets Using Hybrid Algorithms Selection of the topology of a network and correct parameters for the  learning algorithm is a tedious task for designing an optimal Artificial Neural  Network (ANN), which is smaller, faster and with a better generalization  performance. Genetic algorithm (GA) is an adaptive search technique based on  the principles and mechanisms of natural selection and survival of the fittest  from natural evolution. Simulated annealing (SA) is a global optimization  algorithm that can process cost functions possessing quite arbitrary degrees of  nonlinearities, discontinuities and stochasticity but statistically assuring a  optimal solution. In this paper we explain how a hybrid algorithm integrating  the desirable aspects of GA and SA can be applied for the optimal design of an  ANN. This paper is more concerned with the understanding of current  theoretical developments of Evolutionary Artificial Neural Networks (EANNs)  using GAs and other heuristic procedures and how the proposed hybrid and  other heuristic procedures can be combined to produce an optimal ANN.
AI
arleo00spatial
Spatial Cognition and Neuro-Mimetic Navigation: A Model of Hippocampal Place Cell Activity . A computational model of hippocampal activity during spatial cognition and navigation tasks is presented. The spatial representation in our model of the rat hippocampus is built on-line during exploration via two processing streams. An allothetic vision-based representation is built by unsupervised Hebbian learning extracting spatio-temporal properties of the environment from visual input. An idiothetic representation is learned based on internal movement-related information provided by path integration. On the level of the hippocampus, allothetic and idiothetic representations are integrated to yield a stable representation of the environment by a population of localized overlapping CA3-CA1 place fields. The hippocampal spatial representation is used as a basis for goal-oriented spatial behavior. We focus on the neural pathway connecting the hippocampus to the nucleus accumbens. Place cells drive a population of locomotor action neurons in the nucleus accumbens. Reward-based learnin...
ML
greenberg01phidgets
Phidgets: Easy Development of Physical Interfaces through Physical Widgets Physical widgets or phidgets are to physical user interfaces what widgets are to graphical user interfaces. Similar to widgets, phidgets abstract and package input and output devices: they hide implementation and construction details, they expose functionality through a well-defined API, and they have an (optional) on-screen interactive interface for displaying and controlling device state. Unlike widgets, phidgets also require: a connection manager to track how devices appear on-line; a way to link a software phidget with its physical counterpart; and a simulation mode to allow the programmer to develop, debug and test a physical interface even when no physical device is present. Our evaluation shows that everyday programmers using phidgets can rapidly develop physical interfaces.
HCI
bruno02evaluating
Evaluating Top-k Queries over Web-Accessible Databases A query to a web search engine usually consists of a list of keywords, to which the search engine responds with the best or “top ” k pages for the query. This top-k query model is prevalent over multimedia collections in general, but also over plain relational data for certain applications. For example, consider a relation with information on available restaurants, including their location, price range for one diner, and overall food rating. A user who queries such a relation might simply specify the user’s location and target price range, and expect in return the best 10 restaurants in terms of some combination of proximity to the user, closeness of match to the target price range, and overall food rating. Processing top-k queries efficiently is challenging for a number of reasons. One critical such reason is that, in many web applications, the relation attributes might not be available other than through external web-accessible form interfaces, which we will have to query repeatedly for a potentially large set of candidate objects. In this article, we study how to process top-k queries efficiently in this setting, where the attributes for which users specify target values might be handled by external, autonomous sources with a variety of access interfaces. We present a sequential algorithm for processing such queries, but observe that any sequential top-k query processing strategy is bound to require unnecessarily long query processing times, since web accesses exhibit high and variable latency. Fortunately, web sources can be probed in parallel, and each source can typically process concurrent requests, although sources may impose some restrictions on the type and number of probes that they are willing to accept. We adapt our sequential query processing technique and introduce an efficient algorithm that maximizes source-access parallelism to minimize query response time, while satisfying source-access constraints. We evaluate
DB
attardi99theseus
Theseus: Categorization by Context Introduction The traditional approach to document categorization is categorization by content, since information for categorizing a document is extracted from the document itself. In a hypertext environment like the Web, the structure of documents and the link topology can be exploited to perform what we call categorization by context [Attardi 98]: the context surrounding a link in an HTML document is used for categorizing the document referred by the link. Categorization by context is capable of dealing also with multimedia material, since it does not rely on the ability to analyze the content of documents. Categorization by context leverages on the categorization activity implicitly performed when someone places or refers to a document on the Web. By focusing the analysis to the documents used by a group of people, one can build a catalogue tuned to the need of that group. Categorization by context is based on the following assumptions: 1
IR
murthy00concept
Concept Network: A Structure for Context Sensitive Document Representation In this paper we propose a directed acyclic graphical structure called concept network  (CNW) for context sensitive document representation for use in information filtering.  Nodes of CNW represent concepts and links represent the relationships between  the concepts. A concept can either be a phrase or a topic of discourse, or a mode of  discourse. An important feature of the CNW based scheme (CNWBS) is context filters  [Murthy and Keerthi, 1999] which are employed on the links of the graph to enable context  sensitive analysis and representation of documents. Context filters are intended to  filter the noise in the inputs of the concepts based on the context of appearance of their  inputs. The CNWBS automatically finds the paragraphs related to all concepts in the  document. It also provides good comprehensibility in representation; allows sharing of  CNW among a group of users; and, reduces the credit-assignment problem during its construction.  This representation scheme is used for...
IR
nijholt00multimodal
Multimodal Interactions with Agents in Virtual Worlds Introduction  World Wide Web allows interactions and transactions through Web pages using speech and language, either by inanimate or live agents, image interpretation and generation, and, of course the more traditional ways of presenting explicitly predefined information of text, tables, figures, pictures, audio, animation and video. In a task- or domain-oriented way of interaction current technology allows the recognition and interpretation of rather natural speech and language in dialogues. However, rather than the current two-dimensional web-pages, many interesting parts of the Web will become three-dimensional, allowing the building of virtual worlds inhabited by user and task agents, with which the user can interact using different types of modalities, including speech and language interpretation and generation. Agents can work on behalf of users, hence, human computer interaction will make use of `indirect management', rather than interacting through direct manipulation of data 
HCI
wooldridge96logic
A Logic of BDI Agents with Procedural Knowledge In this paper, we present a new logic for specifying the behaviour of multi-agent systems. In this logic, agents are viewed as BDI systems, in that their state is characterised in terms of beliefs, desires, and intentions: the semantics of the BDI component of the logic are based on the wellknown system of Rao and Georgeff. In addition, agents have available to them a library of plans, representing their `know-how': procedural knowledge about how to achieve their intentions. These plans are, in effect, programs, that specify how a group of agents can work in parallel to achieve certain ends. The logic provides a rich set of constructs for describing the structure and execution of plans. Some properties of the logic are investigated, (in particular, those relating to plans), and some comments on future work are presented. 1 Introduction  There is currently much international interest in computer systems that go under the banner of intelligent agents [17]. Crudely, an intelligent agent i...
Agents
bonnet98partial
Partial Answers for Unavailable Data Sources Abstract. Many heterogeneous database system products and prototypes exist today; they will soon be deployed in a wide variety of environments. Most existing systems suffer from an Achilles ’ heel: they ungracefully fail in presence of unavailable data sources. If some data sources are unavailable when accessed, these systems either silently ignore them or generate an error. This behavior is improper in environments where there is a non-negligible probability that data sources cannot be accessed (e.g., Internet). In case some data sources cannot be accessed when processing a query, the complete answer to this query cannot be computed; some work can however be done with the data sources that are available. In this paper, we propose a novel approach where, in presence of unavailable data sources, the answer to a query is a partial answer. A partial answer is a representation of the work that has been done in case the complete answer to a query cannot be computed, and of the work that remains to be done in order to obtain this complete answer. The use of a partial answer is twofold. First, it contains an incremental query that allows to obtain the complete answer without redoing the work that has already been done. Second, the application program can extract information from a partial answer through the use of a secondary query, which we call a parachute query. In this paper, we present a framework for partial answers and we propose three algorithms for the evaluation of queries in presence of unavailable sources, the construction of incremental queries and the evaluation of parachute queries. 1
DB
chakrabarti98automatic
Automatic Resource list Compilation by Analyzing Hyperlink Structure and Associated Text We describe the design, prototyping and evaluation of ARC, a system for automatically compiling a list of authoritative web resources on any (sufficiently broad) topic. The goal of ARC is to compile resource lists similar to those provided by Yahoo! or Infoseek. The fundamental difference is that these services construct lists either manually or through a combination of human and automated effort, while ARC operates fully automatically. We describe the evaluation of ARC, Yahoo!, and Infoseek resource lists by a panel of human users. This evaluation suggests that the resources found by ARC frequently fare almost as well as, and sometimes better than, lists of resources that are manually compiled or classified into a topic. We also provide examples of ARC resource lists for the reader to examine.  
IR
dyreson99efficiently
Efficiently Supporting Temporal Granularities AbstractÐGranularity is an integral feature of temporal data. For instance, a person's age is commonly given to the granularity of years and the time of their next airline flight to the granularity of minutes. A granularity creates a discrete image, in terms of granules,of a (possibly continuous) time-line. We present a formal model for granularity in temporal operations that is integrated with temporal indeterminacy, or ªdon't know whenº information. We also minimally extend the syntax and semantics of SQL-92 to support mixed granularities. This support rests on two operations, scale and cast, that move times between granularities, e.g., from days to months. We demonstrate that our solution is practical by showing how granularities can be specified in a modular fashion, and by outlining a time- and space-efficient implementation. The implementation uses several optimization strategies to mitigate the expense of accommodating multiple granularities. Index TermsÐCalendar, granularity, indeterminacy, SQL-92, temporal database, TSQL2. 1
DB
boyapati00towards
Towards a Comprehensive Topic Hierarchy for News To date, a comprehensive, Yahoo-like hierarchy of topics has yet to be offered for the domain of news. The Yahoo approach of managing such a hierarchy --- hiring editorial staff to read documents and correctly assign them to topics --- is simply not practical in the domain of news. Far too many stories are written and made available online everyday.  While many Machine Learning methods exist for organising documents into topics, these methods typically require a large number of labelled training examples before performing accurately. When managing a large and ever-changing topic hierarchy, it is unlikely that there would be enough time to provide many examples per topic. For this reason, it would be useful to identify extra information within the domain of news that could be harnessed to minimise the number of labelled examples required to achieve reasonable accuracy.  To this end, the notion of a semi-labelled document is introduced. These documents, which are partially labelled by th...
AI
evers00jacob
Jacob project - Documentation The Jacob software system has been built as part of the Jacob project, which is a pilot project of the Virtual Reality Valley Twente initiative. The Jacob project investigates the application of virtual reality techniques and involves the design and construction of an animated agent in a 3-dimensional virtual environment. The project focuses on software engineering aspects, multimodal interaction, and the use of agent technology. In the current version of the Jacob system, an agent called Jacob teaches the user the Towers of Hanoi game; interaction takes place through natural language and manipulations of objects in the virtual environment. The purpose of this report is to provide information needed for further development of the Jacob system. It describes a number of technical details, including the file and directory structure, the software architecture, the event handling mechanism, and the integration of the dialogue management system.  3  Table of Contents 1 Introduction ...........
HCI
45086
Learning Nonlinear Dynamical Systems using an EM Algorithm The Expectation Maximization (EM) algorithm is an iterative procedure for maximum likelihood parameter estimation from data sets with missing or hidden variables[2]. It has been applied to system identification in linear stochastic state-space models, where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simultaneously [9]. We present a generalization of the EM algorithm for parameter estimation in nonlinear dynamical systems. The "expectation" step makes use of Extended Kalman Smoothing to estimate the state, while the "maximization" step re-estimates the parameters using these uncertain state estimates. In general, the nonlinear maximization step is difficult because it requires integrating out the uncertainty in the states. However, if Gaussian radial basis function (RBF) approximators are used to model the nonlinearities, the integrals become tractable and the maximization step can be solved via systems of linear equations.
ML
65714
Experiences with Selecting Search Engines using Meta-Search Search engines are among the most useful and high profile resources on the Internet. The problem of finding information on the Internet has been replaced with the problem of knowing where search engines are, what they are designed to retrieve and how to use them. This paper describes and evaluates SavvySearch, a meta-search engine designed to intelligently select and interface with multiple remote search engines. The primary meta-search issue examined is the importance of carefully selecting and ranking remote search engines for user queries. We studied the efficacy of SavvySearch's incrementally acquired meta-index approach to selecting search engines by analyzing the effect of time and experience on performance. We also compared the meta-index approach to the simpler categorical approach and showed how much experience is required to surpass the simple scheme.  1 Introduction  Search engines are powerful tools for assisting the otherwise unmanageable task of navigating the rapidly ex...
IR
489313
A Contract Decommitment Protocol for Automated Negotiation in Time Variant Environments Negotiation is a fundamental mechanism in distributed multi-agent systems. Since negotiation is a time-spending process, in many scenarios agents have to take into account the passage of time and to react to uncertain events. The possibility to decommit from a contract is considered a powerful technique to manage this aspect. This paper considers interactions among self-interested and autonomous agent, each with their own utility function, and focuses on incomplete information. We define a negotiation model based on asynchronous message passing in which the negotiation doesn't end when an agreement is reached but when the consequences of the contract have happened -- i.e. the action is done. In this model the agent utility functions are time dependent. We present an extension of the contract net protocol that implements the model.
Agents
muslea00selective
Selective Sampling With Redundant Views Selective sampling, a form of active learning, reduces  the cost of labeling training data by asking  only for the labels of the most informative unlabeled  examples. We introduce a novel approach  to selective sampling which we call co-testing. Cotesting  can be applied to problems with redundant  views (i.e., problems with multiple disjoint sets of  attributes that can be used for learning). We analyze  the most general algorithm in the co-testing  family, naive co-testing, which can be used with  virtually any type of learner. Naive co-testing  simply selects at random an example on which the  existing views disagree. We applied our algorithm  to a variety of domains, including three real-world  problems: wrapper induction, Web page classification,  and discourse trees parsing. The empirical  results show that besides reducing the number of  labeled examples, naive co-testing may also boost  the classification accuracy.  Introduction  In order to learn a classifier, supervised learn...
ML
flach99firstorder
A First-Order Approach to Unsupervised Learning . This paper deals with learning first-order logic rules from data lacking an explicit classification predicate. Consequently, the learned rules are not restricted to predicate definitions as in supervised Inductive Logic Programming. First-order logic offers the ability to deal with structured, multi-relational knowledge. Possible applications include first-order knowledge discovery, induction of integrity constraints in databases, multiple predicate learning, and learning mixed theories of predicate definitions and integrity constraints. One of the contributions of our work is a heuristic measure of confirmation, trading off satisfaction and novelty of the rule. The approach has been implemented in the Tertius system. The system performs an optimal best-first search, finding the k most confirmed hypotheses. It can be tuned to many different domains by setting its parameters, and it can deal either with individual-based representations as in propositional learning or with general logi...
DB
340473
Webmining: Learning from the World Wide Web : Automated analysis of the world wide web is a new challenging area relevant in many applications, e.g., retrieval, navigation and organization of information, automated information assistants, and e-commerce. This paper discusses the use of unsupervised and supervised learning methods for user behavior modeling and content-based segmentation and classification of web pages. The modeling is based on independent component analysis and hierarchical probabilistic clustering techniques.  Keywords: Webmining, unsupervised learning, hierarchical probabilistic clustering 1. 
IR
74980
Domain-Specific Knowledge Acquisition For Conceptual Sentence Analysis  The availability of on-line corpora is rapidly changing the field of natural language processing (NLP) from one dominated by theoretical models of often very specific linguistic phenomena to one guided by computational models that simultaneously account for a wide variety of phenomena that occur in real-world text. Thus far, among the best-performing and most robust systems for reading and summarizing large amounts of real-world text are knowledge-based natural language systems. These systems rely heavily on domain-specific, handcrafted knowledge to handle the myriad syntactic, semantic, and pragmatic ambiguities that pervade virtually all aspects of sentence analysis. Not surprisingly, however, generating this knowledge for new domains is ti...
ML
marsella98instructors
An Instructor's Assistant for Team-Training in Dynamic Multi-agent Virtual Worlds . The training of teams in highly dynamic, multi-agent virtual  worlds places a heavy demand on an instructor. We address the instructor  's problem with the PuppetMaster. The PuppetMaster manages a  network of monitors that report on the activities in the simulation in order  to provide the instructor with an interpretation and situation-speci#c  analysis of student behavior. The approach used to model student teams  is to structure the state space into an abstract situation-based model of  behavior that supports interpretation in the face of missing information  about agent's actions and goals.  1 Introduction  Teams of people operating in highly dynamic, multi-agentenvironments must learn to deal with rapid and unpredictable turns of events. Simulation-based training environments inhabited by synthetic agents can be e#ective in providing realistic but safe settings in which to develop skills these environments require #e.g., #14##. To faithfully capture the unpredictable multi-agent...
Agents
dlrg02background
Background Readings for Collection Synthesis  
IR
schmidt99advanced
Advanced Interaction in Context . Mobile information appliances are increasingly used in numerous  different situations and locations, setting new requirements to their interaction  methods. When the user's situation, place or activity changes, the functionality  of the device should adapt to these changes. In this work we propose a layered  real-time architecture for this kind of context-aware adaptation based on  redundant collections of low-level sensors. Two kinds of sensors are  distinguished: physical and logical sensors, which give cues from environment  parameters and host information. A prototype board that consists of eight  sensors was built for experimentation. The contexts are derived from cues using  real-time recognition software, which was constructed after experiments with  Kohonen's Self-Organizing Maps and its variants. A personal digital assistant  (PDA) and a mobile phone were used with the prototype to demonstrate  situational awareness. On the PDA font size and backlight were changed  depending...
HCI
marmasse00locationaware
Location-aware information delivery with comMotion This paper appears in the HUC 2000 Proceedings, pp.157-171, Springer-Verlag
HCI
tieu00boosting
Boosting Image Retrieval We present an approach for image retrieval using a very large number of highly selective features and efficient online learning. Our approach is predicated on the assumption that each image is generated by a sparse set of visual "causes" and that images which are visually similar share causes. We propose a mechanism for computing a very large number of highly selective features which capture some aspects of this causal structure (in our implementation there are over 45,000 highly selective features). At query time a user selects a few example images, and a technique known as "boosting" is used to learn a classification function in this feature space. By construction, the boosting procedure learns a simple classifier which only relies on 20 of the features. As a result a very large database of images can be scanned rapidly, perhaps a million images per second. Finally we will describe a set of experiments performed using our retrieval system on a database of 3000 images. 1. Introductio...
ML
236095
An Efficient Boosting Algorithm for Combining Preferences We study the problem of learning to accurately rank a set of objects by combining a given collection  of ranking or preference functions. This problem of combining preferences arises in several  applications, such as that of combining the results of different search engines, or the "collaborativefiltering  " problem of ranking movies for a user based on the movie rankings provided by other  users. In this work, we begin by presenting a formal framework for this general problem. We then  describe and analyze an efficient algorithm called RankBoost for combining preferences based on  the boosting approach to machine learning. We give theoretical results describing the algorithm's  behavior both on the training data, and on new test data not seen during training. We also describe  an efficient implementation of the algorithm for a particular restricted but common case. We next  discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment,  we used the algorithm to combine different web search strategies, each of which is a query  expansion for a given domain. The second experiment is a collaborative-filtering task for making  movie recommendations.
IR
ryu01application
Application of Moving Objects and Spatiotemporal Reasoning In order to predict future variations of moving objects which general attributes, locations, and regions of  spatial objects are changed over time, spatiotemporal data, domain knowledge, and spatiotemporal operations  are required to process together with temporal and spatial attributes of data. However, conventional researches  on temporal and spatial reasoning cannot be applied directly to the inference using moving objects, because they  have been studied separately on temporal or spatial attribute of data. Therefore, in this paper, we not only define  spatial objects in time domain but also propose a new type of moving objects and spatiotemporal reasoning  model that has the capability of operation and inference for moving objects. The proposed model is made up  of spatiotemporal database, GIS tool, and inference engine for application of spatiotemporal reasoning using  moving objects and they execute operations and inferences for moving objects. Finally, to show the applicability  of the proposed model, a proper domain is established for the battlefield analysis system to support commander's  decision making in the army operational situation and it is experimented with this domain.
AI
235064
Context Filters for Document-Based Information Filtering In this paper we propose a keyPhrase-sense disambiguation methodology called "context filters" for use in keyPhrase based information filtering systems. A context filter finds whether an input keyPhrase has occurred in the required context. Context filters consider various factors of ambiguity. Some of these factors are special to information filtering and they are handled in a structured fashion. The proposed context filters are very comprehensibile. Context filters consider varieties of contexts which are not considered in existing word-sense disambiguation methods but these are all needed for information filtering. The ideas on context filters that we report in this paper form important elements of an Instructible Information Filtering Agent that we are developing. 1. Introduction  Information filtering is the process of separating out irrelevant documents from relevant ones. Its importance has motivated several researchers to develop software agents such as SIFT, InfoScan, iAgent, ...
IR
claypool01sangam
Sangam: Modeling Transformations For Integrating Now and Tomorrow Today many application engineers struggle to not only publish their relational, object or ascii file data on the Web but to also integrate information from diverse sources, often inventing and reinventing a suite of hard-wired integration tools. A model management system that supports the specification and manipulation of not only data models and schemata, but also mappings between the different models in a generic manner has the promise of solving these issues. However, support for modeling and managing such mappings as objects remains an unsolved challenge. In our work, we propose a powerful middleware tool that successfully tackles this challenge. For this, we propose a graph-theoretic framework that allows users to explicitly model mappings between different data models as well as re-structuring within one data model. Our map metamodel is based on a set of re-usable mapping constructs that can in principle be applied on any data model described in our framework. In our work, we have tested these operators for XML and relational model mappings. Using the description of maps at the model level, mappings between specific application schemas and transformations of associated application data can be automated by our framework. Our framework guarantees the correctness of the map, of the generated transformation code, of the output data model, and of the generated application schemas, based on the correctness criteria for the map metamodel. In this paper, we also introduce the model management system    that we are developing to realize our proposed map modeling theory. With  ! we show not only the feasibility of our approach but also demonstrate the re-usability and the ease of end-to-end development of modeling strategies. To further illustrate our ide...
DB
11264
Pruning Classifiers in a Distributed Meta-Learning System JAM is a powerful and portable agent-based distributed data mining system that employs meta-learning techniques to integrate a number of independent classifiers (concepts) derived in parallel from independent and (possibly) inherently distributed databases. Although metalearning promotes scalability and accuracy in a simple and straightforward manner, brute force meta-learning techniques can result in large, inefficient and some times inaccurate meta-classifier hierarchies. In this paper we explore several techniques for evaluating classifiers and we demonstrate that meta-learning combined with certain pruning methods can achieve similar or even better performance results in a much more cost effective manner.  Keywords: classifier evaluation, pruning, metrics, distributed mining, meta-learning.   This research is supported by the Intrusion Detection Program (BAA9603) from DARPA (F30602-96-1-0311), NSF (IRI-96-32225 and CDA-96-25374) and NYSSTF (423115-445).  y  Supported in part by IBM...
ML
pirjanian99behavior
Behavior Coordination Mechanisms - State-of-the-art In behavior-based robotics the control of a robot is shared between a set of purposive perception-action units, called behaviors. Based on selective sensory information, each behavior produces immediate reactions to control the robot with respect to a particular objective, i.e., a narrow aspect of the robot's overall task such as obstacle avoidance or wall following. Behaviors with di erent and possibly incommensurable objectives may produce con icting actions that are seemingly irreconcilable. Thus a major issue in the design of behavior-based control systems is the formulation of e ective mechanisms for coordination of the behaviors' activities into strategies for rational and coherent behavior. This is known as the action selection problem (also refereed to as the behavior coordination problem) and is the primary focus of this overview paper. Numerous action selection mechanisms have been proposed over the last decade and the main objective of this document istogive a qualitative overview of these approaches. 2 1
Agents
18855
Backtracking Algorithms for Disjunctions of Temporal Constraints We extend the framework of simple temporal problems studied originally by Dechter, Meiri and Pearl to consider constraints of the form x1 \Gamma y1  r1  : : :  xn \Gamma  yn  rn , where x1 : : : xn ; y1 : : : yn are variables ranging over the real numbers, r1 : : : rn are real constants, and n  1. We have implemented four progressively more efficient algorithms for the consistency checking problem for this class of temporal constraints. We have partially ordered those algorithms according to the number of visited search nodes and the number of performed consistency checks. Finally, we have carried out a series of experimental results on the location of the hard region. The results show that hard problems occur at a critical value of the ratio of disjunctions to variables. This value is between 6 and 7. Introduction  Reasoning with temporal constraints has been a hot research topic for the last fifteen years. The importance of this problem has been demonstrated in many areas of artifici...
DB
christina99detecting
Detecting Intrusions Using System Calls: Alternative Data Models Intrusion detection systems rely on a wide variety of observable data to distinguish between legitimate and illegitimate activities. In this paper we study one such observable---sequences of system calls into the kernel of an operating system. Using system-call data sets generated by several different programs, each consisting both of normal and intrusive behavior, we compare the ability of different data-modeling methods to represent normal behavior accurately and to recognize intrusions. We compare the following methods: simple enumeration of observed sequences, methods based on relative frequencies of different sequences, a data mining technique, and Hidden Markov Models (HMMs). All of the methods perform adequately, with HMMs giving the best overall results. We discuss the factors affecting the performance of each method, and conclude that for this particular problem, weaker methods than HMMs are likely sufficient.  1 Introduction  In 1996, Forrest and others introduced a simple in...
ML
dick99mocsyn
MOCSYN: Multiobjective Core-Based Single-Chip System Synthesis In this paper, we present a system synthesis algorithm, called MOCSYN, which partitions and schedules embedded system specifications to intellectual property cores in an integrated circuit. Given a system specification consisting of multiple periodic task graphs as well as a database of core and integrated circuit characteristics, MOCSYN synthesizes real-time heterogeneous single-chip hardware-software architectures using an adaptive multiobjective genetic algorithm that is designed to escape local minima. The use of multiobjective optimization allows a single system synthesis run to produce multiple designs which trade off different architectural features. Integrated circuit price, power consumption, and area are optimized under hard real-time constraints. MOCSYN differs from previous work by considering problems unique to single-chip systems. It solves the problem of providing clock signals to cores composing a system-on-a-chip. It produces a bus structure which balances ease of layo...
ML
karlgren98stylistic
Stylistic Experiments For Information Retrieval . A discussion on various experiments to utilize stylistic variation among texts for information retrieval purposes. 1. Stylistics  Texts vary in many ways. Authors make choices when they write a text: they decide how to organize the material they have planned to introduce; they make choices between synonyms and syntactic constructions; they choose an intended audience for the text. Authors will make these choices in various ways and for various reasons: based on personal preferences, on their view of the reader, and on what they know and like about other similar texts.  A style is a consistent and distinguishable tendency to make some of these linguistic choices. Style is, on a surface level, very obviously detectable as the choice between items in a vocabulary, between types of syntactical constructions, between the various ways a text can be woven from the material it is made of. It is the information carried in a text when compared to other texts, or in a sense compared to language...
IR
wijsen99generalizing
Generalizing Temporal Dependencies for Non-Temporal Dimensions Recently, there has been a lot of interest in temporal granularity, and its applications in temporal dependency  theory and data mining. Generalization hierarchies used in multi-dimensional databases and OLAP serve a role  similar to that of time granularity in temporal databases, but they also apply to non-temporal dimensions, like  space.
DB
cassell01more
Simulated Ship Shock Tests/Trials? This paper contrasts the relative, overall utilities of ship Shock Tests/Trials and simulations. A list of advantages
HCI
454077
Reasoning over Conceptual Schemas and Queries in Temporal Databases This paper introduces a new logical formalism,  intended for temporal conceptual modelling,  as a natural combination of the wellknown  description logic DLR and pointbased  linear temporal logic with Since and  Until. The expressive power of the resulting   DLRUS logic is illustrated by providing  a systematic formalisation of the most important  temporal entity-relationship data models  appeared in the literature. We define  a query language (where queries are nonrecursive  Datalog programs and atoms are  complex DLRUS expressions) and investigate  the problem of checking query containment  under the constraints defined by  DLRUS conceptual schemas, as well as the  problems of schema satisfiability and logical  implication. Although it is shown that reasoning  in full DLRUS is undecidable, we  identify the decidable (in a sense, maximal)  fragment DLR  US  by allowing applications  of temporal operators to formulas and entities  only (but not to relation expressions). We obtain  the following hierarchy of complexity results:  (a) reasoning in DLR US with atomic  formulas is EXPTIME-complete, (b) satisfiability  and logical implication of arbitrary  DLR US formulas is EXPSPACE-complete,  and (c) the problem of checking query containment  of non-recursive Datalog queries under   DLR US constraints is decidable in 2EXPTIME.  
DB
352789
Indexing Moving Points We propose three indexing schemes for storing a set S of N points in the plane, each moving  along a linear trajectory, so that a query of the following form can be answered quickly: Given  a rectangle R and a real value t q , report all K points of S that lie inside R at time t q . We  first present an indexing structure that, for any given constant " ? 0, uses O(N=B) disk blocks,  where B is the block size, and answers a query in O((N=B)  1=2+"  + K=B) I/Os. It can also  report all the points of S that lie inside R during a given time interval. A point can be inserted  or deleted, or the trajectory of a point can be changed, in O(log  2  B N) I/Os. Next, we present a  general approach that improves the query time if the queries arrive in chronological order, by  allowing the index to evolve over time. We obtain a tradeoff between the query time and the  number of times the index needs to be updated as the points move. We also describe an indexing  scheme in which the number of I/Os required to answer a query depends monotonically on the  difference between t q and the current time. Finally, we develop an efficient indexing scheme to  answer approximate nearest-neighbor queries among moving points.  An extended abstract of this paper appeared in the Proceedings of the 19th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems.  y  Center for Geometric Computing, Department of Computer Science, Duke University, Box 90129, Durham, NC 27708--0129; pankaj@cs.duke.edu; http://www.cs.duke.edu/  pankaj. Supported in part by National Science Foundation grants EIA--9870734, EIA--9972879, and CCR--9732787, by Army Research Of fice MURI grant DAAH04-- 96--1--0013, by a Sloan fellowship, and by a grant from the U.S.-Israeli Binational Science Foundation.  z  Center ...
DB
mair99investigation
An Investigation of Machine Learning Based Prediction Systems Traditionally, researchers have used either o€-the-shelf models such as COCOMO, or developed local models using statistical techniques such as stepwise regression, to obtain software e€ort estimates. More recently, attention has turned to a variety of machine learning methods such as arti®cial neural networks (ANNs), case-based reasoning (CBR) and rule induction (RI). This paper outlines some comparative research into the use of these three machine learning methods to build software e€ort prediction systems. We brie¯y describe each method and then apply the techniques to a dataset of 81 software projects derived from a Canadian software house in the late 1980s. We compare the prediction systems in terms of three factors: accuracy, explanatory value and con®gurability. We show that ANN methods have superior accuracy and that RI methods are least accurate. However, this view is somewhat counteracted by problems with explanatory value and con®gurability. For example, we found that considerable e€ort was required to con®gure the ANN and that this compared very unfavourably with the other techniques, particularly CBR and least squares regression (LSR). We suggest that further work be carried out, both to further explore interaction between the enduser
ML
esposito00machine
Machine Learning for Intelligent Processing of Printed Documents . A paper document processing system is an information system component which transforms information on printed or handwritten documents into a computer-revisable form. In intelligent systems for paper document processing this information capture process is based on knowledge of the speci#c layout and logical structures of the documents. This article proposes the application of machine learning techniques to acquire the speci#c knowledge required byan  intelligent document processing system, named WISDOM++, that manages printed documents, such as letters and journals. Knowledge is represented by means of decision trees and #rst-order rules automatically generated from a set of training documents. In particular, an incremental decision tree learning system is applied for the acquisition of decision trees used for the classi- #cation of segmented blocks, while a #rst-order learning system is applied for the induction of rules used for the layout-based classi#cation and understanding of d...
IR
zhang99evolving
Evolving Materialized Views in Data Warehouse A data warehouse contains multiple views accessed by queries. One of the most important decisions in designing a data warehouse is the selection of materialized views for the purpose of efficiently implementing decision making. The search space for the selection of materialized views is exponentially large, therefore, heuristics have been used to search a small fraction of the space to get a near optimal solution. In this paper, we explore the use of a genetic algorithm for the selection of materialized views based on multiple global processing plans for many queries. Our experimental studies indicate that the genetic algorithm delivers better solutions than some heuristics. 1 Introduction  A Data Warehouse (DW) is a repository of integrated information available for querying and analysis. DW is an approach to the integration of data from multiple, possibly very large, distributed, heterogeneous databases and other information sources. In this paper, DW data model is based on SPJ (Sele...
ML
64765
Using Decision Tree Confidence Factors for Multiagent Control . Although Decision Trees are widely used for classification tasks, they are typically not used for agent control. This paper presents a novel technique for agent control in a complex multiagent domain based on the confidence factors provided by the C4.5 Decision Tree algorithm. Using Robotic Soccer as an example of such a domain, this paper incorporates a previously-trained Decision Tree into a full multiagent behavior that is capable of controlling agents throughout an entire game. Along with using Decision Trees for control, this behavior also makes use of the ability to reason about action-execution time to eliminate options that would not have adequate time to be executed successfully. This multiagent behavior represents a bridge between low-level and high-level learning in the Layered Learning paradigm. The newly created behavior is tested empirically in game situations. 1 Introduction  Multiagent Systems is the subfield of AI that aims to provide both principles for construction...
ML
medl98multimodal
Multimodal Man-Machine Interface for Mission Planning This paper presents a multimodal interface featuring fusion of multiple modalities for natural human-computer interaction. The architecture of the interface and the methods applied are described, and the results of the real-time multimodal fusion are analyzed. The research in progress concerning a mission planning scenario is discussed and other possible future directions are also presented.  Keywords  Multimodal interfaces, speech recognition, microphonearray, force-feedback tactile glove, gaze tracking, military maps  INTRODUCTION  Current human-machine communication systems predominantly use keyboard and mouse inputs that inadequately approximate human abilities for communication. More natural communication technologies such as speech, sight and touch, are capable of freeing computer users from the constraints of keyboard and mouse. Although they are not sufficiently advanced to be used individually for robust human-machine communication, they have adequately advanced to serve simul...
HCI
yvon94selflearning
Self-Learning Techniques for Grapheme-to-Phoneme Conversion In this article, we present a comprehensive review of various experiences with different self-learning techniques applied to the task of converting a graphemic string into the corresponding phonemic sequence. We also report some experiments carried out both with English words and French proper names. These experiments support the view that taking full advantage of the huge pronunciation dictionaries that we have been developing during the ONOMASTICA project is possible only if the traditional understanding of grapheme-tophoneme conversion as a classification problem is questioned.
ML
warshaw01facilitating
Facilitating Hard Active Database Applications Machine Interface.............................................................................................. 26 3.2.3 Concurrency Control....................................................................................................... 27 3.3 VENUSDB LANGUAGE SEMANTICS: AN EVALUATION ............................................................ 28 3.3.1 Related Work.................................................................................................................... 30 3.3.2 The Mortgage Pool Allocation Problem .......................................................................... 33 3.3.3 Quantitative Results ......................................................................................................... 37 3.3.4 Discussion and Conclusions ............................................................................................ 43  CHAPTER 4 APPLICATION SEMANTICS FOR ACTIVE LOG MONITORING  APPLICATIONS ............................................................................................45  4.1 MOTIVATION ........................................................................................................................... 46 4.1.1 Coupling Modes............................................................................................................... 47 4.1.2 Example 1 ........................................................................................................................ 48 4.2 BACKGROUND.......................................................................................................................... 50 4.2.1 LMAs, Datalog, and Confluence ..................................................................................... 50 4.2.2 Previous Work....................................................
AI
476368
A hybrid projection based and radial basis function architecture: Initial values and global optimization We introduce a hybrid architecture of projection based units and radial basis functions  as a general function estimation scheme. In particular, we introduce an optimization  scheme which has several steps and assures a convergence to a useful solution.  During training a determination whether a Gaussian unit should be removed is applied.  This results in a final architecture with much smaller number of units. The  proposed global constrained optimization does not lead to overfitting which happens  when the RBF width becomes too small, this is achieved by a regularization mechanism.  Classification and regression results are demonstrated on various benchmark data  sets and compared with several variants of RBF networks. The most striking performance  improvement is achieved on the vowel data set [5].  Keywords: Projection units, RBF Units, Hybrid Network Architecture, SMLP, Clustering, Regularization.  1 Introduction  The duality between projection-based approximation and radial kernel...
AI
joslyn99levels
Levels of Control and Closure in Complex Semiotic Systems It is natural to advance closures as "atomic" processes of universal evolution, and to analyze  this concept specifically. While real complex systems like organisms and complex mechanisms  cannot exist at either extreme of complete closure or lack of closure, nevertheless we should  consider the properties of closures in general: the introduction of boundaries; a corresponding  stability; the establishment of system autonomy and identity; and thereby the introduction of  emergent new systems of potentially new types. Our focus should move from simple physical  closure of common objects and classical self-organizing systems to semiotically closed systems  which maintain cyclic relations of perception, interpretation, decision, and action with their  environments. Thus issues arise concerning the use and interpretation of symbols, representations,  and/or internal models (whether explicit or implicit) by the system; and the syntactic,  semantic, and pragmatic relations among the sign tok...
HCI
oliveira00agents
Agents advanced features for negotiation in Electronic Commerce and Virtual Organisations formation process Electronic Commerce technology has changed the way traditional business is being done. Transactions' complexity is increased due both to the huge amount of available information and also to the environment dynamics. Moreover, Electronic Commerce has enabled the arising of new economical structures, as it is the case of Virtual Organisations.  Our research aims at providing flexible and general-purpose systems for intelligent negotiation, both for Electronic Commerce and Virtual Organisation formation.  This paper proposes an Electronic Market architecture implemented through a Multi-Agent system. This architecture includes both a specific market agent which plays the role of market coordinator, as well as agents representing the individual business partners with their own goals and strategies. We also include a sophisticated negotiation protocol through multi-criteria and distributed constraint formalisms. An online, continuous reinforcement learning algorithm has been designed to enab...
IR
eiter00data
A Data Model and Algebra for Probabilistic Complex Values . We present a probabilistic data model for complex values. More precisely, we introduce  probabilistic complex value relations, which combine the concept of probabilistic relations with  the idea of complex values in a uniform framework. We elaborate a model-theoretic definition of  probabilistic combination strategies, which has a rigorous foundation on probability theory. We  then define an algebra for querying database instances, which comprises the operations of selection,  projection, renaming, join, Cartesian product, union, intersection, and difference. We prove that  our data model and algebra for probabilistic complex values generalizes the classical relational data  model and algebra. Moreover, we show that under certain assumptions, all our algebraic operations  are tractable. We finally show that most of the query equivalences of classical relational algebra carry  over to our algebra on probabilistic complex value relations. Hence, query optimization techniques  for class...
DB
eiter00extension
Extension of the Relational Algebra to Probabilistic Complex Values We present a probabilistic data model for complex values. More precisely, we introduce probabilistic complex value relations, which combine the concept of probabilistic relations with the idea of complex values in a uniform framework. We then de ne an algebra for querying database instances, which comprises the operations of selection, projection, renaming, join, Cartesian product, union, intersection, and difference. We finally show that most of the query equivalences of classical relational algebra carry over to our algebra on probabilistic complex value relations. Hence, query optimization techniques for classical relational algebra can easily be applied to optimize queries on probabilistic complex value relations.
DB
mateer99highlyavailable
Highly-Available Firewall Service using Virtual Redirectors This paper introduces the dependable decentralised system architecture developed by the PHDS group. It describes the virtual redirector utilised by this architecture and how a decentralised network layer firewall application is implemented on it. Dependability of the design is achieved through the use of fault-tolerant protocols. The virtual redirector is implemented by a dynamic hashing algorithm, which uses load information and the fault/working status of nodes in the system. The focus of this paper is the implementation of a testbed which is used to compare the performance and availability of the decentralised firewall against a monolithic firewall with equivalent functionality.  Keywords: dependable computing, internet, security, firewall, fault-tolerant protocol  Category: research paper, student paper  Contents  1 Introduction 2 2 The PHDS Architecture 3  2.1 Autonomous Decentralised Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 The PHDS System for Inter...
Agents
414088
Intelligent Data Analysis in Medicine Extensive amounts of knowledge and data stored in medical databases require the  development of specialized tools for storing and accessing of data, data analysis, and effective  use of stored knowledge and data. This paper focuses on methods and tools for  intelligent data analysis, aimed at narrowing the increasing gap between data gathering  and data comprehension. The paper sketches the history of research that led to the  development of current intelligent data analysis techniques, discusses the need for intelligent  data analysis in medicine, and proposes a classification of intelligent data analysis  methods. The scope of the paper covers temporal data abstraction methods and data  mining methods. A selection of methods is presented and illustrated in medical problem  domains. Presently data abstraction and data mining are attracting considerable research  interest. However the two technologies, in spite of the fact that they share their central  objective, namely the intelligen...
ML
brin98anatomy
The Anatomy of a Large-Scale Hypertextual Web Search Engine In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/  To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date.  Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.
IR
446289
Quantum Treemaps and Bubblemaps for a Zoomable Image Browser This paper describes two algorithms for laying out groups of objects in a 2D space-filling manner. Quantum Treemaps are a variation on existing treemap algorithms that are designed for laying out images or other objects of indivisible (quantum) size. They build on the Ordered Treemap algorithm, but guarantees that every generated rectangle will have a width and height that are an integral multiple of an input object size. Bubblemaps also fill space with groups of quantum-sized objects, but generate nonrectangular blobs, and utilize space more efficiently.  Both algorithms have been applied to PhotoMesa, an application that supports browsing of large numbers of images. PhotoMesa uses a Zoomable User Interface with a simple interaction designed for novices and family use.  Keywords  Zoomable User Interfaces (ZUIs), Treemaps, Image Browsers, Animation, Graphics, Jazz.  
HCI
urhan01dynamic
Dynamic Pipeline Scheduling for Improving Interactive Query Performance Interactive query performance is becoming an  important criterion for online systems where  delivering query results in a timely fashion  is critical. Pipelined execution is a promising  query execution style that can produce  the initial portion of the result early and in  a continuous fashion. In this paper we propose  techniques for delivering results faster in  a pipelined query plan. We distinguish between  two cases. For cases where the tuples  in the query result are of the same importance  we propose a dynamic rate-based  pipeline scheduling policy that produces more  results during the early stages of query execution.
DB
holtman98automatic
Automatic Reclustering of Objects in Very Large Databases for High Energy Physics In the very large object database systems planned for some future particle physics experiments, typical physics analysis jobs will traverse millions of read-only objects, many more objects than fit in the database cache. Thus, a good clustering of objects on disk is highly critical to database performance. We present the implementation and performance measurements of a prototype reclustering mechanism which was developed to optimise I/O performance under the changing access patterns in a high energy physics database. Reclustering is done automatically and on-line. The methods used by our prototype differ greatly from those commonly found in proposed general-purpose reclustering systems. By exploiting some special characteristics of the access patterns of physics analysis jobs, the prototype manages to keep database I/O throughput close to the optimum throughput of raw sequential disk access.  Keywords: Object oriented databases, object clustering, object reclustering, automatic reclust...
DB
liu00implementation
Implementation and Performance Analysis of Incremental Equations for Nested Relations View materialization is an important way of improving the  performance of query processing. When an update occurs to  the source data from which a materialized view is derived, the  materialized view has to be updated so that it is consistent  with the source data. This update process is called view maintenance.  The incremental method of view maintenance, which  computes the new view using the old view and the update to  the source data, is widely preferred to full view recomputation  when the update is small in size. The small update size  becomes an important concept for measuring the cheap performance  of the incremental methods. In this paper, we investigate  what is the limit of the small update size, which we call  size limit for the incremental maintenance. When the size of  an update exceeds the limit, the incremental maintenance is no  longer cheaper than the view recomputation. The investigation  is based on incremental equations for operators in the nested  relational model...
DB
lopesdeoliveira99framework
A Framework for Designing and Implementing the User Interface of a Geographic Digital Library . Geographic data are useful for a large set of applications, such as urban planning and environmental control. These data are, however, very expensive to acquire and maintain. Moreover, their use is often restricted due to a lack of dissemination mechanisms. Digital libraries are a good approach for increasing data availability and therefore reducing costs, since they provide e#cient storage and access to large volumes of data. One major drawback to this approach is that it creates the necessity of providing facilities for a large and heterogeneous community of users to search and interact with these geographic libraries. We present a solution to this problem, based on a framework that allows the design and construction of customizable user interfaces for applications based on Geographic Digital Libraries (GDL). This framework relies on two main concepts: a geographic user interface architecture and a geographic digital library model.  Key words: Digital libraries -- User interfaces -...
DB
erwig00querybytrace
Query-By-Trace: Visual Predicate Specification In Spatio-Temporal Databases In this paper we propose a visual interface for the specification of predicates to be used in queries on spatio-temporal databases. The approach is based on a visual specification method for temporally changing spatial situations. This extends existing concepts for visual spatial query languages, which are only capable of querying static spatial situations. We outline a preliminary user interface that supports the specification on an intuitive and easily manageable level, and we describe the design of the underlying visual language. The visual notation can be used directly as a visual query interface to spatio-temporal databases, or it can provide predicate specifications that can be integrated into textual query languages leading to heterogeneous languages. Key Words Spatio-Temporal Queries, Visual Predicate Specification, Visual Database Interface 1. INTRODUCTION Spatio-temporal databases deal with spatial objects that change over time (for example, they move or they grow): cars, ...
DB
444471
OZONE: A Zoomable Interface for Navigating Ontology Information We present OZONE (Zoomable Ontology Navigator), for searching and browsing ontological  information. OZONE visualizes query conditions and provides interactive, guided browsing for  DAML (DARPA Agent Markup Language) ontologies. To visually represent objects in DAML, we  define a visual model for its classes, properties and relationships between them. Properties can be  expanded into classes for query refinement. The visual query can be formulated incrementally as  users explore class and property structures interactively. Zoomable interface techniques are  employed for effective navigation and usability.  Keywords: Ontology, DAML, Browsing, Zoomable User Interface (ZUI), Jazz, WWW.  
HCI
streitz98roomware
Roomware for Cooperative Buildings: Integrated Design of Architectural Spaces and Information Spaces . In this paper, we introduce the concepts of "cooperative buildings" and  "roomware" and place them in the context of the integrated design of real, physical,  resp. architectural spaces and virtual, resp. digital information spaces. By "roomware  " we mean computer-augmented things in rooms, like doors, walls, furniture,  and others. The general approach is detailed via examples from the i-LAND project  where we develop several "roomware" components in order to realize an interactive  information and cooperation landscape, e.g. an innovative work environment for  creativity teams. We describe the current realization of i-LAND which includes an  interactive electronic wall, an interactive table, computer-augmented chairs, and a  mechanism for assigning physical objects as representatives of information objects  in the virtual world.  Keywords. cooperative buildings, shared workspaces, physical space, architecture,  virtual world, information space, augmented reality, roomware, furnitu...
HCI
jarke99architecture
Architecture and Quality in Data Warehouses: an Extended Repository Approach This paper makes two
DB
dietterich98approximate
Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms This paper reviews five approximate statistical tests for determining whether one learning algorithm out-performs another on a particular learning task. These tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely-used statistical tests are shown to have high probability of Type I error in certain situations and should never be used. These tests are (a) a test for the difference of two proportions and (b) a paired-differences t test based on taking several random train/test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of Type I error. A fourth test, McNemar's test, is shown to have low Type I error. The fifth test is a new test, 5x2cv, based on 5 iterations of 2-fold cross-validation. Experiments show that this test also has acceptable Type I error. The paper also measures the power (ability to detect algorit...
ML
540501
Facilitating Message Exchange though Middle Agents To utilize services provided by other agents, a requesting agent needs to locate and communicate with these service providers. Specifically, in order to interoperate with the providers, the requesting agent should know: 1) the service provider's interface; 2) the ontology that defines concepts used by the provider; and 3) the agent communication language (ACL) the agent uses so that it can parse and understand the communication. Currently deployed Multi-Agent Systems (MAS) encode the interface description and the ontology within a service provider's capability description (or advertisement) that is registered with a Middle Agent; however, this assumes a common ACL between communicating agents. We demonstrate how agents can communicate with each other using a template-based shallow parsing approach to constructing and decomposing messages, thus relaxing assumptions on the ACLs and message formats used.
Agents
125615
Agent-Based Programming Language for Multi-Agent Teams This report specifies a programming language for multi-agent teams. The language aims at providing an abstract level approach to the programming of teams composed of either software or hardware agents (e.g., robots), encapsulating the lower level implementation details (e.g., graphical primitives for icon animation, robot motion primitives) and providing an abstraction level appropriate for multi-agent systems. The overall architecture of the multi-agent team, the language specifications and an example of application to robotic soccer are included. Keywords: Cooperative Robotics, Multi-Agent Systems, Distributed Artificial Intelligence, Robotic Soccer   (tactics) strategy world state WORLD  Relational Rules Behavior11 Behavior12 Behavior1M BehaviorN1 BehaviorN2 BehaviorNM act sense act sense negotiation requests (strategy prescription) Organizational State-Machine (behavior prescription, temporary behavior modification) world + team state Behavior ij : behavior i of agent j  behavior...
Agents
arge99efficient
Efficient Bulk Operations on Dynamic R-trees  In recent years there has been an upsurge of interest in spatial databases. A major issue is how to manipulate efficiently massive amounts of spatial data stored on disk in multidimensional spatial indexes (data structures). Construction of spatial indexes (bulk loading) has been studied intensively in the database community. The continuous arrival of massive amounts of new data makes it important to update existing indexes (bulk updating) efficiently. In this paper we present a simple, yet efficient, technique for performing bulk update and query operations on multidimensional indexes. We present our technique in terms of the so-called R-tree and its variants, as they have emerged as practically efficient indexing methods for spatial data. Our method uses ideas from the buffer tree lazy buffering technique and fully utilizes the available internal memory and the page size of the operating system. We give a theoretical analysis of our technique, showing that it is efficient both in terms of I/O communication, disk storage, and internal computation time. We also present the results of an extensive set of experiments showing that in practice our approach performs better than the previously best known bulk update methods with respect to update time, and that it produces a better quality index in terms of query performance. One important novel feature of our technique is that in most cases it allows us to perform a batch of updates and queries simultaneously. To be able to do so is essential in environments where queries have to be answered even while the index is being updated and reorganized.
DB
harel00characterizing
Characterizing A Media-Enhanced Classroom Server Media-enhanced classrooms are changing the way we teach and learn. Servers for such applications  differ from a typical web server in the nature and geographic spread of the user community and the type  of information accessed from the server. This paper compares the workload of the Classroom 2000 server,  a media-enhanced classroom server, with that of a typical web server. We find several similarities and  differences between the two workloads. One of the most striking differences is that the inter-reference times  for files accessed more than once from the Classroom 2000 server are not independent, as opposed to those  of a web server.  We show that the user interface presented by the Classroom 2000 server affects server workload. We  find that client-side caching improves over time. Finally, in addition to accessing lecture material for their  current courses, students also access archieved material from previous quarters.  1 Introduction  Recent improvements in computational power...
HCI
43488
A Survey of Proxy Cache Evaluation Techniques Proxy caches are increasingly used around the world to reduce bandwidth requirements and alleviate delays associated with the World-Wide Web. In order to compare proxy cache performances, objective measurements must be made. In this paper, we define a space of proxy evaluation methodologies based on source of workload used and form of algorithm implementation. We then survey recent publications and show their locations within this space. 1 Introduction  Proxy caches are increasingly used around the world to reduce bandwidth and alleviate delays associated with the World-Wide Web. This paper describes the space of proxy cache evaluation methodologies and places current research within that space. The primary contributions of this paper are threefold: 1) definition and description of the space of evaluation techniques; 2) appraisal of the di#erent methods within that space; and 3) a survey of cache evaluation techniques from the research literature. In the next section we provide backgro...
DB
hahn95integrating
Integrating Sounds and Motions in Virtual Environments Sounds are often the result of motions of virtual objects in a virtual environment. Therefore, sounds and the motions that caused them should be treated in an integrated way. When sounds and motions do not have the proper correspondence, the resultant confusion can lessen the effects of each. In this paper, we present an integrated system for modeling, synchronizing, and rendering sounds for virtual environments. The key idea of the system is the use of a functional representation of sounds, called timbre trees. This representation is used to model sounds that are parameterizable. These parameters can then be mapped to the parameters associated with the motions of objects in the environment. This mapping allows the correspondence of motions and sounds in the environment. Representing arbitrary sounds using timbre trees is a difficult process that we do not address in this paper. We describe approaches for creating some timbre trees including the use of genetic algorithms. Rendering the...
HCI
fuhr01digital
Digital Libraries: A Generic Classification and Evaluation Scheme Evaluation of digital libraries (DLs) is essential for further  development in this area. Whereas previous approaches were restricted to  certain facets of the problem, we argue that evaluation of DLs should be  based on a broad view of the subject area. For this purpose, we develop  a new description scheme using four major dimensions: data/collection,  system/technology, users, and usage. For each of these dimensions, we  describe the major attributes. Using this scheme, existing DL test beds  can be characterised. For this purpose, we have performed a survey by  means of a questionnaire, which is now continued by setting up a DL  meta-library.
IR
baker98survey
A Survey of Factory Control Algorithms which Can be Implemented in a Multi-Agent Heterarchy: Dispatching, Scheduling, and Pull This paper has not seriously addressed the question of what new algorithms are inspired by the multi-agent heterarchical paradigm, except in two cases. The Market-Driven Contract Net uses a new form of forward / backward scheduling called `cost-based forward / backwards continuum scheduling' which was inspired by the common agent concept of bidding. Also, Duffie's current work on developing agent-based deterministic simulation would be a new form of deterministic simulation as it attempts to automate the human interaction normally required for successful implementation of such systems. But in general we still have an open question of what new algorithms heterarchical agent architectures imply and what would be the performance of these new algorithms. Because a multi-agent heterarchy is a distributed computing paradigm, it would be worthwhile to investigate the communications overhead of these algorithms. Such evaluations are not generally reported. Such research would assure that not only can these algorithms be implemented in a multi-agent heterarchy, but that their implementation does not require excessive communications overhead. 6. REFERENCES [1] H. Hayashi, "The IMS International Collaborative Program," in 24th ISIR, 1993, Japan Industrial Robot Association [2] National Center for Manufacturing Sciences, FOCUS: Exceeding Partner Expectations. Ann Arbor, MI: September, 1994. [3] S. Goldman and K. Preiss, Ed., 21st Century Manufacturing Enterprise Strategy: An Industry-Led View. Bethlehem, PA: Iacocca Institute, Lehigh University, 1991. [4] S. L. Goldman, R. N. Nagel and K. Preiss, Agile Competitors and Virtual Organizations: Strategies for Enriching the Customer. New York, NY: Van Nostrand Reinhold, 1995. [5] P. T. Kidd, Agile Manufacturing: Forging New Frontiers. W...
Agents
mccallum00maximum
Maximum Entropy Markov Models for Information Extraction and Segmentation Hidden Markov models (HMMs) are a powerful  probabilistic tool for modeling sequential data,  and have been applied with success to many  text-related tasks, such as part-of-speech tagging,  text segmentation and information extraction. In  these cases, the observations are usually modeled  as multinomial distributions over a discrete  vocabulary, and the HMM parameters are set  to maximize the likelihood of the observations.  This paper presents a new Markovian sequence  model, closely related to HMMs, that allows observations  to be represented as arbitrary overlapping  features (such as word, capitalization, formatting,  part-of-speech), and defines the conditional  probability of state sequences given observation  sequences. It does this by using the  maximum entropy framework to fit a set of exponential  models that represent the probability of a  state given an observation and the previous state.  We present positive experimental results on the  segmentation of FAQ's.  1. Introdu...
IR
siddiqi98shock
Shock Graphs and Shape Matching We have been developing a theory for the generic representation of 2-D shape, where structural descriptions are derived from the shocks (singularities) of a curve evolution process, acting on bounding contours. We now apply the theory to the problem of shape matching. The shocks are organized into a directed, acyclic shock graph, and complexity is managed by attending to the most significant (central) shape components first. The space of all such graphs is highly structured and can be characterized by the rules of a shock graph grammar. The grammar permits a reduction of a shock graph to a unique rooted shock tree. We introduce a novel tree matching algorithm which finds the best set of corresponding nodes between two shock trees in polynomial time. Using a diverse database of shapes, we demonstrate our system's performance under articulation, occlusion, and changes in viewpoint. Keywords: shape representation; shape matching; shock graph; shock graph grammar; subgraph isomorphism. 1 I...
ML
chien00comparative
A Comparative Study of Version Management Schemes for XML Documents The problem of managing multiple versions for XML documents and semistructured data is of significant  interest in many DB applications and web-related services. Traditional document version control  schemes, such as RCS, suffer from the following two problems. At the logical level, they conceal the  structure of the documents by modeling them as sequences of text lines, and storing a document's evolution  as a line-edit script. At the physical level, they can incur in severe storage or processing costs  because of their inability to trade-off storage with computation. To solve these problems, we propose  version management strategies that preserve the structure of the original document, and apply and extend  DB techniques to minimize storage and processing costs. Therefore, we propose and compare three  schemes for XML version management, namely, the Usefulness-Based Copy Control, the Multiversion  B-Tree, and the Partially Persistent List Method. A common characteristic of these schemes is that  they cluster data using the notion of page usefulness, which by selectively copying current information  from obsolete pages provides for fast version reconstruction with minimal storage overhead. The cost  and performance of these version management schemes are evaluated and compared through extensive  analysis and experimentation.  
DB
bartolini01feedbackbypass
FeedbackBypass: A New Approach to Interactive Similarity Query Processing In recent years, several methods have been  proposed for implementing interactive similarity  queries on multimedia databases. Common to all  these methods is the idea to exploit user feedback  in order to progressively adjust the query parameters  and to eventually converge to an "optimal" parameter  setting. However, all these methods also  share the drawback to "forget" user preferences  across multiple query sessions, thus requiring the  feedback loop to be restarted for every new query,  i.e. using default parameter values. Not only is  this proceeding frustrating from the user's point  of view but it also constitutes a significant waste  of system resources.  In this paper we present FeedbackBypass, a new  approach to interactive similarity query processing.  It complements the role of relevance feedback  engines by storing and maintaining the query  parameters determined with feedback loops over  time, using a wavelet-based data structure (the  Simplex Tree). For each query, a favorable set of  query parameters can be determined and used to  either "bypass" the feedback loop completely for  already-seen queries, or to start the search process  from a near-optimal configuration.  FeedbackBypass can be combined well with  all state-of-the-art relevance feedback techniques  working in high-dimensional vector spaces. Its  storage requirements scale linearly with the dimensionality  of the query space, thus making even  sophisticated query spaces amenable. Experimen-  Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large ...
DB
536952
Making Context Explicit in Communicating Objects INTRODUCTION  One can speak about context only with reference to something (no definition of context out of context): the context of an object, the context of interaction, the context of a problem solving, etc. However, only the context of interaction between agents seems of interest because it is in this context that other contexts are referenced or evolve. For example, if an object, as a telephone, could provide you the context in which the called person is (free, in meeting, phone on voice recorder), you could balance your wish to establish your communication versus the availability of the called person.  Several domains have already elaborated their own working definition of context. In human-machine interaction, a context is a set of information that could be used to define and interpret a situation in which interact agents. In the context-aware applications community, the context is composed of a set of information for characterizing the situation in which interact humans, applic
HCI
wagner97artificial
Artificial Agents and Logic Programming . Artificial agents represent a new paradigm in software engineering and Artificial Intelligence. As complex software-controlled systems they are capable of flexible autonomous behavior in dynamic and unpredictable environments. Over the past few years, researchers in computer science have begun to recognise that the technology of artificial agents provides the key to solving many problems in distributed computing and intelligent control, for which traditional software engineering techniques offer no solution. The field of logic programming includes many important concepts, such as declarativity, unification, meta-logic programming, and deduction rules, from which the new technology of multiagent systems can benefit. 1 Introduction Although the idea of agent systems is intuitively appealing, and there are a number of implemented systems that claim to realize this popular idea, the basic concepts underlying these systems are often not well-understood, and no attempt is made to define t...
Agents
vogt99fusion
Fusion via a Linear Combination of Scores . We present a thorough analysis of the capabilities of the linear combination (LC) model for fusion of information retrieval systems. The LC model combines the results lists of multiple IR systems by scoring each document using a weighted sum of the scores from each of the component systems. We first present both empirical and analytical justification for the hypotheses that such a model should only be used when the systems involved have high performance, a large overlap of relevant documents, and a small overlap of nonrelevant documents. The empirical approach allows us to very accurately predict the performance of a combined system. We also derive a formula for a theoretically optimal weighting scheme for combining 2 systems. We introduce d -- the difference between the average score on relevant documents and the average score on nonrelevant documents -- as a performance measure which not only allows mathematical reasoning about system performance, but also allows the selection of w...
ML
meier01towards
Towards Proximity Group Communication Group communication will undoubtedly be a useful paradigm for many applications of wireless networking in which reliability and timeliness are requirements. Moreover, location-awareness is clearly central to mobile applications such as traffic management and smart spaces. In this paper we introduce our definition of proximity groups in which group membership depends on location and then discuss some requirements for a group management service suitable for proximity groups.
HCI
442161
Incremental Reinforcement Learning for designing Multi-Agent Systems Designing individual agents so that, when put together, they reach a given global goal is not an easy task. One solution to automatically build such large Multi-Agent Systems is to use decentralized learning: each agent learns by itself its own behavior. To that purpose, Reinforcement Learning methods are very attractive as they do not require a solution of the problem to be known before hand. Nevertheless, many hard points need to be solved for such a learning process to be viable. Among others, the credit assignement problem, combinatorial explosion and local perception of the world seem the most crucial and prevent optimal behavior. In this paper, we propose a framework based on a gradual learning of harder and harder tasks until the desired global behavior is reached. The applicability of our paradigm is tested on computer experiments where many agents have to coordinate to reach a global goal. Our results show that incremental learning leads to better performances than more classical techniques. We then discuss several improvements which could lead to even better performances.
Agents
ventura99isocrob
ISocRob - Intelligent Society of Robots Abstract. The SocRob project was born as a challenge for multidisciplinary research on broad and generic approaches for the design of a cooperative robot society, involving Control, Robotics and Artificial Intelligence researchers. A case study on Robotic Soccer played by a team of 3 robots has started two years ago and was first tested last year during RoboCup98. This experience has clearly revealed that the robotic soccer environment is a sufficiently rich, complex and dynamic testbed to study new methodologies both on Robotics and Artificial Intelligence. Therefore, the SocRob team is currently working on the SocRob project improvements and intends to compete at the World Cup of Robotic Soccer, RoboCup99, held in Stockholm, Sweden, in the middle-size league. In this paper the basic aspects of last year implementation as well as the improvements made meanwhile are briefly recalled and presented. Naturally, a special emphasis is given here to the novel solutions proposed for this year implementation, the results obtained and the expected future developments. 1
Agents
kobayashi00information
Information Retrieval on the Web In this paper we review studies on the growth of the Internet and technologies which are useful for information search and retrieval on the Web. We present data on the Internet from several dierent sources, e.g., current as well as projected number of users, hosts and Web sites. Although numerical gures vary, overall trends cited by the sources are consistent and point to exponential growth in the past and in the coming decade. As such, it is not surprising that about 85% of Internet users surveyed claim to be using search engines and search services to nd speci c information of interest. The same surveys show, however, that users are not satis ed with the performance of the current generation of search engines; the slow speed of retrieval, communication delays, and poor quality of retrieved results (e.g., noise and broken links) are commonly cited problems. We discuss the development of new techniques which are targeted to resolve some of the problems associated with Web-...
IR
basin99reflective
Reflective Metalogical Frameworks In computer science we speak of implementing a logic; this is done in a programming language, such as Lisp, called here the implementation language. We also reason about the logic, as in understanding how to search for proofs; these arguments are expressed in the metalanguage and conducted in the metalogic of the object language  being implemented. We also reason about the implementation itself, say to know it is correct; this is done in a programming logic. How do all these logics relate? This paper considers that question and more. We show that by taking the view that the metalogic is primary, these other parts are related in standard ways. The metalogic should be suitably rich so that the object logic can be presented as an abstract data type, and it must be suitably computational (or constructive) so that an instance of that type is an implementation. The data type abstractly encodes all that is relevant for metareasoning, i.e., not only the term constructing functions but also the...
ML
mankoff98bringing
Bringing People and Places Together with Dual Augmentation This paper describes initial work on the Domisilica project at Georgia Tech. We are exploring the dual augmentation of physical and virtual worlds in Domisilica and applying this novel concept to support home life beyond the boundaries of the actual house. We will demonstrate applications of dual augmentation in supporting distributed communities through a home (the Regency) and a specific appliance (CyberFridge), both of which serve as a communications link between physical and virtual worlds. Three specific types of communication are supported: direct (supporting real-time user-to-user interaction), indirect (interaction mediated by devices such as games, a refrigerator, etc), and peripheral (supporting awareness of subtle information). Keywords: Augmented reality, augmented virtuality, home, ubiquitous computing 1 Introduction Physical barriers such as time and distance have traditionally stood in the way of successfully building communities which are not located in a single geogr...
ML
zamir99grouper
Grouper: A Dynamic Clustering Interface to Web Search Results Users of Web search engines are often forced to sift through the long ordered list of document "snippets" returned by the engines. The IR community has explored document clustering as an alternative method of organizing retrieval results, but clustering has yet to be deployed on most major search engines. The NorthernLight search engine organizes its output into "custom folders" based on pre-computed document labels, but does not reveal how the folders are generated or how well they correspond to users' interests.  In this paper, we introduce Grouper -- an interface to the results of the HuskySearch meta-search engine, which dynamically groups the search results into clusters labeled by phrases extracted from the snippets. In addition, we report on the first empirical comparison of user Web search behavior on a standard ranked-list presentation versus a clustered presentation. By analyzing HuskySearch logs, we are able to demonstrate substantial differences in the number of documents f...
IR
418446
Let's Talk! Socially Intelligent Agents for Language Conversation Training Abstract — This paper promotes socially intelligent animated agents for the pedagogical task of English conversation training for native speakers of Japanese. Since student-agent conversations are realized as role-playing interactions, strong requirements are imposed on the agents ’ affective and social abilities. As a novel feature, social role awareness is introduced to animated conversational agents, that are by now strong affective reasoners, but otherwise often lack the social competence observed with humans. In particular, humans may easily adjust their behavior depending on their respective role in a social setting, whereas their synthetic pendants tend to be driven mostly by emotions and personality. Our main contribution is the incorporation of a ‘social filter program ’ to mental models of animated agents. This program may qualify an agent’s expression of its emotional state by the social context, thereby enhancing the agent’s believability as a conversational partner. Our implemented system is web-based and demonstrates socially aware animated agents in a virtual coffee shop environment. An experiment with our conversation system shows that users consider socially aware agents as more natural than agents that violate conventional practices.
Agents
rickel98taskoriented
Task-Oriented Dialogs with Animated Agents in Virtual Reality We are working towards animated agents that can carry on tutorial, task-oriented dialogs with human students. The agent's objective is to help students learn to perform physical, procedural tasks, such as operating and maintaining equipment. Although most research on such dialogs has focused on verbal communication, nonverbal communication can play many important roles as well. To allow a wide variety of interactions, the student and our agent cohabit a threedimensional, interactive, simulated mock-up of the student's work environment. The agent, Steve, can generate and recognize speech, demonstrate actions, use gaze and gestures, answer questions, adapt domain procedures to unexpected events, and remember past actions. This paper focuses on Steve's methods for generating multi-modal behavior, contrasting our work with prior work in task-oriented dialogs, multimodal explanation generation, and animated conversational characters. Introduction  We are working towards animated agents that...
Agents
4363
Least-Squares Temporal Difference Learning TD() is a popular family of algorithms for approximate policy evaluation in large MDPs. TD() works by incrementally updating the value function after each observed transition. It has two major drawbacks: it makes inefficient use of data, and it requires the user to manually tune a stepsize schedule for good performance. For the case of linear value function approximations and = 0, the Least-Squares TD (LSTD) algorithm of Bradtke and Barto (Bradtke and Barto, 1996) eliminates all stepsize parameters and improves data efficiency. This paper extends Bradtke and Barto's work in three significant ways. First, it presents a simpler derivation of the LSTD algorithm. Second, it generalizes from = 0 to arbitrary values of ; at the extreme of = 1, the resulting algorithm is shown to be a practical formulation of supervised linear regression. Third, it presents a novel, intuitive interpretation of LSTD as a model-based reinforcement learning technique.
ML
benzaken98static
Static management of integrity in object-oriented databases: Design and implementation Abstract. In this paper, we propose an efficient technique to statically manage integrity constraints in object-oriented database programming languages. We place ourselves in the context of a simplified database programming language, close to O2, in which we assume that updates are undertaken by means of methods. An important issue when dealing with constraints is that of efficiency. A nave management of such constraints can cause a severe floundering of the overall system. Our basic assumption is that the run-time checking of constraints is too costly to be undertaken systematically. Therefore, methods that are always safe with respect to integrity constraints should be proven so at compile time. The run-time checks should only concern the remaining methods. To that purpose, we propose a new approach, based on the use of predicate transformers combined with automatic theorem proving techniques, to prove the invariance of integrity constraints under complex methods. We then describe the current implementation of our prototype, and report some experiments that have been performed with it on non trivial examples. The counterpart of the problem of program verification is that of program correction. Static analysis techniques can also be applied to solve that problem. We present a systematic approach to undertake the automatic correction of potentially unsafe methods. However, the advantages of the latter technique are not as clear as those of program verification. We will therefore discuss some arguments for and against the use of method correction. 1
DB
oh00efficient
Efficient and Cost-effective Techniques for Browsing and Indexing Large Video Databases We present in this paper a fully automatic content-based approach to organizing and indexing video data. Our methodology involves three steps:  ffl Step 1: We segment each video into shots using a Camera-Tracking technique. This process also extracts the feature vector for each shot, which consists of two statistical variances V ar  BA  and V ar  OA  . These values capture how much things are changing in the background and foreground areas of the video shot.  ffl Step 2: For each video, We apply a fully automatic method to build a browsing hierarchy using the shots identified in Step 1.  ffl Step 3: Using the V ar  BA  and V ar  OA  values obtained in Step 1, we build an index table to support a variance-based video similarity model. That is, video scenes/shots are retrieved based on given values of V ar  BA  and V ar  OA  .  The above three inter-related techniques offer an integrated framework for modeling, browsing, and searching large video databases. Our experimental results indic...
IR
cox96introspective
Introspective Multistrategy Learning: Constructing a Learnung Strategy under Reasoning Failure Officer  praised dog  for barking at  object."  Enables  Detect  Drugs out FK  Initiates  Retrieval    5  6  Missing  Figure 10. Forgetting to fill the tank with gas  A=actual intention; E=expectation; Q=question; C=context; I=index; G=goal  Tank  Out of  Gas  Tank  Full  Tank  Low  Fill Tank  Should have filled  up with gas when tank low Expectation  What  Action  to Do?    KEY: G = goal; I = index;  C = context; Q = question;  E = expectation; A = actual intention  Results  At  Store  connections with related concepts. Other learning goals take multiple arguments. For instance, a knowledge differentiation goal (Cox & Ram, 1995) is a goal to determine a change in a body of knowledge such that two items are separated conceptually. In contrast, a knowledge reconciliation goal (Cox & Ram, 1995) is one that seeks to merge two items that were mistakenly considered separate entities. Both expansion goals and reconciliation goals may include or spawn a knowledge organization goal (Ram, 1993) that seeks to reorganize the existing knowledge so that it is made available to the reasoner at the appropriate time, as well as modify the structure or content of a concept itself. Such reorganization of knowledge affects the conditions under which a particular piece of knowledge is retrieved or the kinds of indexes associated with an item in memory.
ML
amin02dynamic
Dynamic Agent Population in Agent-Based Distance Vector Routing The Intelligent mobile agent paradigm can be applied to a wide variety of intrinsically parallel and distributed applications. Network routing is one such application that can be mapped to an agent-based approach. The performance of any agent-based system will depend on its agent population. Although a lot of research has been conducted on agent-based systems, little consideration has been given to the importance of agent population in dynamic networks. A large number of constituent agents can increase the resource overhead of the system, thereby impeding the overall performance of the network.
Agents
lam01broadcasting
Broadcasting Consistent Data to Read-Only Transactions from Mobile Clients In this paper, we study the data inconsistency problem in data broadcast to mobile transactions. While data items in a mobile computing system are being broadcast, update transactions may install new values for the data items. If the executions of update transactions and broadcast of data items are interleaved without any control, the transactions generated by mobile clients, called mobile transactions, may observe inconsistent data values. In this paper, we propose a new protocol, called Update-First with Order (UFO), for concurrency control between read-only mobile transactions and update transactions. We show that although the protocol is simple, all the schedules are serializable when the UFO protocol is applied. Furthermore, the new protocol possesses many desirable properties for mobile computing systems such as the mobile transactions do not need to set any lock before they read a data item from the "air" and the protocol can be applied to different broadcast algorithms. Its performance has been investigated with extensive simulation experiment. The results show that the protocol can maximize the freshness of the data items provided to mobile transactions and the broadcast overhead is not heavy especially when the arrival rate of the update  transactions is not very high.
HCI
fischer96intelligent
Intelligent agents in virtual enterprises Decreasing innovation cycles, changing market situations as well as growing specialisation in individual market segments demand new ways of economic thinking, increasingly forcing enterprises into cooperations, sometimes even with direct competitors. Presently discussed and designated as the corporate and cooperation model of the future is the so-called virtual enterprise. In this paper, we advocate the use of intelligent agents as a useful metaphor and as a software engineering methodology for the design and the operation of virtual enterprises. We focus on how agents can support the cooperative process of setting up virtual enterprises through the Internet by performing tasks such as presentation, information retrieval and extraction, and the participation in auctions in electronic markets. This paper does not describe completed research; it rather offers a perspective of the high potential of agent-based technology for one of tomorrow's key industrial areas by presenting the main objectives of the new research project AVE
Agents
levene00computing
Computing the Entropy of User Navigation in the Web Navigation through the web, colloquially known as “surfing”, is one of the main activities of users during web interaction. When users follow a navigation trail they often tend to get disoriented in terms of the goals of their original query and thus the discovery of typical user trails could be useful in providing navigation assistance. Herein we give a theoretical underpinning of user navigation in terms of the entropy of an underlying Markov chain modelling the web topology. We present a novel method for online incremental computation of the entropy and a large deviation result regarding the length of a trail to realise the said entropy. We provide an error analysis for our estimation of the entropy in terms of the divergence between the empirical and actual probabilities. We then indicate applications of our algorithm in the area of web data mining. Finally, we present an extension of our technique to higher-order Markov chains by a suitable reduction of a higher-order Markov chain model to a first-order one. Key words. Web user navigation, Web data mining, navigation problem, Markov chain, entropy 1
IR
weiss00building
On Building Flexible Agents . This paper focuses on the challenge of building technical  agents that act exibly in modern computing and information environments.  It is argued that existing agent architectures tend to inherently  limit an agent's exibility because they imply a discrete social and cognitive  behavior space. A generic constraint-centered architectural framework  is proposed that aims at enabling agents to act in a continuous  behavior space and thus to achieve higher exibility.  1 Introduction  Modern computing platforms and information environments are becoming more and more distributed, large, open, dynamic, and heterogeneous. Computers are no longer stand-alone systems, but have become tightly connected both with each other and their users. The increasing technological complexity of such platforms and environments goes together with an increasing complexity of their potential applications. This development has led to a rapidly growing research and application interest in agents as a powerful ...
Agents
33084
Learning Maps for Indoor Mobile Robot Navigation Autonomous robots must be able to learn and maintain models of their environments. Research on mobile robot navigation has produced two major paradigms for mapping indoor environments: grid-based and topological. While grid-based methods produce accurate metric maps, their complexity often prohibits efficient planning and problem solving in large-scale indoor environments. Topological maps, on the other hand, can be used much more efficiently, yet accurate and consistent topological maps are often difficult to learn and maintain in large-scale environments, particularly if momentary sensor data is highly ambiguous. This paper describes an approach that integrates both paradigms: grid-based and topological. Grid-based maps are learned using artificial neural networks and naive Bayesian integration. Topological maps are generated on top of the grid-based maps, by partitioning the latter into coherent regions. By combining both paradigms, the approach presented here gains advantages from both worlds: accuracy/consistency and efficiency. The paper gives results for autonomous exploration, mapping and operation of a mobile robot in populated multi-room environments.
AI
racine97maintaining
Maintaining Unstructured Case Bases  With the dramatic proliferation of case based reasoning systems in commercial applications, many case bases are now becoming legacy systems. They represent a significant portion of an organization's assets, but they are large and difficult to maintain. One of the contributing factors is that these case bases are often large and yet unstructured; they are represented in natural language text. Adding to the complexity is the fact that the case bases are often authored and updated by different people from a variety ofknowledge sources, making it highly likely for a case base to contain redundant and inconsistent knowledge. In this paper, we present methods and a system for maintaining large and unstructured case bases. We focus on two difficult problems in case-base maintenance: redundancy and inconsistency detection. These two problems are particularly pervasive when one deals with an unstructured case base. We will discuss both algorithms and a system for solving these problems. As the ability tocontain the knowledge acquisition problem is of paramount importance, our methods allow one to express relevant domain expertise for detecting both redundancy and inconsistency naturally and effortlessly. Empirical evaluations of the system prove the effectiveness of the methods in several large domains.  
ML
thomas98wearable
A Wearable Computer System with Augmented Reality to Support Terrestrial Navigation To date augmented realities are typically operated in only a small defined area, in the order of a large room. This paper reports on our investigation into expanding augmented realities to outdoor environments. The project entails providing visual navigation aids to users. Awearable computer system with a see-through display, digital compass, and a differential GPS are used to provide visual cues while performing a standard orienteering task. This paper reports the outcomes of a set of trials using an off the shelf wearable computer, equipped with a custom built navigation software package, "map-in-the-hat." 
HCI
dong99maintaining
Maintaining Transitive Closure of Graphs in SQL It is common knowledge that relational calculus and even SQL are not expressive enough to express recursive queries such as the transitive closure. In a real database system, one can overcome this problem by storing a graph together with its transitive closure and maintaining the latter whenever updates to the former occur. This leads to the concept of an incremental evaluation system, or IES. Much is already known about the theory of IES but very little has been translated into practice. The purpose of this paper is to ll in this gap by providing a gentle introduction to and an overview of some recent theoretical results on IES. The introduction is through the translation into SQL of three interesting positive maintenance results that have practical importance { the maintenance of the transitive closure of acyclic graphs, of undirected graphs, and of arbitrary directed graphs. Interestingly, these examples also allow ustoshow the relationship between power and cost in the incremental maintenance of database queries. 1
DB
murray98kaleidoquery
Kaleidoquery: A Visual Query Language for Object Databases In this paper we describe Kaleidoquery, a visual query language for object databases with the same expressive power as OQL. We will describe the design philosophy behind the filter flow nature of Kaleidoquery and present each of the language's constructs, giving examples and relating them to OQL. The Kaleidoquery language is described independent of any implementation details, but a brief description of a 3D interface currently under construction for Kaleidoquery is presented. The queries in this implementation of the language are translated into OQL and then passed to the object database O 2 for evaluation.  KEYWORDS: Visual query language, OQL, object databases, three-dimensional interface.  INTRODUCTION  The lack of a generally accepted and widely supported query language has probably had a significant effect in slowing the uptake of early commercial object-oriented databases. However, the emergence of the Object Query Language (OQL) which is being standardised by the Object Databas...
DB
83259
A Description Logic with Transitive and Inverse Roles and Role Hierarchies transitive roles play an important rôle in the adequate representation of aggregated objects: they allow these objects to be described by referring to their parts without specifying a level of decomposition. In [Horrocks & Gough, 1997], the Description Logic (DL) ALCH R + is presented, which extends ALC with transitive roles and a role hierarchy. It is argued in [Sattler, 1998] that ALCH R + is well-suited to the representation of aggregated objects in applications that require various part-whole relations to be distinguished, some of which are transitive. For example, a medical knowledge base could contain the following entries defining two different parts of the brain, namely the gyrus and the cerebellum. In contrast to a gyrus, a cerebellum is an integral organ and, furthermore, a functional component of the brain. Hence the role is component (which is a non-transitive sub-role of is part) is used to describe the relation between the brain and the cerebellum: is component ⊑ is part gyrus:= (∀consists.brain mass)  ⊓ (∃is part.brain) cerebellum:= organ ⊓ (∃is component.brain) However, ALCH R + does not allow the simultaneous description of parts by means of the whole to which they belong and of wholes by means of their constituent parts: one or other is possible, but not both. To overcome this limitation, we present the DL ALCHI R + which extends ALCH R + with inverse (converse) roles, allowing, for example, the use of has part as well as is part. 1 Using ALCHIR +, we can define a tumorous brain as: tumorous brain:= brain ⊓ (tumorous ⊔ (∃has part.tumorous)) Part of this work was carried out while being a guest at IRST,
AI
brewington99mobile
Mobile Agents in Distributed Information Retrieval A mobile agent is an executing program that can migrate during execution from machine to machine in a heterogeneous network. On each machine, the agent interacts with stationary service agents and other resources to accomplish its task. Mobile agents are particularly attractive in distributed informationretrieval applications. By moving to the location of an information resource, the agent can search the resource locally, eliminating the transfer of intermediate results across the network and reducing end-toend latency. In this chapter, we rst discuss the strengths of mobile agents, and argue that although none of these strengths are unique to mobile agents, no competing technique shares all of them. Next, after surveying several representative mobile-agent systems, we examine one speci c information-retrieval application, searching distributed collections of technical reports, and consider how mobile agents can be used to implement this application e ciently and easily. Then we spend the bulk of the chapter describing two planning services that allow mobile agents to deal with dynamic network environments and information resources: (1) planning algorithms that let an agent choose the best migration path through the network, given its current task and the current network conditions, and (2) planning algorithms that tell an agent howto observe achanging set of documents in a way that detects changes as soon as possible while minimizing overhead. Finally, we consider the types of errors that can occur when information from multiple sources is merged and ltered, and argue that the structure of a mobile-agent application determines the extent to which these errors a ect the nal result. 1
Agents
ashish98optimizing
Optimizing Information Agents by Selectively Materializing Data We present an approach for optimizing the performance  of information agents by materializing useful information  . A critical problem with information agents, particularly  those gathering and integrating information  from Web sources is a high query response time. This  is because the data needed to answer user queries is  present across several differentWeb sources (and in  several pages within a source) and retrieving,extracting  and integrating the data is time consuming. Weaddress  this problem by materializing useful classes of information  and defining them as auxiliary data sources  for the information agent. The key challenge here is to  identify the contentandschema of the classes of information  that would be useful to materialize. We present  an algorithm that identifies such classes by analyzing  patterns in user queries. We describe an implementation  of our approach and experiments in progress. We  also discuss other important problems that we will address  in optimizing information agents.
AI
schnattinger98qualitybased
Quality-Based Learning We introduce a methodology for automating the maintenance of domain-specific taxonomies based on natural language text understanding. A given ontology is incrementally updated as new concepts are acquired from real-world texts. The acquisition process is centered around the linguistic and conceptual "quality" of various forms of evidence underlying the generation and refinement of concept hypotheses. On the basis of the quality of evidence, concept hypotheses are ranked according to credibility and the most credible ones are selected for assimilation into the domain knowledge base. Appeared in:  ECAII'98 - Proceedings of the 13th Biennial European Conference on Artificial Intelligence/em¿, 23-28 August 1998, Brighton Centre, Brighton, UK.pp.160-164 c fl 1998 ECAI 98. 13th European Conference on Artificial Intelligence  Edited by Henri Prade Published in 1998 by John Wiley & Sons, Ltd.  Quality-Based Learning  Klemens Schnattinger and Udo Hahn  1 Abstract. We introduce a formal model f...
ML
ipeirotis01probe
Probe, Count, and Classify: Categorizing Hidden-Web Databases The contents of many valuable web-accessible databases are only accessible through search interfaces and are hence invisible to traditional web "crawlers." Recent studies have estimated the size of this "hidden web" to be 500 billion pages, while the size of the "crawlable" web is only an estimated two billion pages. Recently, commercial web sites have started to manually organize web-accessible databases into Yahoo!-like hierarchical classification schemes. In this paper, we introduce a method for automating this classification process by using a small number of query probes. To classify a database, our algorithm does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of our technique over collections of real documents, including over one hundred web-accessible databases. Our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases.  1. 
IR
jakobi98running
Running Across the Reality Gap: Octopod Locomotion Evolved in a Minimal Simulation . This paper describes experiments in which neural network control architectures were evolved in minimal simulation for an octopod robot. The robot is around 30cm long and has 4 infra red sensors that point ahead and to the side, various bumpers and whiskers, and ten ambient light sensors positioned strategically around the body. Each of the robot's eight legs is controlled by two servo motors, one for movement in the horizontal plane, and one for movement in the vertical plane, which means that the robots motors have a total of sixteen degrees of freedom. The aim of the experiments was to evolve neural network control architectures that would allow the robot to wander around its environment avoiding objects using its infra-red sensors and backing away from objects that it hits with its bumpers. This is a hard behaviour to evolve when one considers that in order to achieve any sort of coherent movement the controller has to control not just one or two motors in a coordinated fashion bu...
AI
hu01overview
An Overview of World Wide Web Search Technologies With over 800 million pages covering most areas of human endeavor, the World  Wide Web is fertile ground for information retrieval. Numerous search technologies  have been applied to Web searches, and the dominant search method has yet to be  identified. This chapter provides an overview of existing Web search technologies and  classifies them into six categories: (i) hyperlink exploration, (ii) information retrieval,  (iii) metasearches, (iv) SQL approaches, (v) content-based multimedia searches, and  (vi) others. A comparative study of some major commercial and experimental search  services is presented, and some future research directions for Web searches are suggested.  Keywords: Survey, World Wide Web, Searches, Search Engines, and Information Retrieval.  1. 
IR
cai01geovibe
GeoVIBE: A Visual Interface to Geographic Digital Library This paper explores the possibilities of visualizing document similarities and differences in both spatial and topical domains. Building on previous studies of geographical information retrieval and textual information retrieval (IR) systems, we report on the development of an information browsing tool, GeoVIBE. The system consists of two types of browsing windows, GeoView and VibeView, that work in coordination for visual navigation in the document space. GeoView imposes a geographical order to the document space based on the idea of hypermaps where "icons" and "footprints" may be embedded in maps as the clickable hotspots linking to relevant documents.
IR
gotoh00topicbased
Topic-Based Mixture Language Modelling This paper describes an approach for constructing a mixture of language models based  on simple statistical notions of semantics using probabilistic models developed for information  retrieval. The approach encapsulates corpus-derived semantic information  and is able to model varying styles of text. Using such information, the corpus texts  are clustered in an unsupervised manner and a mixture of topic-specific language models  is automatically created. The principal contribution of this work is to characterise  the document space resulting from information retrieval techniques and to demonstrate  the approach for mixture language modelling. A comparison is made between manual  and automatic clustering in order to elucidate how the global content information is  expressed in the space. We also compare (in terms of association with manual clustering  and language modelling accuracy) alternative term-weighting schemes and the  effect of singular value decomposition dimension reduction (...
IR
ambroszkiewicz97model
A Model of BDI-Agent in Game-Theoretic Framework . A model of BDI--agent in game--theoretic framework is presented. The desire is represented as agent's goal to achieve a maximum level of utility. A reasoning process based on agent's rational behavior  is proposed. This process determines agent's intention. It is also shown how to use the backward induction consistently with the assumption of the common knowledge of rationality. 1 Introduction  We are going to discuss the following problem:  How does a rational agent use its knowledge in decision making ?  Since the problem is general, we put it in a game--theoretic framework. In the theory of games, agent's rationality is understood as a way of maximizing the utility of the agent relatively to its knowledge. The knowledge may concern the game that is to be played as well as the agents participating in a play. The main task of the paper is to model BDI-agent that is supposed to live  in the world of dynamic games. Agent's belief is identified with the knowledge about the game and abo...
Agents
holden99adaptive
Adaptive Fuzzy Expert System for Sign Recognition The Hand Motion Understanding (HMU) system is a vision-based Australian sign  language recognition system that recognises static and dynamic hand signs. It uses  a visual hand tracker to extract 3D hand configuration data from a visual motion  sequence, and a classifier that recognises the changes of these 3D kinematic data as  a sign. This paper presents the HMU classifier that uses an adaptive fuzzy inference  engine for sign recognition. Fuzzy set theory allows the system to express the sign  knowledge in natural and imprecise descriptions. The HMU classifier has an  adaptive engine that trains the system to be adaptive to the errors caused by the  tracker or the motion variations exhibited amongst the signers. The HMU system is  evaluated with 22 static and dynamic Auslan signs, and recognised 20 signs before  training, and 21 signs after training of the HMU classifier.  Keywords: Sign Language, Sign Recognition, Fuzzy Logic, Adaptive Fuzzy  System, Expert System.  1. Introduction ...
HCI
421280
Will We Have a Wet Summer? Soft Computing Models for Long-term Rainfall Forecasting Long-term rainfall prediction is very important to countries thriving on agro-based economy. In general, climate and rainfall are highly non-linear phenomena in nature giving rise to what is known as "butterfly effect". The parameters that are required to predict the rainfall are enormously complex and subtle so that uncertainty in a prediction using all these parameters is enormous even for a short period. Soft computing is an innovative approach to construct computationally intelligent systems that are supposed to possess humanlike expertise within a specific domain, adapt themselves and learn to do better in changing environments, and explain how they make decisions. Unlike conventional artificial intelligence techniques the guiding principle of soft computing is to exploit tolerance for imprecision, uncertainty, robustness, partial truth to achieve tractability, and better rapport with reality (Zadeh 1998). In this paper, we analysed 87 years of rainfall data in Kerala state, the southern part of Indian Peninsula situated at latitude-longitude pairs (8029 ' N - 76057 ' E). We attempted to train 5 soft computing based prediction models with 40 years of rainfall data. For performance evaluation, network predicted outputs were compared with the actual rainfall data. Simulation results reveal that soft computing techniques are promising and efficient.
ML
124100
Classification on Pairwise Proximity Data We investigate the problem of learning a classification task on data represented in terms of their pairwise proximities. This representation does not refer to an explicit feature representation of the data items and is thus more general than the standard approach of using Euclidean feature vectors, from which pairwise proximities can always be calculated. Our first approach is based on a combined linear embedding and classification procedure resulting in an extension of the Optimal Hyperplane algorithm to pseudo-Euclidean data. As an alternative we present another approach based on a linear threshold model in the proximity values themselves, which is optimized using Structural Risk Minimization. We show that prior knowledge about the problem can be incorporated by the choice of distance measures and examine different metrics w.r.t. their generalization. Finally, the algorithms are successfully applied to protein structure data and to data from the cat's cerebral cortex. They show bette...
ML
536016
Inferring Web Communities Through Relaxed Cocitation and Dense Bipartite Graphs Community forming is one of the important activity in the Web. The Web harbors a large number of communities. A community is a group of content creators that manifests itself as a set of interlinked pages. Given a large collection of pages our aim is to find potential communities in the Web. In the literature, Ravi Kumar et al. [18] proposed a trawling method to find potential communities by abstracting a core of the community as a group of pages that form a complete bipartite graph (CBG) (web-page as a node and link as an edge between two nodes). The trawling approach extracts a small group of pages that form a CBG, which is a signature of a potential community.
IR
107700
Variorum: A Multimedia-Based Program Documentation System Conventional software documentation systems are mostly based on textutal descriptions that explain or annotate the program's source code. Typically they also support interactive browsing of high-level control flows, and name-based searching of program primitives such as variable declarations and function definitions. Because these systems rely solely on texts, it is difficult for program authors to describe overall algorithm structures and detailed implementation considerations of the programs in an interactive and flexible fashion. Variorum is a novel software documentation system that allows program authors to record the process of "walking through" their own code using multimedia technology, specifically, text, audio, and digital pen drawing. This approach greatly improves the interactivity and flexibility in the software documentation process. In addition, to broaden its applicability and to reduce the implementation complexity,  Variorum is designed to inter-operate with the WWW t...
AI
ware00rotating
Rotating Virtual Objects with Real Handles Times for virtual object rotations reported in the literature are of the order of ten seconds or more and this is far longer than it takes to manually orient a "real" object, such as a cup. This is a report of a series of experiments designed to investigate the reasons for this difference and to help design interfaces for object manipulation. The results suggest that two major factors are important. Having the hand physically in the same location as the virtual object being manipulated is one. The other is based on whether the object is being rotated to a new, randomly determined orientation, or is always rotated to the same position. Making the object held in the hand have the same physical shape as the object being visually manipulated was not found to be a significant factor. The results are discussed in the context of interactive virtual environments. Categories and Subject Descriptors: H.1.2 [Models and Principles]: User/Machine Systems -- human factors; I.3.6 [Computer Graphics]:...
HCI
300232
Exploring auction-based leveled-commitment contracting - Part III: Vickrey-type auctioning A key problem addressed in the area of multiagent systems is the automated  assignment of multiple tasks to executing agents. The automation  of multiagent task assignment requires that the individual agents (i) use  a common protocol that prescribes how they have to interact in order to  come to an agreement and (ii) fix their final agreement in a contract that  specifies the commitments resulting from the assignment on which they  agreed. This report describes a novel approach to automated task assignment  in multiagent systems that is based on an auction-based protocol  and on leveled commitment contracting. This approach is applicable in a  broad range of realistic scenarios in which knowledge-intensive negotiation  among agents is not feasible and in which future environmental changes  may require agents to breach their contracts.  1 Introduction  The area of multiagent systems (e.g., [5, 8, 16]), which is concerned with systems composed of technical entities called agents that in...
Agents
glover99architecture
Architecture of a Metasearch Engine that Supports User Information Needs When a query is submitted to a metasearch engine, decisions are made with respect to the underlying search engines to be used, what modifications will be made to the query, and how to score the results. These decisions are typically made by considering only the user's keyword query, neglecting the larger information need. Users with specific needs, such as "research papers" or "homepages," are not able to express these needs in a way that affects the decisions made by the metasearch engine. In this paper, we describe a metasearch engine architecture that considers the user's information need for each decision. Users with different needs, but the same keyword query, may search different sub-search engines, have different modifications made to their query, and have results ordered differently. Our architecture combines several powerful approaches together in a single general purpose metasearch engine.  1 Introduction  Current metasearch engines make several decisions on behalf of the use...
IR
hollerer99exploring
Exploring MARS: Developing Indoor and Outdoor User Interfaces to a Mobile Augmented Reality System We describe an experimental mobile augmented reality system (MARS) testbed that employs different user interfaces to allow outdoor and indoor users to access and manage information that is spatially registered with the real world. Outdoor users can experience spatialized multimedia presentations that are presented on a head-tracked, see-through, head-worn display used in conjunction with a hand-held pen-based computer. Indoor users can get an overview of the outdoor scene and communicate with outdoor users through a desktop user interface or a head- and hand-tracked immersive augmented reality user interface.  Key words: Augmented Reality. Wearable Computing. Mobile Computing. Hypermedia.  GPS.  1 Introduction  As computers increase in power and decrease in size, new mobile and wearable computing applications are rapidly becoming feasible, promising users access to online resources always and everywhere. This new flexibility makes possible a new kind of application---one that exploits ...
HCI
mannila99prediction
Prediction with Local Patterns using Cross-Entropy Sets of local patterns in the forms of rules and co-occurrence counts are produced by many data mining methods such as association rule algorithms. While such patterns can yield useful insights it is not obvious how to synthesize local sparse information into a coherent global predictive model. We study the use of a cross-entropy approach to combining local patterns. Each local pattern is viewed as a constraint on an appropriate high-order joint distribution of interest. Typically, a set of patterns returned by a data mining algorithm under-constrains the high-order model. The cross-entropy criterion is used to select a specific distribution in this constrained family relative to a prior. We review the iterative-scaling algorithm which is an iterative technique for finding a joint distribution given constraints. We then illustrate the application of this method to two specific problems. The first problem is combining information about frequent itemsets. We show that the cross-entropy a...
ML
348716
A Cognitive Bias Approach to Feature Selection and Weighting for Case-Based Learners . Research in psychology, psycholinguistics, and cognitive science has discovered and examined numerous psychological constraints on human information processing. Short term memory limitations, a focus of attention bias, and a preference for the use of temporally recent information are three examples. This paper shows that psychological constraints such as these can be used e#ectively as domain-independent sources of bias to guide feature set selection and weighting for case-based learning algorithms.  We first show that cognitive biases can be automatically and explicitly encoded into the baseline instance representation: each bias modifies the representation by changing features, deleting features, or modifying feature weights. Next, we investigate the related problems of cognitive bias selection and cognitive bias interaction for the feature weighting approach. In particular, we compare two cross-validation algorithms for bias selection that make di#erent assumptions about the indep...
ML
530219
Selection of Behavioral Parameters: Integration of Discontinuous Switching via Case-Based Reasoning with Continuous Adaptation via Learning Momentum This paper studies the effects of the integration of two learning algorithms, Case-Based Reasoning (CBR) and Learning Momentum (LM), for the selection of behavioral parameters in real-time for robotic navigational tasks. Use of CBR methodology in the selection of behavioral parameters has already shown significant improvement in robot performance [3, 6, 7, 14] as measured by mission completion time and success rate. It has also made unnecessary the manual configuration of behavioral parameters from a user. However, the choice of the library of CBR cases does affect the robot's performance, and choosing the right library sometimes is a difficult task especially when working with a real robot. In contrast, Learning Momentum does not depend on any prior information such as cases and searches for the &quot;right &quot; parameters in real-time. This results in high mission success rates and requires no manual configuration of parameters, but it shows no improvement in mission completion time [2]. This work combines the two approaches so that CBR discontinuously switches behavioral parameters based on given cases whereas LM uses these parameters as a starting point for the real-time search for the &quot;right&quot; parameters. The integrated system was extensively evaluated on both simulated and physical robots. The tests showed that on simulated robots the integrated system performed as well as the CBR only system and outperformed the LM only system, whereas on real robots it significantly outperformed both CBR only and LM only systems.
ML
257148
TouchCounters: Designing Interactive Electronic Labels for Physical Containers We present TouchCounters, an integrated system of electronic modules, physical storage containers, and shelving surfaces for the support of collaborative physical work. Through physical sensors and local displays, TouchCounters record and display usage history information upon physical storage containers, thus allowing access to this information during the performance of real-world tasks. A distributed communications network allows this data to be exchanged with a server, such that users can access this information from remote locations as well.  Based upon prior work in ubiquitous computing and tangible interfaces, TouchCounters incorporate new techniques, including usage history tracking for physical objects and multi-display visualization. This paper describes the components, interactions, implementation, and conceptual approach of the TouchCounters system.  Keywords  Tangible interfaces, ubiquitous computing, distributed sensing, visualization  INTRODUCTION  For decades, research i...
HCI
fegaras98query
Query Unnesting in Object-Oriented Databases There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way. This paper presents a new query unnesting algorithm that generalizes many unnesting techniques proposed recently in the literature. Our system is capable of removing  any form of query nesting using a very simple and efficient algorithm. The simplicity of the system is due to the use of the monoid comprehension calculus as an intermediate form for OODB queries. The monoid comprehension calculus treats op...
DB
452225
Abstractions and Infrastructures for the Design and Development of Mobile Agent Organizations Internet applications can take advantage of a paradigm based on  autonomous and mobile agents. However, suitable abstractions  and infrastructures are required for the effective engineering of  such applications. In this paper, we argue that a conceptual  framework for context-dependent coordination, supported by an  infrastructure based on programmable media, can promote a  modular and easy to manage approach to the design and  development of mobile agent applications in terms of  computational organizations. The MARS coordination  infrastructure is presented as an implementation of a  coordination infrastructure promoting context-dependent  coordination. A case study in the area of workflow management is  introduced to clarify the concepts presented.  Keywords: Mobile Agents, Agent Organizations, Coordination  Infrastructures, Agent-oriented Software Engineering.  1 
Agents
12060
On Using Regression for Range Data Fusion In the paper, we consider an occupancy-based approach for range data fusion, as it is used in mobile robotics. We identify two major problems of this approach. The first problem deals with the combination rule which in many cases assumes the independence of range data, contrary to the usual situation. The second problem concerns the redundancy of stored and processed data, which results from using the grid representation of the occupancy function and which is the main obstacle to building 3D occupancy world models. We propose a solution to these problems by proposing a new range data fusion technique based on regression. This technique uses the evidence theory in assigning occupancy values, which we argue is advantageous for fusion, and builds the occupancy function by fitting the sample data provided by a sensor with a piecewise linear function. Having developed a general framework for our approach, we apply it to building 3D world models from visual range data, where 3D world models ...
AI
fegaras99voodoo
VOODOO: A Visual Object-Oriented Database Language for ODMG OQL This paper presents a simple and effective visual language to express ODMG OQL queries. The  language is expressive enough to allow most types of query nesting, aggregation, universal and existential  quantifications, group-by, and sorting, and at the same time is uniform and very simple to learn and  use. Our visual language is strongly typed in the sense that queries constructed in our system are always  type-correct. In addition, there is sufficient type information displayed by the system that guides every  stage of the query construction. The main difference between our language and other related visual query  languages is that we use only one generic visual construct, called a template, instead of inventing a new  one for each OQL syntactic feature.  1 Introduction  Query and data visualizations are key components of many commercial relational database systems, such as Microsoft Access and Paradox. Object-oriented databases offer excellent opportunities for visual data browsing b...
DB
harik99parameterless
A Parameter-Less Genetic Algorithm From the user's point of view, setting the parameters of a genetic algorithm (GA) is far from a trivial task. Moreover, the user is typically not interested in population sizes, crossover probabilities, selection rates, and other GA technicalities. He is just interested in solving a problem, and what he would really like to do, is to hand-in the problem to a blackbox algorithm, and simply press a start button. This paper explores the development of a GA that fulfills this requirement. It has no parameters whatsoever. The development of the algorithm takes into account several aspects of the theory of GAs, including previous research works on population sizing, the schema theorem, building block mixing, and genetic drift. 
ML
56124
Document Categorization and Query Generation on the World Wide Web Using WebACE We present WebACE, an agent for exploring and categorizing documents on the World Wide Web based on a user profile. The heart of the agent is an unsupervised categorization of a set of documents, combined with a process for generating new queries that is used to search for new related documents and for filtering the resulting documents to extract the ones most closely related to the starting set. The document categories are not given a priori. We present the overall architecture and describe two novel algorithms which provide significant improvement over Hierarchical Agglomeration Clustering and AutoClass algorithms and form the basis for the query generation and search component of the agent. We report on the results of our experiments comparing these new algorithms with more traditional clustering algorithms and we show that our algorithms are fast and scalable. y  Authors are listed alphabetically.  1 Introduction  The World Wide Web is a vast resource of information and services t...
IR
83140
Improving Performance Of Case-Based Classification Using Context-Based Relevance Classification involves associating instances with particular classes by maximizing intra-class similarities and minimizing inter-class similarities. Thus, the way similarity among instances is measured is crucial for the success of the system. In case-based reasoning, it is assumed that similar problems have similar solutions. The case-based approach to classification is founded on retrieving cases from the case base that are similar to a given problem, and associating the problem with the class containing the most similar cases. Similarity-based retrieval tools can advantageously be used in building flexible retrieval and classification systems. Case-based classification uses previously classified instances to label unknown instances with proper classes. Classification accuracy is affected by the retrieval process -- the more relevant the instances used for classification, the greater the accuracy. The paper presents a novel approach to case-based classification. The algorithm is bas...
ML
zhang99query
Query Formulation from High-level Concepts for Relational Databases A new query formulation system based on a semantic graph model is presented. The graph provides a semantic model for the data in the database with userdefined relationships. The query formulator allows users to specify their requests and constraints in highlevel concepts. The query candidates are formulated based on the user input by a graph search algorithm and ranked according to a probabilistic information measure. English-like query descriptions can also be provided for users to resolve ambiguity when multiple queries are formulated from a user input. For complex queries, we introduce an incremental approach, which assists users to achieve a complex query goal by formulating a series of simple queries. A prototype system with a multimodal interface using the high-level query formulation techniques has been implemented on top of a cooperative database system (CoBase) at UCLA.  1 Introduction  Many database applications require users to formulate ad-hoc queries instead of invocation ...
DB
502516
Storing and Querying Multiversion XML Documents using Durable Node Numbers Managing multiple versions of XML documents represents an important problem for many traditional applications, such as software configuration control, as well as new ones, such as link permanence of web documents. Research on managing multiversion XML documents seeks to provide efficient and robust techniques for storing, retrieving and querying such documents. In this paper, we present a novel approach to version management that achieves these objectives by a scheme based on Durable Node Numbers and timestamps for the elements of XML documents. We first present efficient storage and retrieval techniques for multiversion documents. Then, we explore the indexing and clustering strategies needed to assure efficient support for complex queries on content and on document evolution.
DB
luo02hybrid
A Hybrid Model For Sharing Information Between Fuzzy, Uncertain And Default Reasoning Models In Multi-Agent Systems This paper develops a hybrid model which provides a unified framework for the fol-  lowing four kinds of reasoning: 1) Zadeh's fuzzy approximate reasoning; 2) truthqualification  uncertain reasoning with respect to fuzzy propositions; 3) fuzzy default  reasoning (proposed, in this paper, as an extension of Reiter's default reasoning); and 4)  truth-qualification uncertain default reasoning associated with fuzzy statements (developed  in this paper to enrich fuzzy default reasoning with uncertain information). Our  hybrid model has the following characteristics: 1) basic uncertainty is estimated in terms  of words or phrases in natural language and basic propositions are fuzzy; 2) uncertainty,  linguistically expressed, can be handled in default reasoning; and 3) the four kinds of rea-  soning models mentioned above and their combination models will be the special cases of  our hybrid model. Moreover, our model allows the reasoning to be performed in the case  in which the information is fuzzy, uncertain and partial. More importantly, the problems  of sharing the information among heterogeneous fuzzy, uncertain and default reasoning  models can be solved efficiently by using our model. Given this, our framework can be  used as a basis for information sharing and exchange in knowledge-based multi-agent  systems for practical applications such as automated group negotiations. Actually, to  build such a foundation is the motivation of this paper
AI
zhang01evolutionary
An Evolutionary Approach to Materialized Views Selection in a Data Warehouse Environment A data warehouse contains multiple views accessed by queries. One of the most important decisions in designing a data warehouse is selecting views to materialize for the purpose of eciently supporting decision making. The search space for possible materialized views is exponentially large. Therefore heuristics have been used to search for a near optimal solution. In this paper, we explore the use of an evolutionary algorithm for materialized view selection based on multiple global processing plans for queries. We apply a hybrid evolutionary algorithm to solve three related problems. The rst is to optimize queries. The second is to choose the best global processing plan from multiple global processing plans. The third is to select materialized views from a given global processing plan. Our experiment shows that the hybrid evolutionary algorithm delivers better performance than either the evolutionary algorithm or heuristics used alone in terms of the minimal query and maintenance cost and the evaluation cost to obtain the minimal cost.  Keywords  Evolutionary algorithms, Materialised view selection, Data warehousing, Data mining.  I. 
DB
298502
Scalable Information Organization We present three scalable extensions of the star algorithm for information organization that use  sampling. The star algorithm organizes a document collection into clusters that are naturally induced  by the topic structure of collection, via a computationally efficient cover by dense subgraphs. We  also provide supporting data from extensive experiments.  1 Introduction  Our goal is to develop a completely automated information organization system for digital libraries, automated tools for librarians to classify this information, automatic tools to create reference pointers into such collections, and automated tools that allow users to locate information effectively.  We focus on static and dynamic digital collections of unstructured text. We consider the problem of determining the topic structure of text data, without a priori knowledge of the number of topics in the data or any other information about their composition. We assume that the collections may be static (for example, digi...
IR
11805
Placing a Robot Manipulator Amid Obstacles for Optimized Execution This paper presents an efficient algorithm for optimizing the base location of a robot manipulator in an environment cluttered with obstacles, in order to execute specified tasks as fast as possible. The algorithm uses randomized motion planning techniques and exploits geometric "coherence " in configuration space to achieve fast computation. The performance of the algorithm is demonstrated on both synthetic examples and real-life CAD data from the automotive industry. The computation time ranges from under a minute for simple problems to a few minutes for more complex ones. 1 Introduction The base placement of a robot manipulator is an important issue in many robotics applications. Given a description of a robot manipulator and its environment, the goal is to find a base location for the manipulator so that specified tasks are executed as efficiently as possible. In this paper, we present an algorithm that makes use of randomized motion planning techniques to compute simultaneously ...
AI
shan98introduction
Introduction to the Relationlog System Advanced applications require construction, efficient access and management of large databases with rich data structures and inference mechanisms. However, such capabilities are not directly supported by the existing database systems. In this paper, we describe Relationlog, a persistent deductive database system that is able to directly support the storage, efficient access and inference of data with complex structures. 1 Introduction  Advanced applications require construction, efficient access and management of large databases with rich data structures and inference mechanisms. However, such capabilities are not directly supported by the existing database systems. Deductive databases have the potential to meet the demands of advanced applications. They grew out of the integration of logic programming and relational database technologies. They are intended to combine the best of the two approaches, such as representational and operational uniformity, inference capabilities, recursion,...
DB
sycara98context
In-Context Information Management through Adaptive Collaboration of Intelligent Agents Although the number and availability of electronic information sources are increasing, current information technology requires manual manipulation and userspecification of all details. Once accessed, information must be filtered in the context of the user's task. Current systems lack the ability to get contextual information or use it to automate filtering. At Carnegie Mellon University, we have been engaged in the RETSINA project, which aims to develop a reusable multiagent software infrastructure that allows heterogeneous agents on the Internet, possibly developed by different designers, to collaborate with each other to manage information in the context of user-specified tasks. In this chapter, we will provide a brief overview of the whole system and then focus on its capability for in-context information management.   This research has been supported in part by DARPA contract F30602-98-2-0138, and by ONR Grant N00014-96-1222.  1 Introduction  The Web is full of information resourc...
IR
dix99places
Places to stay on the move: Software architectures for mobile user interfaces Architectural design has an important effect on usability, most notably on temporal properties. This paper investigates software architecture options for mobile user-interfaces, in particular those for collaborative systems. One of the new features of mobile systems as compared with fixed networks is the connection point to the physical network, the point of presence (PoP), which forms an additional location for code and data. This allows architectures that bring computation closer to the users hence reducing feedback and feedthrough delays. A consequence of using PoPs is that code and data have to be mobile within the network leading to potential security problems.  Keywords: mobile computing, collaborative work, CSCW, software architecture, clientserver Introduction  At first sight it seems that software architectures are about the internals of system design and not a necessary concern for the user interface. However, the merging of computing and communication systems and the maturin...
HCI
243827
Case-Based BDI Agents: an Effective Approach for Intelligent Search on the World Wide Web We present a simple randomized algorithm which solves linear programs with n constraints and d variables in expected minfO(d  2  2  d  n); e  2  p  d ln(n=  p  d )+O(  p  d+ln n)  g  time in the unit cost model (where we count the number of arithmetic operations on the numbers in the input); to be precise, the algorithm computes the lexicographically smallest nonnegative point satisfying n given linear inequalities in d variables. The expectation is over the internal randomizations    Work by the first author has been supported by a Humboldt Research Fellowship. Work by the second and third authors has been supported by the German-Israeli Foundation for Scientific Research and Development (G.I.F.). Work by the second author has been supported by Office of Naval Research Grant N00014-90-J-1284, by National Science Foundation Grants CCR-89-01484 and CCR-90-22103, and by grants from the U.S.-Israeli Binational Science Foundation, and the Fund for Basic Research administered by the Israeli...
Agents
196348
A Framework for Automated Construction and Transformation of Case-Based Reasoning Systems Case Based Reasoning systems have gained immense popularity over the recent  years as problem-solving tools. Most case based reasoning systems, however, are developed  essentially from scratch using proprietary systems and applications on a limited  number of platforms. Although methods have been proposed to describe the structure  of a case based reasoner, none of these have been very successful outside their  application domains. In this paper, we #rst describe common methods for automating  CBR system construction. We then describe a general model for common CBR implementation,  and describe in detail a framework of platform-independent construction  of systems based on this model. We discuss an implementation of such a system using  Java, and #nally describe ways systems can be developed using this framework.  1 Introduction  Achieving widespread case-based reasoning support for corporate memories will require #exibility in integrating implementations with existing organizational i...
DB
brazier02agent
Agent Factory: Generative Migration of Mobile Agents in Heterogeneous Environments In most of today's agent systems migration of agents requires homogeneity in the programming language and/or agent platform in which an agent has been designed. In this paper an approach is presented with which heterogeneity is possible: agents can migrate between non-identical platforms, and need not be written in the same language. Instead of migrating the "code" (including data and state) of an agent, a blueprint of an agent's functionality is transferred. An agent factory generates new code on the basis of this blueprint. This approach of generative mobility not only has implications for interoperability but also for security, as discussed in this paper.
Agents
317766
View-based Query Processing and Constraint Satisfaction View-based query processing requires to answer a query posed to a database only on the basis of the information on a set of views, which are again queries over the same database. This problem is relevant in many aspects of database management, and has been addressed by means of two basic approaches, namely, query rewriting and query answering. In the former approach, one tries to compute a rewriting of the query in terms of the views, whereas in the latter, one aims at directly answering the query based on the view extensions. We study view-based query processing for the case of regular-path queries, which are the basic querying mechanisms for the emergent field of semistructured data. Based on recent results, we first show that a rewriting is in general a co-NP function wrt to the size of view extensions. Hence, the problem arises of characterizing which instances of the problem admit a rewriting that is PTIME. A second contribution of the work is to establish a tight connection between view-based query answering and constraint-satisfaction problems, which allows us to show that the above characterization is going to be difficult. As a third contribution of our work, we present two methods for computing PTIME rewritings of specific forms. The first method, which is based on the established connection with constraint-satisfaction problems, gives us rewritings expressed in Datalog with a fixed number of variables. The second method, based on automata-theoretic techniques, gives us rewritings that are formulated as unions of conjunctive regular-path queries with a fixed number of variables.
DB
373513
Learning to Create Customized Authority Lists The proliferation of hypertext and the popularity  of Kleinberg's HITS algorithm have  brought about an increased interest in link  analysis. While HITS and its older relatives  from the Bibliometrics provide a method for  finding authoritative sources on a particular  topic, they do not allow individual users to  inject their own opinions on what sources are  authoritative. This paper presents a technique  for learning a user's internal model of  authority. We present experimental results  based on Cora on-line index, a database of  approximately one million on-line computer  science literature references.  1. Introduction  Bibliometrics (White & McCain, 1989; Small, 1973) involves studying the structure that emerges from sets of linked documents. Traditionally, these links have taken the form of citations among journal articles, although Kleinberg (1997) and others (e.g., Brin & Page, 1998) have found that they adapt well to sets of hyperlinked documents. Bibliometric techniques exis...
IR
172324
A Framework for Workflow Management Systems Based on Objects, Rules and Roles : The goal of this paper is to present an approach for the development of workflow management systems supporting both reusability and adaptability, i.e., customization due to frequently changing requirements in an organization. The principal contribution is the introduction of an object-oriented application framework for constructing such workflow management systems balancing between reusability and adaptability. The underlying techniques are an object-oriented workflow model, object evolution via an integrated role model, and the support of business policies via an integrated rule model.  Categories and Subject Descriptors: H.4.1 [Office Automation]: Workflow Management; D.2.11 [Software Architectures]: Domain-specific architectures General Terms: Design Additional Key Words and Phrases: Object-Oriented Frameworks, Event/Condition/Action Rule, Role Modeling, Context Dependent Behavior INTRODUCTION  This paper examines the application of the framework idea to the development of workflo...
DB
he00comparative
A Comparative Study on Chinese Text Categorization Methods This paper reports our comparative evaluation of three machine learning methods on Chinese text categorization. Whereas a wide range of methods have been applied to English text categorization, relatively few studies have been done on Chinese text categorization. Based on a re-constructed People's Daily corpus, a series of controlled experiments evaluate three machine learning methods, namely k Nearest Neighbor (kNN) algorithm, Support Vector Machines (SVM), and Adaptive Resonance Associative Map (ARAM), in terms of their capabilities in mining categorization knowledge from high dimensional, sparse, and relatively noisy document feature vectors. Experiments reveal that all three methods produce satisfactory performance on the test corpus while ARAM exhibits a marginally better generalization capability, especially from relatively small and noisy training sets.  Keywords: Chinese text categorization, supervised learning.  2  1 Introduction  Text categorization refers to the task of auto...
IR
wooldridge96practical
Practical Reasoning with Procedural Knowledge (A Logic of BDI Agents with Know-How) . In this paper, we present a new logic for specifying the behaviour of  multi-agent systems. In this logic, agents are viewed as BDI systems, in that their  state is characterised in terms of beliefs, desires, and intentions: the semantics  of the BDI component of the logic are based on the well-known system of Rao  and Georgeff. In addition, agents have available to them a library of plans, representing  their `know-how': procedural knowledge about how to achieve their intentions.  These plans are, in effect, programs, that specify how a group of agents  can work in parallel to achieve certain ends. The logic provides a rich set of constructs  for describing the structure and execution of plans. Some properties of the  logic are investigated, (in particular, those relating to plans), and some comments  on future work are presented.  1 Introduction  There is currently much international interest in computer systems that go under the banner of intelligent agents [16]. Crudely, an intel...
Agents
heinonen96www
WWW Robots and Search Engines The Web robots are programs that automatically traverse through networks. Currently, their most visible and familiar application is to provide indices for search engines, such as Lycos and Alta Vista, and semiautomatically maintained topic references or subject directories. In this article, we survey the state-of-art of the Web robots, and the search engines that utilize the results of robot searches. We also present notions about robot ethics and distributed Web robots.
IR
369039
On the Integration of IR and Databases : Integration of information retrieval (IR) in database management  systems (DBMSs) has proven di#cult. Previous attempts to integration su#ered  from inherent performance problems, or lacked desirable separation between  logical and physical data models. To overcome these problems, we discuss a  database approach based on structural object-orientation. We implement IR  techniques using extensions in an object algebra called MOA. MOA has been  implemented on top of the database backend Monet, a state-of-the-art highperformance  database kernel with a binary relational interface. Our prototype  implementation of the inference network retrieval model using MOA and Monet  demonstrates the feasibility of this approach. We conclude with a discussion of  the advantages of our database design.  INTRODUCTION  Information retrieval (IR) is concerned with the retrieval of (usually text) documents that are likely to be relevant to the user's information need as expressed by his request (van Rijsb...
DB
jain00comparison
A Comparison of Mobile Agent and Client-Server Paradigms for Information Retrieval Tasks in Virtual Enterprises In next-generation enterprises it will become increasingly important to retrieve information efficiently and rapidly from widely dispersed sites in a virtual enterprise, and the number of users who wish to do using wireless and portable devices will increase significantly. This paper considers the use of mobile agent technology rather than traditional clientserver computing for information retrieval by mobile and wireless users in a virtual enterprise. We argue that to be successful mobile agent platforms must coexist with, and be presented to the applications programmer sideby -side with, traditional client-server middleware like CORBA and DCOM, and we sketch a middleware architecture for doing so. We then develop an analytical model that examines the claimed performance benefits of mobile agents over client-server computing for a mobile information retrieval scenario. Our evaluation of the model shows that mobile agents are not always better than client-server calls in terms of average response times; they are only beneficial if the space overhead of the mobile agent code is not too large or if the wireless link connecting the mobile user to the fixed servers of the virtual enterprise is error-prone.
IR
brandt01cryptographic
Cryptographic Protocols for Secure Second-Price Auctions In recent years auctions have become more and more important in the  field of multiagent systems as useful mechanisms for resource allocation, task  assignment and last but not least electronic commerce. In many cases the Vickrey  (second-price sealed-bid) auction is used as a protocol that prescribes how the  individual agents have to interact in order to come to an agreement. The main  reasons for choosing the Vickrey auction are the existence of a dominant strategy  equilibrium, the low bandwidth and time consumption due to just one round  of bidding and the (theoretical) privacy of bids. This paper specifies properties  that are needed to ensure the accurate and secret execution of Vickrey auctions  and provides a classification of different forms of collusion. We approach the two  major security concerns of the Vickrey auction: the vulnerability to a lying auctioneer  and the reluctance of bidders to reveal their private valuations. We then  propose a novel technique that allows to securely perform second-price auctions.  This is achieved using the announcement of encrypted binary bidding lists on a  blackboard. Top-down, bottom-up and binary search techniques are used to interactively  find the second highest bid step by step without revealing unnecessary  information.  1 
Agents
lau00version
Version Space Algebra and its Application to Programming by Demonstration Machine learning research has been very  successful at producing powerful, broadlyapplicable  classification learners. However,  many practical learning problems do not fit  the classification framework well, and as a  result the initial phase of suitably formulating  the problem and incorporating the relevant  domain knowledge can be very di#cult  and typically consumes the majority of the  project time. Here we propose a framework  to systematize and speed this process, based  on the notion of version space algebra. We  extend the notion of version spaces beyond  concept learning, and propose that carefullytailored  version spaces for complex applications  can be built by composing simpler, restricted  version spaces. We illustrate our  approach with SMARTedit, a programming  by demonstration application for repetitive  text-editing that uses version space algebra  to guide a search over text-editing action sequences.  We demonstrate the system on a  suite of repetitive text-editing ...
ML
gandon02multiagent
A Multi-Agent Architecture for Distributed Corporate Memories This paper presents an approach to design a  multi-agent system managing a corporate  memory in the form of a distributed semantic  web and describes the resulting architecture.
IR
carro99concurrency
Concurrency in Prolog Using Threads and a Shared Database Concurrency in Logic Programming has received much attention in the past. One problem with many proposals, when applied to Prolog, is that they involve large modifications to the standard implementations, and/or the communication and synchronization facilities provided do not fit as naturally within the language model as we feel is possible. In this paper we propose a new mechanism for implementing synchronization and communication for concurrency, based on atomic accesses to designated facts in the (shared) database. We argue that this model is comparatively easy to implement and harmonizes better than previous proposals within the Prolog control model and standard set of built-ins. We show how in the proposed model it is easy to express classical concurrency algorithms and to subsume other mechanisms such as Linda, variable-based communication, or classical parallelism-oriented primitives. We also report on an implementation of the model and provide performance and resource consumption data. 1
AI
dumas98handling
Handling Temporal Grouping and Pattern-Matching Queries in a Temporal Object Model This paper presents a language for expressing temporal pattern-matching queries, and a set of temporal grouping operators for structuring histories following calendar-based criteria. Pattern-matching queries are shown to be useful for reasoning about successive events in time while temporal grouping may be either used to aggregate data along the time dimension or to display histories. The combination of these capabilities allows to express complex queries involving succession in time and calendar-based conditions simultaneously. These operators are embedded into the TEMPOS temporal data model and their use is illustrated through examples taken from a geographical application. The proposal has been validated by a prototype on top of the O 2 DBMS.  Keywords: temporal databases, temporal object model, temporal query language, pattern-matching queries, temporal grouping, calendar, granularity, O 2 .  1 Introduction  In most modern DBMS, time is one of the primitive datatypes provided for d...
DB
jakobi98minimal
Minimal Simulations For Evolutionary Robotics this paper, the line is drawn between controller and environment. 3.1.1 Drawing the line between controller and environment
ML
165504
Clustering Categorical Data: An Approach Based on Dynamical Systems We describe a novel approach for clustering collections of sets, and its application to the analysis and mining of categorical data. By "categorical data," we mean tables with fields that cannot be naturally ordered by a metric --- e.g., the names of producers of automobiles, or the names of products offered by a manufacturer. Our approach is based on an iterative method for assigning and propagating weights on the categorical values in a table; this facilitates a type of similarity measure arising from the cooccurrence of values in the dataset. Our techniques can be studied analytically in terms of certain types of non-linear dynamical systems. We discuss experiments on a variety of tables of synthetic and real data; we find that our iterative methods converge quickly to prominently correlated values of various categorical fields. 1 Introduction  Much of the data in databases is categorical: fields in tables whose attributes cannot naturally be ordered as numerical values can. The pro...
IR
markatos98effective
Effective Resource Discovery on the World Wide Web this paper, we present USEwebNET, a resource discovery tool built on top of traditional search engines. USEwebNET registers each user's interests and repeatedly queries several search engines for URLs matching a user's registered interests. USEwebNET keeps track of which URLs have been visited by each user. Thus, when a user invokes USEwebNET, (s)he is presented only with new or "unvisited" URLs. We view USEwebNET as a value-added service on top of existing search engines and information providers, which helps users effectively find "what's new" in the rapidly evolving web of our world. Introduction
IR
chakrabarti02accelerated
Accelerated Focused Crawling through Online Relevance Feedback The organization of HTML into a tag tree structure, which is rendered by browsers as roughly rectangular regions with embedded text and HREF links, greatly helps surfers locate and click on links that best satisfy their information need. Can an automatic program emulate this human behavior and thereby learn to predict the relevance of an unseen HREF target page w.r.t. an information need, based on information limited to the HREF source page? Such a capability would be of great interest in focused crawling and resource discovery, because it can fine-tune the priority of unvisited URLs in the crawl frontier, and reduce the number of irrelevant pages which are fetched and discarded.
IR
bettini01xklaim
X-Klaim and Klava: Programming Mobile Code Highly distributed networks have now become a common infrastructure for a new kind of wide-area distributed applications whose key design principle is network awareness, namely the ability to deal with dynamic changes of the network environment. Network-aware computing has called for new programming languages that exploit the mobility paradigm as the basic interaction mechanism. In this paper we present the Klaim (Kernel Language for Agent Interaction and Mobility) framework for programming mobile code applications, namely the X-Klaim programming language and the Java-based run-time system Klava. In particular, we illustrate how Klava handles mobile code. Finally, an example is shown that is implemented using this framework.
Agents
290879
Support Vector Machines for Face Authentication The paper studies Support Vector Machines (SVMs) in the context of face  authentication. Our study supports the hypothesis that the SVM approach  is able to extract the relevant discriminatory information from the training  data. We believe this is the main reason for its superior performance over  benchmark methods. When the representation space already captures and  emphasises the discriminatory information content as in the case of Fisherfaces,  SVMs loose their superiority. SVMs can also cope with illumination  changes, provided these are adequately represented in the training data.  However, on data which has been sanitised by feature extraction (Fisherfaces)  and/or normalisation, SVMs can get over-trained, resulting in the loss of the  ability to generalise. SVMs involve many parameters and can employ different  kernels. This makes the optimisation space rather extensive, without the  guarantee that it has been fully explored to find the best solution.  1 Introduction  Verificat...
ML
531895
An Architecture for Building Multi-Device Thin-Client Web User Interfaces We describe a new approach to providing adaptable thin client  interfaces for web-based information systems. Developers specify web-based  interfaces using a high-level mark-up language based on the logical structure of  the user interface. At run-time this single interface description is used to  automatically provide an interface for multiple web devices e.g. desk-top  HTML and mobile WML-based systems, as well as highlight, hide or disable  interface elements depending on the current user and user task. Our approach  allows developers to much more easily construct and maintain adaptable webbased  user interfaces than other current approaches.
HCI
hsu02genetic
Genetic Programming and Multi-Agent Layered Learning by Reinforcements We present an adaptation of the standard genetic program (GP) to hierarchically decomposable, multi-agent learning problems. To break down a problem that requires cooperation of multiple agents, we use the team objective function to derive a simpler, intermediate objective function for pairs of cooperating agents. We apply GP to optimize first for the intermediate, then for the team objective function, using the final population from the earlier GP as the initial seed population for the next. This layered learning approach facilitates the discovery of primitive behaviors that can be reused and adapted towards complex objectives based on a shared team goal.
ML
gal01managing
Managing Periodically Updated Data in Relational Databases: A Stochastic Modeling Approach Recent trends in information management involve the periodic transcription  of data onto secondary devices in a networked environment, and the proper  scheduling of these transcriptions is critical for efficient data management. To assist  in the scheduling process, we are interested in modeling data obsolescence, that is,  the reduction of consistency over time between a relation and its replica. The modeling  is based on techniques from the field of stochastic processes, and provides several  stochastic models for content evolution in the base relations of a database, taking  referential integrity constraints into account. These models are general enough to  accommodate most of the common scenarios in databases, including batch insertions  and life spans both with and without memory. As an initial "proof of concept" of the  applicability of our approach, we validate the insertion portion of our model framework  via experiments with real data feeds. We also discuss a set of transcription  protocols which make use of the proposed stochastic model.
IR
kalfoglou01integration
On the Integration of Technologies for Capturing and Navigating Knowledge With Ontology-Driven Services Nowadays, many distinct communities are researching on technologies for knowledge capturing, modelling, and navigation. Moreover,  advances in Internet technology makes it possible to perform most of these tasks on heterogeneous and distributed environments such as the Web. These advances though, have raise the need for knowledge services to accommodate the ever increasing number of Web users. To provide such a service one needs to combine key technologies for different aspects of knowledge management: capturing, modelling, navigating. This should be tightly integrated with the intended service. We describe such an integration effort in this paper. Our domain is a Web-based news repository and we aimed to provide personalised ontology-driven services on the top of it. We used knowledge capturing technologies to populate the underlying ontologies, knowledge modelling techniques to provide reasoning capabilities for the ontology-driven service, and navigating technologies to overlay Web-pages with the ontology-driven service.  Keywords  Knowledge capture, ontology-driven services, knowledge navigation 1. 
IR
muller00eventdriven
An Event-Driven Sensor Architecture for Low Power Wearables In this paper we define a software architecture for low power wearable computers. The architecture is event driven, and is designed so that application programs have access to sensor data without the need for polling. The software architecture is designed to allow for an underlying hardware architecture that can achieve maximal power efficiency by switching off parts of the hardware that are known not to be required.  INTRODUCTION  Wearable Computing places particular demands on software. It is characterised by having to accommodate a potentially large number of input devices which are simultaneously providing contextual data; a need to minimise processor activity to reduce battery consumption [1]; and the requirement for simplicity to aid debugging applications which may be used in adverse conditions.  As a result of our experiences with software which had none of the above features, we have developed an eventdriven sensor architecture and used it successfully for several applications...
HCI
24678
Overview of Datalog Extensions with Tuples and Sets Datalog (with negation) is the most powerful query language for relational database with a well-defined declarative semantics based on the work in logic programming. However, Datalog only allows inexpressive flat structures and cannot directly support complex values such as nested tuples and sets common in novel database applications. For these reasons, Datalog has been extended in the past several years to incorporate tuple and set constructors. In this paper, we examine four different Datalog extensions: LDL, COL, Hilog and Relationlog. 1 Introduction  Databases and logic programming are two independently developed areas in computer science. Database technology has evolved in order to effectively and efficiently organize, manage and maintain large volumes of ever increasingly complex data reliably in various memory devices. The underlying structure of databases has been the primary focus of research which leads to the development of data models. The most well-known and widely used da...
DB
215383
Composition of Services with Mobile Code Mobile code is slowly gaining acceptance but it is still not clear where it is really useful. If not used judiciously it may incur greater complexity of programming and degradation of performances. The goal of this paper is to show that mobile code is particularly well suited as a glue for the composition of immobile services, where flexibility and extensibility are necessary. To support our claim we describe two services and one application that have been programmed with mobile code in the context of active networking. We study the impact on the flexibility, complexity and performances of the resulting systems. We observe positive effects on flexibility and complexity and acceptable performance penalties.  1. Introduction  The main idea of active networking is to make mobile code a core functionality of the network. Traditionally network elements like routers only transport packets and do not try to decode or modify the enclosed payload. In an Active Network, the nodes can perform cus...
Agents
306529
Multi-Robot Learning in a Cooperative Observation Task . An important need in multi-robot systems is the development of mechanisms that enable robot teams to autonomously generate cooperative behaviors. This paper rst briey presents the Cooperative Multi-robot Observation of Multiple Moving Targets (CMOMMT) application as a rich domain for studying the issues of multi-robot learning of new behaviors. We discuss the results of our handgenerated algorithm for CMOMMT, and then describe our research in generating multi-robot learning techniques for the CMOMMT application, comparing the results to the hand-generated solutions. Our results show that, while the learning approach performs better than random, naive approaches, much room still remains to match the results obtained from the hand-generated approach. The ultimate goal of this research is to develop techniques for multi-robot learning and adaptation that will generalize to cooperative robot applications in many domains, thus facilitating the practical use of multi-robot teams in a wid...
ML
363813
On Wrapping Query Languages and Efficient XML Integration Modern applications/portals, e-commerce, digital libraries, etc.) require integrated access to various information sources (from traditional RDBMS to semistructured Web repositories), fast deployment and low maintenance cost in a rapidly evolving environment. Because of its flexibility, there is an increasing interest in using XML as a middleware model for such applications. XML enables fast wrapping and declarative integration. However, query processing in XML-based integration systems is still penalized by the lack of an algebra with adequate optimization properties and the difficulty to understand source query capabilities. In this paper, we propose an algebraic approach to support efficient query evaluation in XML integration systems. We define a general purpose algebra suitable for semistructured or XML query languages. We showhow this algebra can be used, with appropriate type information, to also wrap more structured query languages such as OQL or SQL. Finally,we develop new optimizat...
DB
conrad00xml
XML Conceptual Modeling using UML . The eXtensible Markup Language (XML) is increasingly finding  acceptance as a standard for storing and exchanging structured and  semi-structured information. With its expressive power, XML enables a  great variety of applications relying on such structures - notably product  catalogs, digital libraries, and electronic data interchange (EDI). As  the data schema, an XML Document Type Definition (DTD) is a means  by which documents and objects can be structured. Currently, there is  no suitable way to model DTDs conceptually. Our approach is to model  DTDs and thus classes of documents on the basis of UML (Unified Modeling  Language). We consider UML to be the connecting link between  software engineering and document design, i.e., it is possible to design  object-oriented software together with the necessary XML structures. For  this reason, we describe how to transform the static part of UML, i.e.  class diagrams, into XML DTDs. The major challenge for the transformation  is to defi...
IR
liu99overview
Overview of the ROL2 Deductive Object-Oriented Database System This paper presents an overview of ROL2, a novel deductive object-oriented database system developed at the University of Regina. ROL2 supports in a rule-based framework nearly all important object-oriented features such as object identity, complex objects, typing, information hiding, rule-based methods, encapsulation of such methods, overloading, late binding, polymorphism, class hierarchies, multiple structural and behavioral inheritance with overriding, blocking, and conflict handling. It is so far the only deductive system that supports all these features in a pure rule-based framework.
DB
timm00multiagent
Multiagent Architecture for D-Sifter -- A modern approach to flexible information filtering in dynamic environments  
IR
fayyad98initialization
Initialization of Iterative Refinement Clustering Algorithms Iterative refinement clustering algorithms (e.g. K-Means, EM) converge to one of numerous local minima. It is known that they are especially sensitive to initial conditions. We present a procedure for computing a refined starting condition from a given initial one that is based on an efficient technique for estimating the modes of a distribution. The refined initial  starting condition leads to convergence to "better" local minima. The procedure is applicable to a wide class of clustering algorithms for both discrete and continuous data. We demonstrate the application of this method to the Expectation Maximization (EM) clustering algorithm and show that refined initial points indeed lead to improved solutions. Refinement run time is considerably lower than the time required to cluster the full database. The method is scalable and can be coupled with a scalable clustering algorithm to address the large-scale clustering in data mining.  1 Background  Clustering has been formulated in var...
ML
71059
Reliable Communication for Highly Mobile Agents . The provision of a reliable communication infrastructure for mobile agents is still an open research issue. The challenge to reliability we address in this work does not come from the possibility of faults, but rather from the mere presence of mobility, which complicates the problem of ensuring the delivery of information even in a fault-free network. For instance, the asynchronous nature of message passing and agent migration may cause situations where messages forever chase a mobile agent that moves frequently from one host to another. Current solutions rely on conventional technologies that either do not provide a solution for the aforementioned problem, because they were not designed with mobility in mind, or enforce continuous connectivity with the message source, which in many cases defeats the very purpose of using mobile agents.  In this paper, we propose an algorithm that guarantees delivery to highly mobile agents using a technique similar to a distributed snapshot. A numbe...
Agents
glover99recommending
Recommending Web Documents Based on User Preferences Making recommendations requires treating users as individuals. In this paper, we describe a metasearch engine available at NEC Research Institute that allows individual search strategies to be used. Each search strategy consists of a different set of sources, different query modification rules and a personalized ordering policy. We combine these three features with a dynamic interface that allows users to see the "current best" recommendations displayed at all times, and allows results to be displayed immediately upon retrieval. We present several examples where a single query produces different results, ordered based on different factors, accomplished without the use of training, or a local database.
IR
graham00towards
Towards a Distributed, Environment-Centered Agent Framework . This paper will discuss the internal architecture for an agent framework  called DECAF (Distributed Environment Centered Agent Framework). DECAF  is a software toolkit for the rapid design, development, and execution of  "intelligent" agents to achieve solutions in complex software systems. From a  research community perspective, DECAF provides a modular platform for evaluating  and disseminating results in agent architectures, including communication,  planning, scheduling, execution monitoring, coordination, diagnosis, and  learning. From a user/programmer perspective, DECAF distinguishes itself by  removing the focus from the underlying components of agent building such as  socket creation, message formatting, and agent communication. Instead, users  may quickly prototype agent systems by focusing on the domain-specific parts  of the problem via a graphical plan editor, reusable generic behaviors [9], and  various supporting middle-agents [10]. This paper will briefly describe the...
Agents
labrou99agent
Agent Communication Languages: The Current Landscape this article--- suggest a paradigm for software development that emphasizes autonomy both at design time and runtime, adaptivity, and cooperation. This approach seems appealing in a world of distributed, heterogeneous systems. Languages for communicating agents promise to play the role that natural languages played for their human counterparts. An agent communication language that allows agents to interact while hiding the details of their internal workings will result in agent communities that tackle problems no individual agent could.
Agents
32742
Corpus-Based Stemming using Co-occurrence of Word Variants Stemming is used in many information retrieval (IR) systems to reduce variant word forms  to common roots. It is one of the simplest applications of natural language processing to IR,  and one of the most effective in terms of user acceptance and consistent, though small, retrieval  improvements. Current stemming techniques do not, however, reflect the language use in specific  corpora and this can lead to occasional serious retrieval failures. We propose a technique  for using corpus-based word variant co-occurrence statistics to modify or create a stemmer. The  experimental results generated using English newspaper and legal text and Spanish text demonstrate  the viability of this technique and its advantages relative to conventional approaches.  Categories and Subject Descriptors: H.3.1. [Information Storage and Retrieval]: Content Analysis and Indexing -- indexing methods; linguistic processing; H.3.3. [Information Storage and Retrieval]: Information Search and Retrieval -- query f...
DB
popescul01probabilistic
Probabilistic Models for Unified Collaborative and Content-Based Recommendation in Sparse-Data Environments Recommender systems leverage product and  community information to target products to  consumers. Researchers have developed collaborative  recommenders, content-based recommenders,  and a few hybrid systems. We propose  a unified probabilistic framework for merging  collaborative and content-based recommendations.  We extend Hofmann's (1999) aspect  model to incorporate three-way co-occurrence  data among users, items, and item content. The  relative influence of collaboration data versus  content data is not imposed as an exogenous parameter,  but rather emerges naturally from the  given data sources. However, global probabilistic  models coupled with standard EM learning algorithms  tend to drastically overfit in the sparsedata  situations typical of recommendation applications.  We show that secondary content information  can often be used to overcome sparsity.  Experiments on data from the ResearchIndex  library of Computer Science publications  show that appropriate mixture models incorporating  secondary data produce significantly better  quality recommenders than k-nearest neighbors  (k-NN). Global probabilistic models also allow  more general inferences than local methods like  k-NN.   
IR
150514
Representing and Querying Changes in Semistructured Data Semistructured data may be irregular and incomplete and does not necessarily conform to a fixed schema. As with structured data, it is often desirable to maintain a history of changes to data, and to query over both the data and the changes. Representing and querying changes in semistructured data is more difficult than in structured data due to the irregularity and lack of schema. We present a model for representing changes in semistructured data and a language for querying over these changes. We discuss implementation strategies for our model and query language. We also describe the design and implementation of a "query subscription service" that permits standing queries over changes in semistructured information sources. 1 Introduction  Semistructured data is data that has some structure, but it may be irregular and incomplete and does not necessarily conform to a fixed schema (e.g, HTML documents). Recently, there has been increased interest in data models and query languages for s...
DB
196762
Navigational Plans For Data Integration We consider the problem of building data integration systems when the data sources are webs of data, rather than sets of relations. Previous approaches to modeling data sources are inappropriate in this context because they do not capture the relationships between linked data and the need to navigate through paths in the data source in order to obtain the data. We describe a language for modeling data sources in this new context. We show that our language has the required expressive power, and that minor extensions to it would make query answering intractable. We provide a sound and complete algorithm for reformulating a user query into a query over the data sources, and we show how to create query execution plans that both query and navigate the data sources.  Introduction  The purpose of data integration is to provide a uniform  interface to a multitude of data sources. Data integration applications arise frequently as corporations attempt to provide their customers and employees wit...
DB
potter01heterogeneity
Heterogeneity in the Coevolved Behaviors of Mobile Robots: The Emergence of Specialists Many mobile robot tasks can be most efficiently  solved when a group of robots is utilized. The type  of organization, and the level of coordination and  communication within a team of robots affects the  type of tasks that can be solved. This paper examines  the tradeoff of homogeneity versus heterogeneity  in the control systems by allowing a team of  robots to coevolve their high-level controllers given  different levels of difficulty of the task. Our hypothesis  is that simply increasing the difficulty of a task  is not enough to induce a team of robots to create  specialists. The key factor is not difficulty per se,  but the number of skill sets necessary to successfully  solve the task. As the number of skills needed  increases, the more beneficial and necessary heterogeneity  becomes. We demonstrate this in the  task domain of herding, where one or more robots  must herd another robot into a confined space.  1 
ML
241799
Coordinating Mutually Exclusive Resources using GPGP Hospital Patient Scheduling is an inherently distributed problem because of the way real hospitals are organized. As medical procedures have become more complex, and their associated tests and treatments have become interrelated, the current ad hoc patient scheduling solutions have been observed to break down. We propose a multi-agent solution using the Generalized Partial Global Planning (GPGP) approach that preserves the existing human organization and authority structures, while providing better system-level performance (increased hospital unit throughput and decreased patient stay time). To do this, we extend GPGP with a new coordination mechanism to handle mutually exclusive resource relationships. Like the other GPGP mechanisms, the new mechanism can be applied to any problem with the appropriate resource relationship. We evaluate this new mechanism in the context of the hospital patient scheduling problem, and examine the effect of increasing interrelations between tasks performed...
Agents
308951
Learning Motor Skills By Imitation: A Biologically Inspired Robotic Model. This article presents a biologically inspired model for motor skills imitation. The model is composed of modules whose functinalities are inspired by corresponding brain regions responsible for the control of movement in primates. These modules are high-level abstractions of the spinal cord, the primary and premotor cortexes (M1 and PM), the cerebellum, and the temporal cortex. Each module is modeled at a connectionist level. Neurons in PM respond both to visual observation of movements and to corresponding motor commands produced by the cerebellum. As such, they give an abstract representation of mirror neurons. Learning of new combinations of movements is done in PM and in the cerebellum. Premotor cortexes and cerebellum are modeled by the DRAMA neural architecture which allows learning of times series and of spatio-temporal invariance in multimodal inputs. The model is implemented in a mechanical simulation of two humanoid avatars, the imitator and the imitatee. Three types of sequences learning are presented: (1) learning of repetitive patterns of arm and leg movements; (2) learning of oscillatory movements of shoulders and elbows, using video data of a human demonstration; 3) learning of precise movements of the extremities for grasp and reach
ML
biskup00decomposition
Decomposition of Database Classes under Path Functional Dependencies and Onto Constraints Based on F-logic, we specify an advanced data model with object-oriented and  logic-oriented features that substantially extend the relational approach. For this  model we exhibit and study the counterpart to the well-known decomposition of  a relation scheme according to a nontrivial nonkey functional dependency. For  decomposing a class of a database schema the transformation of pivoting is used.  Pivoting separates apart some attributes of the class into a newly generated class.  This new class is declared to be a subclass of the result class of the so-called pivot  attribute. Moreover the pivot attribute provides the link between the original class  and the new subclass. We identify the conditions for the result of pivoting being  equivalent with its input: the expressive power of path functional dependencies,  the validity of the path functional dependency between the pivot attribute and  the transplanted attributes, and the validity of the onto-constraint guaranteeing  that value...
DB
kirste01reference
A Reference Model for Situation-Aware Assistance As computers are becoming more and more ubiquitous, moving from the desktop into the infrastructure of our everyday life, they begin to influence the way we interact with this environment -- the (physical) entities that we operate upon in order to achieve our daily goals. The most important aspect of future human-computer interaction therefore is the way, computers support us in efficiently managing our personal environment. Conventionally, human-computer interaction looks at a process, where only two partners are involved: the human and the computer. However, looking at the computer as a mediator between the user and his environment, we have to acknowledge a more complex communication process. This paper proposes a reference model that identifies the fundamental components which are involved in human-computer-environment interaction.
HCI
prokopenko00cyberoos
Cyberoos2000: Experiments with Emergent Tactical Behaviour this paper is that, rather than defining situated or tactical reasoning ad hoc, it is desirable to categorise agents according to their functionality and reactions to the environment, and identify corresponding classes of action theories and agent architectures. Then, the reasoning exhibited by agents of a certain type (and validated by particular action theories) can be declared to be situated, tactical, strategic, social and so on. In other words, the principal target is a systematic description of increasing levels of agent reasoning abilities. The results reported in [8, 9, 10] demonstrated that this is achievable at the situated level. Preliminary results on the systematic models for basic tactical behaviour were obtained as well [11]. This work intends to use this framework in experimenting with emergent tactical teamwork; and thus build up empirical results and intuition necessary to advance the systematic methodology towards collaborative goal-oriented agents.
Agents
druin99designing
Designing PETS: A Personal Electronic Teller of Stories in Robots For Kids, Morgan Kaufmann, San Francisco, CA. Druin, A. and Hendler J. (eds.)  Page 2  Who, or What, Is PETS? Figure 1: PETS, Spaceship, and MyPETS software What Does PETS Do?  PETS is a Personal Electronic Teller of Stories, a robotic story telling environment for elementary school age children (Druin et al. 1999a). The PETS kit contains a box of fuzzy stuffed animal parts and an authoring application on a personal computer (Figures 1 and 3). Children can build a robotic animal, or pet, by connecting animal parts such as torso, head, paws, ears, and wings. After they construct their pet, they can write and tell stories using the My PETS software. Just as the robotic animal is constructed from discrete components, My PETS is also constructive. This application enables children to create emotions, draw emotive facial expressions, name their robotic companion, and compile a library of stori
HCI
cardie93using
Using Decision Trees to Improve Case-Based Learning This paper shows that decision trees can be used to improve the performance of casebased learning (CBL) systems. We introduce a performance task for machine learning systems called semi-flexible prediction that lies between the classification task performed by decision tree algorithms and the flexible prediction task performed by conceptual clustering systems. In semi-flexible prediction, learning should improve prediction of a specific set of features known a priori rather than a single known feature (as in classification) or an arbitrary set of features (as in conceptual clustering). We describe one such task from natural language processing and present experiments that compare solutions to the problem using decision trees, CBL, and a hybrid approach that combines the two. In the hybrid approach, decision trees are used to specify the features to be included in k-nearest neighbor case retrieval. Results from the experiments show that the hybrid approach outperforms both the decision ...
ML
506993
Evaluating Look-to-Talk: A Gaze-Aware Interface in a Collaborative Environment We present “look-to-talk”, a gaze-aware interface for directing a spoken utterance to a software agent in a multiuser collaborative environment. Through a prototype and a Wizard-of-Oz (WOz) experiment, we show that &quot;look-totalk” is indeed a natural alternative to speech and other paradigms.
HCI
198191
Formal Analysis of Models for the Dynamics of Trust based on Experiences . The aim of this paper is to analyse and formalise the dynamics of trust in the light of experiences. A formal framework is introduced for the analysis and specification of models for trust evolution and trust update. Different properties of these models are formally defined. 1 Introduction  Trust is the attitude an agent has with respect to the dependability/capabilities of some other agent (maybe itself) or with respect to the turn of events. The agent might for example trust that the statements made by another agent are true. The agent might trust the commitment of another agent with respect to a certain (joint) goal. The agent might trust that another agent is capable of performing certain tasks. The agent might trust itself to be able to perform some tasks. The agent might trust that the current state of affairs will lead to a state of affairs that is agreeable to its own intentions, goals, commitments, or desires. In [1], [2] the importance of the notion trust is shown for agent...
Agents
537134
Comparing Statistical and Content-Based Techniques for Answer Validation on the Web Answer Validation is an emerging topic in Question Answering, where open  domain systems are often required to rank huge amounts of candidate answers. We  present a novel approach to answer validation based on the intuition that the amount  of implicit knowledge which connects an answer to a question can be estimated by  exploiting the redundancy of Web information. Two techniques are considered in this  paper: a statistical approach, which uses the Web to obtain a large amount of pages,  and a content-based approach, which analyses text snippets retrieved by the search  engine. Both the approaches do not require to download the documents. Experiments  carried out on the TREC-2001 judged-answer collection show that a combination of  the two approaches achieves a high level of performance (i.e. about 88% success rate).
IR
zhang01personalized
Personalized Web-Document Filtering Using Reinforcement Learning Abstract- Document filtering is increasingly deployed in Web environments to reduce information overload of users. We formulate online information filtering as a reinforcement learning problem, i.e. TD(0). The goal is to learn user profiles that best represent his information needs and thus maximize the expected value of user relevance feedback. A method is then presented that acquires reinforcement signals automatically by estimating user’s implicit feedback from direct observations of browsing behaviors. This “learning by observation ” approach is contrasted with conventional relevance feedback methods which require explicit user feedbacks. Field tests have been performed which involved 10 users reading a total of 18,750 HTML documents during 45 days. Compared to the existing document filtering techniques, the proposed learning method showed superior performance in information quality and adaptation speed to user preferences in online filtering. 1
ML
sandholm01market
Market Clearability Market mechanisms play a central role in AI as a  coordination tool in multiagent systems and as an  application area for algorithm design. Mechanisms  where buyers are directly cleared with sellers, and  thus do not require an external liquidity provider,  are highly desirable for electronic marketplaces for  several reasons. In this paper we study the inherent  complexity of, and design algorithms for, clearing  auctions and reverse auctions with multiple indistinguishable  units for sale. We consider settings  where bidders express their preferences via  price-quantity curves, and settings where the bids  are price-quantity pairs. We show that markets  with piecewise linear supply/demand curves and  non-discriminatory pricing can always be cleared  in polynomial time. Surprisingly, if discriminatory  pricing is used to clear the market, the problem  becomes NP-Complete (even for step function  curves). If the price-quantity curves are all  linear, then, in most variants, the problem admits  a poly-time solution even for discriminatory pricing.  When bidders express their preferences with  price-quantity pairs, the problem is NP-Complete,  but solvable in pseudo-polynomial time. With free  disposal, the problem admits a poly-time approximation  scheme, but no such approximation scheme  is possible without free disposal. We also present  pseudo-polynomial algorithms for XOR bids and  or-of-xors bids, and analyze the approximability.  1 
Agents
karkanis00image
Image Recognition and Neuronal Networks: Intelligent Systems for the Improvement of Imaging Information this paper we have concentrated on describing issues related to the development and use of artificial neural network-based intelligent systems for medical image interpretation. Research in intelligent systems to-date remains centred on technological issues and is mostly application driven. However, previous research and experience suggests that the successful implementation of computerised systems (e.g., [34] [35]), and decision support systems in particular (e.g., [36]), in the area of healthcare relies on the successful integration of the technology with the organisational and social context within which it is applied. Therefore, the successful implementation of intelligent medical image interpretation systems 9 should not only rely on their technical feasibility and effectiveness but also on organisational and social aspects that may rise from their applications, as clinical information is acquired, processed, used and exchanged between professionals. All these issues are critical in healthcare applications because they ultimately reflect on the quality of care provided.
IR
bowman01hybrid
Hybrid Shipping Architectures: A Survey Recent advances in relational database systems include distributed systems that can choose to execute portions of query processing functionality at server or client sites. A symmetric problem that has received little attention is the partitioning of client application functionality between client and server. This report presents a survey of the literature related to both of these partitioning problems.
DB
marsh01evaluating
Evaluating Guidelines for Reducing User Disorientation When Navigating in Virtual Environments Navigation in virtual environments can be difficult. One contributing factor is user disorientation.
HCI
rennie01improving
Improving Multi-class Text Classification with Naive Bayes The Problem: There are billions of text documents available in electronic form. More and more are becoming available every day. The Web itself contains over a billion documents. Millions of people send e-mail every day. Academic publications and journals are becoming available inelectronicform. Thesecollections and many others represent a massive amount of information that is easily accessible. However, seeking value in this huge collection requires organization. Many web sites offer a hierarchically-organized view of the Web. E-mail clients offer a systems for filtering e-mail. Numerous academic communities have a Web site that allows searching on papers and shows an organization of papers. However, organizing documents by hand or creating rules for filtering is painstaking and labor-intensive. This can be greatly aided by automated classifier systems. The accuracy of such systems determines their usefulness. We propose to use the Support Vector Machine (SVM) in conjunction with Error-Correcting Output Codes (ECOC) to improve the state-of-the-art in text classification. Motivation & Previous Work: In 1998, Joachims published results on a set of binary text classification experiments using the SVM [4]. The SVM yielded lower error than many other classification techniques. Yang followed later with experiments of her own on the same data set [5]. She used improved versions of Naive Bayes (NB) and kNN but still found that the SVM performed at least as well as all other classifiers she tried. She also found that the linear SVM performed as well as polynomial and RBF versions. Both papers used the SVM for binary text classification,
IR
52576
An Autonomous Spacecraft Agent Prototype . This paper describes the New Millennium Remote Agent (NMRA) architecture for autonomous spacecraft control systems. The architecture supports challenging requirements of the autonomous spacecraft domain not usually addressed in mobile robot architectures, including highly reliable autonomous operations over extended time periods in the presence of tight resource constraints, hard deadlines, limited observability, and concurrent activity. A hybrid architecture, NMRA integrates traditional real-time monitoring and control with heterogeneous components for constraint-based planning and scheduling, robust multi-threaded execution, and model-based diagnosis and reconfiguration. Novel features of this integrated architecture include support for robust closed-loop generation and execution of concurrent temporal plans and a hybrid procedural/deductive executive. We implemented a prototype autonomous spacecraft agent within the architecture and successfully demonstrated the prototype in the c...
AI
wiederhold96conceptual
The Conceptual Basis for Mediation Services Mediator modules comprise a layer of intelligent middleware services in information systems, linking data resources and application programs. Earlier programs that led to the concept of mediation were either constructed to support specific applications or provided extended services from databases. Intelligent mediators are being built now by careful domain knowledge acquisition and hand crafting the required code. In this paper we present the conceptual underpinning for automating the mediation process. Automation does not extend to fully automatic code generation, since additional knowledge is necessary to provide added value. The generation concept is based on the extraction of a hierarchical domain model out of the general network representing the available resources. Associated with the method are domain ontologies. Ontologies list the terms used by the models, and document their relationships. These terms provide the semantic foundation needed to perform the generation. This paper...
HCI
276915
Mixed-Initiative Interaction spect of effective multiagent collaboration to solve problems or perform tasks. In our minimal human-computer configuration, such tasks could include systems designed to interact with a user to design a kitchen, find the best airfare, coordinate an emergency relief mission, or teach the user how to use new equipment. Mixed-initiative  refers to a flexible interaction strategy, where each agent can contribute to the task what it does best. Furthermore, in the most general cases, the agents' roles are not determined in advance, but opportunistically negotiated between them as the problem is being solved. At any one time, one agent might have the initiative---controlling the interaction---while the other works to assist it, contributing to the interaction as required. At other times, the roles are reversed, and at other times again the agents might be working independently, assisting each other only when specifically asked. The agents dynamically adapt their interaction st
HCI
lim98extracting
Extracting Structures of HTML Documents Using a High-Level Stack Machine Information on the Web, which are conglomeration of heterogeneous data such as texts, images and audio clips, are often accessed through documents written according to the HTML specification [7]. According to the HTML specification, HTML documents are semistructured in nature. We propose a high-level stack machine (HSM) which accesses an HTML document through its URL and constructs a semistructured data graph (SDG) of the document. The SDG of an HTML document H precisely captures the structure of the semistructured data embedded in H based on the dependency relationship [11] among the data objects in H . HSM is configurable to accommodate a user's interest with respect to the HTML elements in H to be considered during the construction process of the SDG of H .  1 Introduction  During the early days of the World-Wide Web (WWW or Web), users heavily relied on the mouse-button-click navigation method through hyperlinks provided by Web browsers to retrieve information of interest and soon ...
DB
lim00comparison
A Comparison of Prediction Accuracy, Complexity, and Training Time of Thirty-three Old and New Classification Algorithms Twenty-two decision tree, nine statistical, and two neural network algorithms are compared on thirty-two datasets in terms of classification accuracy, training time, and (in the case of trees) number of leaves. Classification accuracy is measured by mean error rate and mean rank of error rate. Both criteria place a statistical, spline-based, algorithm called POLYCLASS at the top, although it is not statistically significantly different from twenty other algorithms. Another statistical algorithm, logistic regression, is second and third with respect to the two accuracy criteria. The most accurate decision tree algorithm is QUEST with linear splits, which ranks fourth and fifth, respectively. Although spline-based statistical algorithms tend to have good accuracy, they also require relatively long training times. POLYCLASS, for example, is third last in terms of median training time. It often requires hours of training compared to seconds for other algorithms. The QUEST and logistic re...
ML
clarkson00recognizing
Recognizing User's Context from Wearable Sensors: Baseline System INTRODUCTION We describe a baseline system for training and classifying natural situations. It is a baseline system because it will provide the reference implementation of the context classifier against which we can compare more sophisticated machine learning techniques. It should be understood that this system is a precursor to a system for understanding all types of observable context not just location. We are less interested in obtaining high precision and recall rates than we are in obtaining appropriate model structures for doing higher order tasks like clustering and prediction on a user's life activities. II. BACKGROUND There has been some excellent work on recognizing various kinds of user situations via wearable sensors. Starner [6] uses HMMs and omnidirectional and directional cameras to determine the user's location in a building and current action during a physical game. Aoki also uses a head mounted directional camera to d
HCI
bates98using
Using Events for the Scalable Federation of Heterogeneous Components The thesis of this paper is that, using our eventbased development principles, components that were not designed to interoperate, can be made to work together quickly and easily. The only requirement is that each component must be made event-based by adding an interface for registering interest in events and an interface for injecting actions. A component notifies an event to a distributed client if the parameters of an event, internal to the component, match the parameters of a particular registration. Heterogeneous components can be federated using event-based rules; rules can respond to events from any component by injecting actions into any other component. We show that the event paradigm is scalable by illustrating how event-based components can be located worldwide, using a federation of event brokers. Additionally, we illustrate with 3 event-based systems we have developed: a component-based multimedia system, a multi-user virtual worlds system and an augmented reality system for mobile users. Finally, we show how the event paradigm is also scalable enough to allow event federation of entire systems, not just single components. We illustrate by showing how we have federated the operation of the 3 featured eventbased systems. This enables, for example, real-world mobile users to appear as avatars in the appropriate locations in the VR world, and for these avatars to move in response to actual user movements.
HCI
415056
Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data We present conditional random elds, a framework  for building probabilistic models to segment  and label sequence data. Conditional random  fields offer several advantages over hidden  Markov models and stochastic grammars  for such tasks, including the ability to relax  strong independence assumptions made in those  models. Conditional random fields also avoid  a fundamental limitation of maximum entropy  Markov models (MEMMs) and other discriminative  Markov models based on directed graphical  models, which can be biased towards states  with few successor states. We present iterative  parameter estimation algorithms for conditional  random fields and compare the performance of  the resulting models to HMMs and MEMMs on  synthetic data.  1. Introduction  The need to segment and label sequences arises in many different problems in several scientific fields. Hidden Markov models (HMMs) and stochastic grammars are well understood and widely used probabilistic models for such problems. I...
IR
525666
UML for Behavior-Oriented Multi-Agent Simulations Developing multi-agent simulations seems to be rather straight  forward, as active entities in the original correspond to active agents in  the model. Thus plausible behaviors can be produced rather easily. However,  for real world applications they must satisfy some requirements  concerning verification, validation and reproducibility. Using a standard  framework for designing a multi-agent model one can gain further advantages  like fast learnability, wide understandability and possible transfer.
Agents
520488
Node Similarity in Networked Information Spaces Netvorked information spaces contain information entities, corresponding to nodes, vhich  are (:orlrle(:l.ed [)y associm.i(ms, (:or'r'esporldirlg 1.o links irl (.he nel.wor'k. Exarrq)les of nel.wor'ked  information spaces are: the World Wide Web, vhere information entities are veb pages, and  associations are hyperlinks; the scientific literature, vhere information entities are articles and  associations are references to other articles. SimilariW betveen information entities in a net-  vorked information space can be defined not only based on the content of the information  entities, but also based on the connectivity established by the associations present. This paper  explores the definition of similariW based on connectivity only, and proposes several algorithms  [)r' I, his [mr'pose. Our' rrlei, r'i(:s I,ake mJvard,age o[' I, he local rleigh[)or'hoo(ts o[' I, he rmcJes irl I, he rlel,-  is no required, as long as a query engine is available for fo]]oving ]inks and extracting he  necessary local neighbourhoods for similarity estimation. Tvo variations of similarity estimation  beveen vo nodes are described, one based on he separate local neighbourhoods of he  nodes, and another based on he join local neighbourhood expanded from boh nodes a he  same ime. The algorithms are imp]emened and evaluated on he citation graph of computer  science. The immediate application of his vork is in finding papers similar o a given paper  [he Web.
IR
461688
Visual Web Information Extraction with Lixto We present new techniques for supervised  wrapper generation and automated web information  extraction, and a system called Lixto  implementing these techniques. Our system  can generate wrappers which translate relevant  pieces of HTML pages into XML. Lixto,  of which a working prototype has been implemented,  assists the user to semi-automatically  create wrapper programs by providing a fully  visual and interactive user interface. In this  convenient user-interface very expressive extraction  programs can be created. Internally,  this functionality is reflected by the new logicbased  declarative language Elog. Users never  have to deal with Elog and even familiarity  with HTML is not required. Lixto can be used  to create an "XML-Companion" for an HTML  web page with changing content, containing  the continually updated XML translation of  the relevant information.  1 
IR
375693
Recent Publications of the "Multimedia Information Access" Research Group elligence, pages 7--35, Buenos Aires, AR, 1999.  [ Straccia, 1997a ] Umberto Straccia. A four-valued fuzzy propositional logic. In Proceedings of IJCAI-97, 15th International Joint Conference on Artificial Intelligence, pages 128--133, Nagoya, JP, 1997.  [ Straccia, 1997b ] Umberto Straccia. A sequent calculus for reasoning in four-valued description logics. In Proceedings of TABLEAUX-97, International Conference on Analytic Tableaux and Related Methods, pages 343--357, Pont-a-Mousson, FR, 1997. Published in the "Lecture Notes in Computer Science" series, number 1227, Springer Verlag, Heidelberg, DE.  [ Straccia, 1998 ] Umberto Straccia. A fuzzy description logic. In Proceedings of AAAI-98, 15th Conference of the American Association for Artificial Intelligence, pages 594--599, Madison, US, 1998.  [ Straccia, 1999 ] Umberto Straccia. Four-valued fuzzy description logics for representing multimedia objects content. In Fabio Crestani and Ga
IR
90507
An Evaluation of Statistical Approaches to Text Categorization Abstract. This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.
IR
locatelli00simulated
Simulated Annealing Algorithms For Continuous Global Optimization INTRODUCTION  In this paper we consider Simulated Annealing algorithms (SA in what follows) applied to continuous global optimization problems, i.e. problems with the following form  f  = min  x2X  f(x); (1.1) where X ` !  n  is a continuous domain, often assumed to be compact, which, combined with the continuity or lower semicontinuity of f , guarantees the existence of the minimum value f  . SA algorithms are based on an analogy with a physical phenomenon: while at high temperatures the molecules in a liquid move freely, if the temperature is slowly decreased the thermal mobility of the molecules is lost and they form a pure crystal which also corresponds to a state of minimum energy. If the temperature is decreased too quickly (the so called quenching) a liquid metal rather ends up in a polycrystalline or amorphous state with
AI
fensel98ontobroker
Ontobroker: Or How to Enable Intelligent Access to the WWW . The World Wide Web (WWW) is currently one of the most important  electronic information sources. However, its query interfaces and the provided  reasoning services are rather limited. Ontobroker consists of a number of  languages and tools that enhance query access and inference service in the  WWW. It provides languages to annotate web documents with ontological  information, to represent ontologies, and to formulate queries. The tool set of  Ontobroker allows us to access information and knowledge from the web and to  infer new knowledge with an inference engine based on techniques from logic  programming. This article provides several examples that illustrate these  languages and tools and the kind of service that is provided. We also discuss the  bottlenecks of our approach that stem from the fact that the applicability of  Ontobroker requires two time-consuming activities: (1) developing shared  ontologies that reflect the consensus of a group of web users and (2) annotating  we...
IR
mcilraith01semantic
Semantic Web Services hose properties, capabilities,  interfaces, and effects are encoded in an unambiguous,  machine-understandable form.  The realization of the Semantic Web is underway  with the development of new AI-inspired content  markup languages, such as OIL,  3  DAML+OIL  (www.daml.org/2000/10/daml-oil), and DAML-L (the  last two are members of the DARPA Agent Markup  Language (DAML) family of languages).  4  These languages  have a well-defined semantics and enable the  markup and manipulation of complex taxonomic and  logical relations between entities on the Web. A fundamental  component of the Semantic Web will be the markup of Web services to make them computer-interpretable, use-apparent, and agent-ready. This article addresses precisely this component.  We present an approach to Web service markup that provides an agent-independent declarative API capturing the data and metadata associated with a service together with specifications of its pro
IR
herlea01secure
Secure Meeting Scheduling with Agenta When people want to schedule a meeting, the agendas of the participants must be compared to nd a time suitable for all of them. However, at the same time participants want to keep their agendas private. This paper presents a negotiation protocol which tries to solve this contradiction. The protocol is implemented in the agenTa system using mobile software agents, hereby alleviating communication overhead and allowing disconnected operation.  Keywords: mobile agents, secure distributed computation, meeting scheduling  1. 
Agents
sloman98damasio
Damasio, Descartes, Alarms and Meta-management This paper discusses some of the requirements for the control architecture of an intelligent human-like agent with multiple independent dynamically changing motives in a dynamically changing only partly predictable world. The architecture proposed includes a combination of reactive, deliberative and meta-management mechanisms along with one or more global "alarm" systems. The engineering design requirements are discussed in relation our evolutionary history, evidence of brain function and recent theories of Damasio and others about the relationships between intelligence and emotions.  1. INTRODUCTION  Stan Franklin, the organiser of this symposium, wrote "Minds are the control structures of autonomous agents" [5, p 412]. The claim that minds are essentially concerned with control, echoing the seminal ideas of Norbert Wiener [16] is one with which I strongly concur though as argued in [11], we need to go far beyond the early idea of control systems with fixed architecture and changes on...
AI
wilson01framework
A Framework for Linking Distributed Simulations Using Software Agents This paper presents the basic ideas behind the use of software agent technology for distributed simulation and data assimilation. A software agent is an autonomous computer program that operates on behalf of someone or something. A mobile agent has the ability to migrate during execution from machine to machine in a heterogeneous network, while a stationary agent executes only on the system on which it began execution.
Agents
buckinghamshum00scholonto
ScholOnto: An Ontology-Based Digital Library Server for Research Documents and Discourse . The internet is rapidly becoming the first place for researchers to publish documents, but at present they receive little support in searching, tracking, analyzing or debating concepts in a literature from scholarly perspectives. This paper describes the design rationale and implementation of ScholOnto, an ontology-based digital library server to support scholarly interpretation and discourse. It enables researchers to describe and debate via a semantic network the contributions a document makes, and its relationship to the literature. The paper discusses the computational services that an ontology-based server supports, alternative user interfaces to support interaction with a large semantic network, usability issues associated with knowledge formalization, new work practices that could emerge, and related work.  2 1 Introduction  It is becoming standard practice for researchers to publish their documents on the internet (or intranets), via personal, institutional and discipline-spe...
IR
norman96alarms
Alarms: An implementation of motivated agency . Autonomous agents in the real world must be capable of asynchronous goal generation. However, one consequence of this ability is that the agent may generate a substantial number of goals, but only a small number of these will be relevant at any one time. Therefore, there is a need for tractable mechanisms to manage a changing and potentially large number of goals. This paper presents both a framework for the design of agents that have the capability to generate and manage their own top level goals, "motivated agency", and an implementation of part of this agent architecture, "alarms". The alarm-processing machinery serves to focus the attention of an agent on a limited number of the most salient goals regardless of the number of possible goals that the agent can pursue or their distribution in time. In this way, a resource-bounded autonomous agent can employ modern planning methods to greater effect. 1 Introduction  If an autonomous agent is required to interact with an environment t...
Agents
burnett01forms
Forms/3: A First-Order Visual Language to Explore the Boundaries of the Spreadsheet Paradigm Although detractors of functional programming sometimes claim that functional programming is too difficult or counterintuitive for most programmers to understand and use, evidence to the contrary can be found by looking at the popularity of spreadsheets. The spreadsheet paradigm, a first-order subset of the functional programming paradigm, has found wide acceptance among both programmers and end users. Still, there are many limitations with most spreadsheet systems.  In this paper, we discuss language features that eliminate several of these limitations without deviating from the first-order, declarative evaluation model. The language used to illustrate these features is a research language called Forms/3. Using Forms/3, we show that procedural abstraction, data abstraction, and graphics output can be supported in the spreadsheet paradigm. We show that, with the addition of a simple model of time, animated output and GUI I/O also become viable. To demonstrate generality, we also presen...
HCI
306088
Is Paper Safer? The Role of Paper Flight Strips in Air Traffic Control Air traffic control is a complex, safety-critical activity, with well-established and successful work practices. Yet many attempts to automate the existing system have failed because controllers remain attached to a key work artifact: the paper flight strip. This article describes a four-month intensive study of a team of Paris en route controllers in order to understand their use of paper flight strips. The article also describes a comparison study of eight different control rooms in France and the Netherlands. Our observations have convinced us that we do not know enough to simply get rid of paper strips, nor can we easily replace the physical interaction between controllers and paper strips.  These observations highlight the benefits of strips, including qualities difficult to quantify and replicate in new computer systems. Current thinking offers two basic alternatives: maintaining the existing strips without computer support and bearing the financial cost of limiting the air traff...
HCI
olson98probabilistic
A Probabilistic Formulation for Hausdorff Matching Matching images based on a Hausdorff measure has become popular for computer vision applications. However, no probabilistic model has been used in these applications. This limits the formal treatment of several issues, such as feature uncertainties and prior knowledge. In this paper, we develop a probabilistic formulation of image matching in terms of maximum likelihood estimation that generalizes a version of Hausdorff matching. This formulation yields several benefits with respect to previous Hausdorff matching formulations. In addition, we show that the optimal model position in a discretized pose space can be located efficiently in this formation and we apply these techniques to a mobile robot self-localization problem. 1 Introduction  The use of variants of the Hausdorff distance has recently become popular for image matching applications (see, for example, [6, 9, 11, 16, 18, 19]). While these methods have been largely successful, they have lacked a probabilistic formulation of th...
AI
guo00learning
Learning Similarity for Texture Image Retrieval A novel algorithm is proposed to learn pattern similarities for texture image retrieval. Similar patterns in different texture classes  are grouped into a cluster in the feature space. Each cluster is isolated from others by an enclosed boundary, which is represented by several  support vectors and their weights obtained from a statistical learning  algorithm called support vector machine (SVM). The signed distance of  a pattern to the boundary is used to measure its similarity. Furthermore,  the patterns of different classes within each cluster are separated  by several sub-boundaries, which are also learned by the SVMs. The  signed distances of the similar patterns to a particular sub-boundary  associated with the query image are used for ranking these patterns.  Experimental results on the Brodatz texture database indicate that the  new method performs significantly better than the traditional Euclidean  distance based approach.
IR
marquez00machine
Machine Learning and Natural Language Processing In this report, some collaborative work between the fields of Machine Learning (ML) and Natural Language Processing (NLP) is presented. The document is structured in two parts. The first part includes a superficial but comprehensive survey covering the state--of--the--art of machine learning techniques applied to natural language learning tasks. In the second part, a particular problem, namely Word Sense Disambiguation (WSD), is studied in more detail. In doing so, four algorithms for supervised learning, which belong to different families, are compared in a benchmark corpus for the WSD task. Both qualitative and quantitative conclusions are drawn. This document stands for the complementary documentation for the conference "Aprendizaje autom 'atico aplicado al procesamiento del lenguaje natural", given by the author within the course: "Curso de Industrias de la Lengua: La Ingenier'ia Lingu'istica en la Sociedad de la Informaci'on", Fundaci'on Duques de Soria. Soria. July 2000. 1 Con...
ML
artale00expressivity
On the Expressivity and Complexity of Temporal Conceptual Modelling The contribution of this paper is twofold. On the one hand, it introduces T DLR, a novel temporal  logic for temporal conceptual modelling, motivated as the obvious generalisation of the successful DLR  Description Logic. Tight decidability and complexity results are proved for T DLR and the monodic  fragment of it (T DLR ). Moreover, the decidability of conjunctive query containment under T DLR  constraints is proved. On the other hand, the paper provides a formal semantic characterisation of all  the important temporal conceptual modelling constructs (for valid time representation) as found in the  literature. To the best of our knowledge, this is the first systematic formalisation of the constructs present  in most temporal conceptual modelling systems. This systematic characterisation as T DLR theories is an  evidence of the adequacy of the T DLR temporal Description Logic for temporal conceptual modelling.  1 Introduction  In this paper the novel T DLR temporal logic is introduced...
DB
sadri00abduction
Abduction with Negation as Failure for Active and Reactive Rules . Recent work has suggested abductive logic programming as a  suitable formalism to represent active databases and intelligent agents. In  particular, abducibles in abductive logic programs can be used to represent  actions, and integrity constaints in abductive logic programs can be  used to represent active rules of the kind encountered in active databases  and reactive rules incorporating reactive behaviour in agents. One would  expect that, in this approach, abductive proof procedures could provide  the engine underlying active database management systems and the behaviour  of agents. We analyse existing abductive proof procedures and  argue that they are inadequate in handling these applications. The inadequacy  is due to the inappropriate treatment of negative literals in  integrity constraints. We propose a new abductive proof procedure and  give examples of how this proof procedure can be used to achieve active  behaviour in (deductive) databases and reactivity in agents. Final...
DB
romer01smart
Smart Playing Cards: A Ubiquitous Computing Game Abstract. Recent technological advances allow for turning parts of our everyday environment into so–called smart environments. In this paper we present the “Smart Playing Cards ” application, a ubiquitous computing game that augments a classical card game with information–technological functionality, in contrast to developing new games around the abilities of available technology. Furthermore, we present the requirements such an application makes on a supporting software infrastructure for ubiquitous computing.
HCI
baker98distributional
Distributional Clustering of Words for Text Classification This paper applies Distributional Clustering (Pereira  et al. 1993) to document classification. The approach clusters words into groups based on the distribution of class labels associated with each word. Thus, unlike some other unsupervised dimensionality-reduction techniques, such as Latent Semantic Indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy. Experimental results obtained on three real-world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2% accuracy---significantly better than Latent Semantic Indexing (Deerwester et al. 1990), class-based clustering (Brown et al. 1992), feature selection by mutual information (Yang and Pederson 1997), or Markovblanket -based feature selection (Koller and Sahami 1996). We also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clusteri...
ML
shneiderman00direct
Direct Annotation: A Drag-and-Drop Strategy for Labeling Photos Annotating photos is such a time-consuming, tedious and error-prone data entry task that it discourages most owners of personal photo libraries. By allowing users to drag labels such as personal names from a scrolling list and drop them on a photo, we believe we can make the task faster, easier and more appealing. Since the names are entered in a database, searching for all photos of a friend or family member is dramatically simplified. We describe the user interface design and the database schema to support direct annotation, as implemented in our PhotoFinder prototype.  Keywords: direct annotation, direct manipulation, graphical user interfaces, photo libraries, drag-and-drop, label placement  1. Introduction  Adding captions to photos is a time-consuming and error prone task for professional photographers, editors, librarians, curators, scholars, and amateur photographers. In many professional applications, photos are worthless unless they are accurately described by date, time, loc...
HCI
60136
Text Classification by Bootstrapping with Keywords, EM and Shrinkage When applying text classification to complex tasks, it is tedious and expensive to hand-label the large amounts of training data necessary for good performance. This paper presents an alternative approach to text classification that requires no labeled documents; instead, it uses a small set of keywords per class, a class hierarchy and a large quantity of easilyobtained unlabeled documents. The keywords are used to assign approximate labels to the unlabeled documents by termmatching. These preliminary labels become the starting point for a bootstrapping process that learns a naive Bayes classifier using Expectation-Maximization and hierarchical shrinkage. When classifying a complex data set of computer science research papers into a 70-leaf topic hierarchy, the keywords alone provide 45% accuracy. The classifier learned by bootstrapping reaches 66% accuracy, a level close to human agreement on this task. 1 Introduction  When provided with enough labeled training examples, a variety of ...
IR
laaksonen99picsom
PicSOM - A Framework for Content-Based Image Database Retrieval using Self-Organizing Maps We have developed an image retrieval system which uses Tree Structured Self-Organizing Maps (TS-SOMs) as the method for retrieving images similar to a given set of reference images in a database. It also provides a framework for the research on algorithms and methods for contentbased retrieval of images. A novel technique introduced in this paper facilitates automatic combination of the responses from multiple TS-SOMs and their hierarchical levels. The system tries to adapt to the user's preferences in selecting which images resemble each other in the particular sense the user is interested of. This mechanism implements a relevance feedback technique on content-based image retrieval. The image queries are performed through the World Wide Web and the queries are iteratively reøned as the system exposes more images to the user. 1 Introduction  Content-based image retrieval from unannotated image databases has been an object for ongoing research for a long period [12]. Digital image and v...
IR
greenberg99adapting
Adapting the Locales Framework for Heuristic Evaluation of Groupware Heuristic evaluation is a rapid, cheap and effective way for identifying usability problems in single user systems. However, current heuristics do not provide guidance for discovering problems specific to groupware usability. In this paper, we take the Locales Framework and restate it as heuristics appropriate for evaluating groupware. These are: 1) Provide locales; 2) Provide awareness within locales; 3) Allow individual views; 4) Allow people to manage and stay aware of their evolving interactions; and 5) Provide a way to organize and relate locales to one another. To see if these new heuristics are useful in practice, we used them to inspect the interface of Teamwave Workplace, a commercial groupware product. We were successful in identifying the strengths of Teamwave as well as both major and minor interface problems.  KEY WORDS:  Groupware evaluation, heuristic evaluation, inspection methods, locales framework.  1 INTRODUCTION  HCI researchers and practitioners now have a good rep...
HCI
takahashi00location
Location Oriented Integration of Internet Information - Mobile Info Search - Information on the Internet is becoming more attractive and  useful for our daily life. It provides things on the town, happenings on  the city, and learning of the real world. If we can utilize such information  for the interaction between the human and the city, it can enhance the  value and the function of the city. In this paper we introduce the research  project "Mobile Info Search" in which we study the method of integrating  heterogeneous information in a location-oriented way for providing it in  a handy form with mobile computing. We have a prototype of Mobile  Info Search at http://www.kokono.net/, a location-based "search engine".  Local information such as yellow pages, maps, and relevant Web pages  at any location of Japan are provided with a simple interface. From the  analysis of test services, we will discuss the user issues and information  source issues; What kind of local information is welcomed? What can  we learn from collected documents? Through the experience of handling  various contents related to the real-world, we describe the potential of  the Internet information for the digital city efforts.  1 
IR
77796
Point-based Temporal Extensions of SQL and their Efficient Implementation . This chapter introduces a new approach to temporal extensions of SQL. The main difference from most of the current proposals is the use single time points, rather than intervals or various other complexvalues for references to time, while still achieving efficient query evaluation. The proposed language, SQL/TP, extends the syntax of SQL/92 to handle temporal data in a natural way: it adds a single data type to represent a linearly ordered universe of time instants. The semantics of the new language naturally extends the standard SQL semantics and eliminates or fixes many of the problems connected with defining a precise semantics to temporal query languages based on explicit interval-valued temporal attributes. The efficient query evaluation procedure is based on a compilation technique that translates SQL/TP queries to SQL/92. Therefore existing off-shelf database systems can be used as back-ends for implementations based on this approach to manage temporal data. 1 Why another temp...
DB
mamoulis98integration
Integration of Spatial Join Algorithms for Joining Multiple Inputs Several techniques that compute the join between two spatial datasets have been proposed during the last decade. Among these methods, some consider existing indices for the joined inputs, while others treat datasets with no index, thus providing solutions for the case where at least one input comes as an intermediate result of another database operator. In this paper we analyze previous work on spatial joins and propose a novel algorithm, called slot index spatial join (SISJ), that efficiently computes the spatial join between two inputs, only one of which is indexed by an R-tree. Going one step further, we show how SISJ and other spatial join algorithms can be implemented as operators in a database environment that joins more than two spatial inputs. We study the differences between relational and spatial multi-way joins, and propose a dynamic programming algorithm that optimizes the execution of complex spatial queries. Contact Author: Dimitris Papadias Tel: ++852-23586971 http://www...
DB
feris00tracking
Tracking Facial Features Using Gabor Wavelet Networks . This work presents a new method for automatic facial feature tracking in video sequences. In this method, a discrete face template is represented as a linear combination of continuous 2D odd-Gabor wavelet functions. The weights and 2D parameters (position, scale and orientation) of each wavelet are determined optimally so that the maximum of image information is preserved for a given number of wavelets. We have used this representation to achieve effective facial feature tracking that is robust to homogeneous illumination changes and affine deformations of the face image. Moreover, the tracking approach considers the overall geometry of the face, being robust to facial feature deformations such as eye blinking and smile. The number of wavelets in the representation may be chosen with respect to the available computational resources, even allowing real-time processing. 1
HCI
campos01searching
Searching the Unsearchable: Inducing Serendipitous Insights Although no serious efforts seem to have been devoted yet to the theoretical and experimental study of the phenomenon, the web is recognizably a well suited medium for information encountering, the accidental discovery of information that is not sought for. This is the very essence of serendipity, the faculty of making fortunate and unexpected discoveries by accident. This paper presents Max, a sof t ware agent that uses simple information retrieval techniques and heuristic search to wander on the Internet and uncover useful, and not sought for, information that may stimulate serendipitous insights.
IR
453602
Relevance Feedback and Personalization: A Language Modeling Perspective Many approaches to personalization involve learning short-term and long-term user models. The user models provide context for queries and other interactions with the information system. In this paper, we discuss how language models can be used to represent context and support context-based techniques such as relevance feedback and query disambiguation.  1. Overview  From some perspectives, personalization has been studied in information retrieval for some time. If the goal of personalization is to improve the effectiveness of information access by adapting to individual users' needs, then techniques such as relevance feedback and filtering would certainly be considered to support personalization. There has also been considerable research done, mostly in the 1980s, on user modeling for information retrieval. This research had essentially the same goal as current research on personalization, which is to build a model of a user's interests and preferences over time. Filtering systems, too...
IR
bellatreche00what
What can Partitioning do for your Data Warehouses and Data Marts? Efficient query processing is a critical requirement for data warehousing systems as decision support  applications often require minimum response times to answer complex, ad-hoc queries having aggregations,  multi-ways joins over vast repositories of data. This can be achieved by fragmenting warehouse  data. The data fragmentation concept in the context of distributed databases aims to reduce query  execution time and facilitates the parallel execution of queries. In this paper, we propose a methodology  for applying the fragmentation technique in a Data Warehouse (DW) star schema to reduce the total  query execution cost. We present an algorithm for fragmenting the tables of a star schema. During  the fragmentation process, we observe that the choice of the dimension tables used in fragmenting the  fact table plays an important role on overall performance. Therefore, we develop a greedy algorithm in  selecting "best" dimension tables. We propose an analytical cost model for executing a set of OLAP  queries on a fragmented star schema. Finally, we conduct some experiments to evaluate the utility of  fragmentation for efficiently executing OLAP queries.  Key Words : Data Warehouses, Star schema, Fragmentation, Query Optimization, Performance Evaluation  1 
DB
oyama00cooperative
Cooperative Information Agents for Digital Cities This paper presents an architecture for digital cities and shows the roles of agent
Agents
ciancarini02coordination
Coordination Middleware for XML-centric Applications This paper focuses on coordination middleware for distributed ap-plications based on active documents and XML technologies. It in-troduces the main concepts underlying active documents and XML Then, the paper goes into details about the problem of defining a suitable middleware architecture to effectively support coordina-tion activities in applications including active documents and mo-bile agents, by specifically focusing on the role played by XML technologies in that context. According to a simple taxonomy, the characteristics of several middleware systems are analyzed and evaluated. This analysis enables us to identify the advantages and the shortcoming of the different approaches, and to identify the ba-sic requirements of a middleware for XML-centric applications. 1.
Agents
294238
Learning to Recognize Objects A learning account for the problem of object recognition is developed within the PAC (Probably Approximately Correct) model of learnability. The proposed approach makes no assumptions on the distribution of the observed objects, but quantifies success relative to its past experience. Most importantly, the success of learning an object representation is naturally tied to the ability to represent it as a function of some intermediate representations extracted from the image. We evaluate this approach in a large scale experimental study in which the SNoW learning architecture is used to learn representations for the 100 objects in the Columbia Object Image Database (COIL-100). The SNoW-based method is shown to outperform other methods in terms of recognition rates; its performance degrades gracefully when the training data contains fewer views and in the presence of occlusion noise.
ML
458592
Learning Lateral Interactions for Feature Binding and Sensory Segmentation We present a new approach to the supervised learning of lateral interactions  for the competitive layer model (CLM) dynamic feature binding  architecture. The method is based on consistency conditions, which were  recently shown to characterize the attractor states of this linear threshold  recurrent network. For a given set of training examples the learning problem  is formulated as a convex quadratic optimization problem in the lateral  interaction weights. An efficient dimension reduction of the learning  problem can be achieved by using a linear superposition of basis interactions.  We show the successful application of the method to a medical  image segmentation problem of fluorescence microscope cell images.  1 
AI
sparkman01automated
Automated Derivation of Complex Agent Architectures from Analysis Specifications . Multiagent systems have been touted as a way to meet the need for  distributed software systems that must operate in dynamic and complex  environments. However, in order for multiagent systems to be effective, they  must be reliable and robust. Engineering multiagent systems is a non-trivial  task, providing ample opportunity for even experts to make mistakes. Formal  transformation systems can provide automated support for synthesizing  multiagent systems, which can greatly improve their correctness and reliability.  This paper describes a semi-automated transformation system that generates an  agent's internal architecture from an analysis specification in the MaSE  methodology.  1 
Agents
bylund99coordinating
Coordinating Adaptations in Open Service Architectures An Open Service Architecture (OSA) is a software structure that makes an open set of information services available to an open set of users. The World Wide Web constitutes the most outstanding example of an OSA as of today. An important feature of an OSA is personalization, i.e. adapting the user interface, functionality, and information of services to its users. However, designers of such a feature are facing many problems, perhaps the biggest one being coordination. If services fail to coordinate how they adapt to users, chances are that the whole point of performing the adaptation, i.e. helping the user, is lost.  In this thesis, I lay out a framework for describing and reasoning about adaptive systems in Open Service Architectures, with a special emphasis on coordination. This framework is mainly meant for analysis and design, but some of the ideas presented are also suitable as metaphors for implementations. An implementation of an adaptive system that was designed using this fram...
HCI
advani99integrating
Integrating a Modern Knowledge-Based System Architecture with a Legacy VA Database: The ATHENA and EON Projects at Stanford ion  Output:  Guideline-Based  Treatment Advice  Guideline  KB Server:  Protg PSM  Guideline  Interpreter:  EON/PCA PSM  Guideline-Based  Quality Assessment:  MedCritic PSM  VA  Medical  Record:  VISTA/  DHCP  EndUser  GUI  (CPRS)  EndUser  GUI  (CPRS)  Legacy  Database  Mediator:  Athenaeum  PSM  Output:  Assessment of  Physician Actions  Tzolkin  DBMS PSM  Temporal  Abstraction  Tzolkin  DBMS PSM  Temporal  Abstraction  Output:  Guideline-Based  Treatment Advice  Figure 1 ATHENA system incorporating the EON architecture for component-based decision-support. Each EON component carries out a specific problemsolving task for automated decision-support of guideline-based care.  Advani, et al. 2  in the EON architecture to carry it out. Such tasks include the monitoring the execution of an applied guideline, using the EON Protocol Compliance Advisor (PCA) PSM and assessing the quality of the guideline-based treatment, using the MedCritic PSM (see Figure 1).  THE ATHENAEUM MEDIATOR  All th...
DB
najork01highperformance
High-Performance Web Crawling SRC’s charter is to advance the state of the art in computer systems by doing basic and applied research in support of our company’s business objectives. Our interests and projects span scalable systems (including hardware, networking, distributed systems, and programming-language technology), the Internet (including the Web, e-commerce, and information retrieval), and human/computer interaction (including user-interface technology, computer-based appliances, and mobile computing). SRC was established in 1984 by Digital Equipment Corporation. We test the value of our ideas by building hardware and software prototypes and assessing their utility in realistic settings. Interesting systems are too complex to be evaluated solely in the abstract; practical use enables us to investigate their properties in depth. This experience is useful in the short term in refining our designs and invaluable in the long term in advancing our knowledge. Most of the major advances in information systems have come through this approach, including personal computing, distributed systems, and the Internet. We also perform complementary work of a more mathematical character. Some of
IR
scholer02compression
Compression of Inverted Indexes For Fast Query Evaluation Compression reduces both the size of indexes and the time needed to evaluate queries. In this paper, we revisit the compression of inverted lists of document postings that store the position and frequency of indexed terms, considering two approaches to improving retrieval efficiency: better implementation and better choice of integer compression schemes. First, we propose several simple optimisations to well-known integer compression schemes, and show experimentally that these lead to significant reductions in time. Second, we explore the impact of choice of compression scheme on retrieval efficiency. In experiments on large collections of data, we show two surprising results: use of simple byte-aligned codes halves the query evaluation time compared to the most compact Golomb-Rice bitwise compression schemes; and, even when an index fits entirely in memory, byte-aligned codes result in faster query evaluation than does an uncompressed index, emphasising that the cost of transferring data from memory to the CPU cache is less for an appropriately compressed index than for an uncompressed index. Moreover, byte-aligned schemes have only a modest space overhead: the most compact schemes result in indexes that are around 10 % of the size of the collection, while a byte-aligned scheme is around 13%. We conclude that fast byte-aligned codes should be used to store integers in inverted lists.
IR
dwork01rank
Rank Aggregation Methods for the Web We consider the problem of combining ranking results from various sources. In the context of the Web, the main applications include building meta-search engines, combining ranking functions, selecting documents based on multiple criteria, and improving search precision through word associations. Wedevelop a set of techniques for the rank aggregation problem and compare their performance to that of well-known methods. A primary goal of our work is to design rank aggregation techniques that can effectively combat "spam," a serious problem in Web searches. Experiments show that our methods are simple, efficient, and effective.  Keywords: rank aggregation, ranking functions, metasearch, multi-word queries, spam  1. 
IR
bozzano99logic
Logic Programming & Multi-Agent Systems: a Synergic Combination for Applications and Semantics The paper presents an ongoing research project that uses Logic Programming, Linear Logic Programming, and their related techniques for executable specifications and rapid prototyping of Multi-Agent Systems. The MAS paradigm is an extremely rich one and we believe that Logic Programming will play a very effective role in this area, both as a tool for developing real applications and as a semantically well founded language for basing program analysis and proof of properties on.
Agents
logan00distributed
The Distributed Simulation of Multi-Agent Systems Agent-based systems are increasingly being applied in a wide range of areas including telecommunications, business process modelling, computer games, control of mobile robots and military simulations. Such systems are typically extremely complex and it is often useful to be able to simulate an agent-based system to learn more about its behaviour or investigate the implications of alternative architectures. In this paper, we discuss the application of distributed discrete-event simulation techniques to the simulation of multi-agent systems. We identify the efficient distribution of the agents' environment as a key problem in the simulation of agent-based systems, and present an approach to the decomposition of the environment which facilitates load balancing.
Agents
su00replicable
A Replicable Web-Based Negotiation Server For E-Commerce This paper describes our ongoing R&D effort in developing a replicable, Web-based negotiation server to conduct bargaining-type negotiations between clients (i.e., buyers and sellers) in e-commerce. Multiple copies of this server can be paired with existing Web-servers to provide negotiation capabilities. Each client can select a trusted negotiation server to represent his/her interests. Web-based GUI tools are used by clients in a build-time registration process to specify the requirements, constraints, negotiation strategic rules, and preference scoring methods related to the buying or selling of a product. The registration information is used by the negotiation servers to conduct negotiations automatically on behalf of the clients. In this paper, we present the architecture of the negotiation server and the framework for automated negotiations, and describe a number of communication primitives, which make up the negotiation protocol. We have developed a constraint satisfaction processor (CSP) to evaluate a negotiation proposal against the registered constraints. An Event-Trigger-Rule (ETR) server manages events and triggers the execution of strategic rules, which may relax constraints, notify clients, or perform other operations. Strategic rules can be added and modified at run-time to deal with the dynamic nature of negotiations. A cost-benefit analysis performs quantitative analysis of alternative negotiation conditions. We have implemented a prototype system to demonstrate automated negotiations among buyers and suppliers in a supply chain management system.
Agents
271585
Topical Locality in the Web Most web pages are linked to others with related content. This idea, combined with another that says that text in, and possibly around, HTML anchors describe the pages to which they point, is the foundation for a usable WorldWide Web. In this paper, we examine to what extent these ideas hold by empirically testing whether topical locality mirrors spatial locality of pages on the Web. In particular, we find that the likelihood of linked pages having similar textual content to be high; the similarity of sibling pages increases when the links from the parent are close together; titles, descriptions, and anchor text represent at least part of the target page; and that anchor text may be a useful discriminator among unseen child pages. These results show the foundations necessary for the success of many web systems, including search engines, focused crawlers, linkage analyzers, and intelligent web agents.
IR
labrou99interoperability
The Interoperability Problem: Bringing together Mobile Agents and Agent Communication Languages Interoperability is a central issue for both the mobile agents community and the wider agents community. Unfortunately, the interoperability concerns are different between the two communities. As a result, inter-agent communication is an issue that has been addressed in a limited manner by the mobile agents community. Agent communication languages (ACLs) have been developed as tools with the capacity to integrate disparate sources of information and support interoperability but have achieved limited use by mobile agents. We investigate the origins of the differences of perspective on agent-to-agent communication, examine the reasons for the relative lack of interest in ACLs by mobile agents researchers and explore the integration of ACLs into mobile agents frameworks.  Copyright 1999 IEEE. Published in the Proceedings of the Hawaii International Conference On System Sciences, January 5-8, 1999, Maui, Hawaii. 1. Introduction  There are two kinds of discussions that have plagued agent-re...
Agents
sandholm01side
Side Constraints and Non-Price Attributes in Markets In most real-world (electronic) marketplaces, there  are other considerations besides maximizing immediate  economic value. We present a sound way  of taking such considerations into account via side  constraints and non-price attributes. Side constraints  have a significant impact on the complexity  of market clearing. Budget constraints, a limit on  the number of winners, and XOR-constraints make  even noncombinatorial markets ##-complete to  clear. The latter two make markets ##-complete  to clear even if bids can be accepted partially. This  is surprising since, as we show, even combinatorial  markets with a host of very similar side constraints  can be cleared in polytime. An extreme equality  constraint makes combinatorial markets polytime  clearable even if bids have to be accepted entirely  or not at all. Finally, we present a way to  take into account additional attributes using a bid  re-weighting scheme, and prove that it does not  change the complexity of clearing. All of the results  hold for auctions as well as exchanges, with  and without free disposal.  1 
Agents
beimel99oneway
One-way Functions are Essential for Single-Server Private Information Retrieval Private Information Retrieval (PIR) protocols allow a user to read information from a database without revealing to the server storing the database which information he has read. Kushilevitz and Ostrovsky [23] construct, based on the quadratic residuosity assumption, a single-server PIR protocol with small communication complexity. Cachin, Micali, and Stadler [5] present a single-server PIR protocol with a smaller communication complexity, based on the (new) \Phihiding assumption. A major question, addressed in the present work, is what assumption is the minimal assumption necessary for the construction of single-server private information retrieval protocols with small communication complexity. We prove that if there is a (0-error) PIR protocol in which the server sends less than n bits then one-way functions exist (where n is the number of bits in the database). That is, even saving one bit compared to the naive protocol, in which the entire database is sent, already requires one-way...
IR
kotz99mobile
Mobile Agents and the Future of the Internet Use of the Internet has exploded in recent years with the appearance of the World-Wide Web. In this paper, we show how current technological trends may lead to a system based substantially on mobile code, and in many cases, mobile agents. We discuss several technical and non-technical hurdles along the path to that eventuality. It seems likely that, within a few years, nearly all major Internet sites will be capable of hosting and willing to host some form of mobile code or mobile agents. 1 Introduction  Rapidly evolving network and computer technology, coupled with the exponential growth of the services and information available on the Internet, will soon bring us to the point where hundreds of millions of people will have fast, pervasive access to a phenomenal amount of information, through desktop machines at work, school and home, through televisions, phones, pagers, and car dashboards, from anywhere and everywhere. Mobile code, and in particular mobile agents, will be an essential...
Agents
mascolo99finegrained
A Fine-Grained Model for Code Mobility . In this paper we take the extreme view that every line of code is potentially mobile, i.e., may be duplicated and/or moved from one program context to another on the same host or across the network. Our motivation is to gain a better understanding of the range of constructs and issues facing the designer of a mobile code system, in a setting that is abstract and unconstrained by compilation and performance considerations traditionally associated with programming language design. Incidental to our study is an evaluation of the expressive power of Mobile  Unity, a notation and proof logic for mobile computing. 1 Introduction  The advent of world-wide networks, the emergence of wireless communication, and the growing popularity of the Java language are contributing to a growing interest in dynamic and reconfigurable systems. Code mobility is viewed by many as a key element of a class of novel design strategies which no longer assume that all the resources needed to accomplish a task are...
Agents
rousseau97socialpsychological
A Social-Psychological Model for Synthetic Actors In the Virtual Theater project, we provide synthetic actors that portray fictive characters by improvising their behavior in a multimedia environment. Actors are either autonomous or avatars directed by users. Their improvisation is based on the directions they receive and the context. Directions can take different forms: high-level scenarios, user commands, and personality changes in the character portrayed. In this paper, we look at this last form of direction. We propose a social-psychological model, in which we can define personality traits that depend on the values of moods and attitudes. We show how such a model can be exploited by synthetic actors to produce performances that are theatrically interesting, believable, and diverse. An application, the Cybercafé, is used to test those features.  Content Areas: synthetic actors, improvisation, believability. ____________________________________________________________________________________________  3  1. Introduction  In the Virt...
Agents
green01sdlip
SDLIP + STARTS = SDARTS A Protocol and Toolkit for Metasearching In this paper we describe how we combined SDLIP and STARTS, two complementary protocols for searching over distributed document collections. The resulting protocol, which we call SDARTS, is simple yet expressible enough to enable building sophisticated metasearch engines. SDARTS can be viewed as an instantiation of SDLIP with metasearchspecific elements from STARTS. We also report on our experience building three SDARTS-compliant wrappers: for locally available plain-text document collections, for locally available XML document collections, and for external webaccessible collections. These wrappers were developed to be easily customizable for new collections. Our work was developed as part of Columbia University's Digital Libraries Initiative--Phase 2 (DLI2) project, which involves the departments of Computer Science, Medical Informatics, and Electrical Engineering, the Columbia University libraries, and a large number of industrial partners. The main goal of the project is to provide personalized access to a distributed patient-care digital library.
IR
godbole01document
Document Classification as an Internet service: Choosing the best classifier This project investigates some of the issues involved in a new proposal for expanding the scope of the field of Data Mining by providing mining models as services on the Internet. This idea can widely increase the reach and accessibility of Data Mining to common people because one of the primary stumbling blocks in the adoption of mining is the extremely high level of expertise and data resources needed in building a robust mining model. We feel this task should be left to the specialists with access to data and resources, who can provide their most up to date model as a service on the Internet for public use.
IR
146066
MIRROR: A State-Conscious Concurrency Control Protocol for Replicated Real-Time Databases Data replication is one of the main techniques by which database systems can hope to meet the stringent temporal constraints of current time-critical applications, especially Web-based directory and electronic commerce services. A pre-requisite for realizing the benefits of replication, however, is the development of high-performance concurrency control mechanisms. We present in this paper MIRROR (Managing Isolation in Replicated Realtime Object Repositories), a concurrency control protocol specifically designed for firm-deadline applications operating on replicated real-time databases. MIRROR augments the optimistic two-phase locking (O2PL) algorithm developed for non real-time databases with a novel and simple to implement state-based conflict resolution mechanism to fine-tune real-time performance. Using a detailed simulation model, we compare MIR-ROR’s performance against the real-time versions of a representative set of classical protocols for a range of transaction workloads and system configurations. Our performance studies show that (a) the relative performance characteristics of replica concurrency control algorithms in the real-time environment could be significantly different from their performance in a traditional (non-real-time) database system, (b) MIRROR provides the best performance in both fully and partially replicated environments for real-time applications with low to moderate update frequencies, and (c) MIRROR’s conflict resolution mechanism works almost as well as more sophisticated (and difficult to implement) strategies.
DB
roth01scalable
A Scalable and Secure Global Tracking Service for Mobile Agents Abstract. In this paper, we propose a global tracking service for mobile agents, which is scalable to the Internet and accounts for security issues as well as the particularities of mobile agents (frequent changes in locations). The protocols we propose address agent impersonation, malicious location updates, as well as security issues that arise from profiling location servers, and threaten the privacy of agent owners. We also describe the general framework of our tracking service, and some evaluation results of the reference implementation we made.
Agents
284454
Computing Geographical Scopes of Web Resources Many information resources on the web are relevant primarily to limited geographical communities. For instance, web sites containing information on restaurants, theaters, and apartment rentals are relevant primarily to web users in geographical proximity to these locations. In contrast, other information resources are relevant to a broader geographical community. For instance, an on-line newspaper may be relevant to users across the United States. Unfortunately, most current web search engines largely ignore the geographical scope of web resources. In this paper, we introduce techniques for automatically computing the geographical scope of web resources, based on the textual content of the resources, as well as on the geographical distribution of hyperlinks to them. We report an extensive experimental evaluation of our strategies using real web data. Finally, we describe a geographically-aware search engine that we have built using our techniques for determining the geographical scope of web resources. 1
IR
marsh00using
Using Cinematography Conventions to Inform Guidelines For the Design and Evaluation of Virtual Off-Screen Space Many usability problems are associated with navigation and exploration of virtual space. In an attempt to find methods that support navigation within virtual space, this paper describes an investigation of cinematography conventions. In particular, this will focus on conventions that suggest to spectators the existence of additional space other than that contained within the confines or borders of the projection screen. Referred to as off-screen space, this paper builds upon these conventions and proposes guidelines to inform the design of visual cues to suggest virtual off-screen space. Visual cues will appear natural and transparent, they will help to guide participants through the smooth and continuously animated VE, and thus, maintain the illusion of interacting within a larger 3D virtual space than that contained within the restricted Field-Of-View (FOV) of the display screen. Introduction The 3 rd dimension of a Virtual Environment (VE) creates a space. Within ...
HCI
nascimento99evaluation
Evaluation of Access Structures for Discretely Moving Points Several applications require management of data which is spatially dynamic, e.g., tracking of battle ships or moving cells in a blood sample. The capability of handling the temporal aspect, i.e., the history of such type of data, is also important. This paper presents and evaluates three temporal extensions of the R-tree, the 3D R-tree, the 2+3 R-tree and the HR-tree, which are capable of indexing spatiotemporal data. Our experiments focus on discretely moving points (i.e., points standing at a specific location for a time period and then moving "instantaneously", and so on and so forth). We explore several parameters, e.g., initial spatial distribution, spatial query area and temporal query length. We found out that the HR-tree usually outperforms the other candidates, in terms of query processing cost, specially when querying time points and small time intervals. However, the main side effect of the HR-tree is its storage requirement, whichismuch larger than that of the o...
DB
schaller99objectivitydb
Objectivity/DB Benchmark Objectivity read/write benchmarks and storage overhead measurements are presented. Keywords:  1 Introduction  Most of the coming HEP experiments will use object-oriented database management systems (ODBMSs) as data store. A detailed understanding of the the ODBMS performance is crucial to enable highperformance analysis scenarios. This paper presents performance measurements for di#erent access patterns using the commercial ODBMS Objectivity/DB. Measurements were made for sequential reading, sequential writing and selective reading. Furthermore an experimental and analytical analysis of the storage overhead introduced by Objectivity/DB is given. The ODBMS performance depends on the underlying data medium. At the moment there are no alternatives to hard drives for secondary storage existing. Although the performance/price ratios of both processors and disks are improving, the rate of improvement is greater for processors. Hence, the disk subsystem is emerging as a bottleneck factor in s...
DB
440927
CREAM - Creating relational metadata with a component-based, ontology-driven annotation framework Richly interlinked, machine-understandable data constitutes the basis for  the Semantic Web. Annotating web documents is one of the major techniques for creating  metadata on the Web. However, annotation tools so far are restricted in their  capabilities of providing richly interlinked and truely machine-understandable data.  They basically allow the user to annotate with plain text according to a template structure,  such as Dublin Core. We here present CREAM (Creating RElational, Annotationbased  Metadata), a framework for an annotation environment that allows to construct  relational metadata, i.e. metadata that comprises class instances and relationship instances.  These instances are not based on a fix structure, but on a domain ontology.  We discuss some of the requirements one has to meet when developing such a framework,  e.g. the integration of a metadata crawler, inference services, document management  and information extraction, and describe its implementation, viz. Ont-O-Mat  a component-based, ontology-driven annotation tool.  
IR
495550
Models for Information Integration: Turning Local-as-View Into Global-as-View There are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the concepts in the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. The goal of this paper is to verify whether we can transform a data integration system built with the local-as-view approach into a system following the global-as-view approach. We study the problem in a setting where the global schema is expressed in the relational model with inclusion dependencies, and the queries used in the integration systems (both the queries on the global schema, and the views in the mapping) are expressed in the language of conjunctive queries. The result we present is that such a transformation exists: we can always transform a local-as-view system into a global-as-view system such that, for each query, the set of answers to the query wrt the former is the same as the set of answers wrt the latter.
DB
kwedlo99evolutionary
An Evolutionary Algorithm Using Multivariate Discretization for Decision Rule Induction We describe EDRL-MD, an evolutionary algorithm-based  system, for learning decision rules from databases. The main novelty of  our approach lies in dealing with continuous - valued attributes. Most of  decision rule learners use univariate discretization methods, which search  for threshold values for one attribute at the same time. In contrast to  them, EDRL-MD simultaneously searches for threshold values for all  continuous-valued attributes, when inducing decision rules. We call this  approach multivariate discretization. Since multivariate discretization is  able to capture interdependencies between attributes it may improve  the accuracy of obtained rules. The evolutionary algorithm uses problem  specific operators and variable-length chromosomes, which allows it to  search for complete rulesets rather than single rules. The preliminary  results of the experiments on some real-life datasets are presented.
ML
baldan01mutaclp
MuTACLP: A language for temporal reasoning with multiple theories In this paper we introduce MuTACLP, a knowledge representation language which provides facilities for modeling and handling temporal information, together with some basic operators for combining different temporal knowledge bases. The proposed approach stems from two separate lines of research: the general studies on meta-level operators on logic programs introduced by Brogi et al. [6, 8] and Temporal Annotated Constraint Logic Programming (TACLP) de ned by Frühwirth [14]. In MuTACLP atoms are annotated with temporal information which are managed via a constraint theory, as in TACLP. Mechanisms for structuring programs and combining separate knowledge bases are provided through meta-level operators. The language is given two different and equivalent semantics, a top-down semantics which exploits meta-logic, and a bottom-up semantics based on an immediate consequence operator.
DB
moghaddam98beyond
Beyond Eigenfaces: Probabilistic Matching for Face Recognition We propose a novel technique for direct visual matching of images for the purposes of face recognition and database search. Specifically, we argue in favor of a probabilistic measure of similarity, in contrast to simpler methods which are based on standard L2 norms (e.g., template matching) or subspace-restricted norms (e.g., eigenspace matching). The proposed similarity measure is based on a Bayesian analysis of image differences: we model two mutually exclusive classes of variation between two facial images: intra-personal (variations in appearance of the same individual, due to different expressions or lighting) and extra-personal (variations in appearance due to a difference in identity). The high-dimensional probability density functions for each respective class are then obtained from training data using an eigenspace density estimation technique and subsequently used to compute a similarity measure based on the a posteriori probability of membership in the intrapersonal class,...
DB
wang98quasistable
Quasi-Stable Semantics of Logic Programs this paper, we introduce a new semantic theory for logic programs. We choose the Well-Founded Semantics (WFS) [23] as our starting point because it has many desirable features. For any logic program there exists a unique well-founded partial model which can be defined in a constructive way. WFS naturally extends the semantics for a large class of logic programs, including stratified and locally stratified programs (see [17] and [23]). Despite its merits, it can be argued that the WFS is too "sceptical". For many programs it gives the empty set as their intended meaning. It is thus silent on all atoms though it may be reasonable to expect that something should be concluded from the information given. For an illustrative example, see program P 1 in section 4. There are many proposals for extending the WFS such as the generalised well-founded semantics GWFS [1], the well-founded-by-case-semantics WFSC [20], the extended well-founded semantics WFSE [11], the strong well-founded semantics WFS S [5], and the O-semantics [15]. The relationship between these semantic theories for logic programming is also studied in [6]. Here we propose another nondeterministic extension of the WFS. The other dominant semantic model for logic programs is the stable model semantics [10]. Compared with the WFS, the stable model semantics is "credulous": it derives much more information than the WFS though this is non-deterministic or "disjunctive" in the sense that there may be several stable models for a given logic program. The chief drawback of this semantics is that a stable model is not defined for all logic programs. In addition stable model semantics gives rise to anomalies in some circumstances. See the program P 3 in section 4, borrowed from [23]. A modification of the stable model seman...
DB
111958
Shopbots and Pricebots Shopbots are agents that automatically search the Internet to obtain information about prices and other attributes of goods and services. They herald a future in which autonomous agents profoundly influence electronic markets. In this study, a simple economic model is proposed and analyzed, which is intended to quantify some of the likely impacts of a proliferation of shopbots and other economically-motivated software agents. In addition, this paper reports on simulations of pricebots --- adaptive, pricesetting agents which firms may well implement to combat, or even take advantage of, the growing community of shopbots. This study forms part of a larger research program that aims to provide insights into the impact of agent technology on the nascent information economy.  1 Introduction  Shopbots, agents that automatically search the Internet for goods and/or services on behalf of consumers, herald a future in which autonomous agents become an essential component of nearly every facet o...
Agents
prentzas02webbased
A Web-based Intelligent Tutoring System Using Hybrid Rules as its Representational Basis In this paper, we present the architecture and describe the functionality of a Web-based Intelligent Tutoring System (ITS), which uses neurules for knowledge representation. Neurules are a type of hybrid rules integrating symbolic rules with neurocomputing. The use of neurules as the knowledge representation basis of the ITS results in a number of advantages.
AI
wroblewski00analyzing
Analyzing Relational Databases Using Rough Set Based Methods One of the most important problems in KDD applications is a size of real-world databases. In practical problems data may contain millions of records in many data tables bounded by relations. On the other hand, most of theoretical works and practical applications concentrate on relatively small data sets collected in single tables. The paper proposes a new approach to the problem of analysis of relational databases. The proposed methodology woks in adaptive way: if a considered data table is not sufficient to create satisfactory set of rules, new attributes (generated by arithmetical operations or based on database relations) are added to information system.
ML
yagiura99variable
A Variable Depth Search Algorithm for the Generalized Assignment Problem : A variable depth search procedure (abbreviated as VDS) is a generalization of the local search method, which was rst successfully applied by Lin and Kernighan to the traveling salesman problem and the graph partitioning problem. The main idea is to adaptively change the size of neighborhood so that it can eectively traverse larger search space while keeping the amount of computational time reasonable. In this paper, we propose a heuristic algorithm based on VDS for the generalized assignment problem, which is one of the representative combinatorial optimization problems known to be NP-hard. To the authors' knowledge, most of the previously proposed algorithms (with some exceptions) conduct the search within the feasible region; however, there are instances for which the search within feasible region is not advantageous because the feasible region is very small or is combinatorially complicated to search. Therefore, we allow in our algorithm to search into the infeasible region as w...
ML
dumais01optimizing
Optimizing Search by Showing Results In Context We developed and evaluated seven interfaces for integrating semantic category information with Web search results. List interfaces were based on the familiar ranked-listing of search results, sometimes augmented with a category name for each result. Category interfaces also showed page titles and/or category names, but re-organized the search results so that items in the same category were grouped together visually. Our user studies show that all Category interfaces were more effective than List interfaces even when lists were augmented with category names for each result. The best category performance was obtained when both category names and individual page titles were presented. Either alone is better than a list presentation, but both together provide the most effective means for allowing users to quickly examining search results. These results provide a better understanding of the perceptual and cognitive factors underlying the advantage of category groupings and provide some practical guidance to Web search interface designers.  Keywords  User Interface, World Wide Web, Search, User Study, Usability, Text Categorization, Focus-In-Context  
IR
massink99towards
Towards Hybrid Interface Specification for Virtual Environments . Many new multi-modal interaction techniques have been proposed for interaction in a virtual world. Often these techniques are of a hybrid nature combining continuous interaction, such as gestures and moving video, with discrete interaction, such as pushing buttons to select items. Unfortunately the description of the behavioural aspects of these interaction techniques found in the literature is informal and incomplete. This can make it hard to compare and evaluate their usability. This paper investigates the use of HyNet to give concise and precise specifications of hybrid interaction techniques. HyNet is an extension of high-level Petri Nets developed for specification and verification of hybrid systems, i.e. mathematical models including both continuous and discrete elements. 1 Introduction New technologies for virtual environments (VEs) have been eagerly embraced by VE users and developers. The process of diffusing this technology into a wider range of products has, i...
HCI
przymusinski98autoepistemic
Autoepistemic Logic of Knowledge and Beliefs In recent years, various formalizations of non-monotonic reasoning and different semantics for normal and disjunctive logic programs have been proposed, including autoepistemic logic, circumscription, CWA, GCWA, ECWA, epistemic specifications, stable, well-founded, stationary and static semantics of normal and disjunctive logic programs. In this paper we introduce a simple non-monotonic knowledge representation framework which isomorphically contains all of the above mentioned non-monotonic formalisms and semantics as special cases and yet is significantly more expressive than each one of these formalisms considered individually. The new formalism, called the Autoepistemic Logic of Knowledge and Beliefs, AELB, is obtained by augmenting Moore's autoepistemic logic, AEL, already employing the knowledge operator , L, with an additional belief operator , B. As a result, we are able to reason not only about formulae F which are known to be true (i.e., those for which LF holds) but also abou...
AI
younes00calibrating
Calibrating Parameters of Cost Functionals We propose a new framework for calibrating parameters of energy  functionals, as used in image analysis. The method learns parameters  from a family of correct examples, and given a probabilistic construct  for generating wrong examples from correct ones. We introduce a  measure of frustration to penalize cases in which wrong responses are  preferred to correct ones, and we design a stochastic gradient algorithm  which converges to parameters which minimize this measure of  frustration. We also present a first set of experiments in this context,  and introduce extensions to deal with data-dependent energies.  keywords: Learning, variational method, parameter estimation, image reconstruction, Bayesian image models  1  1 Description of the method  Many problems in computer vision are addressed through the minimization of a cost functional U . This function is typically defined on a large, finite, set \Omega  (for example the set of pictures with fixed dimensions), and the minimizer of x ...
ML
bruyninckx00autonomous
Autonomous Compliant Motion: the Bayesian approach This paper gives an overview of the dierent levels of sensor processing complexity found in forcecontrolled tasks, and explains which techniques from Bayesian probability theory are appropriate to cope with uncertainties and missing information at each of the dierent levels. The paper reduces all approaches for \intelligent" compliant motion sensor processing to a basic set of just four classes. Some of these algorithms have already been tested experimentally, while others are still beyond the current state-of-theart. The major contribution of this paper is to bring a clear structure to the eld, which should eventually result in an easier integration of dierent research results, and a more precise discussion about their relative merits and innovations.  1 Introduction  Force sensors are only one of many possible sensors that make robot controllers more autonomous, i.e., make them work in an environment that need not be as modelled and structured as the current industrial settings i...
ML
wactlar00informedia
Informedia - Search and Summarization in the Video Medium The Informedia system provides "full-content" search and retrieval of current and past TV and radio news and documentary broadcasts. The system implements a fully automatic intelligent process to enable daily content capture, analysis and storage in on-line archives. The current library consists of approximately a 2,000 hours, 1.5 terabyte library of daily CNN News captured over the last 3 years and documentaries from public television and government agencies. This database allows for rapid retrieval of individual "video paragraphs" which satisfy an arbitrary spoken or typed subject area query based on a combination of the words in the soundtrack, images recognized in the video, plus closed-captioning when available and informational text overlaid on the screen images. There are also capabilities for matching of similar faces and images, generation of related map-based displays. The latest work attempts to produce a visualization and summarization of the content across all the stories ...
HCI
tabuada01feasible
Feasible Formations of Multi-Agent Systems Formations of multi-agent systems, such as mobile robots, satellites. and aircraft, require individual agents to satisfy their kinematic equations while constantly maintaining inter-agent constraints. In this paper, we develop a systematic framework for studying formation feasibility of multi-agent systems. In particular, we consider undirected formations for centralized formations, and directed formations for decentralized formations. In each case, we determine differential geometric conditions that guarantee formation feasibility given the individual agent kinematics. Our framework also enables us to extract a smaller control system that describes the group kinematics while maintaining all formation constraints.
Agents
239748
Towards Flexible Teamwork in Persistent Teams: Extended Report Teamwork is a critical capability in multi-agent environments. Many such environments mandate that the agents and agent-teams must be persistent i.e., exist over long periods of time. Agents in such persistent teams are bound together by their long-term common interests and goals. This paper focuses on flexible teamwork in such persistent teams. Unfortunately, while previous work has investigated flexible teamwork, persistent teams remain unexplored. For flexible tamwork, one promising approach that has emerged is model-based, i.e., providing agents with general models of teamwork that explicitly specify their commitments in teamwork. Such models enable agents to autonomously reason about coordination. Unfortunately, for persistent teams, such models may lead to coordination and communication actions that while locally optimal, are highly problematic for the team's long-term goals. We present a decisiontheoretic technique to enable persistent teams to overcome such limitations of the m...
Agents
509763
Medical Document Information Retrieval through Active User Interfaces This paper reports our preliminary design and implementation towards the development of Kavanah, a system to help users retrieve information and discover knowledge for a medical domain application. The goal of this system is to adaptively react to the dynamic changes in the user's interests and preferences in searching for information within the context of the on-going information retrieval task. The context in which the user seeks information is modeled by an active user interface through analyzing the user's interactions with the system to dynamically construct an ontology of concepts representing the user's information seeking context. We implement the system using Unified Medical Language System knowledge base as a test bed.
IR
cho02parallel
Parallel Crawlers In this paper we study how we can design an effective parallel crawler. As the size of the Web grows, it becomes imperative to parallelize a crawling process, in order to finish downloading pages in a reasonable amount of time. We first propose multiple architectures for a parallel crawler and identify fundamental issues related to parallel crawling. Based on this understanding, we then propose metrics to evaluate a parallel crawler, and compare the proposed architectures using 40 million pages collected from the Web. Our results clarify the relative merits of each architecture and provide a good guideline on when to adopt which architecture. 1
IR
mertz00pushing
Pushing the limits of ATC user interface design beyond S&M interaction: the DigiStrips experience Most designs proposed for air traffic control workstations are based on user interface designs from the early 1980s, though research in user interaction has produced innovations since then. Project Toccata federates a series of research held at CENA on new user interfaces and services for ATC. Virtuosi and DigiStrips are two prototypes developed for Toccata, which make use of touch screens and served as a basis for research on the use of graphical design techniques in user interfaces. This paper describes the lessons learnt in that experience and argues that techniques such as animation, font design, careful use of graphical design techniques can augment the possibilities of user interface design and improve the usability of systems. We finally analyze the possible implications on ATC workstation design. KEY WORDS: touch-screen, animation, graphical design, feedback, gesture recognition, informal assessment, mutual awareness, electronic flight strips. INTRODUCTION In the last deca...
HCI
252229
Agent-based Distributed Planning and Scheduling in Global Manufacturing Scheduling and resource allocation problems are pervasive and important in the management of industrial and government organizations. With advent of new technology and fast evolvement in industry, the enterprise is gradually moving toward global manufacturing for efficient operational management and competent strategic decision making. In the past two decades, researchers and practitioners have been applying various techniques (such as artificial intelligence, optimization methodologies, information system design, human factors, etc.) to design and develop planning and scheduling methodologies and systems for different applications. However, the inherent complexity of problems, short life-cycle of planner/scheduler, unrealistic investment of generous-purpose systems, and profligacy of scattering computing resource, accessibility, integration and re-configurability become the essential factors for new planners/schedulers in global manufacturing. The specific objectives of this research ...
HCI
kullbach99querying
Querying as an Enabling Technology in Software Reengineering In this paper it is argued that different kinds of reengineering technologies can be based on querying. Several reengineering technologies are presented as being integrated into a technically oriented reengineering taxonomy. The usefulness of querying is pointed out with respect to these reengineering technologies.  To impose querying as a base technology in reengineering examples are given with respect to the EER/GRAL approach to conceptual modeling and implementation. This approach is presented together with GReQL as its query part. The different reengineering technologies are finally reviewed in the context of the GReQL query facility.  1 Introduction  Reengineering may be viewed as any activity that either improves the understanding of a software or else improves the software itself [2].  According to this view software reengineering can be "partitioned" into two kinds of activities. The first kind of activities is concerned with understanding such as source code retrieval, browsin...
DB
470345
Developing a Market Timing System using Grammatical Evolution This study examines the potential of an evolutionary  automatic programming methodology, Grammatical  Evolution, to uncover a series of useful  fuzzy technical trading rules for the ISEQ, the official  equity index of the Irish Stock Exchange. Index  values for the period 29/03/93 to 4/12/1997 are  used to train and test the model. The preliminary  findings indicate that the methodology has much  potential.  1 
ML
415300
A New Clustering Algorithm For Segmentation Of Magnetic Resonance Images 
AI
480065
London Calling: GIS, VR, and the Victorian Period The Bolles Collection of Tufts University represents a comprehensive and integrated collection of sources on the history and topography of Victorian London. Texts, images, maps, and three-dimensional reconstructions are all interconnected forming a body of material that transcends the limits of print publication and exploits the flexibility of the electronic medium. The Perseus Digital Library has incorporated Geographic Information System and Virtual Reality technologies in a set of tools intended to help readers synthesize and visualize the numerous temporal and spatial  interconnections between Bolles Collection materials. The tools, which are applicable to any large assemblage of related documents, also help readers grasp the complex temporal-spatial interactions that shape historical materials in general.
IR
172950
Logic-Based User-Defined Aggregates for the Next Generation of Database Systems this paper, we provide logic-based foundations for the extended aggregate constructs required by advanced database applications. In particular, we focus on data mining applications and show that they require user-defined aggregates extended with early returns. Thus, we propose a simple formalization of extended user-defined aggregates using the nondeterministic construct of choice. We obtain programs that have a formal semantics based on the concept of total stable models, but are also amenable to efficient implementation. Our formalization leads to a simple syntactic characterization of user-defined aggregates that are monotone with respect to set containment. Therefore, these aggregates can be freely used in recursive programs, and the fixpoints for such programs can be computed efficiently using the standard techniques of deductive databases. We describe the many new applications of user-defined aggregates, and their implementation for the logical data language LDL++. Finally, we discuss the transfer of this technology to SQL databases. 1 Introduction
DB
huget02agent
Agent UML Class Diagrams Revisited Multiagent system designers already use Agent UML in order to represent the interaction protocols [8] [2]. Agent UML is a graphical modeling language based on UML. As UML, Agent UML provides several types of representation covering the description of the system, of the components, the dynamics of the system and the deployment. Since agents and objects are not exactly the same, one can guess that the UML class diagram has to be changed for describing agents. The aim of this paper is to present how to extend Agent UML class diagrams in order to represent agents. We then compare our approach to Bauer's approach [1].
Agents
feder00computing
Computing the Median with Uncertainty We consider a new model for computing with uncertainty. It is desired to compute a function f(X1 ; : : : ; Xn) where X1 ; : : : ; Xn are unknown, but guaranteed to lie in specified intervals I1 ; : : : ; In . It is possible to query the precise value of any X j at a cost c j . The goal is to pin down the value of f to within a precision ffi at a minimum possible cost. We focus on the selection function f which returns the value of the kth smallest argument. We present optimal offline and online algorithms for this problem.  1 Introduction  Consider the following model for computing with uncertainty. We wish to compute a function f(X1 ; : : : ; Xn) over n real-valued arguments. The values of the variables X1 , : : :, Xn are not known in advance; however, we are provided with real intervals I1 ; : : : ; In along with a guarantee that for each j, X j 2 I j . Furthermore, it is possible to query the true value x j of each X j at a cost c j . The goal is to pin down the value of f into an i...
DB
470572
Support Vector Machine Active Learning with Applications to Text Classification . Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.  Keywords: Active Learning, Selective Sampling, Support Vector Machines, Classification, Relevance Feedback  Abbreviations: SVM -- Support Vector Machine; TSVM -- Transductive Support Vector Machine 1. 
IR
nahm01mining
Mining Soft-Matching Rules from Textual Data Text mining concerns the discovery of knowledge  from unstructured textual data. One important task  is the discovery of rules that relate specific words  and phrases. Although existing methods for this  task learn traditional logical rules, soft-matching  methods that utilize word-frequency information  generally work better for textual data. This paper  presents a rule induction system, TEXTRISE, that  allows for partial matching of text-valued features  by combining rule-based and instance-based learning.  We present initial experiments applying TEX-  TRISE to corpora of book descriptions and patent  documents retrieved from the web and compare  its results to those of traditional rule and instance  based methods.  1 Introduction  Text mining, discovering knowledge from unstructured natural-language text, is an important data mining problem attracting increasing attention [Hearst, 1999; Feldman, 1999; Mladenic, 2000] . Existing methods for mining rules from text use a hard, logical ...
ML
liu99face
Face Recognition Using Shape and Texture We introduce in this paper a new face coding and recognition method which employs the Enhanced FLD (Fisher Linear Discrimimant) Model (EFM) on integrated shape (vector) and texture (`shape-free' image) information. Shape encodes the feature geometry of a face while texture provides a normalized shape-free image by warping the original face image to the mean shape, i.e., the average of aligned shapes. The dimensionalities of the shape and the texture spaces are first reduced using Principal Component Analysis (PCA). The corresponding but reduced shape and texture features are then integrated through a normalization procedure to form augmented features. The dimensionality reduction procedure, constrained by EFM for enhanced generalization, maintains a proper balance between the spectral energy needs of PCA for adequate representation, and the FLD discrimination requirements, that the eigenvalues of the within-class scatter matrix should not include small trailing values after the dimensionality reduction procedure as they appear in the denominator.
ML
449007
Direct Policy Search using Paired Statistical Tests Direct policy search is a practical way to solve reinforcement learning problems involving continuous state and action spaces. The goal becomes finding policy parameters that maximize a noisy objective function. The Pegasus method converts this stochastic optimization problem into a deterministic one, by using fixed start states and fixed random number sequences for comparing policies (Ng & Jordan, 1999). We evaluate Pegasus, and other paired comparison methods, using the mountain car problem, and a difficult pursuer-evader problem. We conclude that: (i) Paired tests can improve performance of deterministic and stochastic optimization procedures. (ii) Our proposed alternatives to Pegasus can generalize better, by using a different test statistic, or changing the scenarios during learning. (iii) Adapting the number of trials used for each policy comparison yields fast and robust learning. 1.
ML
243755
Design-to-Criteria Scheduling: Real-Time Agent Control Design-to-Criteria builds custom schedules for agents that  meet hard temporal constraints, hard resource constraints,  and soft constraints stemming from soft task interactions or  soft commitments made with other agents. Design-to-Criteria  is designed specifically for online application -- it copes with  exponential combinatorics to produce these custom schedules  in a resource bounded fashion. This enables agents to respond  to changes in problem solving or the environment as  they arise.  Introduction  Complex autonomous agents operating in open, dynamic environments must be able to address deadlines and resource limitations in their problem solving. This is partly due to characteristics of the environment, and partly due to the complexity of the applications typically handled by software agents in our research. In open environments, requests for service can arrive at the local agent at any time, thus making it difficult to fully plan or predict the agent's future workload. In dyn...
Agents
pentland99digital
The Digital Doctor: An Experiment in Wearable Telemedicine Consultation with various specialists and review of medical literature are key elements in superior, modern medical care. Because this information can be expensive and inconvenient to access, physicians and patients must typically compromise ideal care practices with practical realities. Awearable computer with the ability to transmit and receive text and image data without a direct connection during an examination can remove the need for such compromise, potentially allowing both better care and lower cost. In this paper we report on experiments conducted at the University of Rochester's Strong Hospital in which a wearable computer is usedtocreate patient records and provide remote consultations during dermatological examinations.
HCI
273716
Optimizing the Efficiency of Parameterized Local Search within Global Search: A Preliminary Study Application-specific, parameterized local search algorithms (PLSAs), in which optimization accuracy can be traded-off with run-time, arise naturally in many optimization contexts. We introduce a novel approach, called simulated heating, for systematically integrating parameterized local search into global search algorithms (GSAs) in general and evolutionary algorithms in particular. Using the framework of simulated heating, we investigate both static and dynamic strategies for systematically managing the trade-off between PLSA accuracy and optimization effort. We show quantitatively that careful management of this trade-off is necessary to achieve the full potential of a GSA/PLSA combination. Furthermore, we provide preliminary results which demonstrate the effectiveness of our simulated heating techniques in the context of code optimization for embedded software implementation, a practical problem that involves vast and complex search spaces.  1 Motivation  For many useful optimizatio...
AI
mertz98conception
Conception Par Maquettage Rapide : Application Des Crans Tactiles Pour Le Contrle Arien. In this paper we present a joint use of tactile screen and animation. We first recall why this two techniques are valuable for air traffic controller computer interaction and then describe the current trends for these techniques. We then describe the methodology we used, based on paper and video fast prototyping. It allowed us to quickly design the first computer-based prototypes. These prototypes demonstrated that tightly coupling tactile screen and animation make the computer human interaction more natural. These results can easily be applied in future air traffic controller computer interfaces studied at the CENA.  Keywords  Paper prototype, video prototype, touch input screen, gestures, animation, air traffic control,  computer human interactions, mutual awareness  INTRODUCTION  Le dplacement et l'organisation d'objets informatiques sur un cran peuvent-ils tre aussi naturels, efficaces, explicites, rapides que la manipulation du papier ? Aujourd'hui, Non ! Cette question est import...
HCI
75123
Towards Lifetime Maintenance of Case Base Indexes for Continual Case Based Reasoning Abstract. One of the key areas of case based reasoning is how to main-tain the domain knowledge in the face of a changing environment. During case retrieval, a key process of CBR, feature-value pairs attached to the cases are used to rank the cases for the user. Different feature-value pairs may have different importance measures in this process, often represented by feature weights attached to the cases. How to maintain the weights so that they are up to date and current is one of the key factors deter-mining the success of CBR. Our focus in this paper is on the lifetime maintenance of the feature-weights in a case base. Our task is to de-sign a CBR maintenance system that not only learns a user's preference in the selection of cases but also tracks the user's evolving preferences in the cases. Our approach is to maintain feature weighting in a dy-namic context through an integration with a learning system inspired by a back-propagation neural network. In this paper we explain the new system architecture and reasoning algorithms, contrasting our approach with the previous ones. The effectiveness of the system is demonstrated through experiments in a real world application domain. 1
ML
auer00hybrid
Hybrid Tracking for Augmented Reality  
HCI
li98automatic
Automatic Identification of Text In Digital Video Key Frames Scene and graphic text can provide important supplemental index information in video sequences. In this paper we address the problem automatically identifying text regions in digital video key frames. The text contained in video frames is typically very noisy because it is aliased and/or digitized at a much lower resolution than typical document images, making identification, extraction and recognition difficult. The proposed method is based on the use of a hybrid wavelet/neural network segmenter on a series of overlapping small windows to classify regions which contain text. To detect text over a wide range of font sizes, the method is applied to a pyramid of images and the regions identified at each level are integrated. 1. Introduction  The increasing availability of online digital imagery and video has rekindled interest in the problems of how to index multimedia information sources automatically and how to browse and manipulate them efficiently. Traditionally, images and video seq...
ML
bonchi01web
Web Log Data Warehousing and Mining for Intelligent Web Caching We introduce intelligent web caching algorithms that employ predictive models of web requests; the general idea is to extend the LRU policy of web and proxy servers by making it sensible to web access models extracted from web log data using data mining techniques. Two approaches have been studied in particular, frequent patterns and decision trees. The experimental results of the new algorithms show substantial improvement over existing LRU-based caching techniques, in terms of hit rate. We designed and developed a prototypical system, which supports data warehousing of web log data, extraction of data mining models and simulation of the web caching algorithms.
DB
106003
Spoken Document Retrieval for TREC-7 at Cambridge University This paper presents work done at Cambridge University, on the TREC-7 Spoken Document Retrieval (SDR) Track. The broadcast news audio was transcribed using a 2-pass gender-dependent HTK speech recogniser which ran at 50 times real time and gave an overall word error rate of 24.8%, the lowest in the track. The Okapi-based retrieval engine used in TREC-6 by the City/Cambridge University collaboration was supplemented by improving the stop-list, adding a bad-spelling mapper and stemmer exceptions list, adding word-pair information, integrating part-of-speech weighting on query terms and including some pre-search statistical expansion. The final system gave an average precision of 0.4817 on the reference and 0.4509 on the automatic transcription, with the R-precision being 0.4603 and 0.4330 respectively. The paper also presents results on a new set of 60 queries with assessments for the TREC-6 test document data used for development purposes, and analyses the relationship between recognition accuracy, as defined by a pre-processed term error rate, and retrieval performance for both sets of data. 1.
IR
309072
A General Learning Approach to Multisensor Based Control using Statistic Indices We propose a concept for integrating multiple sensors in real-time robot control. To increase the controller robustness under diverse uncertainties, the robot systematically generates series of sensor data (as robot state) while memorising the corresponding motion parameters. From the collection of (multi-) sensor trajectories, statistical indices like principal components for each sensor type can be extracted. If the sensor data are preselected as output relevant, these principal components can be used very efficiently to approximately represent the original perception scenarios. After this dimension reduction procedure, a non-linear fuzzy controller, e.g. a B-spline type, can be trained to map the subspace projection into the robot control parameters. We apply the approach to a real robot system with two arms and multiple vision and force/torque sensors. These external sensors are used simultaneously to control the robot arm performing insertion and screwing operations. The successful experiments show that the robustness as well as the precision of robot control can be enhanced by integrating multiple additional sensors using this concept. 1
ML
rhodes99wearable
Wearable Computing Meets Ubiquitous Computing: Reaping the best of both worlds This paper describes what we see as fundamental diculties in both the pure ubiquitous computing and pure wearable computing paradigms when applied to context-aware applications. In particular, ubiquitous computing and smart room systems tend to have dif- culties with privacy and personalization, while wearable systems have trouble with localized information, localized resource control, and resource management between multiple people. These diculties are discussed, and a peer-to-peer network of wearable and ubiquitous computing components is proposed as a solution. This solution is demonstrated through several implemented applications.  1 Introduction  Ubiquitous computing and wearable computing have been posed as polar opposites even though they are often applied in very similar applications. Here we rst outline the advantages and disadvantages of each and propose that the two perspectives have complementary problems. We then attempt to demonstrate that the failing of both ubiquitous...
HCI
essa99computer
Computers Seeing People this paper, we present methods that give machines the ability to see people, interpret their actions and interact with them. We present the motivating factors behind this work, examples of how such computational methods are developed and their applications. The basic reason for providing machines the ability to see people really depends on the task we are associating with a machine. An industrial vision system aimed at extracting defects on an assembly line need not know anything about people. Similarly, a computer used for email and text writing need not see and perceive the users gestures and expressions. However, if our interest is to build intelligent machines that can work with us, support our needs and be our helpers, than it maybe required for these machines to know more about who they are supporting and helping. If our computers are to do more then support our text-based needs like writing papers, spreadsheets, and communicating via email; perhaps take on a role of being a personal assistant, then the ability to see a person is essential. Such an ability to perceive people is something that we take for granted in our everyday interactions with each other. At present our model of a machine or more specifically of a computer is something that is placed in the corner of the room. It is deaf, dumb, and blind, having no sense of the environment that it is in or of the person that is near it. We communicate with this computer using a coded sequence of tappings on a keyboard. Imagine a computer that knows that you are near it, that you are looking at it, knows who you are and what you are trying to do. Imagine a machine that can interpret a video signal based on who is in the scene and what they are doing. Such abilities in a computer are hard to imagine, unless it has...
HCI
scheffer01active
Active Learning of Partially Hidden Markov Models We consider the task of learning hidden Markov models (HMMs) when only partially (sparsely) labeled observation sequences are available for training. This setting is motivated by the information extraction problem, where only few tokens in the training documents are given a semantic tag while most tokens are unlabeled. We first describe the partially hidden Markov model together with an algorithm for learning HMMs from partially labeled data. We then present an active learning algorithm that selects "difficult" unlabeled tokens and asks the user to label them. We study empirically by how much active learning reduces the required data labeling effort, or increases the quality of the learned model achievable with a given amount of user effort.
IR
pelillo99continuousbased
Continuous-based Heuristics for Graph and Tree Isomorphisms, with Application to Computer Vision We present a new (continuous) quadratic programming approach for graph- and tree-isomorphism problems which is based on an equivalent maximum clique formulation. The approach is centered around a fundamental result proved by Motzkin and Straus in the mid-1960s, and recently expanded in various ways, which allows us to formulate the maximum clique problem in terms of a standard quadratic program. The attractive feature of this formulation is that a clear one-to-one correspondence exists between the solutions of the quadratic programs and those in the original, combinatorial problems. To approximately solve the program we use the so-called "replicator" equations, a class of straightforward continuous- and discrete-time dynamical systems developed in various branches of theoretical biology. We show how, despite their inherent inability to escape from local solutions, they nevertheless provide experimental results which are competitive with those obtained using more sophisticated mean-fiel...
ML
degaris99building
Building An Artificial Brain Using An FPGA Based "CAM-Brain Machine" This paper reports on recent progress made in ATR's attempt to build a 10,000 evolved neural net module artificial brain to control the behaviors of a life sized robot kitten. 1. Introduction This paper presents progress in ATR's Artificial Brain (CAM-Brain) Project. The broad aim of ATR's artificial brain project, is to build/grow/evolve an artificial brain containing a billion artificial neurons by the year 2001. The basic ideas of the CAM-Brain Project are as follows. Use cellular automata (CA) as the foundation upon which to grow and evolve neural network circuits, with user defined functionality. The state of each cellular automata cell can be stored in one or two bytes of RAM. Since nowadays it is possible to have a gigabyte of RAM in one's workstation, there is a huge space in which to store the CA cell states, more than enough to contain an artificial brain. The next consideration in the CAM-Brain Project was how to evolve these neural networks quickly enough for "Brain Buildin...
AI
74239
Markovian Models for Sequential Data Hidden Markov Models (HMMs) are statistical models of sequential data that have been used successfully in many machine learning applications, especially for speech recognition. Furthermore, in the last few years, many new and promising probabilistic models related to HMMs have been proposed. We first summarize the basics of HMMs, and then review several recent related learning algorithms and extensions of HMMs, including in particular hybrids of HMMs with artificial neural networks, Input-Output HMMs (which are conditional HMMs using neural networks to compute probabilities), weighted transducers, variable-length Markov models and Markov switching state-space models. Finally, we discuss some of the challenges of future research in this very active area. 1 Introduction  Hidden Markov Models (HMMs) are statistical models of sequential data that have been used successfully in many applications in artificial intelligence, pattern recognition, speech recognition, and modeling of biological ...
ML
gimbel01data
Data Stream Quality: Towards Parallel and . . . Perhaps the single most critical factor that limits today's utility of Knowledge Discovery in Databases (KDD) is scalability. Scalability is the main consequence of ever-increasing collections of mass data whereas data mining algorithms impose limits by their inherent computational complexity. One line of attack is to reduce the amount of data during preprocessing, that is, by exploring the raw data for
DB
19199
TRIPS: An Integrated Intelligent Problem-Solving Assistant We discuss what constitutes an integrated system in AI, and why AI researchers should be interested in building and studying them. Taking integrated systems to be ones that integrate a variety of components in order to perform some task from start to finish, we believe that such systems (a) allow us to better ground our theoretical work in actual tasks, and (b) provide an opportunity for much-needed evaluation based on task performance. We describe one particular integrated system we have developed that supports spoken-language dialogue to collaboratively solve planning problems. We discuss how the integrated system provides key advantages for helping both our work in natural language dialogue processing and in interactive planning and problem solving, and consider the opportunities such an approach affords for the future. Content areas: AI systems, natural language understanding, planning and control, problem solving, user interfaces Introduction It is an interesting time to be an A...
DB
yu99methodology
A Methodology to Retrieve Text Documents from Multiple Databases In this paper, we present a methodology for finding the n most similar documents across multiple text  databases for any given query and for any positive integer n. This methodology consists of two steps.  First, the contents of databases are indicated approximately by database representatives. Databases  are ranked using their representatives in a certain order with respect to the given query. We provide  a necessary and sufficient condition to rank the databases optimally. In order to satisfy this necessary  and sufficient condition, we provide three estimation methods. One estimation method is intended for  short queries; the other two are for all queries. Second, we provide an algorithm, OptDocRetrv, to  retrieve documents from the databases according to their rank and in a particular way. We show that  if the databases containing the n most similar documents for a given query are ranked ahead of other  databases, our methodology will guarantee the retrieval of the n most similar d...
IR
538839
Monadic Datalog and the Expressive Power of Languages for Web Information Extraction Research on information extraction from Web pages (wrapping) has seen much activity in recent times (particularly systems implementations), but little work has been done on formally studying the expressiveness of the formalisms proposed or on the theoretical foundations of wrapping. In this paper, we first study monadic datalog as a wrapping language (over ranked or unranked tree structures). Using previous work by Neven and Schwentick, we show that this simple language is equivalent to full monadic second order logic (MSO) in its ability to specify wrappers. We believe that MSO has the right expressiveness required for Web information extraction and thus propose MSO as a yardstick for evaluating and comparing wrappers. Using the above result, we study the kernel fragment Elog- of the Elog wrapping language used in the Lixto system (a visual wrapper generator). The striking fact here is that Elog- exactly captures MSO, yet is easier to use. Indeed, programs in this language can be entirely visually specified. We also formally compare Elog to other wrapping languages proposed in the literature.
IR
dipietro99experiences
Experiences in the use of Mobile Agents for developing distributed applications Introduction  The recent development of telecommunication networks has contributed to the success of distributed systems, and has stimulated the research for a new generation of applications, such as the access to remote databases, the Web, and e-commerce. Traditional programming in distributed systems has been based on the well-known client/server paradigm. An alternative to such traditional mechanisms has recently been spreading, and is based on the use of environments that give a sort of "code mobility" [2]. By this term we mean the possibility to change dynamically at run-time the binding between the software components of an application and their physical location within a network of computers.  Code mobility is the main feature on which mobile agent systems are based: a mobile agent can be deemed as a software module able to autonomously perform the task assigned by the user, by moving (if necessary) from a node of the network to the other, in order to collect the informa
Agents
526689
Optimizing Search Engines using Clickthrough Data This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Ma- chine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimiza- tion framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.
IR
french99effective
Effective and Efficient Automatic Database Selection We examine a class of database selection algorithms that require only document frequency information. The CORI algorithm is an instance of this class of algorithms. In previous work, we showed that CORI is more effective than gGlOSS when evaluated against a relevance-based standard. In this paper, we introduce a family of other algorithms in this class and examine components of these algorithms and of the CORI algorithm to begin identifying the factors responsible for their performance. We establish that the class of algorithms studied here is more effective and efficient than  gGlOSS and is applicable to a wider variety of operational environments. In particular, this methodology is completely decoupled from the database indexing technology so is as useful in heterogeneous environments as in homogeneous environments.  1 Introduction  Database or collection selection [2, 6, 10, 8, 13, 14, 15, 9] is a fundamental problem in distributed searching. Given an environment containing many dat...
IR
lamma99strategies
Strategies in Combined Learning via Logic Programs Abstract. We discuss the adoption of a three-valued setting for inductive concept learning. Distinguishing between what is true, what is false and what is unknown can be useful in situations where decisions have tobetaken on the basis of scarce, ambiguous, or downright contradictory information. In a three-valued setting, we learn a de nition for both the target concept and its opposite, considering positive and negative examples as instances of two disjoint classes. To this purpose, we adopt Extended Logic Programs (ELP) under a Well-Founded Semantics with explicit negation (WFSX) as the representation formalism for learning, and show how ELPs can be used to specify combinations of strategies in a declarative way also coping with contradiction and exceptions. Explicit negation is used to represent the opposite concept, while default negation is used to ensure consistency and to handle exceptions to general rules. Exceptions are represented by examples covered by the de nition for a concept that belong to the training set for the opposite concept. Standard Inductive Logic Programming techniques are employed to learn the concept and its opposite. Depending on the adopted technique, we can learn the most general or the least general
Agents
amitay00incommonsense
InCommonSense - Rethinking Web Search Results The World Wide Web is a rich annotation system which allows people to relate to documents and sites from different perspectives. People describe, comment, relate or mock other Web pages in the context of their document. This richness is currently not reflected in snippets presented by Web search engines, where a search result is represented by the text found in the Web document alone. This paper proposes a new method for representing documents in Web search engines' results. This method is based on recent trends in search engine technology and provides descriptions of the retrieved documents, assembled from people's commentary and annotations on the Web. This paper suggests a new way for automatically retrieving and reusing people's annotations on the Web, incorporating these annotations into a search engine for creating a hybrid directory-search-engine, allowing for both automatic retrieval and on-the-fly human authored summaries.  I. INTRODUCTION  It is common knowledge that many use...
IR
bongard00legion
The Legion System: A Novel Approach to Evolving Heterogeneity for Collective Problem Solving . We investigate the dynamics of agent groups evolved to perform  a collective task, and in which the behavioural heterogeneity of the  group is under evolutionary control. Two task domains are studied: solutions  are evolved for the two tasks using an evolutionary algorithm called  the Legion system. A new metric of heterogeneity is also introduced,  which measures the heterogeneity of any evolved group behaviour. It  was found that the amount of heterogeneity evolved in an agent group  is dependent of the given problem domain: for the rst task, the Legion  system evolved heterogeneous groups; for the second task, primarily homogeneous  groups evolved. We conclude that the proposed system, in  conjunction with the introduced heterogeneity measure, can be used as  a tool for investigating various issues concerning redundancy, robustness  and division of labour in the context of evolutionary approaches to collective  problem solving.  1 Introduction  Investigations into heterogeneous a...
ML
40506
A Collaborative Internet Documents Access Scheme Using Acird In this paper, we present a collaborative intelligent Internet multi-web sites documents search system using ACIRD. ACIRD is a system that automatically learns the classification knowledge from Web pages and applies the knowledge to automatic classification of Web pages to some classes in a class hierarchy. Data mining technique is used to learn the association of terms to discover the hidden semantic connections between terms. With the capabilities of ACIRD, it is straightforward to extend ACIRD to collaborate multi-web site document access. Based on the learned classification knowledge, a collaborative two-phase search engine is proposed, which dispatches queries to distributed Web sites to match documents and presents hierarchically navigable results to the Internet users rather than conventional ranked flat results. 1. INTRODUCTION The rapid growth of the Internet has changed the way of working and living that the Internet becomes a major source of information and means of commun...
IR
shankar00weight
Weight Adjustment Schemes for a Centroid Based Classifier In recent years we have seen a tremendous growth in the volume of text documents available on the Internet, digital  libraries, news sources, and company-wide intra-nets. Automatic text categorization, which is the task of assigning  text documents to pre-specified classes (topics or themes) of documents, is an important task that can help both in  organizing as well as in finding information on these huge resources. Similarity based categorization algorithms such  as k-nearest neighbor, generalized instance set and centroid based classification have been shown to be very effective  in document categorization. A major drawback of these algorithms is that they use all features when computing the  similarities. In many document data sets, only a small number of the total vocabulary may be useful for categorizing  documents. A possible approach to overcome this problem is to learn weights for different features (or words in  document data sets). In this report we present two fast iterativ...
IR
mladenic98turning
Turning Yahoo into an Automatic Web-Page Classifier . The paper describes an approach to automatic Web-page classification based on the Yahoo hierarchy. Machine learning techniques developed for learning on text data are used here on the hierarchical classification structure. The high number of features is reduced by taking into account the hierarchical structure and using feature subset selection based on the method known from information retrieval. Documents are represented as feature-vectors that include n-grams instead of including only single words (unigrams) as commonly used when learning on text data. Based on the hierarchical structure the problem is divided into subproblems, each representing one on the categories included in the Yahoo hierarchy. The result of learning is a set of independent classifiers, each used to predict the probability that a new example is a member of the corresponding category. Experimental evaluation on real-world data shows that the proposed approach gives good results. For more than a half of testing...
IR
149759
Towards Combining Fuzzy and Logic Programming Techniques Many problems from AI have been successfully solved using fuzzy techniques. On the other hand, there are many other AI problems, in which logic programming (LP) techniques have been very useful. Since we have two successful techniques, why not combine them?
AI
wan01realtime
Real-Time FRP Functional reactive programming (FRP) is a declarative programming paradigm where the basic notions are continuous, time-varying behaviors and discrete, event-based reactivity. FRP has been used successfully in many reactive programming domains such as animation, robotics, and graphical user interfaces. The success of FRP in these domains encourages us to consider its use in real-time applications, where it is crucial that the cost of running a program be bounded and known before run-time. But previous work on the semantics and implementation of FRP was not explicitly concerned about the issues of cost. In fact, the resource consumption of FRP programs in the current implementation is often hard to predict. As a first step
HCI
115410
OMS Rapid Prototyping System for the Development of Object-Oriented Database Application Systems We present an object-oriented data model and system that supports the development of database application systems through a combination of rapid prototyping and refinement. Prototyping is performed on an abstract application model and is independent of any implementation platform -- supporting not only the design stage, but also analysis and requirements modelling. The underlying model, OM, has a two-level structure which reflects the two aspects of object-oriented database application systems -- data modelling and programming. We are therefore able to cleanly integrate ideas from both the database and software engineering communities and achieve compatibility with object models used in both.  Keywords: Object-oriented database systems, rapid prototyping, database design. 1 Introduction  The design and implementation of object-oriented database application systems presents a software engineering challenge in that it encompasses both application program development and database design. ...
DB
nissen99repository
Repository Support For Multi-Perspective Requirements Engineering Abstract | Relationships among di erent modeling perspectives have been systematically investigated focusing either on given notations (e.g. UML) or on domain reference models (e.g. ARIS/SAP). In contrast, many successful informal methods for business analysis and requirements engineering (e.g. JAD) emphasize team negotiation, goal orientation and exibility of modeling notations. This paper addresses the question how much formal and computerized support can be provided in such settings without destroying their creative tenor. Our solution is based on a novel modeling language design, M-Telos, that integrates the adaptability and analysis advantages of the logic-based meta modeling language Telos with a module concept covering the structuring mechanisms of scalable software architectures. It comprises four components: (1) A modular conceptual modeling formalism organizes individual perspectives and their interrelationships. (2) Perspective schemata are linked to a conceptual meta meta model of shared domain terms, thus giving the architecture a semantic meaning and enabling adaptability and extensibility of the network of perspectives. (3) Inconsistency management across perspectives is handled in a goal-oriented manner, by formalizing analysis goals as meta rules which are automatically customized to perspective schemata. (4) Continuous incremental maintenance of inconsistency information is provided by exploiting recent view maintenance techniques from deductive databases. The approach has been fully implemented as an extension to the ConceptBase meta database management system and is currently experimentally applied in the context of business analysis and data warehouse design.
DB
wijsen01query
On Query Optimization in a Temporal SPC Algebra Tuples of a temporal relation are equipped with a valid time period. A simple extension of the SPC (SelectionProjection  -Cross product) algebra for temporal relations is defined, which conforms to primitives in existing temporal  query languages. In particular, temporal projection involves coalescing of time intervals, which results in  non-monotonic queries. Also the "select-from-where" normal form is no longer available in this temporal extension.
DB
groh99automated
Automated Knowledge and Information Fusion from multiple text-based sources using Formal Concept Analysis This report explores the space of this problem and develops some steps towards its solutions. Our experience with knowledge representation languages and dynamic hyperlinking of HTML documents using conceptual graphs seems complementary to the multiple source knowledge fusion task [40]. The reasons are that conceptual graphs are based on lexical, hierarchically structured ontologies of semantically related terms and these structures lend themselves to traditional information retrival engines, at least so far as term extraction from text is concerned. Furthermore, conceptual graphs provide for typical and necessary conditions, logical inference and term signatures that aid with disambiguiation of semantic intent. Conceptual graphs also permit knowledge fusion by way of the maximal join operation. Conceptual graphs are not without their research challenges however. Who or what can automatically generate the CGs and how the approach scales to real-world  multiple source fusion are open questions. To address the first of these issues. There are two schools of thought on who or what generates the CGs needed for knowledge fusion from multiple sources. The first of these is that index expressions generated by a meta-level information retrival engine called the Hyperindex Browser (the HiB) [5, 6, 7, 4] can be used to automatically generate knowledge annotations as input to an inference engine (such as the one used by our research group's WebKB software [38, 39, 40]). The idea here is that we can efficiently extract and fuse knowledge structures from example documents to construct a knowledge exemplar and that these exemplars are subsequently used to compute the closure of the knowledge base and generate pattern recognition classifiers (see Figure 1.1). Domain background knowled...
DB
443913
A Tutorial Dialogue System with Knowledge-Based Understanding and Classification of Student Explanations We are engaged in a research project to create a tutorial  dialogue system that helps students to explain the reasons  behind their problem-solving actions, in order to help them  learn with greater understanding. Currently, we are pilottesting  a prototype system that is able to analyze student  explanations, stated in their own words, recognize the types  of omissions that we typically see in these explanations, and  provide feedback. The system takes a knowledge-based  approach to natural language understanding and uses a  statistical text classifier as a backup. The main features are:  robust parsing, logic-based representation of semantic  content, representation of pedagogical content knowledge in  the form of a hierarchy of partial and complete explanations,  and reactive dialogue management. A preliminary  evaluation study indicates that the knowledge-based natural  language component correctly classifies 80% of  explanations and produces a reasonable classification for all  but 6% of explanations.  1 
HCI
calvanese99unifying
Unifying Class-Based Representation Formalisms The notion of class is ubiquitous in computer science and is central in many formalisms for the representation of structured knowledge used both in knowledge representation and in databases. In this paper we study the basic issues underlying such representation formalisms and single out both their common characteristics and their distinguishing features. Such investigation leads us to propose a unifying framework in which we are able to capture the fundamental aspects of several representation languages used in different contexts. The proposed formalism is expressed in the style of description logics, which have been introduced in knowledge representation as a means to provide a semantically well-founded basis for the structural aspects of knowledge representation systems. The description logic considered in this paper is a subset of first order logic with nice computational characteristics. It is quite expressive and features a novel combination of constructs that has not been studied before. The distinguishing constructs are number restrictions, which generalize existence and functional dependencies, inverse roles, which allow one to refer to the inverse of a relationship, and possibly cyclic assertions, which are necessary for capturing real world
DB
shavlik99instructable
An Instructable, Adaptive Interface for Discovering and Monitoring Information on the World-Wide Web We are creating a customizable, intelligent interface to the World-Wide Web that assists a user in locating specific, current, and relevant information. The Wisconsin Adaptive Web Assistant (Wawa) is capable of accepting instructions regarding what type of information the users are seeking and how to go about looking for it. Wawa compiles these instructions into neural networks, which means that the system's behavior can be modified via training examples. Users can create these training examples by rating pages retrieved by Wawa,  but more importantly the system uses techniques from reinforcement learning to internally create its own examples (users can also later provide additional instructions) . Wawa uses these neural networks to guide its autonomous navigation of the Web, thereby producing an interface to the Web that users periodically instruct and which in the background searches the Web for relevant information, including periodically revisiting pages that change regularly.  Key...
IR
butz00augmenting
Augmenting Buildings with Infrared Information We describe a building information and navigation system based on Palm Pilot PDAs and a set of strong infrared transmitters, located throughout a building. The infrared senders stream localized data, thus effectively augmenting areas of space with localized information. This information can be perceived by just entering those areas with the PDA in your hand. We show that this form of augmentation of an environment can serve a multitude of purposes and requires neither the employment of classic 3D augmented reality nor to carry around wearable computers nor to wear head mounted displays.  1 Introduction  The IRREAL system is a building information system that by its pure working principle can also be used as a navigation system within buildings. It is based on the Palm Pilot family of PDAs most of which feature a builtin infrared port located in the middle of the top end of the device, so that it points away from the user when the device is being held in front of the user. By using this...
HCI
das98rule
Rule Discovery From Time Series We consider the problem of finding rules relating patterns  in a time series to other patterns in that series,  or patterns in one series to patterns in another series.  A simple example is a rule such as "a period  of low telephone call activity is usually followed by a  sharp rise in call volume". Examples of rules relating  two or more time series are "if the Microsoft stock  price goes up and Intel falls, then IBM goes up the  next day," and "if Microsoft goes up strongly for one  day, then declines strongly on the next day, and on  the same days Intel stays about level, then IBM stays  about level." Our emphasis is in the discovery of local  patterns in multivariate time series, in contrast to traditional  time series analysis which largely focuses on  global models. Thus, we search for rules whose conditions  refer to patterns in time series. However, we do  not want to define beforehand which patterns are to be  used; rather, we want the patterns to be formed from  the data in t...
ML
su00whatnext
WhatNext: A Prediction System for Web Requests using N-gram Sequence Models As an increasing number of users access information on the web, there is a great opportunity to learn from the server logs to learn about the users' probable actions in the future. In this paper, we present an n-gram based model to utilize path profiles of users from very large data sets to predict the users' future requests. Since this is a prediction system, we cannot measure the recall in a traditional sense. We, therefore, present the notion of applicability to give a measure of the ability to predict the next document. Our model is based on a simple extension of existing point-based models for such  predictions, but our results show for n-gram based prediction when n is greater than three, we can  increase precision by 20% or more for two realistic web logs. Also we present an efficient method that can compress our model to 30% of its original size so that the model can be loaded in main  memory. Our result can potentially be applied to a wide range of applications on the web, inc...
DB
bergo01text
Text Categorization and Prototypes this document. However in accordance with what we mentioned previously, a member of a category does not necessarily have to have all the quintessential features of a category and this constitutes one of the big problems in choosing an algorithm for making a category representative
IR
deraedt98three
Three Companions for First Order Data Mining . Three companion systems, Claudien, ICL and Tilde, are presented. They use a common representation for examples and hypotheses: each example is represented by a relational database. This contrasts with the classical inductive logic programming systems such as Progol  and Foil. It is argued that this representation is closer to attribute value learning and hence more natural. Furthermore, the three systems can be considered first order upgrades of typical data mining systems, which induce association rules, classification rules or decision trees respectively. 1 Introduction  Typical data mining algorithms employ a limited attribute value representation, where each example consists of a single tuple in a relational database. This representation is inadequate for problem-domains that require reasoning about the structure of the domain, such as e.g. in bio-chemistry [7], natural language processing [14],. . . This paper presents three companion systems, where each example corresponds to a...
DB
18915
Logic Programs for Intelligent Web Search . We present a general framework for the information extraction from web pages based on a special wrapper language, called token-templates. By using token-templates in conjunction with logic programs we are able to reason about web page contents, search and collect facts and derive new facts from various web pages. We give a formal definition for the semantics of logic programs extended by token-templates and define a general answer-completecalculus for these extendedprograms. These methodsand techniquesare used to build intelligent mediators and web information systems.  Keywords: information extraction; template based wrappers; mediators; logic programming; theory reasoning; deductive web databases; softbots; logic robots  1. Introduction  In the last few years it became appearant that there is an increasing need for more intelligent World-Wide-Web information systems. The existing information systems are mainly document search engines, e.g. Alta Vista, Yahoo, Webcrawler, based on in...
IR
blok00top
Top N optimization issues in MM databases Introduction  In multi media (MM) DBMSs the usual way of operation in case of a MM retrieval query is to compute some ranking based on statistics and distances in feature spaces. The MM objects are then sorted by descending relevance relative to the given query. Since users are limited in their capabilities of reviewing all objects in that ranked list only a reasonable top of say N objects is returned.  However, this can turn out to be a quite time consuming process. The first reason is that the number of objects (i.e. documents) in the DBMS is usually very large (10  6  or even more). From the information retrieval field it is known that usually half of all objects (e.g. documents) contains at least one query term; so, even considering only these objects might be very time consuming. The same may hold for MM in general.  The problem of top N MM query optimization is to find techniques to limit the set of objects taken into consideration during the
DB
382181
Super Logic Programs Recently, considerable interest and research e#ort has been given to the problem of finding a suitable extension of the logic programming paradigm beyond the class of normal logic programs. In order to demonstrate that a class of programs can be justifiably called an extension of logic programs one should be able to argue that:  .  the proposed syntax of such programs resembles the syntax of logic programs but it applies to a significantly broader class of programs;  .  the proposed semantics of such programs constitutes an intuitively natural extension of the semantics of normal logic programs;  .  there exists a reasonably simple procedural mechanism allowing, at least in principle, to compute the semantics;  .  the proposed class of programs and their semantics is a special case of a more general non-monotonic formalism which clearly links it to other well-established non-monotonic formalisms. In this paper we propose a specific class of extended logic programs which will be (modestly) called super logic programs or just super-programs. We will argue that the class of super-programs satisfies all of the above conditions, and, in addition, is su#ciently flexible to allow various application-dependent extensions and modifications. We also provide a brief description of a Prolog implementation of a query-answering interpreter for the class of super-programs which is available via FTP and WWW.  Keywords: Non-Monotonic Reasoning, Logics of Knowledge and Beliefs, Semantics of Logic Programs and Deductive Databases.  # An extended abstract of this paper appeared in the Proceedings of the Fifth International Conference on Principles of Knowledge Representation and Reasoning (KR'96), Boston, Massachusetts, 1996, pp. 529--541.  + Partially supported by the National Science Fou...
DB
mayol00wearable
Wearable Visual Robots This paper presents a wearable active visual sensor which is able to achieve a level of decoupling of camera movement from the wearer’s posture and movements. This decoupling is the result of a combination of an active sensing approach, inertial information and visual sensor feedback. The issues of sensor placement, robot kinematics and their relation to wearability are discussed. The performance of the prototype head is evaluated for some essential visual tasks. The paper also discusses potential application scenarios for this kind of wearable robot. 1
HCI
puzicha99theory
A Theory of Proximity Based Clustering: Structure Detection by Optimization In this paper, a systematic optimization approach for clustering proximity or similarity data is developed. Starting from fundamental invariance and robustness properties, a set of axioms is proposed and discussed to distinguish different cluster compactness and separation criteria. The approach covers the case of sparse proximity matrices, and is extended to nested partitionings for hierarchical data clustering. To solve the associated optimization problems, a rigorous mathematical framework for deterministic annealing and mean--field approximation is presented. Efficient optimization heuristics are derived in a canonical way, which also clarifies the relation to stochastic optimization by Gibbs sampling. Similarity-based clustering techniques have a broad range of possible applications in computer vision, pattern recognition, and data analysis. As a major practical application we present a novel approach to the problem of unsupervised texture segmentation, which relies on statistical...
IR
boyle00grouplab
Grouplab at SkiGraph Collaboration among distributed workgroup members is hampered by the lack of good tools to support informal interactions. These tools either fail to provide teleawareness or enable smooth transitions into and out of informal interactions. Video media spaces---always-on video links---have been proposed as a solution to this problem. However, the "always-on" nature of video media spaces results in a conflict between the desire to provide awareness and the need to preserve privacy. The present study examines distortion filtration applied to always-on video as means of resolving this tension. Our discussions include the inter-related concepts of informal interactions, awareness, and privacy; and the treatment afforded by existing distributed collaboration support tools. We then outline the present study, where our goal is to understand the effect of distortion filtration on awareness and privacy.  Keywords  Tele-awareness, telepresence, privacy, informal interaction, video media spaces, di...
HCI
453999
Reasoning over Conceptual Schemas and Queries in Temporal Databases This paper introduces a new logical formalism,  intended for temporal conceptual modelling,  as a natural combination of the wellknown  description logic DLR and pointbased  linear temporal logic with Since and  Until. The expressive power of the resulting   DLRUS logic is illustrated by providing  a systematic formalisation of the most important  temporal entity-relationship data models  appeared in the literature. We define  a query language (where queries are nonrecursive  Datalog programs and atoms are  complex DLRUS expressions) and investigate  the problem of checking query containment  under the constraints defined by  DLRUS conceptual schemas, as well as the  problems of schema satisfiability and logical  implication. Although it is shown that reasoning  in full DLRUS is undecidable, we  identify the decidable (in a sense, maximal)  fragment DLR  US  by allowing applications  of temporal operators to formulas and entities  only (but not to relation expressions). We obtain  the following hierarchy of complexity results:  (a) reasoning in DLR US with atomic  formulas is EXPTIME-complete, (b) satisfiability  and logical implication of arbitrary  DLR US formulas is EXPSPACE-complete,  and (c) the problem of checking query containment  of non-recursive Datalog queries under   DLR US constraints is decidable in 2EXPTIME.  
DB
448769
From Design to Intention: Signs of a Revolution ce such environment and be influenced by it.  ii. openness: software systems will be subject to decentralized management and will dynamically change their structure: new components can be dynamically created or destroyed and, via mobility, will be able, to roam in and out the permeable boundaries of different software systems. Thus, the problem of openness is currently much broader than being simply a problem of interoperability;  iii. locality in control: the components of software systems will represent autonomous loci of control. In fact, most components of software systems will be active, and will have local control over their activities, although will be in need of coordinating these activities with other active components.  iv. locality in interactions: despite living in a fully connected world, software components interact with each other accordingly to local (geographical or logical) patterns. In other words, systems will have to be modeled around clusters of locally interactin
Agents
scime01websifter
WebSifter: An Ontological Web-Mining Agent for EBusiness The World Wide Web provides access to a great deal of information on a vast  array of subjects. A user can begin a search for information by selecting a Web page and following the embedded links from page to page looking for clues to  the desired information. An alternative method is to use one of the Web-based  search engines to select the Web pages that refer to the general subject of the  information desired. In either case, a vast amount of information is retrieved.
IR
419282
Lessons Learned from the Scientist's Expert Assistant Project During the past two years, the Scientist's Expert Assistant (SEA) team has been prototyping proposal development tools for the Hubble Space Telescope in an effort to demonstrate the role of software in reducing support costs for the Next Generation Space Telescope (NGST). This effort has been a success. The Hubble Space Telescope has adopted two SEA prototype tools, the Exposure Time Calculator and Visual Target Tuner, for operational use. The Space Telescope Science Institute is building a new set of observing tools based on SEA technology. These tools will hopefully be a foundation that is easily adaptable to other observatories including NGST.  The SEA project has aggressively pursued the latest software technologies including Java, distributed computing, XML, Web distribution, and expert systems. Some technology experiments proved to be dead ends, while other technologies were unexpectedly beneficial. We have also worked with other projects to foster collaboration between the vario...
AI
papadias01indexing
Indexing Spatio-Temporal Data Warehouses Spatio-temporal databases store information about the positions of individual objects over time. In many applications however, such as traffic supervision or mobile communication systems, only summarized data, like the average number of cars in an area for a specific period, or phones serviced by a cell each day, is required. Although this information can be obtained from operational databases, its computation is expensive, rendering online processing inapplicable. A vital solution is the construction of a spatiotemporal data warehouse. In this paper, we describe a framework for supporting OLAP operations over spatiotemporal data. We argue that the spatial and temporal dimensions should be modeled as a combined dimension on the data cube and present data structures, which integrate spatiotemporal indexing with pre-aggregation. While the well-known materialization techniques require a-priori knowledge of the grouping hierarchy, we develop methods that utilize the proposed structures for efficient execution of ad-hoc group-bys. Our techniques can be used for both static and dynamic dimensions.  1. 
DB
robbert01astrolabe
Astrolabe: A Robust and Scalable Technology for Distributed System Monitoring, Management, and Data Mining this paper, we describe a new information management service called Astrolabe. Astrolabe monitors the dynamically changing state of a collection of distributed resources, reporting summaries of this information to its users. Like DNS, Astrolabe organizes the resources into a hierarchy of domains, which we call zones to avoid confusion, and associates attributes with each zone. Unlike DNS, zones are not bound to specific servers, the attributes may be highly dynamic, and updates propagate quickly; typically, in tens of seconds
IR
joachims99transductive
Transductive Inference for Text Classification using Support Vector Machines This paper introduces Transductive Support Vector Machines (TSVMs) for text classification.  While regular Support Vector Machines  (SVMs) try to induce a general decision  function for a learning task, Transductive  Support Vector Machines take into account  a particular test set and try to minimize  misclassifications of just those particular  examples. The paper presents an analysis  of why TSVMs are well suited for text  classification. These theoretical findings are  supported by experiments on three test collections.  The experiments show substantial  improvements over inductive methods, especially  for small training sets, cutting the number  of labeled training examples down to a  twentieth on some tasks. This work also proposes  an algorithm for training TSVMs efficiently,  handling 10,000 examples and more. 
IR
tice00practical
A Practical, Robust Method for Generating Variable Range Tables In optimized programs the location in which the current value of a single source variable may reside typically varies as the computation progresses. A debugger for optimized code needs to know all of the locations -- both registers and memory addresses -- in which a variable resides, and which locations are valid for which portions of the computation. Determining this information is known as the data location problem [3, 7]. Because optimizations frequently move variables around (between registers and memory or from one register to another) the compiler must build a table to keep track of this information. Such a table is known as a variable range table [3]. Once a variable range table has been constructed, finding a variable's current location reduces to the simple task of looking up the appropriate entry in the table.  The difficulty lies in collecting the data for building the table. Previous methods for collecting this data depend on which optimizations the compiler performs and ho...
HCI
duch00computational
Computational Intelligence Methods and Data Understanding Abstract. Experts in machine learning and fuzzy system frequently identify understanding the data with the use of logical rules. Reasons for inadequacy of crisp and fuzzy rule-based explanations are presented. An approach based on analysis of probabilities of classification p(Ci|X;ρ) as a function of the size of the neighborhood ρ of the given case X is presented. Probabilities are evaluated using Monte Carlo sampling or – for some models – using analytical formulas. Coupled with topographically correct visualization of the data in this neighborhood this approach, applicable to any classifiers, gives in many cases a better evaluation of the new data than rule-based systems. Two real life examples of such interpretation are presented. 1 Introduction. Classification and prediction are the two most common applications of computational intelligence (CI) methods, i.e. methods designed for solving problems that are effectively non-algorithmic. Although sometimes classification by a black box is sufficient – if it is reliable – domain experts use software to help them to understand
IR
king98practical
Practical Dependency Analysis through a Share Quotient Def , the domain of definite Boolean functions, expresses (sure) dependencies between the program variables of, say, a constraint program. The domain Share, on the other hand, captures the (possible) variable sharing between the variables of a logic program. The connection between these domains has been explored in the domain comparison, quotienting and decomposition literature. We develop this link further and show how the meet (as well as the join) of Def can be modelled with efficient (quadratic) operations on Share. We show how the connection leads to an attractive way of constructing a dependency analysis and how widening can make the approach practical.
DB
1529
A Cubist approach to Object Recognition We describe an appearance-based object recognition system using a keyed, multi-level context representation reminiscent of certain aspects of cubist art. Specifically, we utilize distinctive intermediatelevel features in this case automatically extracted 2-D boundary fragments,, as keys, which are then verified within a local context, and assembled within a loose global context to evoke an overall percept. This system demonstrates good recognition of a variety of 3D shapes, ranging from sports cars and fighter planes to snakes and lizards with full orthographic invariance. We report the results of large-scale tests, involving over 2000 separate test images, that evaluate performance with increasing number of items in the database, in the presence of clutter, background change, and occlusion, and also the results of some generic classification experiments where the system is tested on objects never previously seen or modeled. To our knowledge, the results we report are the best in the l...
DB
354467
Intention Reconsideration in Theory and Practice . Autonomous agents operating in complex dynamic environments need the ability to integrate robust plan execution with higher level reasoning. This paper describes work to combine low level navigation techniques drawn from mobile robotics with deliberation techniques drawn from intelligent agents. In particular, we discuss the combination of a navigation system based on fuzzy logic with a deliberator based on the belief/desire/intention (BDI) model. We discuss some of the subtleties involved in this integration, and illustrate it with an example.  1 INTRODUCTION  Milou the robot works in a food factory. He has to regularly go and fetch two food samples (potato crisps) from two production lines in two different rooms, A and B, and take them to an electronic tester in the quality control lab. Milou must now plan his next delivery. He decides to get the sample from A first, since room A is closer than B. While going there, however, he finds the main door to that room closed. Milou knows t...
Agents
bertino99enhancing
Enhancing the Expressive Power of the U-Datalog Language U-Datalog has been developed with the aim of providing a set-oriented logical update language, guaranteeing update parallelism in the context of a Datalog-like language. In U-Datalog, updates are expressed by introducing constraints (+p(X), to denote insertion, and p(X), to denote deletion) inside Datalog rules. A U-Datalog program can be interpreted as a CLP program. In this framework, a set of updates (constraints) is satisable if it does not represent an inconsistent theory, that is, it does not require the insertion and the deletion of the same fact. This approach resembles a very simple form of negation. However, on the other hand, U-Datalog does not provide any mechanism to explicitly deal with negative information, resulting in a language with limited expressive power. In this paper, we provide a semantics, based on stratication, handling the use of negated atoms in U-Datalog programs and we show which problems arise in dening a compositional semantics.
DB
250815
An Instructor's Assistant for Team-Training in dynamic multi-agent virtual worlds Teams of people operating in highly dynamic, multi-agent environments must learn to deal with rapid and unpredictable turns of events. Simulation-based training environments inhabited by synthetic agents can be effective in providing realistic but safe settings in which to develop the skills these environments require. However such training environments present a problem for the instructor who must evaluate and control rapidly evolving training sessions. We address the instructors' problem with a pedagogical agent called the PuppetMaster. The PuppetMaster manages a network of spy agents that report on the activities in the simulation in order to provide the instructor with an interpretation and situation-specific analysis of student behavior. The approach used to model student teams is to structure the state space into an abstract situation-based model of behavior that supports interpretation in the face of missing information about agent's actions and goals.  
Agents
mcgowan02who
Who do you want to be today? Web Personae for personalised information access Personalised context sensitivity is the Holy Grail of web information retrieval. As a first step towards this goal, we present the Web Personae personalised search and browsing system. We use well-known information retrieval techniques to develop and track user models. Web Personae differ from previous approaches in that we model users with multiple profiles, each corresponding to a distinct topic or domain. Such functionality is essential in heterogeneous environments such as the Web. We introduce Web Personae, describe an algorithm for learning such models from browsing data, and discuss applications and evaluation methods.
IR
441191
Inducing Content Based User Models with Inductive Logic Programming Techniques . In this paper we describe an approach for conceptual user
IR
bonnet99query
Query Processing in a Device Database System Data Types Today's object-relational and object-oriented databases support Abstract Data Type (ADT) objects that are single attribute values encapsulating a collection of related data. The critical feature of an ADT that makes it suitable for representing devices is encapsulation. % Note that there are natural parallels between devices and ADTs. Both ADTs and devices provide controlled access to encapsulated data and functionality through a well-defined interface. We build upon this observation by modeling each type of device in the network as an ADT in the database -- an actual ADT object in the database corresponds then to a physical device in the real world. The public interface of the ADT corresponds to the functionality supported by the device. Methods on the device are executed by sending requests to the device, which evaluates the methods and answers with return values. Differences between the different device types are reflected by differences between the abstract dat...
DB
rao96agentspeakl
AgentSpeak(L): BDI Agents speak out in a logical computable language Abstract. Belief-Desire-Intention (BDI) agents have been investigated by many researchers from both a theoretical specification perspectiveand a practical design perspective. However, there still remains a large gap between theory and practice. The main reason for this has been the complexity of theorem-proving or modelchecking in these expressive specification logics. Hence, the implemented BDI systems have tended to use the three major attitudes as data structures, rather than as modal operators. In this paper, we provide an alternative formalization of BDI agents by providing an operational and proof-theoretic semantics of a language AgentSpeak(L). This language can be viewed as an abstraction of one of the implemented BDI systems (i.e., PRS) and allows agent programs to be written and interpreted in a manner similar to that of horn-clause logic programs. We show how to perform derivations in this logic using a simple example. These derivations can then be used to prove the properties satisfied by BDI agents. 1
Agents
194986
Evaluation of Eye Gaze Interaction Eye gaze interaction can provide a convenient and natural addition to user-computer dialogues. We have previously reported on our interaction techniques using eye gaze [10]. While our techniques seemed useful in demonstration, we now investigate their strengths and weaknesses in a controlled setting. In this paper, we present two experiments that compare an interaction technique we developed for object selection based on a where a person is looking with the most commonly used selection method using a mouse. We find that our eye gaze interaction technique is faster than selection with a mouse. The results show that our algorithm, which makes use of knowledge about how the eyes behave, preserves the natural quickness of the eye. Eye gaze interaction is a reasonable addition to computer interaction and is convenient in situations where it is important to use the hands for other tasks. It is particularly beneficial for the larger screen workspaces and virtual environments of the future, and it will become increasingly practical as eye tracker technology matures.
HCI
71092
The Dynamics of Database Views . The dynamics of relational database can be specified by means of Reiter's formalism based on the situation calculus. The specification of transaction based database updates is given in terms of Successor State Axioms (SSAs) for the base tables of the database. These axioms completely describe the contents of the tables at an arbitrary state of the database that is generated by the execution of a legal primitive transaction, and thus solve the frame problem for databases. In this paper we show how to derive action--effect based SSAs for views from the SSAs for the base tables. We prove consistency properties for those axioms. In addition, we establish the relationship between the derived SSA and the view definition as a static integrity constraint of the database. We give applications of the derived SSAs to the problems of view maintenance, and checking, proving, and enforcement of integrity constraints. 1 Introduction  The situation calculus (SC) [MH1] is a family of many sorted lang...
DB
476601
Techniques for Specialized Search Engines It is emerging that it is very difficult for the major search engines to provide a comprehensive and up-to-date search service of the Web. Even the largest search engines index only a small proportion of static Web pages and do not search the Web' s backend databases that are estimated to be 500 times larger than the static Web. The scale of such searching introduces both technical and economic problems. What is more, in many cases users are not able to retrieve the information they desire because of the simple and generic search interface provided by the major search engines. A necessary response to these search problems is the creation of specialized search engines. These search engines search just for information in a particular topic or category on the Web. Such search engines will have smaller and more manageable indexes and have a powerful domainspecific search interface. This paper discusses the issues in this area and gives an overview of the techniques for building specialized search engines. Keywords: specialized search engine, information retrieval, focused crawling, taxonomy, Web search. 1.
IR
howe97examining
Examining Locally Varying Weights for Nearest Neighbor Algorithms . Previous work on feature weighting for case-based learning algorithms has tended to use either global weights or weights that vary over extremely local regions of the case space. This paper examines the use of coarsely local weighting schemes, where feature weights are allowed to vary but are identical for groups or clusters of cases. We present a new technique, called class distribution weighting (CDW), that allows weights to vary at the class level. We further extend CDW into a family of related techniques that exhibit varying degrees of locality, from global to local. The class distribution techniques are then applied to a set of eleven concept learning tasks. We find that one or more of the CDW variants significantly improves classification accuracy for nine of the eleven tasks. In addition, we find that the relative importance of classes, features, and feature values in a particular domain determines which variant is most successful. 1 Introduction  The k-nearest-neighbor (k-NN)...
ML
22140
Evolutionary Design and Multi-objective Optimisation : In this paper we explore established methods for optimising multi-objective functions whilst addressing the problem of preliminary design. Methods from the literature are investigated and new ones introduced. All methods are evaluated within a collaborative project for whole system airframe design and the basic problems and difficulties of preliminary design methodology are discussed (Cvetkovic, Parmee and Webb 1998). Our Genetic Algorithm is expanded to integrate different methods for optimising multi--objective functions. All presented methods are also analysed in the context of whole system design, discussing their advantages and disadvantages. The problem of qualitative versus quantitative characterisation of relative importance of objectives (such as `objective A is much more important then objective B') in multi--objective optimisation framework is also addressed and some relationships with fuzzy preferences (Fodor and Roubens 1994) and preference ordering established. Several ...
ML
303963
Towards Automating the Evolution of Linguistic Competence in Artificial Agents The goal of this research is to understand and automate the mechanisms by which language can emerge among artificial, knowledge-based and rational agents. We use the paradigm of rationality defined by decision theory, and employ the formal model of negotiation studied in game theory to allow the emergence and enrichment of an agent communication language. 1 Introduction The aim of our research is to understand and automate the mechanisms by which language can emerge among artificial, knowledge-based and rational agents. Our ultimate goal is to be able to design and implement agents that, upon encountering other agent(s) with which they do not share an agent communication language, are able to initiate creation of, and further able to evolve and enrich, a mutually understandable agent communication language (ACL). This paper outlines the overall approach we are taking, and identifies some of the basic concepts and tools that we think are necessary to accomplish our goal. First, the a...
Agents
194227
NetView: A Framework for Integration of Large-Scale Distributed Visual Databases Many visual databases (including image and video databases) are being designed in various locations. The integration of such databases enables users to access data across the world in a transparent manner. In this article, we present a system framework, termed NetView, which supports global content-based query access to various visual databases over the Internet. An integrated metaserver is designed to facilitate such access. The metaserver contains three main components: the metadatabase, the metasearch agent, and the query manager. The metadatabase is organized to include the metadata about individual visual databases which reflect the semantics of each visual database. The query manager extracts heterogeneous features such as text, texture, and color in the query for suitable matching of the metadata. The metasearch agent derives a list of relevant database sites to the given query by matching their feature content with the metadata. Such a capability can significantly reduce the a...
IR
maedche02bootstrapping
Bootstrapping an Ontology-based Information Extraction System Automatic intelligent web exploration will benefit from shallow information extraction techniques if the latter can be brought to work within many different domains. The major bottleneck for this, however, lies in the so far difficult and expensive modeling of lexical knowledge, extraction rules, and an ontology that together define the information extraction system. In this paper we present a bootstrapping approach that allows for the fast creation of an ontology-based information extracting system relying on several basic components, viz. a core information extraction system, an ontology engineering environment and an inference engine. We make extensive use of machine learning techniques to support the semi-automatic, incremental bootstrapping of the domain-specific target information extraction system.  Keywords. Ontologies, information extraction, machine learning  1 
IR
michaud01using
Using Motives and Artificial Emotion for Long-Term Activity of an Autonomous Robot To operate over a long period of time in the real world, autonomous mobile robots must have the capabilityofrecharging themselves whenever necessary. In addition to be able to nd and dockintoacharging station, robots must be able to decide when and for how long to recharge. This decision is inuenced by the energetic capacity of their batteries and the contingencies of their environments. To deal with this temporality issue and based on researchworks in psychology, this paper investigates the use of motives and articial emotions to regulate the recharging need of autonomous robots. A bipolar model of articial emotion is presented, designed to be generic and not specically congured for a particular task. The paper also describes the use of the approach in two specic applications, the AAAI Mobile Robot Challenge and experiments involving a group of robots share one charging station in an enclosed area.  1. 
Agents
cox99model
A Model Independent Source Code Repository Software repositories, used to support program development and maintenance, invariably require an abstract model of the source code. This requirement restricts the repository user to the analyses and queries supported by the data model of the repository. In this work, we present a software repository system based on an existing information retrieval system for structured text. Source code is treated as text, augmented with supplementary syntactic and semantic information. Both the source text and supplementary information can then be queried to retrieve elements of the code. No transformations are necessary to satisfy the requirements of a database storage model. As a result, the system is free of many of the limitations imposed by existing systems.  1 Introduction  In order to store computer source code in a database, the source code must be abstracted in some manner so that it satisfies the requirements of the data model supported by the chosen database system. Databases applied to t...
DB
bird01olac
The OLAC Metadata Set and Controlled Vocabularies As language data and associated  technologies proliferate and as the  language resources community rapidly  expands, it has become difficult to  locate and reuse existing resources.  Are there any lexical resources for  such-and-such a language? What  tool can work with transcripts in  this particular format? What is a  good format to use for linguistic data  of this type? Questions like these  dominate many mailing lists, since  web search engines are an unreliable  way to find language resources.  This paper describes a new digital  infrastructure for language resource  discovery, based on the Open Archives  Initiative, and called OLAC -- the  Open Language Archives Community.  The OLAC Metadata Set and the  associated controlled vocabularies  facilitate consistent description and  focussed searching. We report progress  on the metadata set and controlled  vocabularies, describing current issues  and soliciting input from the language  resources community.  1 
IR
white98towards
Towards Multi-Swarm Problem Solving in Networks This paper describes how multiple interacting swarms of adaptive mobile agents can be used to solve problems in networks. The paper introduces a new architectural description for an agent that is chemically inspired and proposes chemical interaction as the principal mechanism for inter-swarm communication. Agents within a given swarm have behavior that is inspired by the foraging activities of ants, with each agent capable of simple actions and knowledge of a global goal is not assumed. The creation of chemical trails is proposed as the primary mechanism used in distributed problem solving arising from self-organization of swarms of agents. The paper proposes that swarm chemistries can be engineered in order to apply the principal ideas of the Subsumption Architecture in the domain of mobile agents. The paper presents applications of the new architecture in the domain of communications networks and describes the essential elements of a mobile agent framework that is being considered fo...
Agents
liu99olog
OLOG: A Deductive Object Database Language (Extended Abstract) )  Mengchi Liu  Department of Computer Science, University of Regina  Regina, Saskatchewan, Canada S4S 0A2  mliu@cs.uregina.ca  http://www.cs.uregina.ca/mliu  Abstract. Deductive object-oriented databases are intended to combine  the best of the deductive and object-oriented approaches. However,  some important object-oriented features are not properly supported in  the existing proposals. This paper proposes a novel deductive language  that supports important structurally object-oriented features such as  object identity, complex objects, typing, classes, class hierarchies, multiple  property inheritance with overriding, conict-handling, and blocking,  and schema denitions in a uniform framework. The language eectively  integrates useful features in deductive and object-oriented database languages.  The main novel feature is the logical semantics that cleanly accounts  for those structurally object-oriented features that are missing in  object-oriented database languages. Therefor...
DB
4799
Data Mining Approaches for Intrusion Detection In this paper we discuss our research in developing general and systematic methods for intrusion detection. The key ideas are to use data mining techniques to discover consistent and useful patterns of system features that describe program and user behavior, and use the set of relevant system features to compute (inductively learned) classifiers that can recognize anomalies and known intrusions. Using experiments on the sendmail system call data and the network tcpdump data, we demonstrate that we can construct concise and accurate classifiers to detect anomalies. We provide an overview on two general data mining algorithms that we have implemented: the association rules algorithm and the frequent episodes algorithm. These algorithms can be used to compute the intra- and inter- audit record patterns, which are essential in describing program or user behavior. The discovered patterns can guide the audit data gathering process and facilitate feature selection. To meet the challenges of both efficient learning (mining) and real-time detection, we propose an agent-based architecture for intrusion detection systems where the learning agents continuously compute and provide the updated (detection) models to the detection agents. 
ML
vilalta01rule
Rule Induction of Computer Events Introduction  Monitoring systems are able to capture an assortment of different events from a network environment. A single event signals an abnormal situation on either one host, e.g., "cpu utilization is above a critical threshold", or the network, e.g., "communication link is down". A monitoring system can capture thousands of events in a short time period (several days); applying data analysis techniques (e.g., machine-learning, data-mining) on those events may reveal useful patterns characterizing a network problem. Data analysis techniques have proved useful in the area of network fault management.  In this paper we consider the following scenario. A computer network is under continuous monitoring; a user is interested in identifying what triggers a specific kind of events, which we refer to as target events. The user would like to know what events correlate to each target event within a fixed time window. We describe a data-mining technique that takes as input 
ML
haynes96learning
Learning Cases to Resolve Conflicts and Improve Group Behavior Groups of agents following fixed behavioral rules can be limited in performance and efficiency. Adaptability and flexibility are key components of intelligent behavior which allow agent groups to improve performance in a given domain using prior problem solving experience. We motivate the usefulness of individual learning by group members in the context of overall group behavior. In particular, we propose a framework in which individual group members learn cases to improve their model of other group members. We use a testbed problem from the distributed AI literature to show that simultaneous learning by group members can lead to significant improvement in group performance and efficiency over agent groups following static behavioral rules. Introduction  An agent is rational if when faced with a choice from a set of actions, it chooses the one that maximizes the expected utilities of those actions. Implicit in this definition is the assumption that the preference of the agent for diffe...
ML
88449
Inverted Files Versus Signature Files for Text Indexing Two well-known indexing methods are inverted files and signature files. We have undertaken a detailed comparison of these two approaches in the context of text indexing, paying particular attention to query evaluation speed and space requirements. We have examined their relative performance using both experimentation and a refined approach to modeling of signature files, and demonstrate that inverted files are distinctly superior to signature files. Not only can inverted files be used to evaluate typical queries in less time than can signature files, but inverted files require less space and provide greater functionality. Our results also show that a synthetic text database can provide a realistic indication of the behavior of an actual text database. The tools used to generate the synthetic database have been made publicly available.
IR
fauvet99applying
Applying Temporal Databases to Geographical Data Analysis This paper reports an experience in which a temporal database was used to analyze the results of a survey on human behaviors and displacements in a ski resort. This survey was part of a broader study about the use of the resort 's infrastructure, based on the time-geography methodology. As such, the presented experience may be seen as an attempt to implement some concepts of the time-geography using temporal database technology. Throughout the paper, some shortcomings of current temporal data models regarding human displacements analysis are pointed out, and possible solutions are briefly sketched.  Keywords: temporal databases, time-geography, human displacements analysis.  1 
DB
410405
An Adaptive and Distributed Framework for Advanced IR It has been often noticed that modern IR ((Gregory, 1991), (Alan, 1991)) should exhibit capabilities that are sensitive to the document content, integrate interactivity, multimodality and multilinguality over a large scale and support the very dynamic nature of the current needs for information access (so to be adaptable to chanes of the sources, language and content/style). This paper discuss the architectural design aspects of TREVI (Text Retrieval and Enrichment for Vital Information - ESPRIT project EP23311), a distributed Object-Oriented Java/CORBA driven system for NLP-driven news classification, enrichment and delivery. The advanced features of TREVI include the extensive use of a well defined model ((Mazzucchelli, 1999)) based on a typed mechanism for static/dynamic control of the distributed process and on a principled representation of linguistic types into computational OO data structures and the adaptivity of the employed linguistic processors (namely, the robust and lexica...
IR
380343
Fast Supervised Dimensionality Reduction Algorithm with Applications to Document Categorization & Retrieval Retrieval techniques based on dimensionality reduction, such as Latent Semantic Indexing (LSI), have been shown to improve the quality of the information being retrieved by capturing the latent meaning of the words present in the documents. Unfortunately, the high computational and memory requirements of LSI and its inability to compute an effective dimensionality reduction in a supervised setting limits its applicability. In this paper we present a fast supervised dimensionality reduction algorithm that is derived from the recently developed cluster-based unsupervised dimensionality reduction algorithms. We experimentally evaluate the quality of the lower dimensional spaces both in the context of document categorization and improvements in retrieval performance on a variety of different document collections. Our experiments show that the lower dimensional spaces computed by our algorithm consistently improve the performance of traditional algorithms such as C4.5, k-nearest- neighbor, ...
IR
98111
How Do Program Understanding Tools Affect How Programmers Understand Programs? In this paper, we explore the question of whether program understanding tools enhance or change the way that programmers understand programs. The strategies that programmers use to comprehend programs vary widely. Program understanding tools should enhance or ease the programmer 's preferred strategies, rather than impose a fixed strategy that may not always be suitable. We present observations from a user study that compares three tools for browsing program source code and exploring software structures. In this study, 30 participants used these tools to solve several high-level program understanding tasks. These tasks required a broad range of comprehension strategies. We describe how these tools supported or hindered the diverse comprehension strategies used.  Keywords: Fisheye views, program comprehension, program understanding tools, reverse engineering, software maintenance, software visualization, user study. 1 Introduction  Program understanding tools should help programmers to ...
ML
sachdev98exploration
Explorations in Asynchronous Teams The subject of this thesis is the A-Teams formalism. This formalism facilitates the organization of multiple algorithms, encapsulated as autonomous agents, into cooperating teams to solve difficult problems. The ATeams formalism is one of many agent-based systems, and I start by providing a taxonomy of agent-based systems that allows us to see they how A-Teams relate to other agent-based systems. A-Teams are constructed from memories that store solutions and agents that work on those solutions. ATeams are open to the addition of new memories as well as of new agents. Sets of memories and agents can also be combined in different ways to create a variety of customized A-Teams. As new memories and agents are created, they can be added to existing repositories and reused for future applications. The automatic construction of problem-specific custom A-Teams from repositories of components has been a long standing goal of research in A-Teams. Current guidelines for A-Team construction requir...
Agents
527800
Data Quality in e-Business Applications In e-Business scenarios, an evaluation of the quality of exchanged  data is essential for developing service-based applications and  correctly performing cooperative activities. Data of low quality can spread  all over the cooperative system, but at the same time, improvement can  be based on comparing data, correcting them and disseminating high  quality data. In this paper, an XML-based broker service for managing  data quality in cooperative systems is presented, which selects the best  available data from different services. Such a broker also supports data  quality improvements based on feedbacks to source services.
DB
karnik99security
Security in the Ajanta Mobile Agent System This paper describes the security architecture of Ajanta
Agents
451495
Coordination Infrastructure for Virtual Enterprises Virtual Enterprises (VE) and Workflow Management Systems (WFMS) require deployable and flexible infrastructures, promoting the integration of heterogenous resources and services, as well as the development of new VE's business processes in terms of workflow (WF) rules coordinating the activities of VE's component enterprises. In this paper, we argue that a suitable general-purpose coordination infrastructure may well fit the needs of VE management in a highly dynamic and unpredictable environment like the Internet, by providing engineers with the abstractions and run-time support to address heterogeneity of different sorts, and to represent WF rules as coordination laws. We discuss the requirements for VE infrastructures, and suggest why VE management and WFMS may be seen as coordination problems. Then, we introduce the TuCSoN coordination model and technology, and show, both in principle and in a simple case study, how such a coordination infrastructure can support the design and deve...
Agents
119174
Processing Fuzzy Spatial Queries: A Configuration Similarity Approach . Increasing interest for configuration similarity is currently developing in the context of Digital Libraries, Spatial Databases and Geographical Information Systems. The corresponding queries retrieve all database configurations that match an input description (e.g., "find all configurations where an object x 0 is about 5km northeast of another x 1 , which, in turn, is inside object x 2 "). This paper introduces a framework for configuration similarity that takes into account all major types of spatial constraints (topological, direction, distance). We define appropriate fuzzy similarity measures for each type of constraint to provide flexibility and allow the system to capture real-life needs. Then we apply pre-processing techniques to explicate constraints in the query, and present algorithms that effectively solve the problem. Extensive experimental results demonstrate the applicability of our approach to images and queries of considerable size. 1. INTRODUCTION As opposed to visu...
IR
jalali-sohi01integrated
An Integrated Secure Web Architecture For Protected Mobile Code Distribution : IPR (Intellectual Property Rights) protection is one of the key elements to be considered in the development of mobile code technologies (applets, agents, etc.) due to the mobile nature of this kind of software and the power of servers. The absence of protection would increase the risk of piracy to such a level that the economy of this sector would be weakened, perhaps even destroyed. FILIGRANE (FlexIbLe IPR for Software AGent ReliANcE) Framework  Complementary to the legal provisions (anti-piracy laws), IPR protection is one of the absolute elements in the development of these new markets. In the course of ESPRIT project FILIGRANE (FlexIbLe IPR for Software AGent ReliANcE), we developed an integrated Web architecture and associated security framework and protocol for the trading of mobile code in Internet. The term mobile code includes all kinds of mobile Java software (applets and agents and Java beans, cardlets, etc.).  1. 
Agents
314295
Learning to Order Things wcohen,schapire,singer¡ There are many applications in which it is desirable to order rather than classify instances. Here we consider the problem of learning how to order, given feedback in the form of preference judgments, i.e., statements to the effect that one instance should be ranked ahead of another. We outline a two-stage approach in which one first learns by conventional means a preference function, of the form PREF¢¤£¦¥¨§� ©, which indicates whether it is advisable to rank £ before §. New instances are then ordered so as to maximize agreements with the learned preference function. We show that the problem of finding the ordering that agrees best with a preference function is NP-complete, even under very restrictive assumptions. Nevertheless, we describe a simple greedy algorithm that is guaranteed to find a good approximation. We then discuss an on-line learning algorithm, based on the “Hedge ” algorithm, for finding a good linear combination of ranking “experts.” We use the ordering algorithm combined with the on-line learning algorithm to find a combination of “search experts, ” each of which is a domain-specific query expansion strategy for a WWW search engine, and present experimental results that demonstrate the merits of our approach. 1
IR
aguil00querying
Querying XML Documents in Xyleme this paper on query processing. The query language supporting Xyleme graphical user interface is an extension of OQL [6, 10] and provides a mix of database and information retrieval characteristics. It is consistent with the requirements published by the W3C XML Query Working Group [20] and similar to many languages proposed by the database community for text or semistructured databases [1, 5, 7, 11]. Still, due to the fact that we are considering millions of documents, query processing acquires in Xyleme a truly distinct flavor. Although we rely mainly on database optimization techniques (notably, the use of an algebra as in [5, 8, 9, 12, 15]), we need to adapt them to meet new and interesting challenges:
DB
524967
Searching People on the Web According to Their Interests Due to lack of structural data and information explosion on the World Wide Web (WWW), searching for useful information is becoming increasingly a difficult task. Traditional search engines on the Web render some form of assistance, but perform sub-optimally when dealing with context sensitive queries. To overcome this, niche search engines serving specific Web communities evolved. These engines index only pages of high quality and relevance to a specific domain and make use of context information for searching. This poster presents BullsI Search (publicly available at http://dm2.comp.nus.edu.sg), a fielded domain specific search engine that helps Web users locate computer science faculty members' homepages and email addresses by specifying a research interest, teaching interest or name. The system is able to automatically extract and index research interests, teaching interests, owners' email addresses and names from a set of discovered homepages. Experimental evaluation shows that the proposed algorithms are very accurate and are independent of structures in different homepages.
IR
307268
Appearance-Based Place Recognition for Topological Localization This paper presents a new appearance-based place recognition system for topological localization. The method uses a panoramic vision system to sense the environment. Color images are classified in real-time based on nearest-neighbor learning, image histogram matching, and a simple voting scheme. The system has been evaluated with eight cross-sequence tests in four unmodified environments, three indoors and one outdoors. In all eight cases, the system successfully tracked the mobile robot's position. The system correctly classified between 87% and 98% of the input color images. For the remaining images, the system was either momentarily confused or uncertain, but never classified an image incorrectly. 1. INTRODUCTION Localization is a fundamental problem in mobile robotics. Most mobile robots must be able to locate themselves in their environment in order to accomplish their tasks. Since mobile robot localization is a prerequisite for most applications, research has been very active i...
ML
pinheirodasilva00generating
Generating User Interface Code in a Model Based User Interface Development Environment Declarative models play an important role in most software design activities, by allowing designs to be constructed that selectively abstract over complex implementation details. In the user interface setting, Model-Based User Interface Development Environments (MB-UIDEs) provide a context within which declarative models can be constructed and related, as part of the interface design process. However, such declarative models are not usually directly executable, and may be difficult to relate to existing software components. It is therefore important that MB-UIDEs both fit in well with existing software architectures and standards, and provide an effective route from declarative interface specification to running user interfaces. This paper describes how user interface software is generated from declarative descriptions in the Teallach MB-UIDE. Distinctive features of Teallach include its open architecture, which connects directly to existing applications and widget sets, and the genera...
HCI
liu00extended
An Extended Genetic Rule Induction Algorithm This paper describes an extension of a GAbased, separate-and-conquer propositional rule induction algorithm called SIA [24]. While the original algorithm is computationally attractive and is also able to handle both nominal and continuous attributes efficiently, our algorithm further improves it by taking into account of the recent advances in the rule induction and evolutionary computation communities. The refined system has been compared to other GA-based and non GA-based rule learning algorithms on a number of benchmark datasets from the UCI machine learning repository. Results show that the proposed system can achieve higher performance while still produces a smaller number of rules.  1 Introduction  The increasingly widespread use of information system technologies and the internet has resulted in an explosive growth of many business, government and scientific databases. As these terabyte-size databases become prevalent, the traditional approach of using human experts to sift thro...
ML
decker01multiagent
A Multi-Agent System for Automated Genomic Annotation Massive amounts of raw data are currently being generated by biologists while sequencing organisms. Outside of the largest, high-profile projects such as the Human Genome Project, most of this raw data must be analyzed through the piecemeal application of various computer programs and searches of various public web databases. Due to the inexperience and lack of training, both the raw data and any valuable derived knowledge will remain generally unavailable except in published textual forms. Multi-agent information gathering systems have a lot to contribute to these efforts, even at the current state of the art. We have used DECAF, a multi-agent system toolkit based on RETSINA and TAEMS, to construct a prototype multi-agent system for automated annotation and database storage of sequencing data for herpes viruses. The resulting system eliminates tedious and always out-of-date hand analyses, makes the data and annotations available for other researchers (or agent systems), and provides a level of query processing beyond even some high-profile web sites.
Agents
cohen00learning
Learning Concepts by Interaction This paper presents a theory of how robots may learn  concepts by interacting with their environment in an  unsupervised way. First, categories of activities are learned,  then abstractions over those categories result in concepts.  The meanings of concepts are discussed. Robotic systems  that learn categories of activities and concepts are presented.  Introduction  If machines could acquire conceptual knowledge with the same facility as humans, then AI would be much better off. There's no denying the dream of a machine that knows roughly what we know, organized roughly as we organize it, with roughly the same values and motives as we have. It makes sense, then, to ask how this knowledge is acquired by humans and how might it be acquired by machines.  I focus on the origins of conceptual knowledge, the earliest distinctions and classes, the first efforts to carve the world at its joints. One reason is just the desire to get to the bottom of, or in this case the beginning of, anything. ...
ML
randell02well
The Well Mannered Wearable Computer In this paper we describe continuing work being carried out as part of the Bristol Wearable Computing Initiative. We are interested in the use of context sensors to improve the usefulness of wearable computers. A CyberJacket incorporating a Tourist Guide application has been built, and we have experimented with location and movement sensing devices to improve its performance. In particular, we have researched processing techniques for data from accelerometers which enable the wearable computer to determine the user's activity.
HCI
myers98brief
A Brief History of Human Computer Interaction Technology This article summarizes the historical development of major advances in humancomputer interaction technology, emphasizing the pivotal role of university research in the advancement of the field. Copyright 1996 --- Carnegie Mellon University A short excerpt from this article appeared as part of "Strategic Directions in Human Computer Interaction," edited by Brad Myers, Jim Hollan, Isabel Cruz, ACM Computing Surveys, 28(4), December 1996 This research was partially sponsored by NCCOSC under Contract No. N66001-94-C-6037, Arpa Order No. B326 and partially by NSF under grant number IRI-9319969. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NCCOSC or the U.S. Government.  Keywords: Human Computer Interaction, History, User Interfaces, Interaction Techniques.  Brief History of HCI - 1 1. Introduction  Research in Human-Computer Interaction (HCI) has been spec...
HCI
stonebraker01content
Content Integration for E-Business We define the problem of content integration for EBusiness, and show how it differs in fundamental ways from traditional issues surrounding data integration, application integration, data warehousing and OLTP. Content integration includes catalog integration as a special case, but encompasses a broader set of applications and challenges. We explore the characteristics of content integration and required services for any solution. In addition, we explore architectural alternatives and discuss the use of XML in this arena.  1. 
DB
322267
An Indexed Bibliography of Genetic Algorithms and Neural Networks A computer based simulation with artificial adaptive agents for predicting secondary structure from the protein hydrophobicity ffl  [79] Abstract Appl. of GAs to de novo design of therapeutic peptides ffl of a poster] [77] -- Assigning a protein sequence to a three-dimensional fold ffl of a poster] [78] -- Design of a three helix bundle with a "native-like" folded state ffl of a poster] [82] -- Resolving water-mediated and polar ligand recognition using GAs ffl of a poster] [60] actinomycin GAs for docking of ffl D and deoxyguanosine molecules with comparison to the crystal structure of ffl  D-deoxyguanosine complex [118] adaptive A computer based simulation with artificial  ffl agents for predicting secondary structure from the protein hydrophobicity [Abstract] [75] AG-1343 Molecular recognition of the inhibitor ffl by HIV-1 protease: conformationally flexible docking by EP [118] agents A computer based simulation with artificial adaptive ffl for predicting secondary structure from th...
ML
373023
Contractual Agent Societies: Negotiated shared context and social control in open multi-agent systems Information systems for supporting the fluid organizations of the 21  st  century must be correspondingly open and agile, able to automatically configure themselves out of heterogeneous system components, accommodate the dynamic exit and entry of hitherto unknown participants and maintain system stability in the face of limited trust. This paper introduces the concept of Contractual Agent Societies (CAS) as a metaphor for building such open information systems. CAS are open information systems where independently developed agents configure themselves automatically through a set of dynamically negotiated social contracts. Social contracts define the shared context of agent interactions, including ontologies, joint beliefs, joint goals, normative behaviors, etc. In addition, they specify classes of associated exceptions (deviations from ideal behavior) together with associated prevention and resolution mechanisms. A research agenda for developing the infrastructure that will enable the c...
Agents
cumby00relational
Relational Representations that Facilitate Learning Given a collection of objects in the world,  along with some relations that hold among  them, a fundamental problem is how to learn  denitions of some relations and concepts of  interest in terms of the given relations. These  denitions might be quite complex and, inevitably,  might require the use of quanti-  ed expressions. Attempts to use rst order  languages for these purposes are hampered  by the fact that relational inference is  intractable and, consequently, so is the problem  of learning relational denitions.  This work develops an expressive relational  representation language that allows the use  of propositional learning algorithms when  learning relational denitions.  The representation serves as an intermediate  level between a raw description of observations  in the world and a propositional learning  system that attempts to learn denitions  for concepts and relations. It allows for hierarchical  composition of relational expressions  that can be evaluated ecientl...
ML
535300
A Multi-Agent Reflective Architecture for Web Search Assistance Nowadays the Web is overloaded with documents and even finding out  whether some of the items selected by search engines are worth reading is a tedious  and time consuming job. This paper proposes a general framework and specific assistant  agents that are used to enrich browsers with the ability to characterise the user interests  and to influence results of search engines according to such interests. To enrich  Web browsers, a reflective architecture was used that captures control from browser  objects at run time and intertwines further activities when some conditions are verified.
Agents
denoue00annotation
An annotation tool for Web browsers and its applications to information retrieval With bookmark programs, current Web browsers provide a limited support to personalize the Web. We present a new Web annotation tool which uses the Document Object Model Level 2 and Dynamic HTML to deliver a system where speed and privacy are important issues. We report on several experiments showing how annotations improve document access and retrieval by providing user-directed document summaries. Preliminary results also show that annotations can be used to produce user-directed document clustering and classification. Introduction Current Web browsers provide a limited support to personalize the Web, namely bookmarks. In two recent surveys, users report that the use of bookmarks is among the three main problems they have when using the Internet (Abrams, 1998 ; Cockburn, 1999). We see several problems when using bookmarks : The number of bookmarks grows linearly with time (Abrams, 1998) and it becomes a real challenge for many users to organize this amount of data By storing the...
IR
mataric00sensorymotor
Sensory-Motor Primitives as a Basis for Imitation: Linking Perception to Action and Biology to Robotics ing away from the specific coding of the spinal fields, the examples from neurobiology provide the framework for a motor control system based on a small number of additive primitives (or basis behaviors) sufficient for a rich output movement repertoire. Our previous work (Matari'c 1995, Matari'c 1997), inspired by the same biological results, has successfully applied the idea of basis behaviors to control of mobile robots 6  by fitting it directly into the modular behavior-based control paradigm. Applictions of schema theory (Arbib 1992) to behavior-based mobile robots (Arkin 1987) have employed a similar notion of composable behaviors, stemming from foundations in neuroscience (Arbib 1981, Arbib 1989).  The idea of using such primitives for articulator control has been recently studied in robotics. Williamson (1996) and Marjanovi'c, Scassellati & Williamson (1996) developed a 6 DOF (degrees of freedom) robot arm controller. While in the biological and mobile robotics work primitives c...
Agents
cohen98integration
Integration of Heterogeneous Databases Without Common Domains Using Queries Based on Textual Similarity Most databases contain "name constants" like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user's query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIR...
IR
balke00applications
Applications of Quick-Combine for Ranked Query Models In digital libraries queries are often based on the similarity of objects, using several feature attributes like colors, texture or full-text searches. Such multi-feature queries return a ranked result set instead of exact matches. Recently we presented a new algorithm called Quick-Combine [5] for combining multi-feature result lists, guaranteeing the correct retrieval of the k top-ranked results. As benchmarks on practical data promise that we can dramatically improve performance, we want to discuss interesting applications of Quick-Combine in different areas. The applications for the optimization in ranked query models are manifold. Generally speaking we believe that all kinds of federated searches can be supported like e.g. content-based retrieval, knowledge management systems or multi-classifier combination.   
DB
526346
Computational Web Intelligence (CWI): Synergy of Computational Intelligence and Web Technology With explosive growth of e-Business on the Internet, and wireless networks, users face more and more challenging networks-based application problems in terms of intelligent eApplications. To increase the QoI (Quality of Intelligence) of eBusiness, we propose a new research area called Computational Web Intelligence (CWI) based on both Computational Intelligence (CI) and Web Technology (WT). Generally, the intelligent e-brainware using CWI techniques plays an important role in smart e-Business. In this paper, fundamental concepts, basic methods, major applications and future trends of CWI are described to briefly show a general framework of CWI from different aspects.
IR
bahle02efficient
Efficient Phrase Querying with an Auxiliary Index Search engines need to evaluate queries extremely fast, a challenging task given the vast quantities of data being indexed. A significant proportion of the queries posed to search engines involve phrases. In this paper we consider how phrase queries can be efficiently supported with low disk overheads. Previous research has shown that phrase queries can be rapidly evaluated using nextword indexes, but these indexes are twice as large as conventional inverted files. We propose a combination of nextword indexes with inverted files as a solution to this problem. Our experiments show that combined use of an auxiliary nextword index and a conventional inverted file allow evaluation of phrase queries in half the time required to evaluate such queries with an inverted file alone, and the space overhead is only 10% of the size of the inverted file. Further time savings are available with only slight increases in disk requirements.
IR
choi00randomized
A Randomized Approach to Planning Biped Locomotion with Prescribed Motions In this paper, we present a new scheme for planning a natural-looking locomotion of a human-like biped figure. Given start and goal positions in a virtual environment, our scheme finds a sequence of motions to move from the start position to the goal using a set of prescribed, live-captured motion clips. Our scheme consists of three parts: roadmap construction, roadmap search, and motion generation. We randomly sample a set of valid configurations of the biped figure for the environment to construct a directed graph, called a roadmap, that guides the locomotion of the figure. Every edge of the roadmap is attached with a live-captured motion clip. Traversing the roadmap, we obtain the sequence of footprints and that of motion clips. We finally adapt the motion sequence to the constraints specified by the footprint sequence to obtain the locomotion.
AI
15340
Large Scale Terrain Visualization Using The Restricted Quadtree Triangulation Real-time rendering of triangulated surfaces has attracted growing interest in the last few years. However, interactive visualization of very large scale grid digital elevation models is still a hard problem. The graphics load must be controlled by an adaptive surface triangulation and by taking advantage of different levels of detail. Furthermore, the management of the visible scene requires efficient access to the terrain database. We describe a all-in-one visualization system which integrates adaptive triangulation, dynamic scene management and spatial data handling. The triangulation model is based on the restricted quadtree triangulation. Furthermore, we present new algorithms of the restricted quadtree triangulation. These include among others exact error approximation, progressive meshing, performance enhancements and spatial access.  Keywords algorithms, computer graphics, virtual reality, triangulated surfaces, terrain visualization, terascale visualization 1. Introduction  In...
DB
denti00luce
The LuCe Coordination Technology for MAS Design and Development on the Internet Internet-based multi-agent systems call for new metaphors, abstractions, methodologies and enabling techologies specifically tailored to agent-oriented engineering. While coordination models define the framework to manage the space of agent interaction, ruling social behaviours and accomplishing social tasks, their impact on system design and development calls for an effective coordination technology. This paper presents LuCe, a coordination technology that integrates Java, Prolog and the notion of logic tuple centre, a programmable coordination medium, into a coherent framework. The power of the LuCe coordination technology is first discussed in general, then shown in the context of a simple yet significant system: a TicTacToe game among intelligent software agents and human players on the Internet.
Agents
martinez00recognition
Recognition of Partially Occluded and/or Imprecisely Localized Faces Using a Probabilistic Approach New face recognition approaches are needed, because although much progress has been recently achieved in the field (e.g. within the eigenspace domain), still many problems are to be robustly solved. Two of these problems are occlusions and the imprecise localization of faces (which ultimately imply a failure in identification) . While little has been done to account for the first problem, almost nothing has been proposed to account for the second. This paper presents a probabilistic approach that attempts to solve both problems while using an eigenspace representation. To resolve the localization problem, we need to find the subspace (within the feature space, e.g. eigenspace) that represents this error for each of the training image. To resolve the occlusion problem, each face is divided into n local regions which are analyzed in isolation. In contrast with other previous approaches, where a simple voting space is used, we present a probabilistic method that analyzes how "good" a loca...
ML
damiani97structuring
Structuring and Querying the Web through Graph-Oriented Languages . In order to pose effective queries to Web sites, some form of site data model must be implicitly or explicitly shared by users. Many approaches try to compensate for the lack of such a common model by considering the hypertextual structure of Web sites; unfortunately, this structure has usually little to do with data semantics. In this paper a different technique is proposed that allows for both navigational and data model description of Web sites, while allowing for graphical queries. The data model is based on WGlog, a description and query language based on the graph-oriented object database model of GOOD [Gys94] and G-log [Par95] allowing description of data manipulation primitives via graph transformations. WG-log description of the navigational part of a Web site schema is lexically based on standard hypermedia design languages, thus allowing for easy schema generation by current hypermedia authoring environments. The use of WG-log for queries allows graphic query construction ...
IR
320482
Tabling for Non-monotonic Programming this paper we describe tabling as it is implemented in the XSB system
DB
helmer01software
Software Fault Tree and Colored Petri Net Based Specification, Design and Implementation of Agent-Based Intrusion Detection Systems The integration of Software Fault Tree (SFT) which describes intrusions and Colored Petri Nets (CPNs) which specifies design, is examined for an Intrusion Detection System (IDS). The IDS under development is a collection of mobile agents that detect, classify, and correlate system and network activities. Software Fault Trees (SFTs), augmented with nodes that describe trust, temporal, and contextual relationships, are used to describe intrusions. CPNs for intrusion detection are built using CPN templates created from the augmented SFTs. Hierarchical CPNs are created to detect critical stages of intrusions. The agent-based implementation of the IDS is then constructed from the CPNs. Examples of intrusions and descriptions of the prototype implementation are used to demonstrate how the CPN approach has been used in development of the IDS. The main contribution of this paper is an approach to systematic specification, design, and implementation of an IDS. Innovations include (1) using stages of intrusions to structure the specification and design of the IDS, (2) augmentation of SFT with trust, temporal, and contextual nodes to model intrusions, (3) algorithmic construction of CPNs from augmented SFT, and (4) generation of mobile agents from CPNs. 1
Agents
wang00experience
Experience paper: Implementing a Multi-Agent Architecture for Cooperative Software Engineering The paper describes experiences we have earned from implementing a multiagent architecture used to support cooperative software engineering. Before starting to implement a multi-agent architecture, important decisions and considerations must be taken into account. You have to decide how to provide efficient inter-agent communication support, what language should the agents talk, should the agents be stationary or mobile, and what technology should be used to build the architecture. This paper describes how we implemented our multi-agent system, and the experiences we gained from building it. Keywords: Cooperative Software Engineering, Agents, Multi-agent system, KQML, XML, Aglets, JATLite, CORBA. 1 Introduction The last couple of years, distributed computing and agent technology have become more and more popular. When researchers are developing prototypes, the choice of technologies and how to use different technologies is getting more and more complicated. This paper descri...
Agents
amer-yahia00boundingschemas
On Bounding-Schemas for LDAP Directories . As our world gets more networked, ever increasing amounts  of information are being stored in LDAP directories. While LDAP directories  have considerable flexibility in the modeling and retrieval of  information for network applications, the notion of schema they provide  for enabling consistent and coherent representation of directory information  is rather weak. In this paper, we propose an expressive notion  of bounding-schemas for LDAP directories, and illustrate their practical  utility. Bounding-schemas are based on lower bound and upper bound  specifications for the content and structure of an LDAP directory. Given  a bounding-schema specification, we present algorithms to efficiently determine:  (i) if an LDAP directory is legal w.r.t. the bounding-schema,  and (ii) if directory insertions and deletions preserve legality. Finally,  we show that the notion of bounding-schemas has wider applicability,  beyond the specific context of LDAP directories.  1 Introduction  X.500 styl...
DB
prasad99learning
Learning Situation-Specific Coordination in Cooperative Multi-agent Systems Achieving effective cooperation in a multi-agent system is a difficult problem for a number of reasons such as limited and possiblyout-dated views of activities of other agents and uncertainty about the outcomes of interacting non-local tasks. In this paper, we present a learning system called COLLAGE, that endows the agents with the capability to learn how to choose the most appropriate coordination strategy from a set of available coordination strategies. COLLAGE relies on meta-level information about agents' problem solving situationsto guide them towards a suitable choice for a coordination strategy. We present empirical results that strongly indicate the effectiveness of the learning algorithm. Keywords: Multi-agent Systems, Coordination, Learning  1 Introduction  Coordination is the process of effectively managing interdependencies between activities distributed across agents so as to derive maximum benefit from them[21, 6]. Based on structure and uncertainty in their environmen...
Agents
108321
Case-Based Classification Using Similarity-Based Retrieval Classification involves associating instances with particular classes by maximizing intra-class similarities and minimizing inter-class similarities. The paper presents a novel approach to case-based classification. The algorithm is based on a notion of similarity assessment and was developed for supporting flexible retrieval of relevant information. Validity of the proposed approach is tested on real world domains, and the system's performance is compared to that of other machine learning algorithms. 1 Introduction  Classification involves associating instances with particular classes; based on the object description, the classification system determines whether a given object belongs to a specified class. In general, this process consists of: first generating a set of categories and then classifying given objects into the created categories. For the purpose of this paper, it is assumed that the categories are known a priori from a prescribed domain theory [38]. Various reasoning tech...
ML
nehaniv99constructive
Constructive Biology and Approaches to Temporal Grounding in Post-Reactive Robotics Constructive Biology (as opposed to descriptive biology) means understanding biological mechanisms through building systems that exhibit life-like properties. Applications include learning engineering tricks from biological systems, as well as the validation in biological modelling. In particular, biological systems (unlike reactive robots) in the course of development and experience become temporally grounded. Researchers attempting to transcend mere reactivity have been inspired by the drives, motivations, homeostasis, hormonal control, and emotions of animals. In order to contextualize and modulate behavior, these ideas have been introduced into robotics and synthetic agents, while further flexibility is achieved by introducing learning.  Broadening scope of the temporal horizon further requires post-reactive techniques that address not only the action in the now, although such action may perhaps be modulated by drives and affect. Support is needed for expressing and benefitting from...
Agents
pedersen99supporting
Supporting Imprecision in Multidimensional Databases Using Granularities On-Line Analytical Processing (OLAP) technologies are being used widely for business-data analysis, and these technologies are also being used increasingly in medical applications, e.g., for patient-data analysis. The lack of effective means of handling data imprecision, which occurs when exact values are not known precisely or are entirely missing, represents a major obstacle in applying OLAP technology to the medical domain, as well as many other domains. OLAP systems are mainly based on a multidimensional model of data and include constructs such as dimension hierarchies and granularities. This paper develops techniques for the handling of imprecision that aim to maximally reusing these already existing constructs. With imprecise data now available in the database, queries are tested to determine whether or not they may be answered precisely given the available data; if not, alternative queries that are unaffected by the imprecision are suggested. When a user elects to proceed with a query that is affected by imprecision, techniques are proposed that take into account the imprecision in the grouping of the data, in the subsequent aggregate computation, and in the presentation of the imprecise result to the user. The approach is capable of exploiting existing multidimensional query processing techniques such as pre-aggregation, yielding an effective approach with low computational overhead and that may be implemented using current technology. The paper illustrates how to implement the approach using SQL databases.
DB
lee00spectral
The Spectral Independent Components Of Natural Scenes Abstract. We apply independent component analysis (ICA) for learning an efficient color image representation of natural scenes. In the spectra of single pixels, the algorithm was able to find basis functions that had a broadband spectrum similar to natural daylight, as well as basis functions that coincided with the human cone sensitivity response functions. When applied to small image patches, the algorithm found homogeneous basis functions, achromatic basis functions, and basis functions with overall chromatic variation along lines in color space. Our findings suggest that ICAmay be used to reveal the structure of color information in natural images. 1 Learning Codes for Color Images The efficient encoding of visual sensory information is an important task for image processing systems as well as for the understanding of coding principles in the visual cortex. Barlow [1] proposed that the goal of sensory information processing is to transform the input signals such that it reduces the redundancy
ML
dotsch96good
Good Examples in Learning Containment Decision Lists this paper. By our very specific approaches and results in a very particular setting we intend to go a small step towards a better understanding and partial answering of questions like above.
ML
95810
Learning Models of Other Agents Using Influence Diagrams We adopt decision theory as a descriptive paradigm to model rational agents. We use influence diagrams as a modeling representation of agents, which is used to interact with them and to predict their behavior. In this paper, we provide a framework that an agent can use to learn the models of other agents in a multi-agent system (MAS) based on their observed behavior. Since the correct model is usually not known with certainty our agents maintain a number of possible models and assign a probability to each of them being correct. When none of the available models is likely to be correct, we modify one of them to better account for the observed behaviors. The modification refines the parameters of the influence diagram used to model the other agent's capabilities, preferences, or beliefs. The modified model is then allowed to compete with the other models and the probability assigned to it being correct can be arrived at based on how well it predicts the behaviors of the other agent alrea...
AI
388718
Decidable Fragments of First-Order Modal Logics The paper considers the set ML1 of first-order polymodal formulas the modal operators in which can be applied to subformulas of at most one free variable. Using the mosaic technique, we prove a general satisfiability criterion for formulas in ML1 which reduces the modal satisfiability to the classical one. The criterion is then used to single out a number of new, in a sense optimal, decidable fragments of various predicate modal logics. 1 Introduction  The classical decision problem---to single out expressive and decidable fragments of first-order logic---has a long history and hardly needs any justification: after all, classical first-order logic was and still remains in the very center of logical studies, both in mathematics and applications. Here are only three examples (out of dozens) of such fragments (the choice is not accidental---we shall use these results later on):  ffl the fragment containing only monadic predicate symbols [5];  ffl the fragment with only two individual vari...
DB
537391
Designing Data Warehouses with OO Conceptual Models ions of  our work. We believe that our innovative approach  provides a theoretical foundation for the use of OO  databases and object-relational databases in data  warehouses, MDB, and OLAP applications.  We use UML to design data warehouses because it  considers an information system's structural and  dynamic properties at the conceptual level more naturally  than do classic approaches such as the EntityRelationship  model. Further, UML provides powerful  mechanisms---such as the Object Constraint Language   and the Object Query Language    ---for embedding data  warehouse constraints and initial user requirements in  the conceptual model. This approach to modeling a  data warehouse system yields simple yet powerful  extended UML class diagrams that represent main data  warehouse properties at the conceptual level.  MULTIDIMENSIONAL MODELING PROPERTIES  Multidimensional modeling structures information  into facts and dimensions. We define a fact as an item  of interest for an enterpri
DB
kervrann00level
Level Lines as Global Minimizers of Energy Functionals in Image Segmentation We propose a variational framework for determining global  minimizers of rough energy functionals used in image segmentation. Segmentation  is achieved by minimizing an energy model, which is comprised  of two parts: the first part is the interaction between the observed  data and the model, the second is a regularity term. The optimal boundaries  are the set of curves that globally minimize the energy functional.
AI
159727
Facial feature detection by Saccadic Exploration of the Gabor Decomposition The Gabor decomposition is a ubiquitous tool in computer vision. Nevertheless, it is generally considered computationally demanding for active vision applications. We suggest an attention-driven approach to feature detection inspired by the human saccadic system. A dramatic speedup is achieved by computing the Gabor decomposition only on the points of a sparse retinotopic grid. An application to eye detection is presented. Also, a real-time head detection and tracking system based on our approach is briefly discussed. The system features a novel eyeball-mounted camera designed to mimic the dynamic performance of the human eye and is, to the best of our knowledge, the first example of active vision system based on the Gabor decomposition.
ML
67522
A Temporal Description Logic for Reasoning about Actions and Plans A class of interval-based temporal languages for uniformly representing and reasoning about actions and plans is presented. Actions are represented by describing what is true while the action itself is occurring, and plans are constructed by temporally relating actions and world states. The temporal languages are members of the family of Description Logics, which are characterized by high expressivity combined with good computational properties. The subsumption problem for a class of temporal Description Logics is investigated and sound and complete decision procedures are given. The basic languageTL-F is considered rst: it is the composition of a temporal logicTL { able to express interval temporal networks { together with the non-temporal logicF { a Feature Description Logic. It is proven that subsumption in this language is an NP-complete problem. Then it is shown how to reason with the more expressive languagesTLU-FU andTL-ALCF. The former adds disjunction both at the temporal and non-temporal sides of the language, the latter extends the non-temporal side with set-valued features (i.e., roles) and a propositionally complete language. 1.
AI
2102
Text Classification from Labeled and Unlabeled Documents using EM  This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.  We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.  
ML
jantke97necessity
The Necessity Of User Guidance In Case-Based Knowledge Acquisition The intention of the present paper is to justify both theoretically and experimentally that user guidance is inevitable in case-based knowledge acquisition. The methodology of our approach is simple: We choose some paradigmatic idea of case-based learning which can be very briefly expressed as follows:  Given any CBR system, apply it. Whenever it works sucessfully, do not change it. Whenever it fails on some input case, add this experience to the case base. Don't do anything else. Then, we perform a number of knowledge acquisition experiments. They clearly exhibit essential limitations of knowledge acquisition from randomly chosen cases. As a consequence, we develop scenarios of user guidance. Based on these theoretical concepts, we prove a few theoretical results characterizing the power of our approach. Next, we perform a new series of more constrained results which support our theoretical investigations. This paper is based on more than 1 000 000 runs of case-based knowledge acquisi...
ML
carson99blobworld
Blobworld: A System for Region-Based Image Indexing and Retrieval . Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. Each image is automatically segmented into regions ("blobs") with associated color and texture descriptors. Querying is based on the attributes of one or two regions of interest, rather than a description of the entire image. In order to make large-scale retrieval feasible, we index the blob descriptions using a tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both querying and indexing. 1 Introduction  From a user's point of view, the performance of an information retrieval system can be measured by the quality and speed with which it answers the user's information need. Several factors contribute to overall performance:  -- the time required to run each individual query,  -- the quality (precision/recall) of each i...
IR
andre98integrating
Integrating Reactive and Scripted Behaviors in a Life-Like Presentation Agent Animated agents- based either on real video, cartoon-style drawings or even model-based 3D graphics- offer great promise for computer-based presentations as they make presentations more lively and appealing and allow for the emulation of conversation styles known from human-human communication. In this paper, we describe a life-like interface agent which presents multimedia material to the user following the directives of a script. The overall behavior of the presentation agent is partly determined by such a script, and partly by the agent's self-behavior. In our approach, the agent's behavior is defined in a declarative specification language. Behavior specifications are used to automatically generate a control module for an agent display system. The first part of the paper describes the generation process which involves AI planning and a two-step compilation. Since the manual creation of presentation scripts is tedious and error-prone, we also address the automated generation of presentation scripts which may be forwarded to the interface agent. The second part of the paper presents an approach for multimedia presentation design which combines hierarchical planning with temporal reasoning. 1.1 Keywords human-like qualities of synthetic agents, life-like qualities, presentation agents
Agents
wu01capturing
Capturing Natural Hand Articulation Vision-based motion capturing of hand articulation is a challenging task, since the hand presents a motion of high degrees of freedom. Model-based approaches could be taken to approach this problem by searching in a high dimensional hand state space, and matching projections of a hand model and image observations. However, it is highly inefficient due to the curse of dimensionality. Fortunately, natural hand articulation is highly constrained, which largely reduces the dimensionality of hand state space. This paper presents a model-based method to capture hand articulation by learning hand natural constraints. Our study shows that natural hand articulation lies in a lower dimensional configurations space characterized by a union of linear manifolds spanned by a set of basis configurations. By integrating hand motion constraints, an efficient articulated motion-capturing algorithm is proposed based on sequential Monte Carlo techniques. Our experiments show that this algorithm is robust and accurate for tracking natural hand movements. This algorithm is easy to extend to other articulated motion capturing tasks.
HCI
ambroszkiewicz99agent
Agent Virtual Organizations within the Framework of Network Computing: a case study We study the concept of agent virtual organization and show how it relates to the paradigm of Network Based Computing [28]. We also discuss the paradigm of BDI-agent trying to show that sophisticated architecture of BDI-agent can not be efficiently applied for large worlds. As the working example of virtual organization we consider a model of virtual enterprise.  Key words: agent virtual organization, agent-based manufacturing, virtual enterprise formation. 1 Introduction  Autonomous, adaptive and cooperative software mobile agents are well suited for domains that require constant adaptation to changing distributed environment or changing demands. Actually cyberspace and manufacturing enterprise are such domains so that there is increasing interest in applying agent technologies there. Cyberspace, in the shape of the Internet, intranets, and the World Wide Web, has grown phenomenally in recent years. Cyberspace now contains enormous amounts of information and is also being increasingly...
Agents
465022
Supporting Reuse by Delivering Task-Relevant and Personalized Information Technical, cognitive, and social factors inhibit the widespread success of systematic software reuse. Our research is primarily concerned with the cognitive and social challenges faced by software developers: how to motivate them to reuse and how to reduce the difficulty of locating components from a large reuse repository. Our research has explored a new interaction style between software developers and reuse repository systems enabled by information delivery mechanisms. Instead of passively waiting for software developers to explore the reuse repository with explicit queries, information delivery autonomously locates and presents components by using the developers' partially written programs as implicit queries.  We have designed, implemented, and evaluated a system called CodeBroker, which illustrates different techniques to address the essential challenges in information delivery: to make the delivered information relevant to the task-at-hand and personalized to the background knowledge of an individual developer. Empirical evaluations of CodeBroker show that information delivery is effective in promoting reuse.  Categories and Subject Descriptors  D.2.13 [Software Engineering]: Reusable Software -- reusable libraries, reuse models. D.2.2 [Software Engineering]:Design  Tools and Techniques -- computer-aided software engineering, software libraries, user interfaces. H.5.2 [Information Interfaces and Presentation]: User Interfaces -- interaction styles, usercentered design. I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence: intelligent agents.  General Terms  Design, Human Factors.  Keywords  Software reuse, information delivery, software agents, discourse models, user models, high-functionality applications  1. 
HCI
coppin01reality
Reality Browsing: Using Information Interaction and Robotic Autonomy for Planetary Exploration . Reality browsing is a framework that enables distributed control of a team of planetary robots. In it, prioritized  user queries are serviced in a hierarchical data structure consisting of an Internet-accessible world model, data archives  on the remote robots and finally a multiple-robot planner that coordinates query-directed searches. This paper introduces  the reality browser concept and outlines important research issues required for implementation.  
HCI
308750
Long-Term Learning from User Behavior in Content-Based Image Retrieval This article describes an algorithm for obtaining knowledge about the importance of features  from analyzing user log les of a content-based image retrieval system (CBIRS). The  user log les from the usage of the Viper web demonstration system are analyzed over a period  of four months. Within this period about 3500 accesses to the system were made with almost  800 multiple image queries. All the actions of the users were logged in a le.  The analysis only includes multiple image queries of the system with positive and/or negative  input images, because only multiple image queries contain enough information for the  method described. Features frequently present in images marked together positively in the  same query step get a higher weighting, whereas features present in one image marked positively  and another image marked negatively in the same step get a lower weighting. The  Viper system oers a very large number of simple features. This allows the creation of exible  feature ...
IR
roth98learning
Learning to Resolve Natural Language Ambiguities: A Unified Approach We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions, which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem. We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes. In particular, we present an extensive experimental ...
ML
428316
Haptic Perception of Virtual Roughness The texture of a virtual surface can both increase the sense of realism of an object as well as convey information about object identity, type, location, function, and so on. It is crucial therefore that interface designers know the range of textural information available through the haptic modality in virtual environments. The current study involves participants making roughness judgments on pairs of haptic textures experienced through a force-feedback device. The effect of texture frequency on roughness perception is analysed. The potential range and resolution of textural information available through force-feedback interaction are discussed.  Keywords  Haptics, force-feedback, texture perception.  INTRODUCTION  Despite the increasing prevalence of haptics in today's computing environments, the effective representation of haptic information is still a relatively new design problem for human computer interaction research. Force feedback interfaces in particular pose a variety of desi...
HCI
489180
Efficiently Querying Moving Objects with Pre-defined Paths in a Distributed Environment Due to the recent growth of the World Wide Web, numerous spatio-temporal applications can obtain their required information from publicly available web sources. We consider those sources maintaining moving objects with predefined paths and schedules, and investigate different plans to perform queries on the integration of these data sources efficiently. Examples of such data sources are networks of railroad paths and schedules for trains running between cities connected through these networks. A typical query on such data sources is to find all trains that pass through a given point on the network within a given time interval. We show that traditional filter+semi-join plans would not result in efficient query response times on distributed spatio-temporal sources. Hence, we propose a novel spatio-temporal filter, called deviation filter, that exploits both the spatial and temporal characteristics of the sources in order to improve the selectivity. We also report on our experiments in comparing the performances of the alternative query plans and conclude that the plan with spatio-temporal filter is the most viable and superior plan.
DB
may99how
How to Write F-Logic Programs in FLORID - A Tutorial for the Database Language F-Logic CONTENTS 2 Contents  1 Introduction 4 2 A First Example 4 3 Objects and their Properties 6 3.1 Object Names and Variable Names . . . . . . . . . . . . . . . . . . . . . . . . 6 3.1.1 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.1.2 Class Membership and Subclass Relationship . . . . . . . . . . . . . . 8 3.2 Expressing Information about an Object: F-Molecules . . . . . . . . . . . . . 8 3.3 Behavioral Inheritance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.4 Signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4 Nesting Object Properties 11 4.1 F-molecules without any properties . . . . . . . . . . . . . . . . . . . . . . . . 12 5 Predicate Symbols 12 6 Built-in Features 13 6.1 Equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 6.2 Integers, Comparisons and Arithmetics . . . . . . . . . . . .
DB
50337
Compositional Design and Maintenance of Broker Agents A generic broker agent architecture is introduced, designed in a principled manner using the compositional development method for multi-agent systems DESIRE. A flexible, easily adaptable agent architecture results in which, in addition, facilities have been integrated that provide automated support of the agents own maintenance. Therefore, the agent is not only easily adaptable, but it shows adaptive behaviour to meet new requirements, supported by communication with a maintenance agent. 1. Introduction  To support users on the World Wide Web, various types of agents can be, and actually have been, developed. For example, to support brokering processes in electronic commerce, personal assistant agents can be developed that support a user offering products (or services) at the Web, or agents that support search for information on products within a user's scope of interest, or agents that combine both functionalities. Moreover, mediating agents can be developed that communicate both with...
Agents
claypool99combining
Combining Content-Based and Collaborative Filters in an Online Newspaper  The explosive growth of mailing lists, Web sites and Usenet news demands effective filtering solutions. Collaborative filtering combines the informed opinions of humans to make personalized, accurate predictions. Content-based filtering uses the speed of computers to make complete, fast predictions. In this work, we present a new filtering approach that combines the coverage and speed of content-filters with the depth of collaborative filtering. We apply our research approach to an online newspaper, an as yet untapped opportunity for filters useful to the wide-spread news reading populace. We present the design of our filtering system and describe the results from preliminary experiments that suggest merits to our approach. 
IR
boll99emp
EMP - A Database-Driven Electronic Market Place for Business-to-Business Commerce on the Internet Electronic commerce systems for business-to-business commerce on the Internet are still in their infancy. The realization of Internet electronic markets for business-to-business following a n-suppliers : m-customers scenario is still unattainable with todays solutions. Comprehensive Internet electronic commerce systems should provide for easy access to and handling of the system, help to overcome di#erences in time of business, location, language between suppliers and customers, and at the same time should support the entire process of trading for business-to-business commerce. In this paper, we present a DBMS-based electronic commerce architecture and its prototypical implementation for business-to-business commerce according to a n-suppliers : mcustomers  scenario. Business transactions within the electronic market are realized by a set of modular market services. Multiple physically distributed markets can be interconnected transparently to the users and form one virtually central market place. The modeling and management of all market data in a DBMS gives the system a solid basis for reliable, consistent, and secure trading on the market. The generic and modular system architecture can be applied to arbitrary application domains. The system is scalable and can cope with an increasing number of single markets, participants, and market data due to the possibility to replicate and distribute services and data and herewith to distribute data, system, and network load.
DB
williamson00random
Random Probability Functions Random probability functions are required in Monte Carlo simulations for expert systems testing. Here I give an empirical method for generating such functions. While this is a practical problem, its solution raises interesting philosophical questions. The design of expert systems is an important problem in the field of Artificial Intelligence. One task, for instance, is to give a general methodology for diagnosis which can be used to construct computer experts in a whole range of areas such as the diagnosis of hepatitis and fault-finding in circuit boards. Clearly before these artificial experts can be used to make critical decisions they must be thoroughly tested and their reliability ascertained. Two approaches to testing stand out: empirical experiments and Monte Carlo simulations. Empirical testing is certainly desirable, but there may not be enough empirical data readily available. To ascertain the reliability of a particular diagnostic expert system one often requires a considerab...
AI
patterson02context
A Context for Assisted Cognition Assisted Cognition is introduced as an idea which leverages four technologies, ubiquitous computing, state reduction, plan recognition and decision theory, to develop solutions to problems faced by Alzheimer patients and their caregivers. A patient population with General Dementia Scale rating of less than 4 is targeted, and user interface methods from clinical studies are identified. A review of the literature on the component technologies is surveyed and a general architecture for Assisted Cognition is described.
HCI
singhal99document
Document Expansion for Speech Retrieval Advances in automatic speech recognition allow us to search large speech collections using traditional information retrieval  methods. The problem of "aboutness" for documents --- is a document about a certain concept --- has been at the core of document indexing for the entire history of IR. This problem is more difficult for speech indexing since automatic speech transcriptions often contain mistakes. In this study we show that document expansion can be successfully used to alleviate the effect of transcription mistakes on speech retrieval. The loss of retrieval effectiveness due to automatic transcription errors can be reduced by document expansion from 15--27% relative to retrieval from human transcriptions to only about 7--13%, even for automatic transcriptions with word error rates as high as 65%. For good automatic transcriptions (25% word error rate), retrieval effectiveness with document expansion is indistinguishable from retrieval from human transcriptions. This makes speech...
IR
knublauch00towards
Towards a Multi-Agent System for Pro-active Information Management in Anesthesia . Decision-making in anesthesia requires the integration of information  from various clinical data sources, such as the patient, the  laboratory, the surgery and the clinical personnel. However, information  management and processing is aggravated by the highly heterogeneous  and distributed nature of the current clinical data and information  repositories. Furthermore, critical situations such as the arrival of emergency  patients demand the context-sensitive provision of information in  a timely fashion. We argue that multi-agent architectures can help to  overcome these problems. In the clinical information space, autonomous  agents can pro-actively collect, integrate and analyze the required data  and condense and communicate the most relevant information. In this  paper, we presentanapproach for an anesthesia information management  system based on intelligent agents.  1 Introduction  The anesthesiological department is an essential service provider in modern clinical health care...
Agents
346876
DEBORA: Developing an Interface to Support Collaboration in a Digital Library . Interfaces to library systems have largely failed to represent the inherently  collaborative nature of information work. This paper describes how  collaborative functionality is being implemented as part of the DEBORA project  to provide access to digitised Renaissance documents. Work practices of users  of Renaissance documents are described and the collaborative features of the  client software are outlined. Functionalities discussed include annotation, the  creation of virtual books and the inclusion of user-supplied metadata.  1 Introduction  This paper describes the development of collaborative functionality for users of digital libraries in the context of the EU Telematics for Libraries project DEBORA (Digital Access to Books of the Renaissance).  The aim of the DEBORA project is to make Renaissance books more generally available as digital resources and to examine the potential for novel collaborative functionality. The collection being created within DEBORA consists of digiti...
IR
chamberlin00quilt
Quilt: An XML Query Language for Heterogeneous Data Sources The World Wide Web promises to transform human society by making virtually all types of information instantly available everywhere. Two prerequisites for this promise to be realized are a universal markup language and a universal query language. The power and flexibility of XML make it the leading candidate for a universal markup language. XML provides a way to label information from diverse data sources including structured and semi-structured documents, relational databases, and object repositories. Several XML-based query languages have been proposed, each oriented toward a specific category of information. Quilt is a new proposal that attempts to unify concepts from several of these query languages, resulting in a new language that exploits the full versatility of XML. The name Quilt suggests both the way in which features from several languages were assembled to make a new query language, and the way in which Quilt queries can combine information from diverse data sources into a query result with a new structure of its own.
DB
baader98matching
Matching in Description Logics: Preliminary Results Matching of concepts with variables (concept patterns) is a relatively new operation that has been introduced in the context of concept description languages (description logics), originally to help discard unimportant aspects of large concepts appearing in industrial-strength knowledge bases. This paper proposes a new approach to performing matching, based on a "concept-centered" normal form, rather than the more standard "structural subsumption" normal form for concepts. As a result, matching can be performed (in polynomial time) using  arbitrary concept patterns of the description language FL: , thus removing restrictions from previous work. The paper also addresses the question of matching problems with additional "side conditions", which were motivated by practical experience.  1 Introduction  The traditional inference problems for Description Logic (DL) systems (like subsumption) are now wellinvestigated. This means that algorithms are available for solving the subsumption proble...
AI
454621
Personalized Web-Document Filtering Using Reinforcement Learning Abstract- Document filtering is increasingly deployed in Web environments to reduce information overload of users. We formulate online information filtering as a reinforcement learning problem, i.e. TD(0). The goal is to learn user profiles that best represent his information needs and thus maximize the expected value of user relevance feedback. A method is then presented that acquires reinforcement signals automatically by estimating user’s implicit feedback from direct observations of browsing behaviors. This “learning by observation ” approach is contrasted with conventional relevance feedback methods which require explicit user feedbacks. Field tests have been performed which involved 10 users reading a total of 18,750 HTML documents during 45 days. Compared to the existing document filtering techniques, the proposed learning method showed superior performance in information quality and adaptation speed to user preferences in online filtering. 1
IR
ljungstrand99wearboy
The WearBoy: A Platform for Low-cost Public Wearable Devices We introduce the WearBoy -- a wearable, modified Nintendo GameBoy -- as a platform for exploring public wearable devices. We have minimized a Color GameBoy to enable users to comfortably wear it, making the device not much larger than the actual screen. Technical properties of the WearBoy are discussed, along with two applications using the platform. 1. Introduction Currently, many wearable computing prototypes are rather clumsy and heavy to wear, and often rely on several different electronic devices connected together by cables hidden in the user's clothing. This might be necessary for computationally demanding applications, but in many cases the application does not need much computational power, especially not if wireless access to more powerful resources is available. Several such low-end wearable platforms have been built and tested, e.g. the Thinking Tags [1]. These prototypes are usually custom designed around a small microcontroller with some additional features, but commonl...
HCI
162298
Machine Learning for Intelligent Systems Recent research in machine learning has focused on supervised induction for simple classification and reinforcement learning for simple reactive behaviors. In the process, the field has become disconnected from AI's original goal of creating complete intelligent agents. In this paper, I review recent work on machine learning for planning, language, vision, and other topics that runs counter to this trend and thus holds interest for the broader AI research community. I also suggest some steps to encourage further research along these lines. Introduction  A central goal of artificial intelligence has long been to construct a complete intelligent agent that can perceive its environment, generate plans, execute those plans, and communicate with other agents. The pursuit of this dream naturally led many researchers to focus on the component tasks of perception, planning, control, and natural language, or on generic issues that cut across these tasks, such as representation and search. Over ...
ML
452192
The Role of Information Extraction for Textual CBR Abstract. The benefits of CBR methods in domains where cases are text depend on the underlying text representation. Today, most TCBR approaches are limited to the degree that they are based on efficient, but weak IR methods. These do not allow for reasoning about the similarities between cases, which is mandatory for many CBR tasks beyond text retrieval, including adaptation or argumentation. In order to carry out more advanced CBR that compares complex cases in terms of abstract indexes, NLP methods are required to derive a better case representation. This paper discusses how state-of-the-art NLP/IE methods might be used for automatically extracting relevant factual information, preserving information captured in text structure and ascertaining negation. It also presents our ongoing research on automatically deriving abstract indexing concepts from legal case texts. We report progress toward integrating IE techniques and ML for generalizing from case texts to our CBR case representation. 1
IR
roth00using
Using Handheld Devices in Synchronous Collaborative Scenarios . In this paper we present a platform specially designed for groupware  applications running on handheld devices. Common groupware platforms  request desktop computers as underlying hardware platforms. The fundamental  different nature of handheld devices has a great impact on the platform, e.g.  resource limitations have to be considered, the network is slow and unstable.  Often, personal data are stored on handheld devices, thus mechanisms have to  ensure privacy. These considerations lead to the QuickStep platform. Sample  applications developed with QuickStep demonstrate the strengths of the  QuickStep environment.  1 Introduction  Collaborative applications help a group to, e.g., collaboratively create documents, write agendas or schedule appointments. A common taxonomy [3] classifies collaborative applications by time and space, with 'same place' and 'different places' attributes on the space axis and 'same time' (synchronous) and 'different time' (asynchronous) ones on the time ...
HCI
kemme98suite
A Suite of Database Replication Protocols based on Group Communication Primitives This paper proposes a family of replication protocols based on group communication in order to address some of the concerns expressed by database designers regarding existing replication solutions. Due to these concerns, current database systems allow inconsistencies and often resort to centralized approaches, thereby reducing some of the key advantages provided by replication. The protocols presented in this paper take advantage of the semantics of group communication and use relaxed isolation guarantees to eliminate the possibility of deadlocks, reduce the message overhead, and increase performance. A simulation study shows the feasibility of the approach and the flexibility with which different types of bottlenecks can be circumvented.  1 Introduction  Replication is often seen as a mechanism to increase availability and performance in distributed databases. Most of the work done in this area, which we will refer to as traditional replication protocols, is on synchronous  and update...
DB
9546
Learning to Extract Symbolic Knowledge from the World Wide Web The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more e ective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach istodevelop a trainable information extraction system that takes two inputs. The rst is an ontology that de nes the classes (e.g., Company, Person, Employee, Product) and relations (e.g., Employed.By, Produced.By) ofinterest when creating the knowledge base. The second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system that has created a knowledge base describing university people, courses, and research projects.
ML
332626
Scalable Replication in Database Clusters The widespread use of clusters and web farms has increased the importance of data replication. In existing protocols, typical distributed system solutions emphasize fault tolerance at the price of performance while database solutions emphasize performance at the price of consistency. In this paper, we explore the use of data replication in a cluster configuration with the objective of providing both fault tolerance and good performance without compromising consistency. We do this by combining transactional concurrency control with group communication primitives. In our approach, transactions are executed at only one site so that not all nodes incur in the overhead of parsing, optimizing, and producing results. To further reduce latency, we use an optimistic multicast approach that overlaps transaction execution with the total order message delivery. The techniques we present in the paper provide correct executions while minimizing overhead and providing higher scalability.
DB
298782
Efficient and Effective Metasearch for Text Databases Incorporating Linkages among Documents Linkages among documents have a significant impact on the importance of documents, as it can be argued that important documents are pointed to by many documents or by other important documents. Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). In a large-scale metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against the set of representatives of all databases in order to determine the appropriate databases to search. In previous works, the linkage information between documents has not been utilized in determining the appropriate databases to search. In this paper, such information is employed to determine the degree of relevance of a document with respect to a given query. This information is then stored in each database representative to facilitate the selection of databases for each given query. We establish a necessary an...
IR
panzarasa01social
Social Mental Shaping: Modelling the Impact of Sociality on the Mental States of Autonomous Agents This paper presents a framework that captures how the social nature of agents that are situated in a multi-agent environment impacts upon their individual mental states. Roles and social relationships provide an abstraction upon which we develop the notion of social mental shaping. This allows us to extend the standard Belief-DesireIntention model to account for how common social phenomena (e.g. cooperation, collaborative problem-solving and negotiation) can be integrated into a unified theoretical perspective that reflects a fully explicated model of the autonomous agent's mental state.  Keywords: Multi-agent systems, agent interactions, BDI models, social influence.  3  1. 
Agents
323158
Towards Agent-Oriented Information Systems This paper reviews several relevant agent concepts developed in the intelligent agents and multiagent systems area, and makes suggestions how to extend current IS technology on the basis of these concepts. Since it presents work in progress, the 3
Agents
basu01evaluating
Evaluating the Novelty of Text-Mined Rules Using Lexical Knowledge A data-mining system may discover a large body of rules; however, relatively few of these may convey useful new knowledge to the user. Several metrics for evaluating the "interestingness" of mined rules have been proposed. However, most of these measure simplicity (e.g. rule size), certainty (e.g. confidence), or utility (e.g. support). Another important aspect of interestingness is novelty: does the rule represent an association that is currently unknown. In this paper, we present a new method of estimating the novelty of rules discovered by data-mining methods using WordNet, a lexical knowledge-base of English words. We have shown that novelty of a rule can be assessed by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule - the more the average distance, more is the novelty of the rule. We present an experimental evaluation of this novelty metric by applying it to rules mined from book descriptions extracted from A...
IR
shneiderman99creating
Creating Creativity for Everyone: User Interfaces for Supporting Innovation :  A challenge for human-computer interaction researchers and user interface designers is to construct information technologies that support creativity. This ambitious goal can be attained by building on an adequate understanding of creative processes. This paper offers the four-phase genex framework for generating excellence: - Collect: learn from previous works stored in digital libraries - Relate: consult with peers and mentors at early, middle and late stages - Create: explore, compose, and evaluate possible solutions - Donate: disseminate the results and contribute to the digital libraries Within this integrated framework, this paper proposes eight activities that require humancomputer interaction research and advanced user interface design. A scenario about an architect illustrates the process of creative work within a genex environment.  1. 
HCI
433736
Machine Learning for Modeling Dutch Pronunciation Variation This paper describes the use of rule induction techniques for the automatic extraction of phonemic knowledge and rules from pairs of pronunciation lexica. This extracted knowledge allows the adaptation of speech processing systems to regional variants of a language. As a case study, we apply the approach to Northern Dutch and Flemish (the variant of Dutch spoken in Flanders, a part of Belgium) , based on Celex and Fonilex, pronunciation lexica for Northern Dutch and Flemish, respectively. In our study, we compare two rule induction techniques, TransformationBased Error-Driven Learning (TBEDL) (Brill, 1995) and C5.0 (Quinlan, 1993), and evaluate the extracted knowledge quantitatively (accuracy) and qualitatively (linguistic relevance of the rules). We conclude that, whereas classication-based rule induction with C5.0 is more accurate, the transformation rules learned with TBEDL can be more easily interpreted.  1 Introduction  A central component of speech processing systems is a pronun...
ML
parsons98argumentation
Argumentation and Multi-Agent Decision Making This paper summarises our on-going work on mixedinitiative decision making which extends both classical decision theory and a symbolic theory of decision making based on argumentation to a multi-agent domain. Introduction  One focus of our work at Queen Mary and Westfield College is the development of multi-agent systems which deal with real world problems, an example being the diagnosis of faults in electricity distribution networks (Jennings et al. 1996). These systems are mixed-initiative in the sense that they depend upon interactions between agents---no single agent has sufficient skills or resources to carry out the tasks which the multi-agent system as a whole is faced with. Because the systems are built to operate in the real world, the agents are forced to deal with the usual problems of incomplete and uncertain information, and increasingly we are turning to the use of techniques from decision theory, both classical and non-standard, in order to ensure that our agents make so...
Agents
507172
Experiments in Meta-Level Learning with ILP When considering new datasets for analysis with machine  learning algorithms, we encounter the problem of choosing the algorithm  which is best suited for the task at hand. The aim of meta-level learning  is to relate the performance of different machine learning algorithms to  the characteristics of the dataset. The relation is induced on the basis  of empirical data about the performance of machine learning algorithms  on the different datasets.
ML
smyth99surfing
Surfing the Digital Wave - Generating Personalised TV Listings using Collaborative, Case-Based Recommendation Abstract. In the future digital TV will offer an unprecedented level of programme choice. We are told that this will lead to dramatic increases in viewer satisfaction as all viewing tastes are catered for all of the time. However, the reality may be somewhat different. We have not yet developed the tools to deal with this increased level of choice (for example, conventional TV guides will be virtually useless), and viewers will face a significant and frustrating information overload problem. This paper describes a solution in the form of the PTV system. PTV employs user profiling and information filtering techniques to generate web-based TV viewing guides that are personalised for the viewing preferences of individual users. The paper explains how PTV constructs graded user profiles to drive a hybrid recommendation technique, combining case-based and collaborative information filtering methods. The results of an extensive empirical study to evaluate the quality of PTV’s casebased and collaborative filtering strategies are also described. 1
IR
fensel00workshop
Workshop on Intelligent Information Integration (III'99)  
IR
wolski00design
Design of RapidBase - an Active Measurement Database System In data-intensive industrial on-line applications utilizing live process data, one faces an unusual set of database requirements. The process measurement data need to be acquired at great speed, organized in time series and made available for time-based retrieval. Active capabilities and functional extensibility are needed to implement a flexible data-driven processing paradigm. An efficient transaction logging and recovery mechanism is needed in order not to impede the data acquisition flow. RapidBase is a system that meets these requirements. It utilizes a main-memory database, a unique temporal-relational data model for handling time series, and an elaborate trigger subsystem. It is implemented as a server program equipped with interfaces of high power of expression.  1 Introduction  Although the requirement for data management is omnipresent in various advanced applications, the main-stream notion of a database may be not a best choice in all cases. Certain application classes requ...
DB
pan00prosody
Prosody Modeling in Concept-to-Speech Generation: Methodological Issues Generation of Intensive Care data), a system that generates multimedia briefings of a patient's status after having a bypass operation (Dalal et al. 1996; McKeown et al. 1997). We first describe information MAGIC generates in the process of producing language, turning next to the corpora we collected. We then provide a description of the more traditional approach to prosody modeling, using machine learning that generalizes over many examples, followed by a description of our memory-based approach. Our results show the memory-based approach yields a better improvement in quality, measured through subjective judgments of output.  2. Information from Language Generation  In the course of producing language, language generators typically produce a variety of intermediate linguistic representations that contain information which potentially could influence prosody. Some of this information is similar to the kind of information used in TTS, such as part-of-speech (POS) tags or syntactic cons...
ML
eiter98firstorder
First-Order Representation of Stable Models Turi (1991) introduced the important notion of a constrained atom: an atom with associated equality and disequality constraints on its arguments. A set of constrained atoms is a constrained interpretation. We investigate how non-ground representations of both the stable model semantics and the well-founded semantics may be obtained through Turi’s approach. The practical implication of this is that the wellfounded model (or the set of stable models) may be partially pre-computed at compile-time, resulting in the association of each predicate symbol in the program to a constrained atom. Algorithms to create such models are presented, both for the well founded case, and the case of stable models. Query processing reduces to checking whether each atom in the query is true in a stable model (resp. well-founded model). This amounts to showing the atom is an instance of one of some constrained atom whose associated constraint is solvable. Various related complexity results are explored, and the impacts of these results are discussed from the point of view of implementing systems that incorporate the stable and well-founded semantics.
AI
rosales99trajectory
Trajectory Guided Tracking and Recognition of Actions A combined 2D, 3D approach is presented that allows for robust tracking of moving people and  recognition of actions. It is assumed that the system observes multiple moving objects via a single,  uncalibrated video camera. Low-level features are often insufficient for detection, segmentation, and  tracking of non-rigid moving objects. Therefore, an improved mechanism is proposed that integrates  low-level (image processing), mid-level (recursive 3D trajectory estimation), and high-level (action  recognition) processes. A novel extended Kalman filter formulation is used in estimating the relative  3D motion trajectories up to a scale factor. The recursive estimation process provides a prediction  and error measure that is exploited in higher-level stages of action recognition. Conversely, higherlevel  mechanisms provide feedback that allows the system to reliably segment and maintain the  tracking of moving objects before, during, and after occlusion. The 3D trajectory, occlusion, and  segmentation information are utilized in extracting stabilized views of the moving object that are then  used as input to action recognition modules. Trajectory-guided recognition (TGR) is proposed as a  new and efficient method for adaptive classification of action. The TGR approach is demonstrated  using "motion history images" that are then recognized via a mixture-of-Gaussians classifier. The  system was tested in recognizing various dynamic human outdoor activities: running, walking, roller  blading, and cycling. Experiments with real and synthetic data sets are used to evaluate stability of  the trajectory estimator with respect to noise.
AI
440239
An Experimental Performance Evaluation of Incremental Materialized View Maintenance in Object Databases Abstract. The development of techniques for supporting incremental maintenance of materialized views has been an active research area for over twenty years. However, although there has been much research on methods and algorithms, there are surprisingly few systematic studies on the performance of different approaches. As a result, understanding of the circumstances in which materialized views are beneficial (or not) can be seen to lag behind research on incremental maintenance techniques. This paper presents the results of an experimental performance analysis carried out in a system that incrementally maintains OQL views in an ODMG compliant object database. The results indicate how the effectiveness of incremental maintenance is affected by issues such as database size, and the complexity and selectivity of views. 1
DB
grandi00generalized
A Generalized Modeling Framework for Schema Versioning Support Advanced object-oriented applications require the management of schema versions, in order to cope with changes in the structure of the stored data. Two types of versioning have been separately considered so far: branching and temporal. The former arose in application domains like CAD/CAM and software engineering, where different solutions have been proposed to support design schema versions (consolidated versions). The latter concerns temporal databases, where some works considered temporal schema versioning to fulfil advanced needs of other typical objectoriented applications like GIS and the multimedia ones.  In this work, we propose a general model which integrates the two approaches by supporting both design and temporal schema versions. The model is provided with a complete set of schema change primitives for full-fledged version manipulation whose semantics is described in the paper.  Keywords: Schema versioning, Schema evolution, OODBMS, Temporal databases  1 Introduction  In th...
DB
erdem98task
Task Oriented Software Understanding The main factors that affect software understanding are the complexity of the problem solved by the program, the program text, the user's mental ability and experience and the task being performed. This paper describes a planning approach solution to the software understanding problem that focuses on the user's task and expertise. First, user questions about software artifacts have been studied and the most commonly asked questions are identified. These questions are organized into a question model and procedures for answering them are developed. Then, the patterns in user questions while performing certain tasks have been studied and these patterns are used to build generic task models. The explanation system uses these task models in several ways. The task model, along with a user model, is used to generate explanations tailored to the user's task and expertise. In addition, the task model allows the system to provide explicit task support in its interface.  Keywords  software explan...
AI
423028
Lock-free Scheduling of Logical Processes in Parallel Simulation With fixed lookahead information in a simulation model, the overhead of asynchronous conservative parallel simulation lies in the mechanism used for propagating time updates in order for logical processes to safely advance their local simulation clocks. Studies have shown that a good scheduling algorithm should preferentially schedule processes containing events on the critical path. This paper introduces a lock-free algorithm for scheduling logical processes in conservative parallel discrete-event simulation on shared-memory multiprocessor machines. The algorithm uses fetch&add operations that help avoid inefficiencies associated with using locks. The lock-free algorithm is robust. Experiments show that, compared with the scheduling algorithm using locks, the lock-free algorithm exhibits better performance when the number of logical processes assigned to each processor is small or when the workload becomes significant. In models with large number of logical processes, our algorithm sh...
HCI
clark00finding
Finding Text Regions Using Localised Measures We present a method based on statistical properties of local image neighbourhoods for the location of text in real-scene images. This has applications in robot vision, and desktop and wearable computing. The statistical measures we describe extract properties of the image which characterise text, invariant to a large degree to the orientation, scale or colour of the text in the scene. The measures are employed by a neural network to classify regions of an image as text or non-text. We thus avoid the use of different thresholds for the various situations we expect, including when text is too small to read, or when the text plane is not fronto-parallel to the camera. We briefly discuss applications and the possibility of recovery of the text for optical character recognition. 1 Introduction Automatic location and digitisation of text in arbitrary scenes, where the text may or may not be fronto-parallel to the viewing plane, is an area of computer vision which has not yet been ...
HCI
tzouramanis01time
Time Split Linear Quadtree For Indexing Image Databases The Time Split B-Tree (TSBT) is modified for indexing a database of evolving binary images. This is accomplished by embedding ideas from Linear region Quadtrees that make the TSBT able to support spatio-temporal query processing. To improve query performance, additional pointers are added to the leaf-nodes of the TSBT. The resulting access method is called Time Split Linear Quadtree (TSLQ). Algorithms for processing five spatio-temporal queries have been adapted to the new structure. Such queries appear in Multimedia Systems, or Geographical Information Systems (GIS), when searched by content. The TSLQ was implemented and results of extensive experiments on query time performance are presented, indicating that the proposed algorithmic approaches outbalance respective straightforward algorithms. The region data sets used in the experiments were real images of meteorological satellite views and synthetic raster images.
DB
89879
Logic-Based Subsumption Architecture In this paper we describe a logic-based AI architecture based on Brooks'  Subsumption Architecture. We axiomatize each of the layers of control in  his system separately and use independent theorem provers to derive each  layer's output actions given its inputs. We implement the subsumption  of lower layers by higher layers using circumscription. We give formal  semantics to our approach.  1 Introduction  In [?], Brooks proposed a reactive architecture embodying an approach to robot control different on various counts from traditional approaches. He decomposed the problem into layers corresponding to levels of behavior, rather than according to a sequential, functional form. Within this setting he introduced the idea of subsumption, that is, that more complex layers could not only depend on lower, more reactive layers, but could also influence their behavior. The resulting architecture was one that could service simultaneously multiple, potentially conflicting goals in a reactive fashi...
AI
jamil00gql
GQL: A Reasonable Complex SQL for Genomic Databases Validating hypotheses and reasoning about objects is becoming commonplace in biotechnology research. The capability to reason strengthens comparative genomics research by providing the much needed tool to pose intelligent queries in a more convenient and declarative fashion. To be able to reason using Genomic Query Language (GQL), we propose the idea of parameterized views as an extension of SQL's create view construct with an optional with parameter clause. Parameterizing enables traditional SQL views to accept input values and delay the computation of the view until invoked with a call statement. This extension empowers users with the capability of modifying the behavior of predened procedures (views) by sending arguments and evaluating the procedure on demand. We demonstrate that the extension is soundly based, with a parallel in Datalog. We also show that the idea of relational unication proposed here empowers SQL to reason and infer in exactly the same way as an Object-Oriented ...
DB
531878
Experiences Developing a Thin-Client, Multi-Device Travel Planning Application Many applications now require access from diverse humancomputer interaction devices, such as desktop computers, web browsers, PDAs, mobile phones, pagers and so on. We describe our experiences developing a multi-device travel planning application built from reusable components, many of these developed from several different previous projects. We focus on key user interface design and component adaptation and integration issues as encountered in this problem domain. We report on the results of a useability evaluation of our prototype and our current research directions addressing HCI and interface development problems we encountered.
HCI
62580
MindReader: Querying databases through multiple examples Users often can not easily express their queries. For example, in a multimedia/image by content setting, the user might want photographs with sunsets; in current systems, like QBIC, the user has to give a sample query, and to specify the relative importance of color, shape and texture. Even worse, the user might want correlations between attributes, like, for example, in a traditional, medical record database, a medical researcher might want to find "mildly overweight patients", where the implied query would be "weight/height &asymp; 4 lb/inch". Our goal is to provide a user-friendly, but theoretically solid method, to handle such queries. We allow the user to give several examples, and, optionally, their 'goodness' scores, and we propose a novel method to "guess" which attributes are important, which correlations are important, and with what weight. Our contributions are twofold: (a) we formalize the problem as a minimization problem and show how to solve for the optimal solution, completely av...
DB
huget02application
An Application of Agent UML to Supply Chain Management Agent UML is certainly the most well-known graphical modeling language for describing multiagent systems but until now, it is not applied to real-world applications. The aim of our project is to apply Agent UML to the Supply Chain Management. This project has several objectives: (1) it allows to prove that Agent UML can be applied to real-world applications, (2) it allows to discover what is missing in Agent UML and what is wrong and finally, (3) it allows to define a methodology based on Agent UML and several tools. The aim of this paper is to sum up our first results on appying Agent UML to the Supply Chain Management and especially, the paper sketches what diagrams are interesting and what could be done after in our project.
Agents
ferguson95role
On the Role of BDI Modelling for Integrated Control and Coordinated Behavior in Autonomous Agents This paper describes an architecture for controlling and coordinating autonomous agents, building on previous work addressing reactive and deliberative control methods. The proposed multi-layered hybrid architecture allows a rationally bounded, goal-directed agent to reason predictively about potential conflicts by constructing knowledge level models which explain other agents' observed behaviors and hypothesize their beliefs, desires, and intentions; at the same time it enables the agent to operate autonomously, to react promptly to changes in its real-time environment, and to coordinate its actions effectively with other agents. A principal aim of this research is to understand the role different functional capabilities play in constraining an agent's behavior under varying environmental conditions. To this end, an experimental testbed has been constructed comprising a simulated multi-agent world in which a variety of agent configurations and behaviors have been investigated. A numbe...
Agents
461165
Event-Driven FRP Abstract. Functional Reactive Programming (FRP) is a high-level declarative language for programming reactive systems. Previous work on FRP has demonstrated its utility in a wide range of application domains, including animation, graphical user interfaces, and robotics. FRP has an elegant continuous-time denotational semantics. However, it guarantees no bounds on execution time or space, thus making it unsuitable for many embedded real-time applications. To alleviate this problem, we recently developed Real-Time FRP (RT-FRP), whose operational semantics permits us to formally guarantee bounds on both execution time and space. In this paper we present a formally verifiable compilation strategy from a new language based on RT-FRP into imperative code. The new language, called Event-Driven FRP (E-FRP), is more tuned to the paradigm of having multiple external events. While it is smaller than RT-FRP, it features a key construct that allows us to compile the language into efficient code. We have used this language and its compiler to generate code for a small robot controller that runs on a PIC16C66 micro-controller. Because the formal specification of compilation was crafted more for clarity and for technical convenience, we describe an implementation that produces more efficient code. 1
HCI
goecks99automatically
Automatically Labeling Web Pages Based on Normal User Actions For agents attempting to learn a user's interests, the cost of obtaining labeled training instances is prohibitive because the user must directly label each training instance, and few users are willing to do so. We present an approach that circumvents the need for human-labeled pages. Instead, we learn `surrogate' tasks where the desired output is easily measured, such as the number of hyperlinks clicked on a page or the amount of scrolling performed. Our assumption is that these outputs will highly correlate with the user's interests. In other words, by unobtrusively `observing' the user's behavior we are able to learn functions of value. For example, an agent could silently observe the user's browser behavior during the day, then use these training examples to learn such functions and gather, during the middle of the night, pages that are likely to be of interest to the user. Previous work has focused on learning a user profile by passively observing the hyperlinks clicked on and tho...
IR
446625
Automated Derivation of Complex Agent Architectures from Analysis Specifications Multiagent systems have been touted as a way to meet the need for distributed software systems that must operate in dynamic and complex environments. However, in order for multiagent systems to be effective, they must be reliable and robust. Engineering multiagent systems is a non-trivial task, providing ample opportunity for even experts to make mistakes. Formal transformation systems can provide automated support for synthesizing multiagent systems, which can greatly improve their correctness and reliability. This paper describes a semi-automated transformation system that generates an agents internal  architecture from the analysis specification for the MaSE methodology.  1. 
Agents
3733
A Hierarchical Probabilistic Model for Novelty Detection in Text Topic Detection and Tracking (TDT) is a variant of classification in which the classes are not known or fixed in advance. Consider for example an incoming stream of news articles or email messages that are to be classified by topic; new classes must be created as new topics arise. The problem is a challenging one for machine learning. Instances of new topics must be recognized as not belonging to any of the existing classes (detection), and instances of old topics must be correctly classified (tracking)---often with extremely little training data per class. This paper proposes a new approach to TDT based on probabilistic, generative models. Strong statistical techniques are used to address the many challenges: hierarchical shrinkage for sparse data, statistical "garbage collection" for new event detection, clustering in time to separate the different events of a common topic, and deterministic annealing for creating the hierarchy. Preliminary experimental results show promise.  Keyword...
ML
jennings00implementing
Implementing A Business Process Management System Using Adept: A Real-World Case Study : This paper describes how ADEPT's agent-based design and implementation philosophy was used to prototype a business process management system for a real-world application. The application illustrated is based on the British Telecom (BT) business process of providing a quote to a customer for installing a network to deliver a specified type of telecommunications service. Particular emphasis is placed upon the techniques developed for specifying services, for allowing agents with heterogeneous information models to interoperate, for allowing rich and flexible inter-agent negotiation to occur, and on the issues related to interfacing agent-based systems and humans. This paper builds upon the companion paper that provides details of the rationale and design of the ADEPT technology deployed in this application.  1 INTRODUCTION  Many advances have been made in recent years within organisations in preparing a culture of dynamic improvement. From the globalisation of trade, Total Quality Mana...
Agents
467320
A Hybrid Approach to the Profile Creation and Intrusion Detection Anomaly detection involves characterizing the behaviors of individuals or systems and recognizing behavior that is outside the norm. This paper describes some preliminary results concerning the robustness and generalization capabilities of machine learning methods in creating user profiles based on the selection and subsequent classification of command line arguments. We base our method on the belief that legitimate users can be classified into categories based on the percentage of commands they use in a specified period. The hybrid approach we employ begins with the application of expert rules to reduce the dimensionality of the data, followed by an initial clustering of the data and subsequent refinement of the cluster locations using a competitive network called Learning Vector Quantization. Since Learning Vector Quantization is a nearest neighbor classifier, and new record presented to the network that lies outside a specified distance is classified as a masquerader. Thus, this system does not require anomalous records to be included in the training set. 1.
ML
choi01probabilistic
A Probabilistic Approach to Planning Biped Locomotion with Prescribed Motions Typical high-level directives for locomotion of human-like characters are encountered  frequently in animation scripts or interactive systems. In this paper, we  present a new scheme for planning natural-looking locomotion of a biped figure to  facilitate rapid motion prototyping and task-level motion generation. Given start  and goal positions in a virtual environment, our scheme gives a sequence of motions  to move from the start to the goal using a set of live-captured motion clips.
AI
liskov99providing
Providing Persistent Objects in Distributed Systems Abstract. THOR is a persistent object store that provides a powerful programming model. THOR ensures that persistent objects are accessed only by calling their methods and it supports atomic transactions. The result is a system that allows applications to share objects safely across both space and time. The paper describes how the THOR implementation is able to support this powerful model and yet achieve good performance, even in a wide-area, large-scale distributed environment. It describes the techniques used in THOR to meet the challenge of providing good performance in spite of the need to manage very large numbers of very small objects. In addition, the paper puts the performance of THOR in perspective by showing that it substantially outperforms a system based on memory mapped files, even though that system provides much less functionality than THOR. 1
DB
herbrich99support
Support Vector Learning for Ordinal Regression We investigate the problem of predicting variables of ordinal scale. This task is referred to as ordinal regression  and is complementary to the standard machine learning tasks of classification and metric regression. In contrast to statistical models we present a distribution independent formulation of the problem together with uniform bounds of the risk functional. The approach presented is based on a mapping from objects to scalar utility values. Similar to Support Vector methods we derive a new learning algorithm for the task of ordinal regression based on large margin rank boundaries. We give experimental results for an information retrieval task: learning the order of documents w.r.t. an initial query. Experimental results indicate that the presented algorithm outperforms more naive approaches to ordinal regression such as Support Vector classification and Support Vector regression in the case of more than two ranks. 1 Introduction  Problems of ordinal regression arise in many fi...
IR
340329
Multimodal System Processing in Mobile Environments One major goal of multimodal system design is to support more robust performance than can be achieved with a unimodal recognition technology, such as a spoken language system. In recent years, the multimodal literatures on speech and pen input and speech and lip movements have begun developing relevant performance criteria and demonstrating a reliability advantage for multimodal architectures. In the present studies, over 2,600 utterances processed by a multimodal pen/voice system were collected during both mobile and stationary use. A new data collection infrastructure was developed, including instrumentation worn by the user while roaming, a researcher field station, and a multimodal data logger and analysis tool tailored for mobile research. Although speech recognition as a stand-alone failed more often during mobile system use, the results confirmed that a more stable multimodal architecture decreased this error rate by 19-35%. Furthermore, these findings were replicated across different types of microphone technology. In large part this performance gain was due to significant levels of mutual disambiguation in the multimodal architecture, with higher levels occurring in the noisy mobile environment. Implications of these findings are discussed for expanding computing to support more challenging usage contexts in a robust manner.
HCI
dix00impact
IMPACT: A Platform for Heterogenous Agents this report as well as the allocated flight route of the plane. 3.1 Action Base 131  Chapter 3: Actions and Agent Programs Multi Agent Systems, Ushuaia (Oct. 2000) Example 3.3 (STORE Example Revisited)
DB
drori01algorithm
Algorithm for Documents Ranking (DRMR) - Preliminary Results In the framework of a study, which investigated implementation of a model for displaying search results, the possibility of ranking documents that appear in a list of search results was examined. The purpose of this paper is to present the concept of using mutual references between documents as a tool for ranking documents, and to present the findings of a study that investigated the applicability of the concept.  Keywords  Documents ranking, Mutual references, Displaying search results list, Text retrieval systems.  1. 
IR
stockinger01towards
Towards a Cost Model for Distributed and Replicated Data Stores Large, Petabyte-scale data stores need detailed design considerations about distributing and replicating particular parts of the data store in a cost-effective way. Technical issues need to be analysed and, based on these constraints, an optimisation problem can be formulated. In this paper we provide a novel cost model for building a world-wide distributed Petabyte data store which will be in place starting from 2005 at CERN and its collaborating, world-wide distributed institutes. We will elaborate on a framework for assessing potential system costs and influences which are essential for the design of the data store.  1 Introduction  With the growth of the Internet in the last couple of years and expanding technologies in database research, data warehousing, networking and data storage, large distributed data stores with data amounts in the range of Petabytes are emerging [16]. Not only the choice of the optimal data storage system (relational or object-oriented databases, flat files...
DB
schmidt01how
How to Build Smart Appliances In this article smart appliances are characterized as devices that are attentive to their environment. We introduce a terminology for situation, sensor data, context, and context-aware applications because it is important to gain a thorough understanding of these concepts to successfully build such artifacts. In the article the relation between a real-world situation and the data read by sensors is discussed; furthermore, an analysis of available sensing technology is given. Then we introduce an architecture that supports the transformation from sensor data to cues then to contexts as a foundation to make context-aware applications. The article suggests a method to build context-aware devices; the method starts from situation analysis, offers a structured way for selection of sensors, and finally suggests steps to determine recognition and abstraction methods. In the final part of the article the question of how this influences the applications is raised and the areas of user int...
HCI
takahashi98mobile
Mobile Info Search: Information Integration for Location-Aware Computing this paper we introduce a research project entitled Mobile Info Search (MIS), and describe  its goal, architecture, implementation, and experimental results.  The goal of MIS is to collect, structure, and integrate distributed and diverse local information  from the Internet in a practicable form and make it available through a simple interface  to mobile users in various situations or contexts. We do this in a location-oriented way.  The experimental MIS application features a "location-oriented meta search" for Web  database servers, a "location-oriented robot-based search" called kokono Search for distributed  Web documents, and a simple interface based on the latitude and longitude of the user's location,  which are obtained from the user's PHS or GPS.  Analysis of a trial service shows that multiple services, such as maps and textual information,  are often requested together for one location within a 
IR
56231
Multistrategy Learning for Information Extraction Information extraction (IE) is the problem of filling out pre-defined structured summaries from text documents. We are interested in performing IE in non-traditional domains, where much of the text is often ungrammatical, such as electronic bulletin board posts and Web pages. We suggest that the best approach is one that takes into account many different kinds of information, and argue for the suitability of a multistrategy approach. We describe learners for IE drawn from three separate machine learning paradigms: rote memorization, term-space text classification, and relational rule induction. By building regression models mapping from learner confidence to probability of correctness and combining probabilities appropriately, it is possible to improve extraction accuracy over that achieved by any individual learner. We describe three different multistrategy approaches. Experiments on two IE domains, a collection of electronic seminar announcements from a university computer science de...
ML
vianu01web
A Web Odyssey: from Codd to XML INTRODUCTION  The Web presents the database area with vast opportunities and commensurate challenges. Databases and the Web are organically connected at many levels. Web sites are increasingly powered by databases. Collections of linked Web pages distributed across the Internet are themselves tempting targets for a database. The emergence of XML as the lingua franca of the Web brings some much needed order and will greatly facilitate the use of database techniques to manage Web information.  This paper will discuss some of the developments related to the Web from the viewpoint of database theory. As we shall see, the Web scenario requires revisiting some of the basic assumptions of the area. To be sure, database theory remains as valid as ever in the classical setting, and the database industry will continue to representamulti-billion dollar target of applicability for the foreseeable future. But the Web represents an opportunityofanentirely di#erent scale. We are th
IR
malis98positioning
Positioning a coarse-calibrated camera with respect to an unknown object by 2D 1/2 visual servoing In this paper we propose a new vision-based robot control approach halfway between the classical positionbased and image-based visual servoings. It allows to avoid their respective disadvantages. The homography between some planar feature points extracted from two images (corresponding to the current and desired camera poses) is computed at each iteration. Then, an approximate partial-pose, where the translational term is known only up to a scale factor, is deduced, from which can be designed a closed-loop control law controlling the six camera d.o.f.. Contrarily to the position-based visual servoing, our scheme does not need any geometric 3D model of the object. Furthermore and contrarily to the image-based visual servoing, our approach ensures the convergence of the control law in all the task space.
AI
475504
Mobile Agent-based Compound Documents This paper presents a mobile agent-based framework for building mobile compound document, which can each be dynamically composed of mobile agents and can migrate itself over a network as a whole, with all its embedded agents. The key of this framework is that it builds a hierarchical mobile agent system that enables multiple mobile agents to be combined into a single mobile agent. The framework also provides several value-added mechanisms for visually manipulating components embedded in a compound document and for sharing a window on the screen among the components. This paper describes this framework and some experiences in the implementation of a prototype system, currently using Java the both implementation language and component development language, and then illustrates several interesting applications to demonstrate the framework's utility and flexibility.  1. 
Agents
8204
Efficient Data Mining for Path Traversal Patterns Abstract—In this paper, we explore a new data mining capability that involves mining path traversal patterns in a distributed information-providing environment where documents or objects are linked together to facilitate interactive access. Our solution procedure consists of two steps. First, we derive an algorithm to convert the original sequence of log data into a set of maximal forward references. By doing so, we can filter out the effect of some backward references, which are mainly made for ease of traveling and concentrate on mining meaningful user access sequences. Second, we derive algorithms to determine the frequent traversal patterns¦i.e., large reference sequences¦from the maximal forward references obtained. Two algorithms are devised for determining large reference sequences; one is based on some hashing and pruning techniques, and the other is further improved with the option of determining large reference sequences in batch so as to reduce the number of database scans required. Performance of these two methods is comparatively analyzed. It is shown that the option of selective scan is very advantageous and can lead to prominent performance improvement. Sensitivity analysis on various parameters is conducted. Index Terms—Data mining, traversal patterns, distributed information system, World Wide Web, performance analysis.
ML
chien01ecient
Efficient Complex Query Support for Multiversion XML Documents Managing multiple versions of XML documents represents a  critical requirement for many applications. Also, there has been much  recent interest in supporting complex queries on XML data (e.g., regular  path expressions, structural projections, DIFF queries). In this paper,  we examine the problem of supporting efficiently complex queries on  multiversioned XML documents. Our approach relies on a scheme based  on durable node numbers (DNNs) that preserve the order among the  XML tree nodes and are invariant with respect to updates. Using the  document's DNNs various complex queries are reduced to combinations  of partial version retrieval queries. We examine three indexing schemes to  efficiently evaluate partial version retrieval queries in this environment. A  thorough performance analysis is then presented to reveal the advantages  of each scheme.
DB
mcgovern01automatic
Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density This paper presents a method by which a reinforcement  learning agent can automatically discover  certain types of subgoals online. By creating  useful new subgoals while learning, the agent  is able to accelerate learning on the current task  and to transfer its expertise to other, related tasks  through the reuse of its ability to attain subgoals.  The agent discovers subgoals based on commonalities  across multiple paths to a solution. We  cast the task of finding these commonalities as  a multiple-instance learning problem and use the  concept of diverse density to find solutions. We  illustrate this approach using several gridworld  tasks.  1. 
ML
middleton01capturing
Capturing Knowledge of User Preferences: Ontologies in Recommender Systems Tools for filtering the World Wide Web exist, but they are hampered by the difficulty of capturing user preferences in such a dynamic environment. We explore the acquisition of user profiles by unobtrusive monitoring of browsing behaviour and application of supervised machine-learning techniques coupled with an ontological representation to extract user preferences. A multi-class approach to paper classification is used, allowing the paper topic taxonomy to be utilised during profile construction. The Quickstep recommender system is presented and two empirical studies evaluate it in a real work setting, measuring the effectiveness of using a hierarchical topic ontology compared with an extendable flat list.
IR
kim01secret
Secret Computation with Secrets for Mobile Agent using One-time Proxy Signature As an application for electronic commerce, a mobile agent is now used to search for special products or services and is executed for a specific job designated by a customer in the server's environment on behalf of a customer. On the way of performing its role, a mobile agent can be vulnerable to several cryptographic attacks. These attacks can be more serious when done by malicious servers. Among schemes to resolve this problem, the concept of encrypted function for secret computation was proposed in [ST97, KBC00]. However, schemes that employ such encrypted functions enforce the server(host) to execute the functions of customer before verifying the mobile codes even in the case that the codes are maliciously modified. In this paper, we apply proxy signature scheme to the mobile agent system to enhance security and efficiency. Also, we suggest one-time proxy signature scheme to limit the signing power of the server.
Agents
spoelder00man
Man Multi-Agent Interaction in VR: a Case Study with RoboCup. We describe a Virtual Reality system that allows users at different locations to interact with a multi-agent system in a natural way. We use RoboCup (robot soccer) as a case study. A human player who is immersed in a CAVE can interact with the RoboCup simulation in its natural domain, by playing along with a virtual soccer game. The system supports distributed collaboration by allowing humans at different geographic locations to participate and interact in real time. The most difficult problem we address is how to deal with the latency that is induced by the multi-agent simulation and by the wide-area network between different CAVEs. Our navigation software anticipates the movements of the human player and optimizes the interaction (navigation, kicking). Also, it sends a minimal amount of state information over the wide-area network.  1. Introduction  Multi-agent systems are becoming increasingly important in our society. The majority of such systems is in some way related to Internet ...
HCI
kobayashi99information
Information Retrieval on the Web: Selected Topics In this paper we review studies on the growth of the Internet and technologies which are useful for information search and retrieval on the Web. In the rst section, we present data on the Internet from several dierent sources, e.g., current as well as projected number of users, hosts and Web sites. Although the numerical gures vary, the overall trends cited by the sources are consistent and point to exponential growth during the coming decade. And Internet users are increasingly using search engines and search services to nd speci c information of interest. However, users are not satis ed with the performance of the current generation of search engines; the slow speed of retrieval, communication delays, and poor quality of retrieved results (e.g., noise and broken links) are commonly cited problems. The main body of our paper focuses on linear algebraic models and techniques for solving these problems. keywords: clustering, indexing, information retrieval, Internet, late...
IR
385259
Multi-Agent Systems Coalition Formation 5.4 Payoff Division Overview 119  5 Contract Nets, Coalition Formation 119-1  Chapter 5: Contract Nets, Coalition Formation Multi-Agent Systems (6 Lectures), Sept. 2000, Bahia Blanca 5.1 General Contract Nets  How to distribute tasks?  .  Global Market Mechanisms. Implementations use a single centralized mediator . .  Announce, bid, award -cycle. Distributed Negotiation . We need the following: 1. Define a task allocation problem in precise terms.  2. Define a formal model for making bidding and awarding decisions. 5.1 General Contract Nets 120  Chapter 5: Contract Nets, Coalition Formation Multi-Agent Systems (6 Lectures), Sept. 2000, Bahia Blanca Definition 5.1 (Task-Allocation Problem)  A task allocation problem is given by 1. a set of tasks T , 2. a set of agents A A A,  3. a cost function cost i i i : 2  T  -#  R#{}  (stating the costs that agent i i i incurs by handling some tasks), and 4. the initial allocation of tasks  #T  init  1 1 1  , . . . , T  init ...
Agents
38927
A Novel Server Selection Technique for Improving the Response Time of a Replicated Service Server replication is an approach often used to improve the ability of a service to handle a large number of clients. One of the important factors in the efficient utilization of replicated servers is the ability to direct client requests to the best server, according to some optimality criteria. In this paper we target an environment in which servers are distributed across the Internet, and clients identify servers using our application-layer anycasting service. Our goal is to allocate servers to clients in a way that minimizes a client's response time. To that end, we develop an approach for estimating the performance that a client would experience when accessing particular servers. Such information is maintained in a resolver that clients can query to obtain the identity of the server with the best response time. Our performance collection technique combines server push with client probes to estimate the expected response time. A set of experiments is used to demonstrate the propert...
DB
molz00design
Design of a Classification System for Rectangular Shapes Using a Co-Design Environment Pattern localization and classification are CPU time intensive, being normally implemented in software. Custom implementations in hardware allow real-time processing. In practice, in ASIC or FPGA implementations, the digitization process introduces errors that should be taken into account. This paper presents initially the state-of-the-art in this field, analyzing the performance and implementation of each work. After we propose a system for rectangular shapes localization and classification using reconfigurable devices (FPGA) and a signal processor (DSP) available in a flexible codesign platform. The system will be described using C and VHDL languages, for the software and hardware parts respectively. Finally, it is described the classification block of the system, implemented as an Artificial Neural Network (ANN) in a rapid prototyping platform.  1. 
ML
burnett00exception
Exception Handling in the Spreadsheet Paradigm Exception handling is widely regarded as a necessity in programming languages today, and almost every programming language currently used for professional software development supports some form of it. However, spreadsheet systems, which may be the most widely used type of "programming language" today in terms of number of users using it to create "programs" (spreadsheets), have traditionally had only extremely limited support for exception handling. Spreadsheet system users range from end users to professional programmers, and this wide range suggests that an approach to exception handling for spreadsheet systems needs to be compatible with the equational reasoning model of spreadsheet formulas, yet feature expressive power comparable to that found in other programming languages.
HCI
sima01computational
A Computational Taxonomy and Survey of Neural Network Models  We survey and summarize the existing literature on the computational aspects of neural network models, by presenting a detailed taxonomy of the various models according to their computational characteristics. The criteria of classication include e.g. the architecture of the network (feedforward vs. recurrent), time model (discrete vs. continuous) , state type (binary vs. analog), weight constraints (symmetric vs. asymmetric), network size (nite nets vs. infinite families), computation type (deterministic vs. probabilistic) , etc. The underlying results concerning the computational power of perceptron, RBF, winner-take-all, and spiking neural networks are briefly surveyed, with pointers to the relevant literature.   
AI
stanoi98weak
Weak Consistency in Distributed Data Warehouses We propose and analyze a novel multiple-view model of a distributed data warehouse. Views are represented in a hierarchical fashion, incorporating data from base sources as well as possibly other views. Current approaches to maintain consistency in such a model require that data stored in a view derived from base data via different paths be from the same state of the base relation. This type of consistency criterion is too restrictive for some applications. Hence, we propose relaxing the synchronization constraints at the view level and develop a model that allows views to set their own constraints by enforcing individual conditions for all pairs of paths. We define a correctness criteria for updates in this particular model, and analyze the new requirements necessary for maintaining the consistency of data. Finally, we propose an algorithm to ensure that views are updated consistently.  Keywords multi-view data warehouse, distributed system, consistency, updates  1 Introduction  A dat...
DB
paradis98language
A Language for Publishing Virtual Documents on the Web The Web is creating exciting new possibilities for direct and instantaneous publishing of information. However, the apparent ease with which one can publish documents on the Web hides more complex issues such updating and maintaining Web pages. We believe one of the crucial requirements to document delivery is the ability to extract and reuse information from other documents or sources. In this paper we present a descriptive language that allows users to write virtual documents, where dynamic information can be retrieved from various sources, transformed, and included along with static information in HTML documents. The language uses a tree-like structure for the representation of information, and defines a database-like query language for extracting and combining information without a complete knowledge of the structure or the types of information. The data structures and the syntax of the language are presented along with examples. 1 Introduction  In recent years the Web has grown fr...
DB
wuwongse01xml
XML Declarative Description: A Language for the Semantic Web this document, you agree to all provisions of the copyright laws protecting it.  a Web resource's semantics. It employs XML as its bare syntax and enhances XML expressive power by employing Declarative Description theory.  8  A description in XDD is a set of ordinary XML elements, extended XML elements with variables, and the XML elements' relationships in terms of XML clauses. An ordinary XML element denotes a semantic unit and is a surrogate of an information item in the real application domain. An extended XML element represents implicit information or a set of semantic units. Clauses express rules, conditional relationships, integrity constraints, and ontological axioms. We define the precise and formal semantics of an XDD description as a set of ordinary XML elements, without employing other formalisms. Important axioms that are missing in XML and RDF but expressible in XDD include  symmetry, composition-of, and inverse  relations. As an example of the inverse-relation axiom, consider the XML clauses A and  B in Figure 1a. They model the Creator  and   PubRAy8"xW  properties' inverses assumed by some particular domain. Figure 1b then gives an example of representing an RDF statement C, "A creator of a document entitled `XDD language' is John." The semantics of an XDD description, which comprises the clauses A, B, and C, will also contain an RDF statement, "A publication of John is `XDD language"' (Figure 1c), hence allowing inverse inference of such implicit information. Obviously, this axiom cannot be represented in RDF. XDD can directly represent all XMLbased application markup languages. It also can simply represent XML applications that provide common conventions of semantics, syntax, and structures for certain specific domains. In addition to RDF, these domai...
IR
scassellati99knowing
Knowing What to Imitate and Knowing When You Succeed If we are to build robots that can imitate the actions of a human instructor, the robotics community must address a variety  of issues. In this paper, we examine two of these issues. First, how does the robot know which things it should imitate?  Second, how does the robot know when its actions are an adequate imitation of the original? We further describe an  on-going research effort to implement systems for a humanoid robot that address these issues.  1 Introduction  Humans (and other animals) acquire new skills from social interactions with others through direct tutelage, observational conditioning, goal emulation, imitation, and other methods (Galef 1988). These social learning skills provide a powerful mechanism for children to acquire skills  and knowledge from their parents, other adults, and other children. In particular, imitation is an extremely powerful mechanism for social learning which has received a great deal of interest from researchers in the fields of animal behavior...
ML
papakonstantinou98query
Query Rewriting using Semistructured Views We address the problem of query rewriting for MSL, a semistructured language developed at Stanford in the TSIMMIS project for information integration. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting  queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing containment mappings, the chase, and unification -- techniques which were developed for structured, relational data. At the same time we develop an algorithm for equivalence checking of MSL queries. We show that the rewriting algorithm is sound and complete, i.e., it always finds every conjunctive MSL rewriting query of q, and we discuss its complexity. We currently incorporate the algorithm in the TSIMMIS system. 1 Introduction  Recently, many semistructured data models, query and view definition languages have been proposed [GM  +  97, MAG  +  97, BDHS96, AV97a, MM97, KS95, PGMU96,...
DB
aiken98abc
ABC - An Intelligent Robocup Agent This paper will cover the basics of our virtual soccer team, ABC (Aiken, Benzacar, Cocosco) . It provides an overview of the agent's architecture, and in-depth explanation of its various modules. The agent's approach to achieving intelligent behavior, as contrasted with other approaches, is discussed. I. Introduction  The international RoboCup competition is held yearly and pits teams of simulated soccer-playing agents against each other. The simulated environment in which the agents interact is designed to reproduce the challenges associated with an embodied agent: the sensory information provided to each agent is incomplete, noisy, and sporadic. With this imperfect data as a base, the agents are challenged to behave as a cohesive team in attempting to win their matches. This paper discusses the design and implementation of an Artificially Intelligent RoboCup agent, ABC.  Its overall design is modeled after the architecture used by the winning team of the 1997 RoboCup competition, as ...
Agents
della99internet
Internet Agents for Telemedicine Services Telemedicine can be viewed as the telematic support for collaboration among distant medical professionals, which cooperate on shared resources of various kind. In light of this, attention to telematics and informatics concepts particularly oriented towards collaboration should be paid: in particular, the recently appeared agent paradigm seems suitable for the analysis, design and development of telemedicine services because of its committment to intercommunication and sharing of resources. The present paper is aimed at introducing the agent paradigm, from the theoretical basis up to the technological issues, and at describing an agent-based approach to telemedicine, specifically applied to telepathology applications. The described system is based on an agent-based tool (JAMES), namely an agent model and template implemented using Java, which has been used to implement a prototype multipurpose telepathology application based on a federated agency architecture.
IR
page98pagerank
The PageRank Citation Ranking: Bringing Order to the Web The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.
IR
mascardi99agentbased
An Agent-Based Approach to Distributed Simulation, Distributed Prototyping and Software Integration Due to nowadays huge availability of data and software, a software developer must be able  not only to invent good algorithms and implement them efficiently, but also to assemble existing  components to create, timely and economically, a new application.  Prototyping is a software engineering paradigm particularly suitable for the compositional  approach to software development. A working prototype embedding the heterogeneous software  which will be used in the final application proves useful for at least two reasons:  ffl the prototype is definitely closer to the final application;  ffl the re-usability of the legacy software can be evaluated before the final application is  built.  The distribution of the software to be integrated within the prototype must be taken into  account, as well as the distribution of the prototype execution. This would help to gain in  efficiency and closeness to the final application.  The aim of this thesis is to exploit the "multi-agent system" abstracti...
Agents
weiss98learning
Learning to Predict Rare Events in Event Sequences Learning to predict rare events from sequences of events with categorical features is an important, real-world, problem that existing statistical and machine learning methods are not well suited to solve. This paper describes timeweaver, a genetic algorithm based machine learning system that predicts rare events by identifying predictive temporal and sequential patterns. Timeweaver is applied to the task of predicting telecommunication equipment failures from 110,000 alarm messages and is shown to outperform existing learning methods. Introduction An event sequence is a sequence of timestamped observations, each described by a fixed set of features. In this paper we focus on the problem of predicting rare events from sequences of events which contain categorical (non-numerical) features. Predicting telecommunication equipment failures from alarm messages is one important problem which has these characteristics. For AT&T, where most traffic is handled by 4ESS switches, the specific ...
ML
melin01atlas
ATLAS: A generic software platform for speech technology based applications ATLAS is a Java software library that provides a framework for building multilingual  and multi-modal applications, especially dialogue systems, on top of speech  technology components. The design is based on a layered system model, where  ATLAS sits as a middleware between an application-dependent layer and the  speech technology components and implements much of application-independent  functionality in the system. ATLAS is itself layered with interfaces to speech  technology components at the bottom and self-contained dialogue components at  the top. The layered design is both efficient and flexible and is suitable for a  research environment. The framework also provides support for applicationdependent  layers through a structure of an application with sessions interacting  with users through terminals. The terminal concept supports creating audio deviceindependent  applications that run transparently in both telephone and desktop  environments. Several speech technology components are available for use with  the ATLAS framework, including text-to-speech, speech recognition and speaker  verification systems. Four applications that use ATLAS have so far been developed  within student and research projects at the Centre for Speech Technology (CTT),  including a speech controlled telephone banking system (CTT-bank) and an  automated entrance receptionist (PER).  1. 
HCI
kortuem99close
Close Encounters: Supporting Mobile Collaboration through Interchange of User Profiles . This paper introduces the notion of profile-based cooperation as a way to support awareness and informal communication between mobile users during chance encounters. We describe the design of Proem, a wearable system for profile-based cooperation that enables users to publish and exchange personal profile information during physical encounters. The Proem system is used to initiate contact between individuals by identifying mutual interests or common friends. In contrast to most previous research that concentrates on collaboration in well-defined and closed user groups, Proem supports informal communication between individuals who have never met before and who don't know each other. We illustrate the benefits of profile-based cooperation by describing several usage scenarios for the Proem system. 1 Introduction During the course of a day we encounter and meet a large number of people, some of whom we know personally and some of whom we never met before. In everyday languag...
HCI
modha00clustering
Clustering Hypertext With Applications To Web Searching : Clustering separates unrelated documents and groups related documents, and is useful for discrimination, disambiguation, summarization, organization, and navigation of unstructured collections of hypertext documents. We propose a novel clustering algorithm that clusters hypertext documents using words (contained in the document), out-links (from the document) , and in-links (to the document). The algorithm automatically determines the relative importance of words, out-links, and in-links for a given collection of hypertext documents. We annotate each cluster using six information nuggets: summary, breakthrough, review, keywords, citation, and reference. These nuggets constitute high-quality information resources that are representatives of the content of the clusters, and are extremely effective in compactly summarizing and navigating the collection of hypertext documents. We employ web searching as an application to illustrate our results. Keywords: cluster annotation, feature comb...
IR
wache01ontologybased
Ontology-Based Integration of Information - A Survey of Existing Approaches We review the use on ontologies for the integration  of heterogeneous information sources. Based  on an in-depth evaluation of existing approaches to  this problem we discuss how ontologies are used to  support the integration task. We evaluate and compare  the languages used to represent the ontologies  and the use of mappings between ontologies as well  as to connect ontologies with information sources.  We also ask for ontology engineering methods and  tools used to develop ontologies for information integration.  Based on the results of our analysis we  summarize the state of the art in ontology-based information  integration and name areas of further research  activities.  1 Motivation  The so-called information society demands for complete access to available information, which is often heterogeneous and distributed. In order to establish efficient information sharing, many technical problems have to be solved. First, a suitable information source must be located that might conta...
IR
nehaniv99meaning
Meaning for Observers and Agents Claude Shannon formalized the notion of information transmission rate and capacity for pre-existing channels. Wittgenstein in his later work insisted that linguistic meaning be defined in terms of use in language games. C. S. Peirce, the father of semiotics, realized the importance of sign, signified, and interpretant in processes of semiosis. In particular, the connection between sign and signified does not take place in a platonic vacuum but is situated, embodied, embedded, and must be mediated by an interpretant. We introduce a rigorous mathematical notion of meaning, as (1) agent- and observer- perceptible information in interaction games between an agent and its environment or between an agent and other agents, that is (2) useful for satisfying homeostatic and other drives, needs, goals or intentions. With this framework it is possible to address issues of sensor- and actuator- design, origins, evolution, and maintenance for biological and artificial systems. Moreover, correspondences between channels of meaning are exploited by biological entities in predicting the behavior or reading the intent of others, as in predator-prey and social interaction. Social learning, imitation, communication of experience also develop and can be developed on this substrate of shared meaning.
Agents
rickel98animated
Animated Agents for Procedural Training in Virtual Reality: Perception, Cognition, and Motor Control This paper describes Steve, an animated agent that helps students learn to perform physical, procedural tasks. The student and Steve cohabit a three-dimensional, simulated mock-up of the student's work environment. Steve can demonstrate how to perform tasks and can also monitor students while they practice tasks, providing assistance when needed. This paper describes Steve's architecture in detail, including perception, cognition, and motor control. The perception module monitors the state of the virtual world, maintains a coherent representation of it, and provides this information to the cognition and motor control modules. The cognition module interprets its perceptual input, chooses appropriate goals, constructs and executes plans to achieve those goals, and sends out motor commands. The motor control module implements these motor commands, controlling Steve's voice, locomotion, gaze, and gestures, and allowing Steve to manipulate objects in the virtual world. 1 Introduction  To ma...
Agents
subramanian98costbased
Cost-Based Optimization of Decision Support Queries using Transient-Views Next generation decision support applications, besides being capable of processing huge amounts of data, require the ability to integrate and reason over data from multiple, heterogeneous data sources. Often, these data sources differ in a variety of aspects such as their data models, the query languages they support, and their network protocols. Also, typically they are spread over a wide geographical area. The cost of processing decision support queries in such a setting is quite high. However, processing these queries often involves redundancies such as repeated access of same data source and multiple execution of similar processing sequences. Minimizing these redundancies would significantly reduce the query processing cost. In this paper, we (1) propose an architecture for processing complex decision support queries involving multiple, heterogeneous data sources
DB
55934
On using degrees of belief in BDI agents : The past few years have seen a rise in the popularity of the use of mentalistic attitudes such as beliefs, desires and intentions to describe intelligent agents. Many of the models which formalise such attitudes do not admit degrees of belief, desire and intention. We see this as an understandable simplification, but as a simplification which means that the resulting systems cannot take account of much of the useful information which helps to guide human reasoning about the world. This paper starts to develop a more sophisticated system based upon an existing formal model of these mental attributes.  1 Introduction  In the past few years there has been a lot of attention given to building formal models of autonomous software agents; pieces of software which operate to some extent independently of human intervention and which therefore may be considered to have their own goals and the ability to determine how to achieve those goals. Many of these formal models are based on the use of ...
Agents
511540
Persona: A Contextualized and Personalized Web Search Recent advances in graph-based search techniques derived from Kleinberg's work [1] have been impressive. This paper further improves the graph-based search algo- rithm in two dimensions. Firstly, variants of Kleinberg's techniques do not take into account the semantics of the query string nor of the nodes being searched. As a result, polysemy of query words cannot be resolved. This paper presents an interactive query scheme utilizing the simple web ontology provided by the Open Directory Project to resolve meanings of a user query. Secondly, we extend a recently proposed personalized version of the Kleinberg algorithm [3]. Simulation results are presented to illustrate the sensitivity of our technique. We outline the implementation of our algorithm in the Persona personalized web search system.
IR
randell00shopping
The Shopping Jacket: Wearable Computing for the Consumer . As part of the Bristol Wearable Computing Initiative we are  exploring location sensing systems suitable for use with Wearable Computing.  In this paper we present our findings, and in particular a wearable  application - the 'Shopping Jacket' - which relies on a minimal infrastructure  to be effective. We use two positioning devices, 'Pingers' and  GPS. The Pinger is used to signal the presence of a shop, and to indicate  the type of shop and it's website. The GPS is used to disambiguate  which branch of a high street chain we are passing. The wearable uses  this information to determine whether the wearer needs to be alerted  that they are passing an interesting shop, or to direct the wearer around  a shopping mall.  The shopping jacket integrates a wearable CardPC; GPS and Pinger  receivers; a near-field radio link; hand-held display; GSM data telephone;  and a speech interface into a conventional sports blazer.  Keywords - wearable computer, location sensing, GPS, pinger, shoppin...
HCI
sun99from
From Implicit Skills to Explicit Knowledge: A Bottom-Up Model of Skill Learning This paper presents a skill learning model CLARION. Different from existing models of mostly high-level skill learning that use a top-down approach (that is, turning declarative knowledge into procedural knowledge through practice), we adopt a bottom-up approach toward low-level skill learning, where procedural knowledge develops first and declarative knowledge develops later. Our model is formed by integrating connectionist, reinforcement, and symbolic learning methods to perform on-line reactive learning. It adopts a two-level dual-representation framework (Sun, 1995), with a combination of localist and distributed representation. We compare the model with human data in a minefield navigation task, demonstrating some match between the model and human data in several respects.
ML
250432
Cyber-Atvs: Dynamic And Distributed Reconnaissance And Surveilllance Using All Terrain Ugvs This paper describes our current effort to develop robotic vehicles for tactical distributed surveillance. Our research is focused on multi-agent collaboration, reconfigurable systems, efficient perception and sensor fusion, distributed command and control, and task decomposition. In particular, this paper describes the main features and capabilities of our All Terrain Vehicles (ATVs), concentrating on their autonomous navigation capabilities.
Agents
thrun98map
Map Learning and High-Speed Navigation in RHINO This chapter surveys basic methods for learning maps and high speed autonomous navigation for indoor mobile robots. The methods have been developed in our lab over the past few years, and most of them have been tested thoroughly in various indoor environments. The chapter is targeted towards researchers and engineers who attempt to build reliable mobile robot navigation software.  
AI
xu01agentbased
An Agent-based Petri Net Model with Application to Seller/Buyer Design in Electronic Commerce Agents are becoming one of the most important topics in distributed and autonomous decentralized systems (ADS), and there are increasing attempts to use agent technologies to develop software systems in electronic commerce. Such systems are complex and there is a pressing need for system modeling techniques to support reliable, maintainable and extensible design. G-Nets are a type of Petri net defined to support modeling of a system as a set of independent and loosely-coupled modules. In this paper, we first introduce an extension of G-Net, agent-based G-Net, as a generic model for agent design. Then new communication mechanisms are introduced to support asynchronous message passing among agents. To illustrate our formal modeling technique is effective for agent modeling in electronic commerce, a pricenegotiation protocol example between buyers and sellers is provided. Finally, by analyzing an ordinary Petri net reduced from our agent-based G-Net models, we conclude that our agent-based G-Net models are L3-live, concurrent and effective for agent communications. 1.
Agents
302729
Essex Wizards'99 Team Description : This paper describes the Essex Wizards team participated in the  RoboCup'99 simulator league. It is mainly concentrated on a multi-threaded  implementation of simulated soccer agents to achieve real-time performance.  Simulated robot agents work at three distinct phases: sensing, thinking and  acting. POSIX threads are adopted to implement them concurrently. The issues  of decision-making and co-operation are also addressed.  1. Introduction  In the RoboCup simulator environment, the response time of a soccer agent becomes significantly important since the soccer server operates with 100ms cycles for executing actions and 150ms cycles for providing sensory data [12]. Moreover, auditory sensory data can be received at random intervals. It is vital that each agent has bounded response times. If an action is not generated within 100ms, then the agent will stay idle for that cycle and enemy agents that did act might gain an advantage. On the other hand, if more than one action is generat...
Agents
sterritt00exploring
Exploring Dynamic Bayesian Belief Networks for Intelligent Fault Management Systems Systems that are subject to uncertainty in their behaviour are often modelled by Bayesian Belief Networks (BBNs). These are probabilistic models of the system in which the independence relations between the variables of interest are represented explicitly. A directed graph is used, in which two nodes are connected by an edge if one is a 'direct cause'  of the other.  However the Bayesian paradigm does not provide any direct means for modelling dynamic systems. There has been a considerable amount of research effort in recent years to address this. This paper reviews these approaches and proposes a new dynamic extension to the BBN.  This paper proceeds to discuss fault management of complex telecommunications and how the dynamic bayesian models can assist in the prediction of faults.  Keywords: dynamic bayesian belief networks, telecommunication networks, fault management, intelligent systems.  1 
AI
sarwar01itembased
Item-based Collaborative Filtering Recommendation Algorithms Recommender systems apply knowledge discovery techniques to the problem of making personalized recommendations for information, products or services during a live interaction. These systems, especially the k-nearest neighbor collaborative filtering based ones, are achieving widespread success on the Web. The tremendous growth in the amount of available information and the number of visitors to Web sites in recent years poses some key challenges for recommender systems. These are: producing high quality recommendations, performing many recommendations per second for millions of users and items and achieving high coverage in the face of data sparsity. In traditional collaborative filtering systems the amount of work increases with the number of participants in the system. New recommender system technologies are needed that can quickly produce high quality recommendations, even for very large-scale problems. To address these issues we have explored item-based collaborative filtering techniques. Itembased techniques first analyze the user-item matrix to identify relationships between different items, and then use these relationships to indirectly compute recommendations for users. In this paper we analyze different item-based recommendation generation algorithms. We look into different techniques for computing item-item similarities (e.g., item-item correlation vs. cosine similarities between item vectors) and different techniques for obtaining recommendations from them (e.g., weighted sum vs. regression model). Finally, we experimentally evaluate our results and compare them to the basic k-nearest neighbor approach. Our experiments suggest that item-based algorithms provide dramatically better performance than user-based algorithms, while at the same time providing better quality than the best available user-based algorithms. 1
IR
459233
Probability Based Clustering for Document and User Properties . Information Retrieval systems can be improved by exploiting context information such as user and document features. This article presents a model based on overlapping probabilistic or fuzzy clusters for such features. The model is applied within a fusion method which linearly combines several retrieval systems. The fusion is based on weights for the different retrieval systems which are learned by exploiting relevance feedback information. This calculation can be improved by maintaining a model for each document and user cluster. That way, the optimal retrieval system for each document or user type can be identified and applied. The extension presented in this article allows overlapping, probabilistic clusters of features to further refine the process. 1.
IR
147375
Program Comprehension in Multi-Language Systems This paper presents an approach to program comprehension in multi-language systems. Such systems are characterized by a high amount of source codes in various languages for programming, database definition and job control. Coping with those systems requires the references crossing the language boundaries to be analysed.  Using the EER/GRAL approach to graph-based conceptual modeling, models representing relevant aspects of single language are built and integrated into a common conceptual model. Since conceptual modeling focusses on specific problems, the integrated model presented here is especially tailored to multi-language aspects. Software systems are parsed and represented according to this conceptual model and queried by using a powerful graph query mechanism. This allows multi-language cross references to be easily retrieved.  The multi-language conceptual model and the query facilities have been developed in cooperation with the maintenance programmers at an insurance company w...
DB
florescu00integrating
Integrating Keyword Search into XML Query Processing Due to the popularity of the XML data format, several query languages for XML have been proposed, specially devised to handle data whose structure is unknown, loose, or absent. While these languages are rich enough to allow for querying the content and structure of an XML document, a varying or unknown structure can make formulating queries a very difficult task. We propose an extension to XML query languages that enables keyword search at the granularity of XML elements, that helps novice users formulate queries, and also yields new optimization opportunities for the query processor. We present an implementation of this extension on top of a commercial RDBMS; we then discuss implementation choices and performance results.  Keywords  XML query processing, full-text index  1 Introduction  There is no doubt that XML is rapidly becoming one of the most important data formats. It is already used for scientific data (e.g., DNA sequences), in linguistics (e.g., the Treebank database at the U...
DB
heskes98solving
Solving a huge number of similar tasks: a combination of multi-task learning and a hierarchical Bayesian approach In this paper, we propose a machine-learning solution to problems consisting of many similar prediction tasks. Each of the individual tasks has a high risk of overfitting. We combine two types of knowledge transfer between tasks to reduce this risk: multi-task learning and hierarchical Bayesian modeling. Multitask learning is based on the assumption that there exist features typical to the task at hand. To find these features, we train a huge two-layered neural network. Each task has its own output, but shares the weights from the input to the hidden units with all other tasks. In this way a relatively large set of possible explanatory variables (the network inputs) is reduced to a smaller and easier to handle set of features (the hidden units). Given this set of features and after an appropriate scale transformation, we assume that the tasks are exchangeable. This assumption allows for a hierarchical Bayesian analysis in which the hyperparameters can be estimated from the data. Effect...
DB
xu02modeling
Modeling and Verifying Multi-Agent Behaviors Using Predicate Transition Nets In a multi-agent system, how agents accomplish a goal task is usually specified by multi-agent plans built from basic actions (e.g. operators) of which the agents are capable. A critical problem with such an approach is how can the designer make sure the plans are reliable. To tackle this problem, this paper presents a formal approach for modeling and analyzing multi-agent behaviors using Predicate/Transition (PrT) nets, a high-level formalism of Petri nets. We construct a multi-agent model by representing agent capabilities as transitions. To verify a multiagent PrT model, we adapt the planning graphs as a compact structure for the reachability analysis. We also demonstrate that, based on the PrT model, whether parallel actions specified in multi-agent plans can be executed in parallel and whether the plans guarantee the achievement of the goal can be verified by analyzing the dependency relations among the transitions.
Agents
243680
Agents for Process Coherence in Virtual Enterprises SoCom  #  1  "on time"  Abstract SoCom  #  2  "cheap"  Buyer Seller  Abstract SoCom  #  3  Buyer Seller  SoCom Manager  Hoosier Inc.  Register me as  buyer and  seller  Register me as  buyer and  seller  Play Seller in  AbstractSoCom  #1?  Yes  Valvano & Co. Hot Air Bros.  8 9  Concrete  SoCom  created  4  6  7  "high quality"  = Roles  = Agents  Directory  Agent_id Role derived  1  Figure 2. Instantiation of a concrete SoCom  68 March 1999/Vol. 42, No. 3 COMMUNICATIONS OF THE ACM  Adopt role  Need to  initiate  Ask SoCom  manager  Participate  No  No  No  No  No  No  No  Ye s  Yes  Ye s  Yes  Yes  Yes  Ye s  Register  SoCom  manager suggests  a socom  Request  to create  SoCom  Process  request  Stop  Stop  (undefined)  Stop  (Failure)  Instantiate  and  announce  Receipt of a  request  Request to  register  Condition  evaluation  OK?  Find  candidates  Ask  candidates  All  say yes?  Agents decision making SoCom manager's decision making  Because our agents are autonomous, we must  e...
Agents
norling01learning
Learning to Notice: Adaptive Models of Human Operators Agent-based technologies have been used for a number of years to model human operators in complex simulated environments. The BDI agent framework has proven to be particularly suited to this sort of modelling, due to its "natural" composition of beliefs, goals and plans. However one of the weaknesses of the BDI agent model, and many other human operator models (agent-based or otherwise), is its inability to support agent learning. Human operators naturally adapt their behaviour over time, particularly to avoid repeating mistakes. This paper introduces an enhancement to the BDI framework which is based on a descriptive psychological model of decision making called "recognition-primed decision making." This enhancement allows the development of agents that adapt their behaviour in real-time, in the same manner as a person would, providing more realistic human operator models.
Agents
camacho00multiagent
A Multiagent Approach for Electronic Travel Planning In the last years, the amount of information stored in Internet has grown exponentially. This article presents a new approach to cooperative problem solving that use the Web as a source of data. The architecture has been designed using two main Artificial Intelligence techniques: Multiagent System design, and problem solving (planning). Both are used to obtain a new architecture that dynamically obtains knowledge from Internet. The system uses two different types of agents: planning agents and web agents. Planning agents pay attention to the user's queries and solve his/her problems at a high level of abstraction; web agents fill in the details obtaining the required information from Internet. Different partial solutions given by the web agents while combined by the planning agent to obtain a detailed solution (or solutions) to the user queries.
Agents
buffet02adaptive
Adaptive Combination of Behaviors in an Agent rchical structures [PS97].    often require specific manual  preparations  (the shape of the structure).  .  Factored representations [Sal00].    are not scalable, but restricted to a given environment  (number of objects, size of environment...).  8/19  ECAI'02  # # # # # # 9/19  ECAI'02  # # # # # # Idea :  Hypothesis:  A complex behavior is often guided by a set of basic motivations (i.e.  goals).  Idea:  Make use of basic behaviors associated  with the basic motivations  to recombine them in one  complex behavior.  agent  O1  O3  10/19  ECAI'02  # # # # # # Scene decomposition:  O2  agent  O1  O2 O3  O2  O1  +  =?  .  Perceived objects:  O 1 , O 2 et O 3 .  .  Basic behaviors:  avoid holes (b a ,  {#hole#})  push tiles (b p ,  {#hole#, #tile#})  .  (behavior, config) pairs:  (b a ,    2  11/19  ECAI'02  # # # # # # Basic Behaviors:  Example :      }):  behavior pushing  tile O 1 in hole O 2 O2  O1  O3  A basic generic behavior b is defined by  .  a type of configuration  {#T  obj 1
ML
rasheed98adaptive
An Adaptive Penalty Approach for Constrained Genetic-Algorithm Optimization In this paper we describe a new adaptive penalty approach for handling constraints in genetic algorithm optimization problems. The idea is to start with a relatively small penalty coefficient and then increase it or decrease it on demand as the optimization progresses. Empirical results in several engineering design domains demonstrate the merit of the proposed approach.  1 Introduction  Genetic Algorithms (GAs) (Goldberg 1989) are search algorithms that mimic the behavior of natural selection. GAs attempt to find the best solution to some problem (e.g., the maximum of a function) by generating a collection ("population ") of potential solutions ("individuals") to the problem. Through mutation and recombination (crossover) operations, better solutions are hopefully generated out of the current set of potential solutions. This process continues until an acceptably good solution is found. GAs have many advantages over other search techniques, including the ability to deal with qualitativ...
ML
levison96connecting
Connecting Planning And Acting Via Object-Specific Reasoning Instructions from a high-level planner are in general too abstract for a behavioral simulator to execute. In this dissertation I describe an intermediate reasoning system -- the Object-Specific Reasoner -- which bridges the gap between high-level task-actions and action directives of a behavioral system. It decomposes task-actions and derives parameter values for each action directive, thus enabling existing high-level planners to instruct synthetic agents with the same task-action commands that they currently produce. The Object-Specific Reasoner's architecture follows directly from the hypothesis that action representations are underspecified descriptions, and that objects in the same functional category are manipulated in similar ways. The action representation and the object representation are combined to complete the action interpretation, thereby grounding plans in action. The Object-Specific Reasoner provides evidence that a small number of object functional categories, organize...
Agents
527187
Learning Behavioral Parameterization Using Spatio-Temporal Case-Based Reasoning This paper presents an approach to learning an optimal behavioral parameterization in the framework of a Case-Based Reasoning methodology for autonomous navigation tasks. It is based on our previous work on a behavior-based robotic system that also employed spario-temporal case-based reasoning [3] in the selection of behavioral parameters but was not capable of learning new parameterizations. The present method extends the case-based reasoning module by making it capable of learning new and optimizing the existing cases where each case is a set of behavioral parameters. The learning process can either be a separate training process or be part of the mission execution. In either case, the robot learns an optimal parameterization of its behavior for different environments it encounters. The goal of this research is not only to automatically optimize the performance of the robot but also to avoid the manual configuration of behavioral parameters and the initial configuration of a case library, both of which require the user to possess good knowledge of robot behavior and the performance of numerous experiments. The presented method was integrated within a hybrid robot architecture and evaluated in extensive computer simulations, showing a significant increase in the performance over a nonadaptive system and a performance comparable to a non-learning CBR system that uses a hand-coded case library.
ML
guillaume02web
The Web Graph: an Overview this paper, a study is made on a 200 millions vertices graph obtained from a crawl of the Web, and it appears that is is composed of four parts of equivalent sizes. See Figure 3. The first part is the largest strongly connected component of the graph (the second largest is much smaller), which composes the core of the well connected pages. The second part, called IN, is composed of those pages from which the core is reachable, but which are not reachable from the core. Conversly, the third part, called OUT, is the set of pages reachable from the core but from which the core is unreachable. Finally, the dendrites are the pages reachable from one of the three first parts, or from which one of the three first parts is reachable, but which belong to none of the previous parts. Only ten percent of the whole graph do not belong to one of these four parts which compose the bow-tie
IR
veloso98playing
Playing Soccer with Legged Robots Sony has provided a remarkable platform for research and development in robotic agents, namely fully autonomous legged robots. In this paper, we describe our work using Sony's legged robots to participate at the RoboCup'98 legged robot demonstration and competition. Robotic soccer represents a very challenging environment for research into systems with multiple robots that need to achieve concrete objectives, particularly in the presence of an adversary. Furthermore RoboCup'98 offers an excellent opportunity for robot entertainment. We introduce the RoboCup context and briefly present Sony's legged robot. We developed a vision-based navigation and a Bayesian localization algorithm. Team strategy is achieved through pre-defined behaviors and learning by instruction. 1 Introduction  Problem solving in complex domains necessarily involves multiple agents, dynamic environments, and the need for learning from feedback and previous experience. Robotic soccer is an example of one such complex...
AI
knublauch02extreme
Extreme Programming of Multi-Agent Systems The complexity of communication scenarios between agents make multi-agent systems difficult to build. Most of the existing Agent-Oriented Software Engineering methodologies face this complexity by guiding the developers through a rather waterfall-based process with a series of intermediate modeling artifacts. While these methodologies lead to executable prototypes relatively late and are expensive when requirements change, we explore a rather evolutionary approach with explicit support for change and rapid feedback. In particular, we apply Extreme Programming, a modern light-weight methodology from object-oriented software technology, for the design and implementation of multiagent systems. The only modeling artifacts that are being maintained in our approach are a process model with which domain experts and developers design and communicate agent application scenarios, and the executable agent source code including automated test cases. Our methodology has been successfully applied for the development of a prototypical multi-agent system for clinical information logistics. 1.
Agents
lovell98feature
Feature selection using Expected Attainable Discrimination We propose expected attainable discrimination (EAD) as a measure to select discrete valued features for reliable discrimination between two classes of data. EAD is an average of the area under the ROC curves obtained when a simple histogram probability density model is trained and tested on many random partitions of a data set. EAD can be incorporated into various stepwise search methods to determine promising subsets of features, particularly when misclassification costs are difficult or impossible to specify. Experimental application to the problem of risk prediction in pregnancy is described.  Keywords: Receiver operating characteristic (ROC); Area under the ROC curve; Feature selection; Risk prediction in pregnancy; Failure to progress. 1 Introduction  Feature selection is a key step towards solving any practical classification problem (Kittler, 1975). Even though a Bayes classifier can never be improved by omission of features (Van Campenhout, 1982), there are compelling practical...
ML
boloni99multiplane
A Multi-Plane State Machine Agent Model This paper presents a framework for implementing collaborative network agents. Agents  are assembled dynamically from components into a structure described by a multi-plane state  machine model. This organization lends itself to an elegant implementations of remote control,  collaboration, checkpointing and mobility, dening features of an agent system. It supports  techniques, like agent surgery dicult to reproduce with other approaches.  The reference implementation for our model, the Bond agent system, is distributed under  an open source license and can be downloaded from http://bond.cs.purdue.edu.  1 Introduction  The eld of agents is witnessing the convergence of researchers from several elds. Some see agents as a natural extension of the object-oriented programming paradigm, [14, 15]. One of the most popular books on articial intelligence reinterprets the whole eld in terms of agents [2]. Contemporary work on the theory of behavior provides the foundations for theoretical mo...
Agents
schulte99spontaneous
Spontaneous, Short-term Interaction with Mobile Robots major open research directions in mobile robotics. This paper considers a specific type of interaction: short-term and spontaneous interaction with crowds of people. Such patterns of interactions are found when service robots operate in public places, for example information kiosks, receptionists, tour-guide robots applications. We describe our approach to spontaneous short-term interaction: a robot designed to be a believable social agent. The approach has been implemented using a mobile robot with a motorized face as focal point for interaction, an architecture that suggests the robot has moods, and a method for learning how to interact with people. Our system was recently deployed at a Smithsonian museum in Washington, DC. During a two week period it interacted with thousands of people. The robot's interactive capabilities were essential for its high on-task performance, and thus its practical success.
AI
ardissono00plan
A Plan Based Agent Architecture for Interpreting Natural Language Dialogue This paper describes a plan-based agent architecture for modeling NL cooperative dialogue; in particular, the paper focuses on the interpretation dialogue and explanation of its coherence by means of the recognition of the speakers' underlying intentions. The approach we propose makes it possible to analyze and explain in a uniform way several apparently unrelated linguistic phenomena, which have been often studied separately and treated via ad-hoc methods in the models of dialogue presented in the literature. Our model of linguistic interaction is based on the idea that dialogue can be seen as any other interaction among agents: therefore, domain-level and linguistic actions are treated in a similar way. Our agent architecture is based on a two-level representation of the knowledge about acting: at the metalevel, the Agent Modeling plans describe the recipes for plan formation and execution (they are a declarative representation of a reactive planner); at the object level, the domain ...
Agents
261630
Evolving Detectors of 2D Patterns on a Simulated CAM-Brain Machine, an Evolvable Hardware Tool for Building a 75 Million Neuron Artificial Brain . This paper presents some simulation results of the evolution  of 2D visual pattern recognizers to be implemented very shortly on real  hardware, namely the "CAM-Brain Machine" (CBM), an FPGA based  piece of evolvable hardware which implements a genetic algorithm (GA)  to evolve a 3D cellular automata (CA) based neural network circuit module,  of approximately 1,000 neurons, in about a second, i.e. a complete  run of a GA, with 10,000s of circuit growths and performance evaluations.  Up to 65,000 of these modules, each of which is evolved with a  humanly specified function, can be downloaded into a large RAM space,  and interconnected according to humanly specified artificial brain architectures.  This RAM, containing an artificial brain with up to 75 million  neurons, is then updated by the CBM at a rate of 130 billion CA cells  per second. Such speeds will enable real time control of robots and hopefully  the birth of a new research field that we call "brain building". The  first su...
ML
106339
Querying Network Directories Hierarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal pro les, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way far superior to what conventional relational or object-oriented databases o er. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated \queries &quot; involve navigational access. In this paper, we develop the core of a formal data model for network directories, and propose a sequence of e ciently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the e ciency of each algorithm in terms of its I/O complexity. Our data model and query languages share the exibility and utility of the recent proposals for semi-structured data models, while at the same time e ectively addressing the speci c needs of network directory applications, which we demonstrate by means of a representative real-life example. This work was done when the authors were at AT&T Labs{
DB
pohl99machine
Machine Learning and Knowledge Representation in the LaboUr Approach to User Modeling . In early user-adaptive systems, the use of knowledge representation methods for  user modeling has often been the focus of research. In recent years, however, the application  of machine learning techniques to control user-adapted interaction has become popular. In  this paper, we present and compare adaptive systems that use either knowledge representation  or machine learning for user modeling. Based on this comparison, several dimensions  are identified that can be used to distinguish both approaches, but also to characterize user  modeling systems in general. The LaboUr (Learning about the User) approach to user modeling  is presented which attempts to take an ideal position in the resulting multi-dimensional  space by combining machine learning and knowledge representation techniques. Finally, an  implementation of LaboUr ideas into the information server ELFI is sketched.  1 Introduction  While striving to achieve user-adapted interaction, user modeling researchers have often m...
IR
roubos00learning
Learning Fuzzy Classification Rules from Data . Automatic design of fuzzy rule-based classification systems based on labeled data is considered. It is recognized that both classification performance and interpretability are of major importance and e#ort is made to keep the resulting rule bases small and comprehensible. An iterative approach for developing fuzzy classifiers is proposed. The initial model is derived from the data and subsequently, feature selection and rule base simplification are applied to reduce the model, and a GA is used for model tuning. An application to the Wine data classification problem is shown.  1 Introduction  Rule-based expert systems are often applied to classification problems in fault detection, biology, medicinem etc. Fuzzy logic improves classification and decision support systems by allowing the use of overlapping class definitions and improves the interpretability of the results by providing more insight into the classifier structure and decision making process [13]. The automatic determination...
ML
meuss99improving
Improving Index Structures for Structured Document Retrieval Structured document retrieval has established itself as a new research area in the overlap between Database Systems and Information Retrieval. This work proposes a filtering technique, that can be added to already existing index structures of many structured document retrieval systems. This new technique takes the contextual structure information of query and document database into account and reduces the occurrence sets returned by the original index structure drastically. This improves the performance of query evaluation.  A measure is introduced that allows to quantify the added value of the proposed index structure. Based on this measure a heuristic is presented that allows to include only valuable context information in the index structure. 1 Introduction  With the growing importance of Information Retrieval in the presence of a vast amount of structured documents in formalisms like SGML ([ISO86]) or the future WWW language XML ([W3C99]), sophisticated and efficient indexing techn...
IR
goldberg00eigentaste
Eigentaste: A Constant Time Collaborative Filtering Algorithm Abstract. Eigentaste is a collaborative filtering algorithm that uses universal queries to elicit real-valued user ratings on a common set of items and applies principal component analysis (PCA) to the resulting dense subset of the ratings matrix. PCA facilitates dimensionality reduction for offline clustering of users and rapid computation of recommendations. For a database of n users, standard nearest-neighbor techniques require O(n) processing time to compute recommendations, whereas Eigentaste requires O(1) (constant) time. We compare Eigentaste to alternative algorithms using data from Jester, an online joke recommending system. Jester has collected approximately 2,500,000 ratings from 57,000 users. We use the Normalized Mean Absolute Error (NMAE) measure to compare performance of different algorithms. In the Appendix we use Uniform and Normal distribution models to derive analytic estimates of NMAE when predictions are random. On the Jester dataset, Eigentaste computes recommendations two orders of magnitude faster with no loss of accuracy. Jester is
IR
529556
An Autonomous Page Ranking Method for Metasearch Engines this paper, the topics are derived from the user's query; the reputation of each result page on the query topic is computed, and the value used to rank the result pages across all participating search engines, without biasing the ranking towards any of the sources
IR
thomason00desires
Desires and Defaults: A Framework for Planning with Inferred Goals This paper develops a formalism designed to integrate reasoning about desires with planning. The resulting logic, BDP, is capable of modeling a wide range of common-sense practical arguments, and can serve as a more general and flexible model for agent architectures.
Agents
78547
Visual Information Retrieval from Large Distributed On-line Repositories ion ---  VIR systems differ in the level of abstraction in  which content is indexed. For example,  images may be indexed at various levels, such  as at the feature-level (e.g., color, texture, and  shape), object-level (e.g., moving foreground  object), syntax-level (e.g., video shot), and  semantic-level (e.g., image subject), and so  forth. Most automatic VIR systems aim at lowlevel features, while the high-level indexes are  usually generated manually. Interaction  among different levels is an exciting but  unsolved issue.  . Generality ---  VIR systems differ in their specificity of the  domain of visual information. For example,  customized feature sets can be developed to  incorporate specific domain knowledge, such  as those in medical and remote-sensing  applications. Other, more general VIR  systems aim at indexing unconstrained visual  information such as that on the Internet.  . Content Collection ---  VIR systems differ in the methods in which new  visual information is ad...
IR
233063
The Cooperative Problem-Solving Process We present a model of cooperative problem solving that describes the process from its beginning, with some agent recognizing the potential for cooperation with respect to one of its goals, through to team action. Our approach is to characterize the mental states of the agents that lead them to solicit, and take part in, cooperative action. The model is formalized by expressing it as a theory in a quantified multi-modal logic.  Keywords: Multi-agent systems, cooperation, modal logic, temporal logic.  1 Introduction  Agents --- both human and artificial --- can engage in many and varied types of social interaction, ranging from altruistic cooperation through to open conflict. However, perhaps the paradigm example of social interaction is cooperative problem solving (CPS), in which a group of autonomous agents choose to work together to achieve a common goal. For example, we might find a group of people working together to move a heavy object, play a symphony, build a house, or write a jo...
Agents
popescul02towards
Towards Structural Logistic Regression: Combining Relational and Statistical Learning Inductive logic programming (ILP) techniques are useful for analyzing data in multi-table relational databases. Learned rules can potentially discover relationships that are not obvious in ``flattened'' data. Statistical learners, on the other hand, are generally not constructed to search relational data, they expect to be presented with a single table containing a set of feature candidates. However, statistical learners often yield more accurate models than the logical forms of ILP, and can better handle certain types of data, such as counts. We propose a new approach which integrates structure navigation from ILP with regression modeling. Our approach propositionalizes the first-order rules at each step of ILP's relational structure search, generating features for potential inclusion in a regression model. Ideally, feature generation by ILP and feature selection by stepwise regression should be integrated into a single loop. Preliminary results for scientific literature classification are presented using a relational form of the data extracted by ResearchIndex (formerly CiteSeer). We use FOIL and logistic regression as our ILP and statistical components (decoupled at this stage). Word counts and citation-based features learned with FOIL are modeled together by logistic regression. The combination often significantly improves performance when high precision classification is desired.
IR
acharya98active
Active Disks Several application and technology trends indicate that it might be both profitable and feasible to move computation closer to the data that it processes. In this paper, we evaluate Active Disk architectures which integrate significant processing power and memory into a disk drive and allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk. The key idea is to offload bulk of the processing to the disk-resident processors and to use the host processor primarily for coordination, scheduling and combination of results from individual disks. To program Active Disks, we propose a stream-based programming model which allows disklets to be executed efficiently and safely. Simulation results for a suite of seven algorithms from three application domains (commercial data warehouses, image processing and satellite data processing) indicate that for these algorithms, Active Disks outperform conventional-disk architectures. 1 Introduction...
DB
212779
Intelligent Information Access in the Web: ML based User Modeling for high precision Meta-Search It is a well known fact that high precision search for documents concerning a certain topic in the World Wide Web (Www) is a tough problem. Index based search engines vary in recall (with a coverage of at most 30% of the web) and offer a very bad precision by (simple) keyword search. Meta search engines provide a specialised monolithic architecture for information extraction, and integration of heterogenous information resources which ensures a bigger recall and may yield a better precision. Few search engines (as, e. g., HuskySearch) employ intelligent techniques in order to increase precision. On the other hand, many personalized agents for web browsing are currently developed. It is a straightforward idea to incorporate the idea of user modeling with machine learning (Ml) methods into web search services. We propose an abstract prototype, OySTER, which makes use of machine learning based user modeling and which for reasons of robustness, performance and quality is realised as a mult...
IR
lutz99complexity
On the Complexity of Terminological Reasoning TBoxes are an important component of knowledge representation systems based on description logics (DLs) since they allow for a natural representation of terminological knowledge. Largely due to a classical result given by Nebel [ 20 ] , complexity analyses for DLs have, until now, mostly focused on reasoning without (acyclic) TBoxes. In this paper, we concentrate on DLs, for which reasoning without TBoxes is  PSpace-complete, and show that there exist logics for which the complexity of reasoning remains in PSpace if acyclic TBoxes are added and also logics for which the complexity increases. An example for a logic of the former type is ALC while examples for logics of the latter kind include ALC(D) and ALCF . This demonstrates that it is necessary to take TBoxes into account for complexity analyses. Furthermore, we show that reasoning with the description logic ALCRP(D)  is NExpTime-complete regardless of whether TBoxes are admitted or not. 1 Introduction  A core feature of description...
AI
glover00web
Web Search -- Your Way We describe a metasearch engine architecture, in use at NEC Research Institute, that allows users to provide preferences in the form of an information need category. This extra information is used to direct the search process, providing more valuable results than by considering only the query. Using our architecture, identical keyword queries may be sent to different search engines, and results may be scored differently for different users.
IR
dragoni00mental
Mental States Recognition from Communication  Effective and useful communication requires the agents' being able to foresee the effects of their utterances over the addressee's mental state. However, referring to the classical Speech Act Theory, it seems to us that the idea of predicting such effects is rather optimistic since they are not really completely "a priori" foreseeable by the speaker. Along with some obvious main effects, there are other side effects which might be regarded as the result of some kind of plausible inference, particularly abduction, performed by the hearerhimself79  the received communication and over: - its own actual mental state (which can be differentfromen one expected by the speaker), - its image (may be incorrect and incomplete) ofthe81-24 's mental state. In this paper we explore the idea that if (and asfar17 it is possible: 1. to formalize in a declarative manner the mental state ofan502-22165 Agent [1], 2. to postulate a correlation between a speaker's mental state and his uttering a certainsent...
Agents
270207
An Application-Independent Intelligent User Support System Exploiting Action-Sequence Based User Modelling . Many software systems' usability suffers from their complexity, usually caused  by the market-driven trend to bundle a huge amount of features, which are supposed to increase  the product's attractiveness. This attempt, however, more often than not leads to software  with poor usability characteristics, therefore requiring an extensive amount of initial  effort for the users to become familiar with the system. One way to overcome this problem  is by providing user-adapted usage support.  In this paper we present an experimental system for intelligent user support, which has  been developed under the aspect of portability. Focusing on this goal, the system supports a  variety of user- and task-modeling approaches and is independent from the hosting software  application environment, thus being ready to integrate with existing and new applications.  The different user-modeling approaches have been empirically evaluated and compared in  a medical software application embedding our syste...
HCI
298012
Scalable Maintenance in Distributed Data Warehousing Environment The maintenance of data warehouses is becoming an increasingly important topic due to the growing  use, derivation and integration of digital information. Most previous work has dealt with one centralized  data warehouse (DW) only. In this paper, we now focus on environments with multiple data warehouses  that are possibly derived from other data warehouses. In such a large-scale environment, data  updates from base sources may arrive in individual data warehouses in different orders, thus resulting  in inconsistent data warehouse extents. We propose a registry-based solution strategy that addresses  this problem by employing a registry agent responsible for establishing one unique order for the propagation  of updates from the base sources to the data warehouses. With this solution, individual data  warehouse managers can still maintain their respective extents autonomously and independently from  each other, thus allowing them to apply any of the existing incremental maintenance algo...
DB
32746
Learning Decision Trees for Loss Minimization in Multi-Class Problems Many machine learning applications require classifiers that minimize an asymmetric loss function rather than the raw misclassification rate. We study methods for modifying C4.5 to incorporate arbitrary loss matrices. One way to incorporate loss information into C4.5 is to manipulate the weights assigned to the examples from different classes. For 2-class problems, this works for any loss matrix, but for k ? 2 classes, it is not sufficient. Nonetheless, we ask what is the set of class weights that best approximates an arbitrary k \Theta k loss matrix, and we test and compare several methods: a wrapper method and some simple heuristics. The best method is a wrapper method that directly optimizes the loss using a hold-out data set. We define complexity measure for loss matrices and show that this measure can predict when more efficient methods will suffice and when the wrapper method must be applied. 1 Introduction  For most of the history of machine learning research, a central goal has ...
ML
grundy01developing
Developing Adaptable User Interfaces for Component-based Systems Developing software components with user interfaces that can be adapted to diverse reuse situations is challenging. Examples of such adaptations include extending, composing and reconfiguring multiple component user interfaces, and adapting component user interfaces to particular user preferences, roles and subtasks. We describe our recent work in facilitating such adaptation via the concept of user interface aspects, which facilitate effective component user interface design and realisation using an extended, component-based software architecture.  1. Introduction  Component-based software applications are composed from diverse software components to form an application [1, 14, 16, 17]. Typically many of these components have been developed separately, with no knowledge of the user interfaces of other components they may be composed with. This can result in component-based applications with inappropriate, inconsistent interfaces.  For example, two components with user interfaces that ...
HCI
ozden98cyclic
Cyclic Association Rules We study the problem of discovering association rules that display regular cyclic variation over time. For example, if we compute association rules over monthly sales data, we may observe seasonal variation where certain rules are true at approximately the same month each year. Similarly, association rules can also display regular hourly, daily, weekly, etc., variation that is cyclical in nature. We demonstrate that existing methods cannot be naively extended to solve this problem of cyclic association rules. We then present two new algorithms for discovering such rules. The first one, which we call the sequential algorithm, treats association rules and cycles more or less independently. By studying the interaction between association rules and time, we devise a new technique called cycle pruning, which reduces the amount of time needed to find cyclic association rules. The second algorithm, which we call the interleaved algorithm, uses cycle pruning and other optimization techniques f...
DB
5749
Experiments In Information Retrieval From Spoken Documents This paper describes the experiments performed as part of the TREC-97 Spoken Document Retrieval Track. The task was to pick the correct document from 35 hours of recognized speech documents, based on a text query describing exactly one document. Among the experiments we described here are: Vocabulary size experiments to assess the effect of words missing from the speech recognition vocabulary; experiments with speech recognition using a stemmed language model; using confidence annotations that estimate of the correctness of each recognized word; using multiple hypotheses from the recognizer. And finally we also measured the effects of corpus size on the SDR task. Despite fairly high word error rates, information retrieval performance was only slightly degraded for speech recognizer transcribed documents. 1. INTRODUCTION  For the first time, the 1997 Text REtrieval Conference (TREC97) included an evaluation track for information retrieval on spoken documents. In this paper, we describe ...
IR
333040
On the Expressiveness of Distributed Leasing in Linda-like Coordination Languages S. All local authors can be reached via e-mail at the address last-name@cs.unibo.it. Questions and comments should be addressed to tr-admin@cs.unibo.it.  Recent Titles from the UBLCS Technical Report Series  99-2 A Theory of Efficiency for Markovian Processes, M. Bernardo, W.R. Cleaveland, February 1999 (Revisied March 2000).  99-3 A Reliable Registry for the Jgroup Distributed Object Model, A. Montresor, March 1999.  99-4 Comparing the QoS of Internet Audio Mechanisms via Formal Methods, A. Aldini, M. Bernardo, R. Gorrieri, M. Roccetti, March 1999.  99-5 Group-Enhanced Remote Method Invocations, A. Montresor, R. Davoli, O. Babao glu, April 1999.  99-6 Managing Complex Documents Over the WWW: a Case Study for XML, P. Ciancarini, F. Vitali, C. Mascolo, April 1999.  99-7 Data-Flow Hard Real-Time Programs: Scheduling Processors and Communication Channels in a Distributed Environment, R. Davoli, F. Tamburini, April 1999.  99-8 The MPS Computer System Simulator, M. Morsiani, R. Davoli, Apri...
HCI
rui00optimizing
Optimizing Learning in Image Retrieval Combining learning with vision techniques in interactive image retrieval has been an active research topic during the past few years. However, existing learning techniques either are based on heuristics or fail to analyze the working conditions. Furthermore, there is almost no in depth study on how to effectively learn from the users when there are multiple visual features in the retrieval system. To address these limitations, in this paper, we present a vigorous optimization formulation of the learning process and solve the problem in a principled way. By using Lagrange multipliers, we have derived explicit solutions, which are both optimal and fast to compute. Extensive comparisons against state-ofthe-art techniques have been performed. Experiments were carried out on a large-size heterogeneous image collection consisting of 17,000 images. Retrieval performance was tested under a wide range of conditions. Various evaluation criteria, including precision-recall curve and rank measure, have demonstrated the effectiveness and robustness of the proposed technique. 1.
ML
koster99normalization
Normalization and matching in the DORO system This paper is concerned with the use of linguistically motivated phrases as indexing terms in Information Retrieval applications. Apart from the conventional noun phrases, we propose to use verb phrases as index terms for text classification. Techniques for phrase matching through syntactic normalization and semantical matching are described. In particular, we show how to perform syntactic normalization of phrases in order to enhance recall. Semantical normalization is based on lexico-semantical relations, taking into account certain properties of the classification algorithms used. The ideas described here are being implemented in the Document Routing system DORO, in which statistical learning algorithms are applied to document profiles consisting of phrases. This paper describes the rationale behind work in progress, rather than presenting final results.
IR
llc00deep
The Deep Web: Surfacing Hidden Value this document is preliminary. BrightPlanet plans future revisions as better information and documentation is obtained. We welcome submission of improved information and statistics from others involved with the "deep" Web. Mata Hari is a registered trademark and BrightPlanet^TM, CompletePlanet^TM, LexiBot^TM, search filter^TM and A Better Way to Search^TM are pending trademarks of BrightPlanet.com LLC. All other trademarks are the respective property of their registered owners. 2000 BrightPlanet.com LLC. All rights reserved. The Deep Web: Surfacing Hidden Value iii
IR
375253
Context-Aware Telephony over WAP . In this paper we introduce a novel approach to share context to enhance the social quality of remote mobile communication. We provide an analysis of how people start a conversation in situations where they meet physically, especially looking on the influence of the situation. Than this is compared to the way remote communication is initiated using mobile phones. The lack of knowledge about the situation on the other end leads to initiation of calls that are not appropriate in the situation. The solution we propose is to exchange context information before initiating the call. We implemented this concept using the Wireless Application Protocol (WAP). The WML-based Application Context-Call offers a phone interface that provides information about the receiver when setting up a call. The caller can than decide based on that information to place the call, to leave a message or to cancel the call. Privacy issues that arise from this technology are discussed, too. Keywords: Contex...
HCI
45960
Blobworld: Image segmentation using Expectation-Maximization and its application to image querying AbstractÐRetrieving images from large and varied collections using image content as a key is a challenging and important problem. We present a new image representation that provides a transformation from the raw pixel data to a small set of image regions that are coherent in color and texture. This ªBlobworldº representation is created by clustering pixels in a joint color-texture-position feature space. The segmentation algorithm is fully automatic and has been run on a collection of 10,000 natural images. We describe a system that uses the Blobworld representation to retrieve images from this collection. An important aspect of the system is that the user is allowed to view the internal representation of the submitted image and the query results. Similar systems do not offer the user this view into the workings of the system; consequently, query results from these systems can be inexplicable, despite the availability of knobs for adjusting the similarity metrics. By finding image regions that roughly correspond to objects, we allow querying at the level of objects rather than global image properties. We present results indicating that querying for images using Blobworld produces higher precision than does querying using color and texture histograms of the entire image in cases where the image contains distinctive objects. Index TermsÐSegmentation and grouping, image retrieval, image querying, clustering, Expectation-Maximization. 1
DB
307090
Implicit Interest Indicators Recommender systems provide personalized suggestions about  items that users will find interesting. Typically, recommender systems require a user interface that can "intelligently" determine the interest of a user and use this information to make suggestions. The common solution, "explicit ratings", where users tell the system what they think about a piece of information, is well-understood and fairly precise. However, having to stop to enter explicit ratings can alter normal patterns of browsing and reading. A more "intelligent" method is to use implicit ratings, where a rating is obtained by a method other than obtaining it directly from the user. These implicit interest indicators have obvious advantages, including removing the cost of the user rating, and that every user interaction with the system can contribute to an implicit rating.  Current recommender systems mostly do not use implicit ratings, nor is the ability of implicit ratings to predict actual user interest well-unders...
IR
sullivan99model
A Model of Collision Perception for Real-Time Animation A model of human visual perception of collisions is presented, based on twodimensional measures of eccentricity and separation. The model is validated by performing psychophysical experiments. We demonstrate the feasibility of using this model as the basis for perceptual scheduling of interruptible collision detection in a realtime animation of large numbers of visually homogeneous objects. The user's point of fixation may be either tracked or estimated. By using a priority queue scheduling algorithm, perceived collision inaccuracy was approximately halved. The ideas presented are applicable to other tasks where the processing of fine detail leads to a computational bottleneck.
HCI
muslea99hierarchical
A Hierarchical Approach to Wrapper Induction With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of easier extraction tasks. We introduce an inductive algorithm, stalker, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that stalker does significantly better then other approaches; on one hand, stalker requires up to two orders of magnitude fewer examples than other algorithms, while on the other hand...
ML
227347
Evolution of a Central Pattern Generator for the Swimming and Trotting Gaits of the Salamander This paper presents the development of neural controllers for the swimming and the trotting of a salamanderlike animat. Using a genetic algorithm (GA), we extend a connectionist model of the central pattern generator (CPG) controlling the swimming of a lamprey [1] to control the locomotion of a 2D mechanical simulation of a salamander. We study in particular what kind of neural connectivity can produce the traveling undulation of the trunk during swimming and the standing S-wave undulation during trotting. Using a GA, we evolve neural controllers which are made of two segmental oscillators controlling the fore- and hindlimb muscles, which project to a lamprey-like CPG for the trunk muscles. CPGs are successfully evolved which exhibit the swimming gait when external excitatory drive is applied only to the trunk CPG, and the trotting gait when external drive is applied to both the limb and the trunk CPGs. For both types of gaits, the speed of locomotion can be varied with the amplitude o...
ML
23217
Results on Reasoning about Updates in Transaction Logic . Transaction Logic was designed as a general logic of state change for deductive databases and logic programs. It has a model theory, a proof theory, and its Horn subset can be given a procedural interpretation. Previous work has demonstrated that the combination of declarative semantics and procedural interpretation turns the Horn subset of Transaction Logic into a powerful language for logic programming with updates [BK98,BK94,BK93,BK95]. In this paper, we focus not on the Horn subset, but on the full logic, and we explore its potential as a formalism for reasoning about logic programs with updates. We first develop a methodology for specifying properties of such programs, and then provide a sound inference system for reasoning about them, and conjecture a completeness result. Finally, we illustrate the power of the inference system through a series of examples of increasing difficulty. 1 Introduction  Updates are a crucial component of any database programming language. Even the si...
DB
wolf00expert
Expert System Technology in Observing Tools Over the past two years, the Scientist's Expert Assistant team from NASA's Goddard Space Flight Center and the Space Telescope Science Institute has been prototyping tools to support General Observer proposal development for the Hubble Space Telescope and the Next Generation Space Telescope. One aspect of this effort has been the exploration of the use of expert systems in guiding the user in preparing their observing program. The initial goal was to provide the user with a question-and-answer style of interaction where the software would "interview" the user for their science needs and recommend instrument settings. This design ultimately failed. The reasons for this failure, and the resulting evolution of our approach, are an interesting case study in the use of expert system technology for observing tools. Although the interview approach failed we felt that expert systems can still be used in the tools environment. This paper describes our current approach to the use of expert syste...
AI
burkhard99bdi
BDI Design Principles and Cooperative Implementation - A Report on RoboCup Agents This report discusses two major views on BDI deliberation for autonomous agents. The first view is a rather conceptual one, presenting general BDI design principles, namely heuristic options, decomposed reasoning and layered planning, which enable BDI deliberation in realtime domains. The second view is focused on the practical application of the design principles in RoboCup Simulation League. This application not only evaluates the usefulness in deliberation but also the usefulness in rapid cooperative implementation. We compare this new approach, which has been used in the Vice World Champion team AT Humboldt 98, to the old approach of AT Humboldt 97, and we outline the extensions for AT Humboldt 99, which are still under work. Conditions faced by deliberation in multi agent contexts differ significantly from the basic assumption of classical AI search and planning. Traditional game playing methods for example assume a static well-known setting and a fixed round-based interaction of ...
Agents
24549
The FERET Evaluation Methodology for Face-Recognition Algorithms Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests. To date, 14,126 images from 1199 individuals are included in the FERET database, which is divided into development and sequestered portions of the database. In September 1996, the FERET program administered the third in a series of FERET face-recognition tests. The primary objectives of the third test were to (1) assess the state of the art, (2) identify future areas of research, and (3) measure algorithm performance. 1 Introduction  Over the last decade, face recognition has become an active area of research in computer vision, neuroscience, and psychology. Progress has advanced to the point that face-recognition systems are being demonstrated in real-...
DB
barve99modeling
Modeling and optimizing I/O throughput of multiple disks on a bus In modern I/O architectures, multiple disk drives are attached to each I/O controller. A study of the performance of such architectures under I/O-intensive workloads has revealed a performance impairment that results from a previously unknown form of convoy behavior in disk I/O. In this paper, we describe measurements of the read performance of multiple disks that share a SCSI bus under a heavy workload, and develop and validate formulas that accurately characterize the observed performance (to within 12 % on several platforms for I/O sizes in the range 16{128 KB). Two terms in the formula clearly characterize the lost performance seen in our experiments. We describe techniques to deal with the performance impairment, via user-level workarounds that achieve greater overlap of bus transfers with disk seeks, and that increase the percentage of transfers that occur at the full bus bandwidth rather than at the lower bandwidth of a disk head. Experiments show bandwidth improvements of 10-20 % when using these user-level techniques, but only in the case of large I/Os.  
DB
li01webdocument
Web-Document Prediction And Presending Using Association Rule Sequential Classifiers An important data source for data mining is the web-log data that traces the user's web browsing actions. From the web logs, one can build prediction models that predict with high accuracy the user's next requests based on past behavior. To do this with the traditional classification and association rule methods will cause a number of serious problems due to the extremely large data size and the rich domain knowledge that must be applied. Most web log data are sequential in nature and exhibit the "most recent--most important" behavior. To overcome this difficulty, we examine two important dimensions of building prediction models, namely the type of antecedents of rules and the criterion for selecting prediction rules. This thesis proposes a better overall method for prediction model representation and refinement. We show empirically on realistic web log data that the proposed model dramatically outperforms previous ones. How to apply the learned prediction model to the task of presending web documents is also demonstrated.  iv  Dedication  To MY MOTHER only v  Acknowledgments  I would like to thank my senior supervisor, Dr. Qiang Yang, for his guidance, encouragement and invaluable advice during my research. His enthusiasm and devotion to research have given me great inspiration to accomplish this thesis. It would be hard to imagine finishing this thesis without his continuous support. My gratitude also goes to my supervisor Dr. Ke Wang and the external examiner, Dr. Jiawei Han, who share with me their boundless knowledge in the area of Data Mining, and provide me valuable feedback on my thesis.  I also gratefully acknowledge the unselfish help given to me by my old friend Henry Zhang. I have always learnt a lot from his amazing wisdom and creative thought over the crit...
DB
almeida01analyzing
Analyzing Web Robots and Their Impact on Caching Understanding the nature and the characteristics of Web robots is an essential step to analyze their impact on caching. Using a multi-layer hierarchical workload model, this paper presents a characterization of the workload generated by autonomous agents and robots. This characterization focuses on the statistical properties of the arrival process and on the robot behavior graph model. A set of criteria is proposed for identifying robots in real logs. We then identify and characterize robots from real logs applying a multi-layered approach. Using a stack distance based analytical model for the interaction between robots and Web site caching, we assess the impact of robots' requests on Web caches. Our analyses point out that robots cause a significant increase in the miss ratio of a server-side cache. Robots have a referencing pattern that completely disrupts locality assumptions. These results indicate not only the need for a better understanding of the behavior of robots, but also the need of Web caching policies that treat robots' requests differently than human generated requests.
IR
nie01joint
Joint Optimization of Cost and Coverage of Query Plans in Data Integration Existing approaches for optimizing queries in data integration use decoupled strategies--attempting to optimize coverage and cost in two separate phases. Since sources tend to have a variety of access limitations, such phased optimization of cost and coverage can unfortunately lead to expensive planning as well as highly inefficient plans. In this paper we present techniques for joint optimization of cost and coverage of the query plans. Our algorithms search in the space of parallel query plans that support multiple sources for each subgoal conjunct. The refinement of the partial plans takes into account the potential parallelism between source calls, and the binding compatibilities between the sources included in the plan. We start by introducing and motivating our query plan representation. We then briefly review how to compute the cost and coverage of a parallel plan. Next, we provide both a System-R style query optimization algorithm as well as a greedy local search algorithm for searching in the space of such query plans. Finally we present a simulation study that demonstrates that the plans generated by our approach will be significantly better, both in terms of planning cost, and in terms of plan execution cost, compared to the existing approaches.  1. 
DB
freund98self
Self Bounding Learning Algorithms Most of the work which attempts to give bounds on the generalization error of the hypothesis generated by a learning algorithm is based on methods from the theory of uniform convergence. These bounds are a-priori bounds that hold for any distribution of examples and are calculated before any data is observed. In this paper we propose a different approach for bounding the generalization error after the data has been observed. A self-bounding learning algorithm is an algorithm which, in addition to the hypothesis that it outputs, outputs a reliable upper bound on the generalization error of this hypothesis. We first explore the idea in the statistical query learning framework of Kearns [10]. After that we give an explicit self bounding algorithm for learning algorithms that are based on local search. 1 INTRODUCTION Most of the work on the sample complexity of learning is based on uniform convergence theory and attempts to give uniform a-priori bounds. A uniform a-priori bound is a guar...
ML
li99multifaceted
Multifaceted Object Modeling with Roles: A Comprehensive Approach In conventional object-oriented (O-O) database systems, it is assumed silently that fundamental object types and inter-object relationships can be classfied statically, prescribing basic structural and behavioral properties for all the objects in the database. Such a classification-based approach falls short of supporting emerging data-intensive applications requiring more advanced dynamic capabilities. One of such advanced capabilities is the support of modeling subjectivity -- the ability to (among others) provide multiple perspectives of application objects or to model socalled "multifaceted objects" which, on different stages, can exhibit different forms and behavior. This article describes an extended O-O approach that we have been investigating for this purpose. Advanced features embodied by dynamic and versatile role facilities have been introduced into a conventional object-oriented model, which facilitates specifying and modeling such applications in a natural, incremental and...
DB
koch00integrating
Integrating Community Services - A Common Infrastructure Proposal Computer based community systems can provide powerful support in knowledge transfer, both in the direct exchange of information and in finding people to exchange information with. Several such information exchange and expert finding services have already been implemented. The problem of current approaches is, that they are not able to exchange data with each other. User profile information and contributed information have to be entered separately for every community-based information system. So, what one system has learned is not available for other systems. In this paper we present our ideas for a community services infrastructure as an enabling technology for further integration of community related services.
IR
martelli99specification
Specification and Simulation of Multi-Agent Systems in CaseLP Nowadays software applications are characterized by a great complexity. It arises from the need of reusing existing components and properly integrating them. The distribution of the involved entities and their heterogeneity makes it very useful the adoption of the agent-oriented technology. The paper presents the state-of-the-art of CaseLP, an experimental logic-based prototyping environment for multi-agent systems. CaseLP provides a prototyping method and a set of tools and languages which support the prototype realization. At the system specification level, an architectural description language can be adopted to describe the prototype in terms of agents classes, instances, their provided and required services and their communication links. At the agent specification level, a rule-based, not executable language can be used to easily define reactive and proactive agents. An executable, linear logic language can define more sophisticated agents and the system in which they operate. At t...
Agents
nguyen98strict
Strict Archimedean t-Norms and t-Conorms as Universal Approximators In knowledge representation, when we have to use logical connectives, various continuous t-norms and t-conorms are used. In this paper, we show that every continuous t-norm and t-conorm can be approximated, to an arbitrary degree of accuracy, by a strict Archimedean t-norm (t-conorm). Address correspondence to Vladik Kreinovich, Department of Computer Science, University of Texas at El Paso, El Paso, TX 79968, USA, email vladik@cs.utep.edu. International Journal of Approximate Reasoning 199? ?:?--? c fl 199? Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/9?/$7.00  2 1. INTRODUCTION Brief idea. When we represent expert knowledge in expert systems and in intelligent control, it is important to adequately describe not only the experts' statements themselves, but also the experts' degrees of confidence in the corresponding statements. It is also important to adequately describe which operations with these degrees of confidence are best representing the expe...
AI
riedel98active
Active Storage for Large-Scale Data Mining and Multimedia The increasing performance and decreasing cost of processors and memory are causing system intelligence to move into peripherals from the CPU. Storage system designers are using this trend toward “excess ” compute power to perform more complex processing and optimizations inside storage devices. To date, such optimiza-tions have been at relatively low levels of the stor-age protocol. At the same time, trends in storage density, mechanics, and electronics are eliminat-ing the bottleneck in moving data off the media and putting pressure on interconnects and host processors to move data more efficiently. We pro-pose a system called Active Disks that takes advantage of processing power on individual disk drives to run application-level code. Moving por-tions of an application’s processing to execute directly at disk drives can dramatically reduce data traffic and take advantage of the storage par-allelism already present in large systems today. We discuss several types of applications that would benefit from this capability with a focus on the areas of database, data mining, and multime-dia. We develop an analytical model of the speed-ups possible for scan-intensive applications in an Active Disk system. We also experiment with a prototype Active Disk system using relatively low-powered processors in comparison to a data-base server system with a single, fast processor. Our experiments validate the intuition in our model and demonstrate speedups of 2x on 10 disks across four scan-based applications. The model promises linear speedups in disk arrays of hundreds of disks, provided the application data is large enough. Thts research was sponsored by DARPAflTO through Order D306, and issued by Indian Head Division, NSWC under contract NOO174-96-0002. Additional support was prowded by NSF under grants EEC-94-02384 and IRI-9625428 and NSF, ARPA and NASA under NSF Agreement IRI-9411299. We are also Indebted to generous contributions from the member companies of the Parallel Data Consortium: Hewlett-
ML
papadias01efficient
Efficient OLAP Operations in SPatial Data Warehouses Abstract. Spatial databases store information about the position of individual objects in space. In many applications however, such as traffic supervision or mobile communications, only summarized data, like the number of cars in an area or phones serviced by a cell, is required. Although this information can be obtained from transactional spatial databases, its computation is expensive, rendering online processing inapplicable. Driven by the non-spatial paradigm, spatial data warehouses can be constructed to accelerate spatial OLAP operations. In this paper we consider the star-schema and we focus on the spatial dimensions. Unlike the non-spatial case, the groupings and the hierarchies can be numerous and unknown at design time, therefore the wellknown materialization techniques are not directly applicable. In order to address this problem, we construct an ad-hoc grouping hierarchy based on the spatial index at the finest spatial granularity. We incorporate this hierarchy in the lattice model and present efficient methods to process arbitrary aggregations. We finally extend our technique to moving objects by employing incremental update methods. 1
DB
10227
Analog Neural Nets with Gaussian or other Common Noise Distributions cannot Recognize Arbitrary Regular Languages We consider recurrent analog neural nets where the output of each gate is subject to Gaussian noise, or any other common noise distribution that is nonzero on a large set. We show that many regular languages cannot be recognized by networks of this type, and we give a precise characterization of those languages which can be recognized. This result implies severe constraints on possibilities for constructing recurrent analog neural nets that are robust against realistic types of analog noise. On the other hand we present a method for constructing feedforward analog neural nets that are robust with regard to analog noise of this type. 1 Introduction  A fairly large literature (see [Omlin, Giles, 1996] and the references therein) is devoted to the construction of analog neural nets that recognize regular languages. Any physical realization of the analog computational units of an analog neural net in technological or biological systems is bound to encounter some form of "imprecision" or an...
ML
horling99using
Using Diagnosis to Learn Contextual Coordination Rules Knowing when and how to communicate and coordinate with other agents in a multi-agent system is an important efficiency and reliability question. Contextual rules governing this communication must be provided to the agent, or generated at runtime through environmental analysis. In this paper we describe how the TAEMS task modeling language is used to encode such contextual coordination rules, and how runtime diagnosis can be used to dynamically update them. Overview  Communication and coordination is an essential component of most complex multi-agent systems. Contention over shared resources, the desire to employ remote information and the need to coordinate interrelated activities may each require some sort of information transfer between agents to be resolved. To this end, individual actors in a multi-agent system must be able to explicitly or implicitly communicate requests and results, desires and beliefs, to make the system an efficient and cohesive unit. Thus, a set of situation-...
Agents
flake02selforganization
Self-Organization and Identification of Web Communities The vast improvement in information access is not the only advantage resulting from the increasing percentage of hyperlinked human knowledge available on the Web. Additionally, much potential exists for analyzing interests and relationships within science and society. However, the Web's decentralized and unorganized nature hampers content analysis. Millions of individuals operating independently and having a variety of backgrounds, knowledge, goals and cultures author the information on the Web. Despite the Web's decentralized, unorganized, and heterogeneous nature, our work shows that the Web self-organizes and its link structure allows efficient identification of communities. This self-organization is significant because no central authority or process governs the formation and structure of hyperlinks.
IR
530815
Versus: a Model for a Web Repository Web data warehouses can prove useful to applications that process large amounts of Web data. Versus is a model for a Repository for Web data management applications, supporting object versioning and distributed operation. Versus applications control the distribution, and the integration of data. This paper presents the design of Versus and our prototype implementation. Keywords: Web data repository, versioning, distributed database.
DB
wah00constrained
Constrained Genetic Algorithms and their Applications in Nonlinear Constrained Optimization This chapter presents a framework that unifies various search mechanisms for solving constrained nonlinear programming (NLP) problems. These problems are characterized by functions that are not necessarily differentiable and continuous. Our proposed framework is based on the first-order necessary and sufficient condition developed for constrained local minimization in discrete space that shows the equivalence between discrete-neighborhood saddle points and constrained local minima. To look for discrete-neighborhood saddle points, we formulate a discrete constrained NLP in an augmented Lagrangian function and study various mechanisms for performing ascents of the augmented function in the original-variable subspace and descents in the Lagrange-multiplier subspace. Our results show that CSAGA, a combined constrained simulated annealing and genetic algorithm, performs well when using crossovers, mutations, and annealing to generate trial points. Finally, we apply iterative deepening to de...
AI
307172
Declarative Semantics of Belief Queries in MLS Deductive Databases (Extended Abstract) A logic based language, called MultiLog, for multi level secure relational databases has recently been proposed. It has been shown that MultiLog is capable of capturing the notion of user belief, of ltering unwanted and \useless" information in its proof theory. Additionally, it can guard against a previously unknown security breach { the so called surprise stories. In this paper, we outline a possible approach to a declarative characterization of belief queries in MultiLog in a very informal manner. We show that for \simple programs" with belief queries, the semantics is rather straight forward. Semantics for the general Horn programs may be developed based on the understanding of the model theoretic characterization of belief queries developed in this paper.  Keywords: Multi level security, belief queries, declarative semantics, completeness.  Introduction  In a recent research, Jukic and Vrbsky [8] demonstrate that users in the relational MLS model potentially have a cluttered view...
DB
lopez01framework
A Framework for Norm-Based Inter-Agent Dependence A significant class of agent architectures designed for operation in a multi-agent world choose their next actions or plans based on a limited analysis
Agents
flake00efficient
Efficient Identification of Web Communities We dene a community on the web as a set of sites that have more links (in either direction) to members of the community than to non-members. Members of such a community can be eciently identied in a maximum ow / minimum cut framework, where the source is composed of known members, and the sink consists of well-known non-members. A focused crawler that crawls to a xed depth can approximate community membership by augmenting the graph induced by the crawl with links to a virtual sink node. The effectiveness of the approximation algorithm is demonstrated with several crawl results that identify hubs, authorities, web rings, and other link topologies that are useful but not easily categorized. Applications of our approach include focused crawlers and search engines, automatic population of portal categories, and improved ltering.  Categories and Subject Descriptors  H.2.8 [Database Management]: Database Applications| data mining ; H.3.3 [Information Storage and Retrieval]: Information...
IR
szummer02partially
Partially labeled classification with Markov random walks To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems.
IR
zaki00sequence
Sequence Mining in Categorical Domains: Incorporating Constraints We present cSPADE, an efficient algorithm for mining frequent sequences considering a variety of syntactic constraints. These take the form of length or width limitations on the sequences, minimum or maximum gap constraints on consecutive sequence elements, applying a time window on allowable sequences, incorporating item constraints, and finding sequences predictive of one or more classes, even rare ones. Our method is efficient and scalable. Experiments on a number of synthetic and real databases show the utility and performance of considering such constraints on the set of mined sequences. 1.
DB
sukhatme99embedding
Embedding Robots into the Internet With the explosive growth of embedded computing hardware it is possible to conceive many new networked robotic applications for diverse domains ranging from urban search and rescue to house cleaning. Designing reliable software for such systems is a challenging problem. However, communication can facilitate robotics by reducing uncertainty, and robotics can facilitate communication by providing physical mobility. In this article we focus on methods for control and coordination of embedded mobile systems (robots) which interact with other computers on a wireless network situated in human environments.  1 Introduction  Ubiquitous embedded computing [12] is here to stay. Information appliances, laptops, palmtops, and wearable computers are examples of the first wave of this new era. Two factors have contributed to the phenomenal increase in the number of computers in our environment: Moore's law and improved network connectivity. It is now increasingly accepted that appliances of the futu...
HCI
madden01fjording
Fjording the Stream: An Architecture for Queries over Streaming Sensor Data If  industry  visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave  very differently  from traditional database sources:  they  have intermittent connectivity  , are limited  by  severe power constraints, and  ty pically  sample  periodically  and push immediately , keeping no record of historical information. These limitations make traditional database sy tems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over  many  sensors, and show how it can be used to limit sensor resource demands while maintaining high  query  throughput. We evaluate our architecture using traces from a network of traffic sensors deployM on Interstate 80 near  Berkeley  and present performance results that show how  query  throughput, communication costs, and power consumption are  necessarily  coupled in sensor environments.
DB
stanoi99decentralized
Decentralized Incremental Maintenance of Multi-View Data Warehouses (Extended Abstract) )  I. Stanoi D. Agrawal A. El Abbadi Department of Computer Science, University of California Santa Barbara, CA 93106 1 Introduction  Decision support systems make up a big percentage of data base servers. Presently, their size increases due to the necessity of more detailed information, and the extended time range of interest. As a result, data mining queries also span larger sets of data. To achieve fast response time, a subset of the relevant information is sometimes materialized in views, separate from the database sources [4]. A data warehouse is an example of storage that integrates information from multiple sources, which may be stand-alone databases as well as sites such as the Internet. Although data warehousing is a powerful concept for supporting analytical processing, building a data warehouse runs into several pragmatic problems. The cost of building a data warehouse that integrates disparate data sources in an organization can easily exceed millions of dollars. A more sev...
DB
128239
Categorisation by Context Assistance in retrieving of documents on the World Wide Web is provided either by search  engines, through keyword based queries, or by catalogues, which organise documents into  hierarchical collections. Maintaining catalogues manually is becoming increasingly difficult  due to the sheer amount of material, and therefore it will be necessary to resort to  techniques for automatic classification of documents. Classification is traditionally  performed by extracting information for indexing a document from the document itself. The  paper describes the technique of categorisation by context, which exploits the context  perceivable from the structure of HTML documents to extract useful information for  classifying the documents they refer to. We present the results of experiments with a  preliminary implementation of the technique.  1. INTRODUCTION  Most Web search engines (e.g. Altavista^TM [Altavista], HotBot^TM [HotBot], Excite^TM [Excite]) perform search based on the content of docume...
IR
flamm01barrier
Barrier Trees of Degenerate Landscapes The heights of energy barriers separating two (macro-)states are useful for estimating transition frequencies. In non-degenerate landscapes the decomposition of a landscape into basins surrounding local minima connected by saddle points is straightforward and yields a useful definition of macro-states. In this work we develop a rigorous concept of barrier trees for degenerate landscapes. We present a program that efficiently computes such barrier trees, and apply it to two well known examples of landscapes. Keywords: Fitness landscape, Potential energy surface, energy barrier, saddle points, degenerate states Dedicated to Peter Schuster on the occasion of his 60th birthday.
AI
102637
The Diagnosis Frontend of the dlv System This paper presents the Diagnosis Frontend of dlv, which is a knowledge representation system under development at the Technische Universität Wien. The kernel language of the system is an extension of disjunctive logic programming (DLP) by integrity constraints; it offers frontends to several advanced knowledge representation formalisms. The formal model of diagnosis employed in the frontend includes both abductive diagnosis (over DLP theories) and consistency-based diagnosis. For each of the two diagnosis modalities, generic diagnoses, single error diagnoses, and subset minimal diagnoses are considered. We illustrate the use of the frontend by showing the dlv encodings of several diagnosis problems. Thereafter, we discuss implementation issues. Diagnostic reasoning is implemented on the dlv engine through suitable translations of diagnostic problems into disjunctive logic programs, such that their stable models correspond to diagnoses. For the six kinds of diagnostic reasoning problems emerging from above, such reductions are provided
AI
daelemans97featurerelevance
A Feature-Relevance Heuristic for Indexing and Compressing Large Case Bases . This paper reports results with igtree, a formalism for indexing and compressing large case bases in Instance-Based Learning (ibl) and other lazy-learning techniques. The concept of information gain (entropy minimisation) is used as a heuristic feature-relevance function for performing the compression of the case base into a tree. igtree reduces storage requirements and the time required to compute classifications considerably for problems where current ibl approaches fail for complexity reasons. Moreover, generalisation accuracy is often similar, for the tasks studied, to that obtained with information-gain-weighted variants of lazy learning, and alternative approaches such as c4.5. Although  igtree was designed for a specific class of problems --linguistic disambiguation problems with symbolic (nominal) features, huge case bases, and a complex interaction between (sub)regularities and exceptions-- we show in this paper that the approach has a wider applicability when generalising i...
ML
beigl00mediacups
MediaCups: Experience with Design and Use of Computer-Augmented Everyday Artefacts Our view of ubiquitous computing is artefact-centred: in this view, computers are considered as secondary artefacts that enable items of everyday use as networked digital artefacts. This view is expressed in an artefact computing model and investigated in the Mediacup project, an evolving artefact computing environment. The Mediacup project provides insights into the augmentation of artefacts with sensing, processing, and communication capabilities, and into the provision of an open infrastructure for information exchange among artefacts. One of the artefacts studied is the Mediacup itself, an ordinary coffee cup invisibly augmented with computing and context-awareness. The Mediacup and other computeraugmented everyday artefacts are connected through a network infrastructure supporting loosely-coupled spatially-defined communication. Keywords Ubiquitous computing, digital artefacts, context-awareness, networking, embedded systems, Mediacup 1. INTRODUCTION Computers are becoming ubi...
HCI
mankoff00providing
Providing Integrated Toolkit-Level Support for Ambiguity in Recognition-Based Interfaces Recognition technologies are being used extensively in both the commercial and research worlds. But recognizers are still error-prone, and this results in performance problems and brittle dialogues. These problems are a barrier to acceptance and usefulness of recognition systems. Better interfaces to recognition systems, which can help to reduce the burden of recognition errors, are difficult to build because of lack of knowledge about the ambiguity inherent in recognition. We have extended a user interface toolkit in order to model and to provide structured support for ambiguity at the input event level [7]. This makes it possible to build re-usable interface components for resolving ambiguity and dealing with recognition errors. These interfaces can help to reduce the negative effects of recognition errors. By providing these components at a toolkit level, we make it easier for application writers to provide good support for error handling. And we can explore new types of interfaces for resolving a more varied range of ambiguity.
HCI
gray01dagents
D'Agents: Applications and Performance of a Mobile-Agent System D'Agents is a general-purpose mobile-agent system that has been used in several informationretrieval  applications. In this paper, we  rst examine one such application, operational support  for military  eld personnel, where D'Agents greatly simpli  es the task of providing ecient,  application-speci  c access to remote information resources. After describing the application, we  discuss the key dierences between D'Agents and most other mobile-agent systems, notably its  support for strong mobility and multiple agent languages. Finally, we derive a small, simple  application that is representative of many information-retrieval tasks, including those in the  example application, and use this application to compare the scalability of mobile agents and  traditional client/server approaches. The results con  rm and quantify the usefulness of mobile  code, and perhaps more importantly, con  rm that intuition about when to use mobile code is  usually correct. Although signi  cant additional experiments are needed to fully characterize the  complex mobile-agent performance space, the results here help answer the basic question of when  mobile agents should be considered at all, particularly for information-retrieval applications.
Agents
540380
C4-1: Building a community hierarchy for the Web based on Bipartite Graphs In this paper we propose an approach to extract and relate the communities by considering a community signature as a group of content creators that manifests itself as a set of interlinked pages. We abstract a community signature as a group of pages that form a dense bipartite graph (DBG), and proposed an algorithm to extract the DBGs from the given data set. Also, using the proposed approach, the extracted communities can be grouped to form a high-level communities. We apply the proposed algorithm on 10 GB TREC (Text REtrieval Conference) data set and extract a three-level community hierarchy. The extracted community hierarchy facilitates an easy analysis of low-level communities and provides a way to understand the sociology of the Web.
IR
293552
Macroscopic Observation of Multi-Robot Behavior : This paper presents a new approach to the observation and the control of the behavior of multiple autonomous robots. It is the microscopic observation expressed by dynamic equations that has been commonly employed to observe the multi-robot behavior. However, the approach has the difficulties in estimating the behavior and the mutual interactions of robots. Furthermore, it is hard to realize the system by checking up all the factors of the system. It seems that a macroscopic observation defined by state equations is efficient for recognizing the multiple robots behavior. Therefore, we would like to propose a quantitative observation approach. This attempt means the application of the thermodynamic macroscopic state values to the multi-robot systems. The advantage of this approach is that it enables us to observe the behavior of autonomous robots in real world by mapping the characteristic values of them in another conceptual state space. First, we discuss the implication of applying ...
Agents
249633
Requirements for a Translation between Knowledge-level Messages and the Database Structure This paper discusses the requirements of a translator between queries and assertions based on database-independent conceptualizations and their equivalents in the terms of the database structure. To cope with a large number of independent developers of software components that will have to access these databases, and who have different backgrounds and expertise, this translator will allow for several conceptualizations and vocabularies to coexist. It will be possible to individually customize these conceptualizations to keep them simple, concise, and well-aligned to the conceptualizations and jargon used by the domain experts. The translator will be accessed by other software components of the CRISTAL system that have to execute complex queries, like the Viewpoint Facility, user-supplied components ("User Code"), and the so-called ICIST GUI [6]. Schema-independent updates are most relevant for the user-supplied components.
DB
stanley01evolving
Evolving Neural Networks through Augmenting Topologies An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.
ML
balch94communication
Communication in Reactive Multiagent Robotic Systems Abstract. Multiple cooperating robots are able to complete many tasks more quickly and reliably than one robot alone. Communication between the robots can multiply their capabilities and e ectiveness, but to what extent? In this research, the importance of communication in robotic societies is investigated through experiments on both simulated and real robots. Performance was measured for three di erent types of communication for three di erent tasks. The levels of communication are progressively more complex and potentially more expensive to implement. For some tasks, communication can signi cantly improve performance, but for others inter-agent communication is apparently unnecessary. In cases where communication helps, the lowest level of communication is almost as e ective as the more complex type. The bulk of these results are derived from thousands of simulations run with randomly generated initial conditions. The simulation results help determine appropriate parameters for the reactive control system which was ported for tests on Denning mobile robots.
Agents
schneider00disseminating
Disseminating Trust Information in Wearable Communities : This paper describes a framework for managing and distributing trust information in a community of mobile and wearable computer users. Trust information in the form of reputations are used to aid users during their social interactions with the rest of the community.  Keywords: Wearable computing, social networks, social interaction, trust.  Introduction  In our modern world, the use of communication technologies like phone, fax and email has become commonplace. Despite this fact, most social interactions between individuals still occur when we meet people face-to-face. Many of our daily interactions are actually the result of a chance encounter, i.e. a situation in which we meet someone unexpectedly, for example in a hallway or an elevator. In most cases, the majority of the people we encounter every day we don't know and have never met before; however, some are familiar. Any encounter with another person, friend or stranger, is a chance for striking up a conversation and for exchang...
HCI
bauer99where
"Where Are You Pointing At?" A Study of Remote Collaboration in a Wearable Videoconference System This paper reports on an empirical study aimed at evaluating the utility of a reality-augmenting telepointer in a wearable videoconference system. Results show that using this telepointer a remote expert can effectively guide and direct a field worker's manual activities. By analyzing verbal communication behavior and pointing gestures, we were able to determine that experts overwhelmingly preferred pointing for guiding workers through physical tasks. 
HCI
critchlow98metadata
Meta-Data Based Mediator Generation Mediators are a critical component of any data warehouse; they transform data from source formats to the warehouse representation while resolving semantic and syntactic conflicts. The close relationship between mediators and databases requires a mediator to be updated whenever an associated schema is modified. Failure to quickly perform these updates significantly reduces the reliability of the warehouse because queries do not have access to the most current data. This may result in incorrect or misleading responses, and reduce user confidence in the warehouse. Unfortunately, this maintenance may be a significant undertaking if a warehouse integrates several dynamic data sources. This paper describes a meta-data framework, and associated software, designed to automate a significant portion of the mediator generation task and thereby reduce the effort involved in adapting to schema changes. By allowing the DBA to concentrate on identifying the modifications at a high level, instead of r...
DB
kushmerick02adaptive
Adaptive information extraction: Core technologies for information agents Introduction  For the purposes of this chapter, an information agent can be described as a distributed system that receives a goal through its user interface, gathers information relevant to this goal from a variety of sources, processes this content as appropriate, and delivers the results to the users. We focus on the second stage in this generic architecture. We survey a variety of information extraction techniques that enable information agents to automatically gather information from heterogeneous sources.  For example, consider an agent that mediates package-delivery requests. To satisfy such requests, the agent might need to retrieve address information from geographic services, ask an advertising service for freight forwarders that serve the destination, request quotes from the relevant freight forwarders, retrieve duties and legal constraints from government sites, get weather information to estimate transportation delays, etc.  Information extraction (IE) is a form of sh
IR
kushmerick00wrapper
Wrapper Induction: Efficiency and Expressiveness The Internet presents numerous sources of useful information---telephone directories, product catalogs, stock quotes, event listings, etc. Recently, many systems have been built that automatically gather and manipulate such information on a user's behalf. However, these resources are usually formatted for use by people (e.g., the relevant content is embedded in HTML pages), so extracting their content is difficult. Most systems use customized wrapper procedures to perform this extraction task. Unfortunately, writing wrappers is tedious and error-prone. As an alternative, we advocate wrapper induction, a technique for automatically constructing wrappers. In this article, we describe six wrapper classes, and use a combination of empirical and analytical techniques to evaluate the computational tradeoffs among them. We first consider expressiveness: how well the classes can handle actual Internet resources, and the extent to which wrappers in one class can mimic those in another. We then...
IR
roddick00beyond
Beyond Schema Versioning: A Flexible Model for Spatio-Temporal Schema Selection Schema versioning provides a mechanism for handling change in the structure of database systems and has been investigated widely, both in the context of static and temporal databases. With the growing interest in spatial and spatio-temporal data as well as the mechanisms for holding such data, the spatial context within which data items are formatted also becomes an issue. This paper presents a generalised model that accommodates temporal, spatial and spatio-temporal schema versioning within databases.
DB
226124
Ontobroker: How to Enable Intelligent Access to the WWW . The World Wide Web (WWW) is currently one of the most important  electronic information sources. However, its query interfaces and the provided  reasoning services are rather limited. Ontobroker consists of a number of  languages and tools that enhance query access and inference service in the  WWW. It provides languages to annotate web documents with ontological  information, to represent ontologies, and to formulate queries. The tool set of  Ontobroker allows us to access information and knowledge from the web and to  infer new knowledge with an inference engine based on techniques from logic  programming. This article provides several examples that illustrate these  languages and tools and the kind of service that is provided. We also discuss the  bottlenecks of our approach that stem from the fact that the applicability of  Ontobroker requires two time-consuming activities: (1) developing shared  ontologies that reflect the consensus of a group of web users and (2) annotating  we...
IR
164846
Experiments on Automatic Web Page Categorization for IR system This paper describes keyword-based Web page categorization. Our goal is to embed our categorization technique into information retrieval (IR) systems to facilitate the end-users' search task. In such systems, search results must be categorized faster, while keeping accuracy high. Our categorization system uses a knowledge base (KB) to assign categories to Web pages. The KB contains a set of characteristic keywords with weights by category, and is automatically generated from training texts. With the keyword-based approach, the algorithms to extract keywords and assign weights to them should be considered, because the algorithms affect strongly both categorization accuracy and processing speed. Furthermore, we must take two characteristics of Web pages into account: (1) the text length is very variable, which makes it harder to use statistics such as word frequency to calculate keyword weights, and (2) a huge number of distinct words are used, which makes the KB bigger and therefore pro...
IR
peters02using
Using Semantic Networks for Knowledge Representation in an Intelligent Environment Introduction  For many years now, research in intelligent spaces has grown, exploring different ways that a room can react to one or more users and their actions. As usage of these intelligent environments (IEs) grows, however, they will by necessity collect ever-increasing amounts of data about their users, in order to adapt to the user's desires. Information will be collected on the users' interests, who they communicate with, their location, web pages they visit, and numerous other details that we may not even notice. All this information needs to be collected and organized, so that the IE can make quick, correct assumptions about what the user would like to do next.  At the Intelligent Room project (Hanssens et al., 2002), we are beginning to define one such knowledge representation  (KR), using semantic networks as the basis for the representation. This creates inherent advantages, both in ease of adding and changing information as well as inference generation.  2. Knowledge Repre
HCI
dix01metaagent
Meta-Agent Programs There are numerous applications where an agent a needs to reason about the beliefs of another agent, as well as about the actions that other agents may take. In (21) the concept of an agent program is introduced, and a language within which the operating principles of an agent can be declaratively encoded on top of imperative data structures is dened. In this paper we rst introduce certain belief data structures that an agent needs to maintain. Then we introduce the concept of a Meta Agent Program (map), that extends the framework of (21; 19), so as to allow agents to perform metareasoning. We build a formal semantics for maps, and show how this semantics supports not just beliefs agent a may have about agent b's state, but also beliefs about agents b's beliefs about agent c's actions, beliefs about b's beliefs about agent c's state, and so on. Finally, we provide a translation that takes any map as input and converts it into an agent program such that there is a one-one correspondence between the semantics of the map and the semantics of the resulting agent program. This correspondence allows an implementation of maps to be built on top of an implementation of agent programs.
Agents
282608
Temporal Dependencies Generalized for Spatial and Other Dimensions . Recently, there has been a lot of interest in temporal granularity  , and its applications in temporal dependency theory and data mining. Generalization hierarchies used in multi-dimensional databases and OLAP serve a role similar to that of time granularity in temporal databases, but they also apply to non-temporal dimensions, like space. In this paper, we first generalize temporal functional dependencies for non-temporal dimensions, which leads to the notion of roll-up dependency (RUD). We show the applicability of RUDs in conceptual modeling and data mining. We then indicate that the notion of time granularity used in temporal databases is generally more expressive than the generalization hierarchies in multi-dimensional databases, and show how this surplus expressiveness can be introduced in non-temporal dimensions, which leads to the formalism of RUD with negation (RUD  :  ). A complete axiomatization for reasoning about RUD  :  is given. 1 Introduction  Generalization hierarchi...
DB
532865
Ephemeral and Persistent Personalization in Adaptive Information Access to Scholarly Publications on the Web We show how personalization techniques can be exploited to  implement more adaptive and effective information access systems in  electronic publishing. We distinguish persistent (or long term) and  ephemeral (or short term) personalization, and we describe how both of  them can be profitably applied in information filtering and retrieval systems  used, via a specialized Web portal, by physicists in their daily job. By means  of several experimental results, we demonstrate that persistent  personalization is needed and useful for information filtering systems, and  ephemeral personalization leads to more effective and usable information  retrieval systems.
IR
macskassy01intelligent
Intelligent Information Triage In many applications, large volumes of time-sensitive textual information require triage: rapid, approximate prioritization for subsequent action. In this paper, we explore the use of prospective indications of the importance of a time-sensitive document, for the purpose of producing better document filtering or ranking. By prospective, we mean importance that could be assessed by actions that occur in the future. For example, a news story may be assessed (retrospectively) as being important, based on events that occurred after the story appeared, such as a stock price plummeting or the issuance of many follow-up stories. If a system could anticipate (prospectively) such occurrences, it could provide a timely indication of importance. Clearly, perfect prescience is impossible. However, sometimes there is sufficient correlation between the content of an information item and the events that occur subsequently. We describe a process for creating and evaluating approximate information-triage procedures that are based on prospective indications. Unlike many informationretrieval applications for which document labeling is a laborious, manual process, for many prospective criteria it is possible to build very large, labeled, training corpora automatically. Such corpora can be used to train text classification procedures that will predict the (prospective) importance of each document. This paper illustrates the process with two case studies, demonstrating the ability to predict whether a news story will be followed by many, very similar news stories, and also whether the stock price of one or more companies associated with a news story will move significantly following the appearance of that story. We conclude by discussing how the comprehensibility of the learned classifiers can be critical to success. 1.
IR
tveit01survey
A survey of Agent-Oriented Software Engineering Agent-Oriented Software Engineering is the one of the most recent contributions to the field of Software Engineering. It has several benefits compared to existing development approaches, in particular the ability to let agents represent high-level abstractions of active entities in a software system. This paper gives an overview of recent research and industrial applications of both general high-level methodologies and on more specific design methodologies for industry-strength software engineering.
Agents
ishida99digital
Digital City Kyoto: Towards A Social Information Infrastructure . This paper proposes the concept of digital cities as a social  information infrastructure for urban life (including shopping, business,  transportation, education, welfare and so on). We propose the three layer  architecture for digital cities: a) the information layer integrates both WWW  archives and real-time sensory information related to the city, b) the interface  layer provides 2D and 3D views of the city, and c) the interaction layer assists  social interaction among people who are living/visiting in/at the city. We started  a three year project to develop a digital city for Kyoto, the old capital and  cultural center of Japan, based on the newest technologies including GIS, 3D,  animation, agents and mobile computing. This paper introduces the system  architecture and the current status of Digital City Kyoto.  1. Introduction  As the number of Internet users is continuing to increase, various community networks are being tested [1]. The Internet is used not only for research...
Agents
precup01offpolicy
Off-policy temporal-difference learning with function approximation We introduce the first algorithm for off-policy temporal-difference learning that is stable with linear function approximation. Off-policy learning is of interest because it forms the basis for popular reinforcement learning methods such as Q-learning, which has been known to diverge with linear function approximation, and because it is critical to the practical utility of multi-scale, multi-goal, learning frameworks such as options, HAMs, and MAXQ. Our new algorithm combines TD(λ) over state–action pairs with importance sampling ideas from our previous work. We prove that, given training under any ɛ-soft policy, the algorithm converges w.p.1 to a close approximation (as in Tsitsiklis and Van Roy, 1997; Tadic, 2001) to the action-value function for an arbitrary target policy. Variations of the algorithm designed to reduce variance introduce additional bias but are also guaranteed convergent. We also illustrate our method empirically on a small policy evaluation problem. Our current results are limited to episodic tasks with episodes of bounded length. 1 Although Q-learning remains the most popular of all reinforcement learning algorithms, it has been known since about 1996 that it is unsound with linear function approximation (see Gordon, 1995; Bertsekas and Tsitsiklis, 1996). The most telling counterexample, due to Baird (1995) is a seven-state Markov decision process with linearly independent feature vectors, for which an exact solution exists, yet 1 This is a re-typeset version of an article published in the Proceedings
ML
deraedt01three
Three Companions for Data Mining in First Order Logic Three companion systems, Claudien, ICL and Tilde, are  presented. They use a common representation for examples and hypotheses:  each example is represented by a relational database. This contrasts  with the classical inductive logic programming systems such as  Progol and Foil. It is argued that this representation is closer to attribute  value learning and hence more natural.  Furthermore, the three systems can be considered first order upgrades of  typical data mining systems, which induce association rules, classification  rules or decision trees respectively.  1 
ML
billard98experiments
Experiments on human-robot communication with Robota, an imitative learning and communicating doll robot. Imitation 1 and communication behaviours are important means of interaction between humans and robots. In experiments on robot teaching by demonstration, imitation and communication behaviours can be used by the demonstrator to drive the robot's attention to the demonstrated task. In a children game, they play an important role to engage the interaction between the child and the robot and to stimulate the child's interest. In this work, we study how imitation skills can be used for teaching a robot a symbolic communication system to describe its actions and perceptions. We report on experiments in which we study human-robot interactions using a doll robot. Robota is a robot, whose shape is similar to that of a doll, and which has the capacity to learn, imitate and communicate. Through simple phototaxis behaviour, the robot can imitate (mirror) the arms and head's movements of a demonstrator. The robot is controlled by a Dynamical Recurrent Associative memory Architecture (DRAMA), wh...
AI
wang00argumentationbased
Argumentation-Based Abduction in Disjunctive Logic Programming In this paper we propose an argumentation-based semantic framework,  called DAS, for disjunctive logic programming. The basic idea  is to translate a disjunctive logic program into an argumentationtheoretic  framework. One unique feature of our proposed framework  is to consider the disjunctions of negative literals as possible assumptions  so as to represent incomplete information. In our framework,  three semantics PDH, CDH and WFDH are defined by three kinds of  acceptable hypotheses to represent credulous, moderate and skeptical  reasoning in AI, respectively. Further more, our semantic framework  can be extended to a wider class than that of disjunctive programs  (called bi-disjunctive logic programs). In addition to being a first serious  attempt of establishing an argumentation-theoretic framework  for disjunctive logic programming, DAS integrates and naturally extends  many key semantics, such as the minimal models, EGCWA, the  well-founded model, and the disjunctive stable models. In particular,  novel and interesting argumentation-theoretic characterizations of  the EGCWA and the disjunctive stable semantics are shown. Thus  the framework presented in this paper does not only provide a new  way of performing argumentation (abduction) in disjunctive deductive  databases, but also is a simple, intuitive and unifying semantic framework  for disjunctive logic programming.
DB
jalali-sohi01multimodal
A Multimodal Shopping Assistant for Home E-Commerce Electronic Commerce has rapidly grown with the expansion of the Internet. E-commerce has also become a promising field for applying agent and Artificial Intelligence technologies. Software agents help to automate a variety of tasks including those involved in buying and selling products over the Internet. In this paper, we describe a multimodal intelligent Shopping Assistant developed in the EMBASSI project [1]. EMBASSI is a project involving more than twenty big German companies and sponsored by BMBF [3]. Its goal is not to focus on the unlimited possibilities of this technology, but rather on the individual prerequisites of the human in contact with it. Therefore, the user interfaces of a big class of appliances and systems, including shopping and ecommerce, need to be easily and efficiently accessible for everyone, taking into account psychological and ergonomic aspects and using innovative interaction techniques by realization of intelligent anthropomorphous assistants.  Keywords  Intelligent agents, assistant systems, e-commerce, multimodality, home shopping, mobile agents  1. 
Agents
schohn00less
Less is More: Active Learning with Support Vector Machines We describe a simple active learning heuristic  which greatly enhances the generalization behavior  of support vector machines (SVMs) on several  practical document classification tasks. We  observe a number of benefits, the most surprising  of which is that a SVM trained on a wellchosen  subset of the available corpus frequently  performs better than one trained on all available  data. The heuristic for choosing this subset is  simple to compute, and makes no use of information  about the test set. Given that the training  time of SVMs depends heavily on the training  set size, our heuristic not only offers better performance  with fewer data, it frequently does so  in less time than the naive approach of training  on all available data.  1. Introduction  There are many uses for a good document classifier --- sorting mail into mailboxes, filtering spam or routing news articles. The problem is that learning to classify documents requires manually labelling more documents than a typical...
IR
bradley98mathematical
Mathematical Programming for Data Mining: Formulations and Challenges This paper is intended to serve as an overview of a rapidly emerging research and applications area. In addition to providing a general overview, motivating the importance of data mining problems within the area of knowledge discovery in databases, our aim is to list some of the pressing research challenges, and outline opportunities for contributions by the optimization research communities. Towards these goals, we include formulations of the basic categories of data mining methods as optimization problems. We also provide examples of successful mathematical programming approaches to some data mining problems. keywords: data analysis, data mining, mathematical programming methods, challenges for massive data sets, classification, clustering, prediction, optimization. To appear: INFORMS: Journal of Compting, special issue on Data Mining, A. Basu and B. Golden (guest editors). Also appears as Mathematical Programming Technical Report 98-01, Computer Sciences Department, University of Wi...
ML
sure02ontoedit
OntoEdit: Collaborative Ontology Development for the Semantic Web Abstract. Ontologies now play an important role for enabling the semantic web. They provide a source of precisely defined terms e.g. for knowledge-intensive applications. The terms are used for concise communication across people and applications. Typically the development of ontologies involves collaborative efforts of multiple persons. OntoEdit is an ontology editor that integrates numerous aspects of ontology engineering. This paper focuses on collaborative development of ontologies with OntoEdit which is guided by a comprehensive methodology. 1
IR
jiang02path
Path Materialization Revisited: An Efficient Storage Model for XML Data XML is emerging as a new major standard for representing data on the world wide web. Several XML storage models have been proposed to store XML data in di#erent database management systems. The unique feature of model-mappingbased approaches is that no DTD information is required for XML data storage. In this paper, we present a new modelmapping -based storage model, called XParent. Unlike the existing work on model-mapping-based approaches that emphasized on converting XML documents to/from database schema and translation of XML queries into SQL queries, in this paper, we focus ourselves on the e#ectiveness of storage models in terms of query processing. We study the key issues that a#ect query performance, namely, storage schema design (storing XML data across multiple tables) and path materialization (storing path information in databases). We show that similar but di#erent storage models significantly a#ect query performance. A performance study is conducted using three data sets and query sets. The experimental results are presented.  Keywords: Semistructured data, XML database  1 
DB
nigam00analyzing
Analyzing the Effectiveness and Applicability Of Co-Training Recently there has been significant interest in supervised learning algorithms that combine labeled and unlabeled data for text learning tasks. The co-training setting [1] applies to datasets that have a natural separation of their features into two disjoint sets. We demonstrate that when learning from labeled and unlabeled data, algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not. When a natural split does not exist, co-training algorithms that manufacture a feature split may out-perform algorithms not using a split. These results help explain why co-training algorithms are both discriminative in nature and robust to the assumptions of their embedded classifiers.  Categories and Subject Descriptors  I.2.6 [Artificial Intelligence]: Learning; H.3.3 [Information  Storage and Retrieval]: Information Search and Retrieval---  Information Filtering  Keywords  co-training, expectation-maximization, learning with labeled and unlabeled...
IR
ben-ameur02multiitem
Multi-item Auctions for Automatic Negotiation Available resources can often be limited with regard to the number of demands. In this paper we propose an approach for solving this problem which consists of using the mechanisms of multi-item auctions for allocating the resources to a set of software agents. We consider the resource problem as a market in which there are vendor agents and buyer agents which trade on items representing the resources. These agents use multi-item auctions which are viewed here as a process of automatic negotiation, and implemented as a network of intelligent software agents. In this negotiation, agents exhibit different acquisition capabilities which let them act differently depending on the current context or situation of the market. For example, the ”richer ” an agent is, the more items it can buy, i.e. the more resources it can acquire. We present a model for this approach based on the English auction, then we discuss experimental evidence of such a model.
Agents
brumitt00better
Better Living Through Geometry Mark Weiser described ubiquitous computing as, “invisible, everywhere computing that does not live on a personal device of any sort, but is in the woodwork everywhere.”[4] The EasyLiving project[1] is concerned with development of an architecture and technologies for ubiquitous computing environments which allow the dynamic aggregation of diverse I/O devices into a single coherent user experience. Though the need for research in distributed computing, perception, and interfaces is widely recognized, the importance of an explicit geometric world model for enhancing the user’s experience of a ubiquitous computing system has not been well-articulated. This paper introduces three scenarios which benefit from geometric context and describes the EasyLiving Geometric Model.
HCI
fikes99distributed
Distributed Repositories of Highly Expressive Reusable Ontologies We describe an ongoing project to develop technology that will support collaborative construction and effective use of distributed large-scale repositories of highly expressive reusable ontologies. We are focusing on developing a distributed server architecture for ontology construction and use, representation formalisms that remove key barriers to expressing essential knowledge in and about ontologies, ontology construction tools, and tools for obtaining domain models for use in applications from large-scale ontology repositories. We are building on the results of the DARPA Knowledge Sharing Effort, specifically by using the Knowledge Interchange Format (KIF) as a core representation language and the Ontolingua system as a core ontology development environment. In order to enable distributed ontology repositories and services, we are developing a distributed server architecture for ontology construction and use based on ontology servers which provide access via a network API to the contents of ontologies and to information derivable from the contents by a general purpose reasoner. Ontology servers will be analogous to data base servers and will provide services including configuration
AI
ravat99towards
Towards Data Warehouse Design This paper focuses on data warehouse modelling. The conceptual model we defined, is based on object concepts extended with specific concepts like generic classes, temporal classes and archive classes. The temporal classes are used to store the detailed evolutions and the archive classes store the summarised data evolutions. We also provide a flexible concept allowing the administrator to define historised parts and non-historised parts into the warehouse schema. Moreover, we introduce constraints which configure the data warehouse behaviour and these various parts. To validate our propositions, we describe a prototype dedicated to the data warehouse design.  Keywords  Conceptual Data Warehouse Model, Temporal Data, Object  Modelling.  1. INTRODUCTION  In order to make long-term managerial decisions, companies have to exploit very large volumes of data, generally stored in their operational databases. The exploitation of these data is sometimes carried out in an empirical way using trad...
DB
schaller99reclustering
Reclustering of High Energy Physics Data The coming high-energy physics experiments will store Petabytes of data into object databases. Analysis jobs will frequently traverse collections containing millions of stored objects. Clustering is one of the most effective means to enhance the performance of these applications. This paper presents a reclustering algorithm for independent objects contained in multiple possibly overlapping collections on secondary storage. The algorithm decomposes the stored objects into a number of independent chunks and then maps these chunks to a traveling salesman problem. Under a set of realistic assumptions the number of disk seeks is reduced almost to the theoretical minimum. Experimental results obtained from a prototype are included.  1 Introduction  We consider data analysis on secondary storage. In certain kinds of scientific data analysis, e.g. high-energy physics (HEP), a very large number of preselected independent objects is repeatedly processed. Such a set of preselected objects, read b...
DB
mizoguchi00using
Using Ontological Engineering to Overcome Common AI-ED Problems This paper discusses long-term prospects of AI-ED research with the aim of giving a clear view of what we need for further promotion of the research from both the AI and ED points of view. An analysis of the current status of AI-ED research is done in the light of intelligence, conceptualization, standardization and theory-awareness. Following this, an ontology-based architecture with appropriate ontologies is proposed. Ontological engineering of IS/ID is next discussed followed by a road map towards an ontology-aware authoring system. Heuristic design patterns and XML-based documentation are also discussed.  1. INTRODUCTION  Among AI-ED research done to date, several paradigms such as CAI, ICAI, Micro-world, ITS, ILE, and CSCL have been proposed and many systems have been built within each paradigm. Additionally, innovative computer technologies such as hyper-media, virtual reality, internet, WWW have significantly affected the AI-ED community in general. We really have learned a lot ...
AI
bachpedersen98multidimensional
Multidimensional Data Modeling for Complex Data Systems for On-Line Analytical Processing (OLAP) considerably ease the process of analyzing business  data and have become widely used in industry. OLAP systems primarily employ multidimensional  data models to structure their data. However, current multidimensional data models fall short in their  ability to model the complex data found in some real-world application domains. The paper presents  nine requirements to multidimensional data models, each of which is exemplified by a real-world, clinical  case study. A survey of the existing models reveals that the requirements not currently met include  support for many-to-many relationships between facts and dimensions, built-in support for handling  change and time, and support for uncertainty as well as different levels of granularity in the data. The  paper defines an extended multidimensional data model, which addresses all nine requirements. Along  with the model, we present an associated algebra, and outline how to implement the model using relational  databases.
DB
kennedy98anomaly
Anomaly Driven Concept Acquisition In this paper, we identify some principles of Maturana and Varela's autopoiesis theory and Piaget's theory of child development, both of which fall into the constructivist category of epistemology. We then apply them to the problem of autonomous concept acquisition for artificial agents. One consequence of constructivist philosophy is that concept acquistion should be possible in situations (anomalies) which were completely unforeseen by a human designer of the system. Another major consequence is that concepts should not merely be defined on the formal logical level, but should be rooted in sensorimotor interaction with the environment. This requires the existence of an intermediate level in the architecture which allows the construction of original response patterns. We also consider the computational implications of the Piagetian concept of integrating environment-driven and model-driven tendencies when searching for new concepts (known as accommodation and assimilation respectively...
AI
schut01principles
Principles of Intention Reconsideration We present a framework that enables a belief-desire-intention (BDI) agent to dynamically choose its intention reconsideration policy in order to perform optimally in accordance with the current state of the environment. Our framework integrates an abstract BDI agent architecture with the decision theoretic model for discrete deliberation scheduling of Russell and Wefald. As intention reconsideration determines an agent's commitment to its plans, this work increases the level of autonomy in agents, as it pushes the choice of commitment level from design-time to run-time. This makes it possible for an agent to operate effectively in dynamic and open environments, whose behaviour is not known at design time. Following a precise formal definition of the framework, we present an empirical analysis that evaluates the run-time policy in comparison with design-time policies. We show that an agent utilising our framework outperforms agents with fixed policies.
Agents
toenshoff01flexible
Flexible Process Planning And Production Control Using Co-Operative Agent Systems Nowadays, one of the greatest challenges companies have to face is the change towards flexible and demand-driven production. More information has to be handled and a considerable speed-up of development and manufacturing processes is needed. However, the actual situation is characterized by strong borderlines between process planning, production control and scheduling systems, caused by extreme specialisation and independent historical paths of system evolution. This gap implies loss of time and of information. Thus, there is a strong need for innovative concepts for management and control of integrated information logistics, production scheduling and process planning.  Agent-based information technologies like the innovative concept of co-operative agent systems are promising approaches for more flexible and distributed production networks. Agents are autonomously, co-operatively and goal-oriented acting intelligent software units. The use of agents for managing information within production control and process planning makes short term and flexible reaction to unexpected events and disturbances in manufacturing (e.g. break down of machines or lack of other resources as well as unexpected change of market situation) possible. Thus, enterprises will react to changing requirements in a more flexible way and will be able to face the challenges of international competition successfully.  KEYWORDS: Agile Manufacturing, Intelligent Manufacturing, CAPP/CAM, PPC, Operations Management, Distributed Artificial Intelligence, Multiagent Systems  
Agents
giraud-carrier98beyond
Beyond Predictive Accuracy: What? This paper presents  a number of such criteria and discusses the impact they have on meta-level  approaches to model selection
AI
bharat99comparison
A Comparison of Techniques to Find Mirrored Hosts on the WWW We compare several algorithms for identifying mirrored hosts on the World Wide Web. The algorithms operate on the basis of URL strings and linkage data: the type of information easily available from web proxies and crawlers. Identification of mirrored hosts can improve web-based information retrieval in several ways: First, by identifying mirrored hosts, search engines can avoid storing and returning duplicate documents. Second, several new information retrieval techniques for the Web make inferences based on the explicit links among hypertext documents -- mirroring perturbs their graph model and degrades performance. Third, mirroring information can be used to redirect users to alternate mirror sites to compensate for various failures, and can thus improve the performance of web browsers and proxies.  # This work was presented at the Workshop on Organizing Web Space at the Fourth ACM Conference on Digital Libraries 1999.  We evaluated 4 classes of "top-down" algorithms for detecting ...
IR
yim00architecturecentric
Architecture-Centric Object-Oriented Design Method for Multi-Agent Systems This paper introduces an architecture-centric object-oriented design method for MAS  (Multi-Agent Systems) using the extended UML (Unified Modeling Language). The  UML extension is based on design principles that are derived from characteristics of  MAS and concept of software architecture which helps to design reusable and wellstructured  multi-agent architecture. The extension allows one to use original objectoriented  method without syntactic or semantic changes which implies the preservation  of OO productivity, i.e., the availability of developers and tools, the utilization of past  experiences and knowledge, and the seamless integration with other systems.  Keywords: multi-agent systems, architecture, object-oriented development methods  1. Introduction  Software agents provide a new way of analyzing, designing, and implementing complex software systems. Currently, agent technology is used in wide variety of applications with range from comparatively small systems such as
Agents
flake00agents
Agents with Complex Plans: Design and Implementation of CASA We describe the design of CASA, an agent specification language that builds on the formal agent specification approach AgentSpeak (L) and extends it by concepts from concurrent logic programming. With CASA it is possible to design agents with complex behavior patterns like speculative computations and parallel executed strategies. The design of multi agent systems composed of CASA agents is supported by providing predefined message structures and integrating an existing agent communication framework.
Agents
weber00viewpointinvariant
Viewpoint-Invariant Learning and Detection of Human Heads We present a method to learn models of human heads for the purpose of detection from different viewing angles. We focus on a model where objects are represented as constellations of rigid features (parts). Variability is represented by a joint probability density function (pdf) on the shape of the constellation. In a first stage, the method automatically identifies distinctive features in the training set using an interest operator followed by vector quantization. The set of model parameters, including the shape pdf, is then learned using expectation maximization. Experiments show good generalization performance to novel viewpoints and unseen faces. Performance is above    % correct with less than    s computation time per image.
ML
vanhoof99bottomup
Bottom-up Partial Deduction of Logic Programs In this paper, we develop a solid theoretical foundation for a bottom-up program transformation, capable of specialising a logic program with respect to a set of unit clauses. Extending a well-known operator, we define a bottom-up partial deduction operator and prove correctness of the transformation with respect to the S-semantics. We also show how, within this framework, a concrete control strategy can be designed. The transformation can be used as a stand-alone specialisation technique, useful when a program needs to be specialised with respect to its internal structure (e.g., a library of predicates with respect to an abstract data type) instead of a single goal. Moreover, the bottom-up transformation can be usefully combined with a more traditional top-down partial deduction strategy.
DB
chu-carroll99conflict
Conflict Resolution in Collaborative Planning Dialogues In a collaborative planning environment in which the agents are autonomous and heterogeneous, it is  inevitable that discrepancies in the agents' beliefs result in conflicts during the planning process. In such  cases, it is important that the agents engage in collaborative negotiation to resolve the detected conflicts  in order to determine what should constitute their shared plan of actions and shared beliefs. This paper  presents a plan-based model for conflict detection and resolution in collaborative planning dialogues.  Our model specifies how a collaborative system should detect conflicts that arise between the system  and its user during the planning process. If the detected conflicts warrant resolution, our model initiates  collaborative negotiation in an attempt to resolve the conflicts in the agent's beliefs. In addition, when  multiple conflicts arise, our model identifies and addresses the most effective aspect in its pursuit of  conflict resolution. Furthermore, by captur...
Agents
schndelbach01augurscope
The Augurscope: A Mixed Reality Interface for Outdoors The augurscope is a portable mixed reality interface for outdoors. A tripod-mounted display is wheeled to different locations and rotated and tilted to view a virtual environment that is aligned with the physical background. Video from an onboard camera is embedded into this virtual environment. Our design encompasses physical form, interaction and the combination of a GPS receiver, electronic compass, accelerometer and rotary encoder for tracking. An initial application involves the public exploring a medieval castle from the site of its modern replacement. Analysis of use reveals problems with lighting, movement and relating virtual and physical viewpoints, and shows how environmental factors and physical form affect interaction. We suggest that problems might be accommodated by carefully constructing virtual and physical content.
HCI
mortazavi-asl01discovering
Discovering And Mining User Web-Page Traversal Patterns As the popularity of WWW explodes, a massive amount of data is gathered by Web servers in the form of Web access logs. This is a rich source of information for understanding Web user surfing behavior. Web Usage Mining, also known as Web Log Mining, is an application of data mining algorithms to Web access logs to find trends and regularities in Web users' traversal patterns. The results of Web Usage Mining have been used in improving Web site design, business and marketing decision support, user profiling, and Web server system performance. In this thesis we study the application of assisted exploration of OLAP data cubes and scalable sequential pattern mining algorithms to Web log analysis. In multidimensional OLAP analysis, standard statistical measures are applied to assist the user at each step to explore the interesting parts of the cube. In addition, a scalable sequential pattern mining algorithm is developed to discover commonly traversed paths in large data sets. Our experimental and performance studies have demonstrated the effectiveness and efficiency of the algorithm in comparison to previously developed sequential pattern mining algorithms. In conclusion, some further research avenues in web usage mining are identified as well.  iv  Dedication To my parents  v  Acknowledgments I would like to thank my supervisor Dr. Jiawei Han for his support, sharing of his knowledge and the opportunities that he gave me. His dedication and perseverance has always been exemplary to me. I am also grateful to TeleLearning for getting me started in Web Log Analysis. I owe a depth of gratitude to Dr. Jiawei Han, Dr. Tiko Kameda and Dr. Wo-shun Luk for supporting my descision to continue my graduate studies. I am also grateful to Dr. Tiko Kameda for accepting to be my supervis...
DB
wu01towards
Towards a Highly-Scalable and Effective Metasearch Engine A metasearch engine is a system that supports unified access to multiple local search engines. Database selection is one of the main challenges in building a large-scale metasearch engine. The problem is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query. In order to enable accurate selection, metadata that reect the contents of each search engine need to be collected and used. In this paper, we propose a highly scalable and accurate database selection method. This method has several novel features. First, the metadata for representing the contents of all search engines are organized into a single integrated representative. Such a representative yields both computation efficiency and storage efficiency. Second, our selection method is based on a theory for ranking search engines optimally. Experimental results indicate that this new method is very effective. An operational prototype system has been built based on the proposed approach.
IR
caragea01analysis
Analysis and Synthesis of Agents that Learn from Distributed Dynamic Data Sources Abstract. We propose a theoretical framework for specification and analysis of a class of learning problems that arise in open-ended environments that contain multiple, distributed, dynamic data and knowledge sources. We introduce a family of learning operators for precise specification of some existing solutions and to facilitate the design and analysis of new algorithms for this class of problems. We state some properties of instance and hypothesis representations, and learning operators that make exact learning possible in some settings. We also explore some relationships between models of learning using different subsets of the proposed operators under certain assumptions. 1 Learning from Distributed Dynamic Data Many practical knowledge discovery tasks (e.g., learning the behavior of complex computer systems from observations, computer-aided scientific discovery in bioinformatics) present several new challenges in machine learning. The data repositories in such applications tend to be very large, physically distributed,
IR
shasha02algorithmics
Algorithmics and Applications of Tree and Graph Searching Modern search engines answer keyword-based queries extremely efficiently. The impressive speed is due to clever inverted index structures, caching, a domain-independent knowledge of strings, and thousands of machines. Several research efforts have attempted to generalize keyword search to keytree and keygraph searching, because trees and graphs have many applications in next-generation database systems. This paper surveys both algorithms and applications, giving some emphasis to our own work.
IR
dumas99sequencebased
A Sequence-Based Object-Oriented Model for Video Databases Structuration, annotation and composition, are amidst the most crucial modeling issues that video editing and querying in the context of a database entail. In this paper, we propose a sequence-based, object-oriented data model that addresses them in an unified, yet orthogonal way. This orthogonality allows to capture the interactions between these three aspects, i.e. annotations may be attached to any level of video structuration, and the composition operators preserve the structurations and annotations of the argument videos. The proposed model reuses concepts stemming from temporal databases, so that operators defined in this latter setting may be used to query it. Accordingly, the query language for video databases proposed in this paper, is a variant of a temporal extension of ODMG's OQL. The main components of the proposal have been formalized and implemented on top of an object-oriented DBMS.  Keywords: video databases, sequence databases, object-oriented databases, ODMG.  R'esum...
DB
sloman01varieties
Varieties of Affect and the CogAff Architecture Schema In the last decade and a half, the amount of work on affect in general and emotion in particular has grown, in empirical psychology, cognitive science and AI, both for scientific purposes and for the purpose of designing synthetic characters, e.g. in games and entertainments. Such work understandably starts from concepts of ordinary language (e.g. “emotion”, “feeling”, “mood”, etc.). However, these concepts can be deceptive: the words appear to have clear meanings but are used in very imprecise and systematically ambiguous ways. This is often because of explicit or implicit pre-scientific theories about mental states and process. More sophisticated theories can provide a basis for deeper and more precise concepts, as has happened in physics and chemistry. In the Cognition and Affect project we have been attempting to explore the benefits of developing architecture-based concepts, i.e. starting with specifications of architectures for complete agents and then finding out what sorts of states and processes are supported by those architectures. So, instead of presupposing one theory of the architecture and explicitly or implicitly basing concepts on that, we define a space of architectures generated by the CogAff architecture schema, where each supports different collections of concepts. In that space we focus on one architecture H-Cogaff, a particularly rich instance of the CogAff architecture schema, conjectured as a theory of normal adult human information processing. The architecture-based concepts that it supports provide a framework for defining with greater precision than previously a host of mental concepts, including affective concepts. We then find that these map more or less loosely onto various pre-theoretical concepts, such as “emotion”, etc. We indicate some of the variety of emotion concepts generated by the H-Cogaff architecture A different architecture, supporting a different range of mental concepts might be appropriate for exploring affective states of other animals, for instance insects, reptiles, or other mammals, and young children. 1
Agents
langley94oblivious
Oblivious Decision Trees and Abstract Cases In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.
ML
granger02multiscale
Multi-scale EM-ICP: A Fast and Robust Approach for Surface Registration We investigate in this article the rigid registration of large sets of points, generally sampled from surfaces. We formulate this problem as a general Maximum-Likelihood (ML) estimation of the transformation and the matches. We show that, in the specific case of a Gaussian noise, it corresponds to the Iterative Closest Point algorithm (ICP) with the Mahalanobis distance.
AI
boutilier01partialorder
Partial-Order Planning with Concurrent Interacting Actions In order to generate plans for agents with multiple actuators, agent teams, or distributed  controllers, we must be able to represent and plan using concurrent actions with  interacting effects. This has historically been considered a challenging task requiring a  temporal planner with the ability to reason explicitly about time. We show that with  simple modifications, the STRIPS action representation language can be used to represent  interacting actions. Moreover, algorithms for partial-order planning require only small  modifications in order to be applied in such multiagent domains. We demonstrate this fact  by developing a sound and complete partial-order planner for planning with concurrent interacting  actions, POMP, that extends existing partial-order planners in a straightforward  way. These results open the way to the use of partial-order planners for the centralized  control of cooperative multiagent systems.  1. Introduction  In order to construct plans for agents with mul...
Agents
ghavamzadeh01continuoustime
Continuous-Time Hierarchical Reinforcement Learning Hierarchical reinforcement learning (RL) is a general framework which studies how to exploit the structure of actions and tasks to accelerate policy learning in large domains. Prior work in hierarchical RL, such as the MAXQ method, has been limited to the discrete-time discounted reward semi-Markov decision process (SMDP) model. This paper generalizes the MAXQ method to continuous-time discounted and average reward SMDP models. We describe two hierarchical reinforcement learning algorithms: continuous-time discounted reward MAXQ and continuous-time average reward MAXQ. We apply these algorithms to a complex multiagent AGV scheduling problem, and compare their performance and speed with each other, as well as several well-known AGV scheduling heuristics. 1.
ML
beaudouin-lafon00instrumental
Instrumental Interaction: An Interaction Model for Designing Post-WIMP User Interfaces This article introduces a new interaction model called Instrumental Interaction that extends and generalizes the principles of direct manipulation. It covers existing interaction styles, including traditional WIMP interfaces, as well as new interaction styles such as two-handed input and augmented reality. It defines a design space for new interaction techniques and a set of properties for comparing them. Instrumental Interaction describes graphical user interfaces in terms of domain objects and interaction instruments. Interaction between users and domain objects is mediated by interaction instruments, similar to the tools and instruments we use in the real world to interact with physical objects. The article presents the model, applies it to describe and compare a number of interaction techniques, and shows how it was used to create a new interface for searching and replacing text.  Keywords  Interaction model, WIMP interfaces, direct manipulation, post-WIMP interfaces, instrumental ...
HCI
bauer99infobeans
InfoBeans - Configuration of Personalized Information Assistants With the enormous amount of data contained in the WWW, one of the crucial tasks a user has to face is the identification and aggregation of relevant pieces of information to satisfy her current information needs. This paper presents an approach to the system--supported configuration of individualized information services. The programming--by--demonstration approach pursued by the InfoBeans releases the user from learning a programming language or dealing with technical subtleties. The first version of this system will be released this fall. Keywords information assistants, wrapper induction, programming by demonstration, information integration INTRODUCTION The WWW provides an increasing amount of largely unrelated pieces of information like personal homepages as well as dedicated special purpose information systems like weather servers. However, creating useful and relevant information from +ro/oor...hy sources to satisfy specific, vqvo/oovqhy needs is a largely unsupported, cha...
IR
hatzilygeroudis00neurules
Neurules: Improving the Performance of Symbolic Rules In this paper, we present a method for improving the performance of classical symbolic rules.
AI
quan00argos
Argos: Efficient Refresh in an XQL-Based Web Caching System The Web has become a major conduit to information repositories of all kinds. Web caches are employed to store a web view to provide an immediate response to recurring queries. However, the accuracy of the replicates in web caches encounters challenges due to the dynamicity of web data. We are thus developing and evaluating a web caching system equipped with an efficient refresh strategy. With the assistance of a novel index structure, the Aggregation Path Index (APIX), we built Argos, a web cache system based on the GMD XQL query engine. Argos achieves a high degree of self-maintenance by diagnosing irrelevant data update cases and hence greatly improves the refresh performance of the materialized web view. We also report preliminary experimental results assessing the performance of Argos compared to from scratch evaluation.  1. INTRODUCTION  The advent of the web has dramatically increased the proliferation of information of all kinds. XML [17] is rapidly becoming popular for represen...
DB
eliassi-rad00instructable
Instructable and Adaptive Web Agents that Learn to Retrieve and Extract Information . We present a system for rapidly and easily building instructable and selfadaptive Web agents for information-retrieval and information-extraction tasks. Our Wisconsin Adaptive Web Assistant (Wawa) constructs a Web agent by accepting user preferences in form of instructions and adapting the agent's behavior as it encounters new information. Wawa has two neural networks that are responsible for the adaptive capabilities of its agents. User-provided instructions are compiled into these neural networks and are modified via training examples. Users can create these training examples by rating pages that are retrieved by Wawa, but more importantly our system uses techniques from reinforcement learning to internally create its own examples. Users can also provide additional instruction throughout the life of an agent. We evaluate Wawa on a "home-page finder" agent and a "seminarannouncement extractor" agent.  Keywords: Web mining, instructable and adaptive software agents, machine learning,...
ML
dayal01business
Business Process Coordination: State of the Art, Trends, and Open Issues Over the past decade, there has been a lot of  work in developing middleware for integrating  and automating enterprise business processes.  Today, with the growth in e-commerce and the  blurring of enterprise boundaries, there is  renewed interest in business process  coordination, especially for inter-organizational  processes. This paper provides a historical  perspective on technologies for intra- and interenterprise  business processes, reviews the state  of the art, and exposes some open research  issues. We include a discussion of process-based  coordination and event/rule-based coordination,  and corresponding products and standards  activities. We provide an overview of the rather  extensive work that has been done on advanced  transaction models for business processes, and of  the fledgling area of business process  intelligence.  1. 
DB
muscettola98reformulating
Reformulating Temporal Plans For Efficient Execution The Simple Temporal Network formalism permits significant flexibility in specifying the occurrence time of events in temporal plans. However, to retain this flexibility during execution, there is a need to propagate the actual execution times of past events so that the occurrence windows of future events are adjusted appropriately. Unfortunately, this may run afoul of tight real-time control requirements that dictate extreme efficiency. The performance may be improved by restricting the propagation. However, a fast, locally propagating, execution controller may incorrectly execute a consistent plan. To resolve this dilemma, we identify a class of dispatchable networks that are guaranteed to execute correctly under local propagation. We show that every consistent temporal plan can be reformulated as an equivalent dispatchable network, and we present an algorithm that constructs such a network. Moreover, the constructed network is shown to have a minimum number of edges among all such n...
AI
picard98humancomputer
Human-Computer Coupling this article.
HCI
eriksson98sics
SICS MarketSpace - An Agent-Based Market Infrastructure . We present a simple and uniform communication framework for an agent-based market infrastructure, the goal of which is to enable automation of consumer goods markets distributed over the Internet. The framework consists of an information model for participant interests and an interaction model that defines a basic vocabulary for advertising, searching, negotiating and settling deals. The information model is based on structured documents representing contracts and representations of constrained sets of contracts called interests. The interaction model is asynchronous message communication in a speech act based language, similar to, but simpler than, KQML [7] and FIPA ACL [8]. We also discuss integration of an agent-based market infrastructure with the web. 1
Agents
naumann00completeness
Completeness of Information Sources Information quality plays a crucial role in every application that integrates data from autonomous sources. However, information quality is hard to measure and complex to consider for the tasks of information integration, even if the integrating sources cooperate. We present a systematic and formal approach to the measurement of information quality and the combination of such measurements for information integration. Our approach is based on a value model that incorporates both extensional value (coverage) and intensional value (density) of information. For both aspects we provide merge functions for adequately scoring integrated results. Also, we combine the two criteria to an overall completeness criterion that formalizes the intuitive notion of completeness of query results. This completeness measure is a valuable tool to assess source size and to predict result sizes of queries in integrated information systems. We propose this measure as an important step towards the usage of information quality for source selection, query planning, query optimization, and quality feedback to users.
IR
jennings99robust
Robust Finger Tracking with Multiple Cameras This paper gives an overview of a system for robustly tracking the 3D position and orientation of a finger using a few closely spaced cameras. Accurate results are obtained by combining features of stereo range images and color images. This work also provides a design framework for combining multiple sources of information, including stereo range images, color segmentations, shape information and various constraints. This information is used in robust model fitting techniques to track highly over-constrained models of deformable objects: fingers.
HCI
rey01location
Location Systems for Ubiquitous Computing To serve us well, emerging mobile computing applications will need to know the physical location of things so that they can record them and report them to us: What lab bench was I standing by when I prepared these tissue samples? How should our search-and-rescue team move to quickly locate all the avalanche victims? Can I automatically display this stock devaluation chart on the large screen I am standing next to? Researchers are working to meet these and similar needs by developing systems and technologies that automatically locate people, equipment, and other tangibles. Indeed, many systems over the years have addressed the problem of automatic location-sensing. Because each approach solves a slightly different problem or supports different applications, they vary in many parameters, such as the physical phenomena used for location determination, the form factor of the sensing apparatus, power requirements, infrastructure versus portable elements, and resolution in time and space. To make sense of this domain, we have developed a taxonomy to help developers
HCI
jantke93casebased
Case-Based Representation and Learning of Pattern Languages Pattern languages seem to suit case-based reasoning particularly well. Therefore, the problem of inductively learning pattern languages is paraphrased in a case-based manner. A careful investigation requires a formal semantics for case bases together with similarity measures in terms of formal languages. Two basic semantics are introduced and investigated. It turns out that representability problems are major obstacles for case-based learnability. Restricting the attention to so-called proper patterns avoids these representability problems. A couple of learnability results for proper pattern languages are derived both for case-based learning from only positive data and for case-based learning from positive and negative data. Under the so-called competing semantics, we show that the learnability result for positive and negative data can be lifted to the general case of arbitrary patterns. Learning under the standard semantics from positive data is closely related to monotonic language l...
ML
cantu-paz00combining
Combining Evolutionary Algorithms With Oblique Decision Trees to Detect Bent-Double Galaxies Decision trees have long been popular in classification as they use simple and easy-to-understand tests at each node. Most variants of decision trees test a single attribute at a node, leading to axis-parallel trees, where the test results in a hyperplane which is parallel to one of the dimensions in the attribute space. These trees can be rather large and inaccurate in cases where the concept to be learned is best approximated by oblique hyperplanes. In such cases, it may be more appropriate to use an oblique decision tree, where the decision at each node is a linear combination of the attributes. Oblique decision trees have not gained wide popularity in part due to the complexity of constructing good oblique splits and the tendency of existing splitting algorithms to get stuck in local minima. Several alternatives have been proposed to handle these problems including randomization in conjunction with deterministic hill-climbing and the use of simulated annealing. In this paper, we use...
AI
munro99footprints
Footprints in the Snow er than use more formalised information artefacts. When navigating cities people tend to ask other people for advice rather than study maps (Streeter and Vitello, 1985), when trying to find information about pharmaceuticals medical doctors tend to ask other doctors for advice (Tiimpka and Hallberg, 1996), if your child has red spots you might phone your mother or talk to a friend for an opinion. Even when we are not directly looking for information we use a wide range of cues, both from features of the environment and from the behaviour of other people, to manage our activities. Alan Munro observed how people followed crowds or simply sat around at a venue when deciding which shows and street events to attend at the Edinburgh Arts Festival (Munro, 1998). We might be influenced to pick up a book because it appears well thumbed, we walk into a sunny courtyard because it looks attractive or we might decide to see a film because our friends enjoyed it. Not only do we find our ways through 
HCI
blockeel98topdown
Top-Down Induction Of First Order Logical Decision Trees this paper. For instance, the Progol system (Muggleton, 1995) has recently been extended with caching and other efficiency improvements (Cussens, 1997). Another direction of work is the use of sampling techniques, see e.g. (Srinivasan, 1998; Sebag, 1998).  168 CHAPTER 7. SCALING UP Tilde
DB
kantarcioglu02privacypreserving
Privacy-preserving Distributed Mining of Association Rules on Horizontally Partitioned Data Abstract—Data mining can extract important knowledge from large data collections—but sometimes these collections are split among various parties. Privacy concerns may prevent the parties from directly sharing the data and some types of information about the data. This paper addresses secure mining of association rules over horizontally partitioned data. The methods incorporate cryptographic techniques to minimize the information shared, while adding little overhead to the mining task. Index Terms—Data mining, security, privacy. 1
DB
hatzilygeroudis02multiinference
Multi-inference with Multi-neurules Neurules are a type of hybrid rules combining a symbolic and a connectionist representation. There are two disadvantages of neurules. The first is that the created neurule bases usually contain multiple representations of the same piece of knowledge. Also, the inference mechanism is rather connectionism oriented than symbolism oriented, thus reducing naturalness. To remedy these deficiencies, we introduce an extension to neurules, called multineurules, and an alternative inference process, which is rather symbolism oriented. Experimental results comparing the two inference processes are also presented.
AI
cantu-paz00using
Using Evolutionary Algorithms to Induce Oblique Decision Trees This paper illustrates the application of evolutionary algorithms (EAs) to the problem of oblique decision tree induction. The objectives are to demonstrate that EAs can find classifiers whose accuracy is competitive with other oblique tree construction methods, and that at least in some cases this can be accomplished in a shorter time. Experiments were performed with a (1+1) evolution strategy and a simple genetic algorithm on public domain and artificial data sets. The empirical results suggest that the EAs quickly find competitive classifiers, and that EAs scale up better than traditional methods to the dimensionality of the domain and the number of instances used in training.
AI
timm01synthesis
Synthesis And Adaptation Of Multiagent Communication Protocols In The Production Engineering Domain The application of multiagent systems is often based on the claim that there will be an emergent behavior within these systems. To reach the emergent behavior many researcher propagate to plan it within design and analysis of specific systems, as it will not occur by chance.  We are presenting a new way to adaptive agent communication protocols with respect to a possible gain of emergent behavior. Communication protocols can be generated, refined or adapted by the agents autonomously. The approach uses basic concepts of machine learning.  
Agents
georgeff99beliefdesireintention
The Belief-Desire-Intention Model of Agency Introduction  Within the ATAL community, the belief-desire-intention (BDI) model has come to be possibly the best known and best studied model of practical reasoning agents. There are several reasons for its success, but perhaps the most compelling are that the BDI model combines a respectable philosophical model of human practical reasoning, (originally developed by Michael Bratman [1]), a number of implementations (in the IRMA architecture [2] and the various PRS-like systems currently available [7]), several successful applications (including the now-famous fault diagnosis system for the space shuttle, as well as factory process control systems and business process management [8]), and finally, an elegant abstract logical semantics, which have been taken up and elaborated upon widely within the agent research community [14, 16]. However, it could be argued that the BDI model is now becoming somewhat dated: the principles of the architecture were established in the mid-1980s,
Agents
wijngaards01mas
On MAS Scalability In open dynamic multi-agent environments the number of agents can vary significantly within very  short periods of time. Very few (if any) current multi-agent systems have, however, been designed to  cope with large-scale distributed applications. Scalability requires increasing numbers of new agents and  resources to have no noticeable effect on performance nor to increase administrative complexity. In this  paper a number of implications for techniques and management are discussed. Current research on agent  middleware is briefly described.  1 
Agents
pant02myspiders
MySpiders : Evolve your own intelligent Web crawlers Abstract. The dynamic nature of the World Wide Web makes it a challenge to find information that is bothrelevant and recent. Intelligent agents can complement the power of searchengines to meet this challenge. We present a Web tool called MySpiders, which implements an evolutionary algorithm managing a population of adaptive crawlers who browse the Web autonomously. Each agent acts as an intelligent client on behalf of the user, driven by a user query and by textual and linkage clues in the crawled pages. Agents autonomously decide which links to follow, which clues to internalize, when to spawn offspring to focus the search near a relevant source, and when to starve. The tool is available to the public as a threaded Java applet. We discuss the development and deployment of such a system. Keywords: web informational retrieval, topic-driver crawlers, online search, InfoSpiders, MySpiders, applet
IR
kalganova99evolving
Evolving More Efficient Digital Circuits By Allowing Circuit Layout Evolution and Multi-Objective Fitness We use evolutionary search to design combinational logic circuits. The technique is based on evolving the functionality and connectivity of a rectangular array of logic cells whose dimension is defined by the circuit layout. The main idea of this approach is to improve quality of the circuits evolved by the genetic algorithm (GA) by reducing the number of active gates used. We accomplish this by combining two ideas: 1) using multiobjective fitness function; 2) evolving circuit layout. It will be shown that using these two approaches allows us to increase the quality of evolved circuits. The circuits are evolved in two phases. Initially the genome fitness in given by the percentage of output bits that are correct. Once 100% functional circuits have been evolved, the number of gates actually used in the circuit is taken into account in the fitness function. This allows us to evolve circuits with 100% functionality and minimise the number of active gates in circuit structure. The populati...
ML
michaelides98uniform
A Uniform Approach to Programming the World Wide Web We propose a uniform model for programming distributed web applications. The model is based on the concept of web computation places and provides mechanisms to coordinate distributed computations at these places, including peer-to-peer communication between places and a uniform mechanism to initiate computation in remote places. Computations can interact with the flow of http requests and responses, typically as clients, proxies or servers in the web architecture. We have implemented the model using the global pointers and remote service requests provided by the Nexus communication library. We present the model and its rationale, with some illustrative examples, and we describe the implementation.  1 Introduction  Many web applications require a significant amount of computation which may be distributed and requires coordination; these applications use the web infrastructure to good advantage but are often constrained by the architecture, which is fundamentally client-server. A variety...
IR
kitano97robocup
The RoboCup Synthetic Agent Challenge 97 RoboCup Challenge offers a set of challenges for intelligent agent researchers using a friendly competition in a dynamic, real-time, multiagent domain. While RoboCup in general envisions longer range challenges over the next few decades, RoboCup Challenge presents three specific challenges for the next two years: (i) learning of individual agents and teams; (ii) multi-agent team planning and plan-execution in service of teamwork; and (iii) opponent modeling. RoboCup Challenge provides a novel opportunity for machine learning, planning, and multi-agent researchers --- it not only supplies a concrete domain to evalute their techniques, but also challenges researchers to evolve these techniques to face key constraints fundamental to this domain: real-time, uncertainty, and teamwork.  1 Introduction  RoboCup (The World Cup Robot Soccer) is an attempt to promote AI and robotics research by providing a common task, Soccer, for evaluation of various theories, algorithms, and agent architectur...
Agents
poggi00eye
Eye communication in a conversational 3D synthetic agent this paper, we concentrate on the study and generation of coordinated linguistic and gaze communicative acts. In this view we analyse gaze signals according to their functional meaning rather than to their physical actions. We propose a formalism where a communicative act is represented by two elements: a meaning (that corresponds to a set of goals and beliefs that the agent has the purpose to transmit to the interlocutor) and a signal, that is the nonverbal expression of that meaning. We also outline a methodology to generate messages that coordinate verbal with nonverbal signals.
Agents
cremers00diffusionsnakes
Diffusion-snakes using statistical shape knowledge  We present a novel extension of the Mumford-Shah functional  that allows to incorporate statistical shape knowledge at the computational  level of image segmentation. Our approach exhibits various favorable  properties: non-local convergence, robustness against noise, and  the ability to take into consideration both shape evidence in given image  data and knowledge about learned shapes. In particular, the latter property  distinguishes our approach from previous work on contour-evolution  based image segmentation. Experimental results conrm these properties. 
ML
globig94casebased
On Case-Based Learnability of Languages Case-based reasoning is deemed an important technology to alleviate the bottleneck  of knowledge acquisition in Artificial Intelligence (AI). In case-based reasoning,  knowledge is represented in the form of particular cases with an appropriate similarity  measure rather than any form of rules. The case-based reasoning paradigm adopts the  view that an AI system is dynamically changing during its life-cycle which immediately  leads to learning considerations.  Within the present paper, we investigate the problem of case-based learning of  indexable classes of formal languages. Prior to learning considerations, we study the  problem of case-based representability and show that every indexable class is case-based  representable with respect to a fixed similarity measure. Next, we investigate several  models of case-based learning and systematically analyze their strengths as well as  their limitations. Finally, the general approach to case-based learnability of indexable  classes of form...
ML
dignum00towards
Towards socially sophisticated BDI agents We present an approach to social reasoning that integrates prior work on norms and obligations with the BDI approach to agent architectures. Norms and obligations can be used to increase the eficiency of agent reasoning, and their explicit representation supports reasoning about a wide range of behaviour types in a single framework. We propose a modified BDI interpreter loop that takes norms and obligations into account in an agent's deliberation.
Agents
degeratu01latencydependent
Latency-dependent fitness in evolutionary multithreaded Web agents The World Wide Web creates opportunities for search systems using adaptive distributed agents. This paper presents a threaded implementation of InfoSpiders, a client-based system that uses an evolving population of intelligent agents to browse the Web at query time. We consider different fitness functions based on network resource consumption and show that taxing agents in proportion to latency results in better efficiency without penalties in the quality of the retrieved documents. The tool is available to the public as a Java applet.
ML
bryson00hypothesis
Hypothesis Testing for Complex Agents As agents approach animal-like complexity, evaluating  them becomes as difficult as evaluating animals.  This paper describes the application of techniques for  characterizing animal behavior to the evaluation of complex  agents. We describe the conditions that lead to the  behavioral variability that requires experimental methods.  We then review the state of the art in psychological  experimental design and analysis, and show its  application to complex agents. We also discuss a specific  methodological concern of agent research: how the  robots versus simulations debate interacts with statistical  evaluation. Finally, we make a specific proposal for  facilitating the use of scientific method. We propose the  creation of a web site that functions as a repository for  platforms suitable for statistical testing, for results determined  on those platforms, and for the agents that have  generated those results.  Keywords: agent performance, complex systems, behavioral indeterminacy, repl...
Agents
munzner95visualizing
Visualizing the Structure of the World Wide Web in 3D Hyperbolic Space We visualize the structure of sections of the World Wide Web by constructing graphical representations in 3D hyperbolic space. The felicitous property that hyperbolic space has "more room" than Euclidean space allows more information to be seen amid less clutter, and motion by hyperbolic isometries provides for mathematically elegant navigation. The 3D graphical representations, available in the WebOOGL or VRML file formats, contain link anchors which point to the original pages on the Web itself. We use the Geomview/WebOOGL 3D Web browser as an interface between the 3D representation and the actual documents on the Web. The Web is just one example of a hierarchical tree structure with links "back up the tree" i.e. a directed graph which contains cycles. Our information visualization techniques are appropriate for other types of directed graphs with cycles, such as filesystems with symbolic links. 1 Introduction  The dominant paradigm for World Wide Web navigation is pointing and click...
HCI
tran00videograph
VideoGraph: A Graphical Object-based Model for Representing and Querying Video Data . Modeling video data poses a great challenge since they do  not have as clear an underlying structure as traditional databases do.  We propose a graphical object-based model, called VideoGraph, in this  paper. This scheme has the following advantages: (1) In addition to semantics  of video individual events, we capture their temporal relationships  as well. (2) The inter-event relationships allow us to deduce implicit  video information. (3) Uncertainty can also be handled by associating the  video event with a temporal Boolean-like expression. This also allows us  to exploit incomplete information. The above features make VideoGraph  very flexible in representing various metadata types extracted from diverse  information sources. To facilitate video retrieval, we also introduce  a formalism for the query language based on path expressions. Query  processing involves only simple traversal of the video graphs.  1 Introduction  We deal with the modeling aspect of video database manageme...
DB
smith01management
Management of XML Documents in an Integrated Digital Library We describe a generalized toolset developed by the Perseus Project to  manage XML documents in the context of a large, heterogeneous digital  library. The system manages multiple DTDs through mappings from elements  in the DTD to abstract document structures. The abstraction of  document metadata, both structural and descriptive, facilitates the development  of application-level tools for knowledge management and document  presentation. We discuss the implementation of the XML back  end and describe applications for cross citation retrieval, toponym extraction  and plotting, automatic hypertext generation, morphology, and word  co-occurrence.  1 
IR
rundensteiner98capacityaugmenting
Capacity-Augmenting Schema Changes on Object-Oriented Databases: Towards Increased Interoperability The realization of capacity-augmenting schema changes on a shared database while providing continued interoperability to active applications has been recognized as a hard open problem. A novel three-pronged process, called transparent object schema evolution (TOSE), is presented that successfully addresses this problem. TOSE uses the combination of views and versioning to simulate schema changes requested by one application without affecting other applications interoperating on a shared OODB. The approach is of high practical relevance as it builds upon schema evolution support offered by commercial OODBMSs. Keywords: Transparent schema evolution, object-oriented views, object-oriented databases, application migration. 1 Introduction  Current schema evolution technology suffers from the problem that schema updates on a database shared by interoperating applications often have catastrophic consequences [BKKK87, KC88, MS93, PS87, TS93, Zic91]. In such a multi-user environment, a schema c...
DB
weinstein99agentbased
Agent-Based Digital Libraries: Decentralization and Coordination This paper describes agent-based systems and explains why digital libraries should be built with this type of architecture. The primary advantage of agent-based architecture is decentralization, which enables scaling, flexibility, and extensibility. The corresponding requirement is the need to coordinate agent activity. We describe the approach taken by the University of Michigan Digital Library project.  2  1 Introduction Digital libraries are just beginning to evolve. No one is certain what capabilities are needed, nor how they should be organized. It is therefore important to design digital libraries to be as open as possible, so that new collections and services can be easily added to the system. Furthermore, it is essential that libraries be able to scale to become quite large. For us, this implies a decentralized architecture, where there are few if any shared resources and where as much decision making is done as locally as possible. An example of such a distributed system is t...
DB
vanwelie00patterns
Patterns as Tools for User Interface Design . Designing usable systems is difficult and designers need effective  tools that are usable themselves. Effective design tools should be based on  proven knowledge of design. Capturing knowledge about the successful design  of usable systems is important for both novice and experienced designers and  traditionally, this knowledge has largely been described in guidelines. However,  guidelines have shown to have problems concerning selection, validity and applicability.  Patterns have emerged as a possible solution to some of the problems  from which guidelines suffer. Patterns focus on the context of a problem and solution  thereby guiding the designer in using the design knowledge. Patterns for  architecture or software engineering are not identical in structure and user interface  design also requires its own structure for patterns, focusing on usability.  This paper explores how patterns for user interface design must be structured in  order to be effective and usable tools for desig...
HCI